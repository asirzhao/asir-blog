{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1504351062936},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1504351219838},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1504351219838},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1504351219838},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1504351219838},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1504351219838},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1504351219838},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1504351219838},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1504351219838},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1504351219838},{"_id":"themes/next/_config.yml","hash":"81a4f5734f76f3f354a18007bcacaac4db9fe948","modified":1514988833526},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1504351219838},{"_id":"themes/next/bower.json","hash":"7d7938f9da896fe710aa0e9120140e528bf058df","modified":1504351219838},{"_id":"themes/next/README.md","hash":"950ca6e9c0fa607d290a5b1fd883df44725b36b2","modified":1504351219838},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1504351219838},{"_id":"themes/next/README.cn.md","hash":"6d9177e7dad87e6129760e4b559bd3f7a15429d7","modified":1504351219838},{"_id":"themes/next/package.json","hash":"193dad6f59a588908fac082cc46fe067dac1b84d","modified":1504351219842},{"_id":"source/_posts/course-deep-learning-course2-week1.md","hash":"ef166a5bfcfd0ddec05c51c29b85c465f41d2f7e","modified":1508666078611},{"_id":"source/_posts/course-deep-learning-course2-week2.md","hash":"70e34457e15782f707c84492ea8ffd3700f605b1","modified":1508666091259},{"_id":"source/_posts/course-deep-learning-course2-week3.md","hash":"fd4d5a2901e8549fbd8cdfbaa376d7f7ec253a16","modified":1508666101371},{"_id":"source/_posts/course-deep-learning-course4-week2.md","hash":"b3c62af47ea48c0fa109de6c805e5b97a3783907","modified":1513003299792},{"_id":"source/_posts/course-deep-learning-course4-week1.md","hash":"2ba1d0e0ff994f5ca0666ae8fd08535361a1ee1b","modified":1511768420987},{"_id":"source/_posts/course-deep-learning-course3-week2.md","hash":"63d0388eb00664a6df0a22463a84278412a9e7e0","modified":1508666122354},{"_id":"source/_posts/course-deep-learning-course3-week1.md","hash":"aa1fa69e05c3e1cf679e5368837bd987ac4c91cd","modified":1508666113990},{"_id":"source/_posts/ml-convex-opt.md","hash":"fe2668f101a14669340b172528c31ff38a5fa4bb","modified":1504351062936},{"_id":"source/_posts/ml-gd-and-nm.md","hash":"6d7ad0d9af51dede1d06ab282aea39094f6ad83b","modified":1506519224063},{"_id":"source/_posts/ml-imbalanced-data-solution.md","hash":"6a3e85f66233c0534de708e81924a0c8fdd05def","modified":1513696398554},{"_id":"source/_posts/other-hello.md","hash":"42c9e2fc129719066d04a28f169563cf2c898cdf","modified":1510666812274},{"_id":"source/_posts/ml-ridge-lasso.md","hash":"ed958fa27cba733d09076a4b768381ededa5d66b","modified":1504351062936},{"_id":"source/_posts/spark-spark-workflow.md","hash":"72bf7a0bc369e588a5b113a125d3f081ee767012","modified":1515682475734},{"_id":"source/_posts/paper-facebook.md","hash":"1eee845e7367f3e8048643c2eadbedc01c026e99","modified":1504974181646},{"_id":"source/_posts/paper-swish.md","hash":"c86594843ed5071477c3f96d1d13f0c9c15b756c","modified":1508943616593},{"_id":"source/_posts/paper-imbalance.md","hash":"9eff4f0fa6b63e1a8472acfd31f89b7a4b34aaef","modified":1505032413050},{"_id":"source/about/index.md","hash":"0d33b9e1f44badf93c0901048b7ee261ec6c2f23","modified":1510666812274},{"_id":"source/categories/index.md","hash":"4b365c6059e6bf92e31df59be1a2597c25bb4ba3","modified":1510666812274},{"_id":"source/tags/index.md","hash":"6069f89020880bad30d7801a8e1a4c2a77d7bc5b","modified":1510666812274},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1504351219838},{"_id":"themes/next/.git/FETCH_HEAD","hash":"3c3f976002388b4b2034ac039575acb78c9daf06","modified":1514988833506},{"_id":"themes/next/.git/COMMIT_EDITMSG","hash":"b3156d8b05042300c8039b60d832a4630f7f3a3e","modified":1513511632087},{"_id":"themes/next/.git/ORIG_HEAD","hash":"06459013f92dcf5c5e73f657faef8e4f58549f01","modified":1514988833510},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1504351172432},{"_id":"themes/next/.git/config","hash":"78d9993a62e495e294242c6e49c5b9303af75446","modified":1504351219838},{"_id":"themes/next/.git/index","hash":"bd3dd48588b948e58f6b58db008eea5e7a9b04ea","modified":1514988925747},{"_id":"themes/next/.git/packed-refs","hash":"e469c46dd550e49abdad1b31a4bed9708b3ac34f","modified":1504351219838},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1504351219838},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"679cdcc33eda5b33375206b2add1de84cea1615e","modified":1504351219838},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"e9169b65a7e3392c27562f9e11061a3ab76bb600","modified":1504351219838},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1504351219838},{"_id":"themes/next/languages/default.yml","hash":"c0b90d66772e79585cd26a81694ad69c16312d6b","modified":1504351219838},{"_id":"themes/next/languages/fr-FR.yml","hash":"a14d051bbec26cfcae358bdcf1acf62a35fb1a45","modified":1504351219838},{"_id":"themes/next/languages/de.yml","hash":"98aa551443b2a61a74b6f2a218635da6d2f6cf57","modified":1504351219838},{"_id":"themes/next/languages/id.yml","hash":"f8b57daac2e50ace9a6d5051b17208af8139c2ae","modified":1504351219838},{"_id":"themes/next/languages/en.yml","hash":"c0b90d66772e79585cd26a81694ad69c16312d6b","modified":1504351219838},{"_id":"themes/next/languages/ja.yml","hash":"0c99ba4ba7d36c43d002342611d2c656ef498582","modified":1504351219838},{"_id":"themes/next/languages/ko.yml","hash":"043951e82997131dd8be40ff2093ef36849ba725","modified":1504351219838},{"_id":"themes/next/languages/pt-BR.yml","hash":"91584764104ef29293117375fc010b1bdbe9aff6","modified":1504351219838},{"_id":"themes/next/languages/pt.yml","hash":"dfd0b8574177346b78cab29db055fbc44ac309dc","modified":1504351219842},{"_id":"themes/next/languages/ru.yml","hash":"98dd9b6ddd88400a7b02cd7e8adb41e7b842bf57","modified":1504351219842},{"_id":"themes/next/languages/zh-Hans.yml","hash":"c1255b722fc5fdecf1852c3b592edfea9dbb554c","modified":1504351219842},{"_id":"themes/next/languages/zh-hk.yml","hash":"e8072846fd43beadbae394e30a49aa5c92a0a53b","modified":1504351219842},{"_id":"themes/next/languages/zh-tw.yml","hash":"562141bfe450432131af012baa262a3de79a50bc","modified":1504351219842},{"_id":"themes/next/layout/_layout.swig","hash":"ada7ffc71cf05e7236a19e0648bce6d6d6cbc7dc","modified":1504351219842},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1504351219842},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1504351219842},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1504351219842},{"_id":"themes/next/layout/page.swig","hash":"37c874cd720acf0eda8d26e063278f2b6ae8d3a6","modified":1504351219842},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1504351219842},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1504351219842},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1504351219842},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1504351219842},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1504351219842},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1504351219870},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1504351219870},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1504351219870},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/.git/logs/HEAD","hash":"afda003a066187a1e70d4525c8863021dde45f74","modified":1514988833530},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1504351172432},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1504351172432},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1504351172432},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1504351172432},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1504351172432},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1504351172432},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1504351172432},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1504351172432},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1504351172432},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1504351172432},{"_id":"themes/next/layout/_partials/comments.swig","hash":"010ef8c42d2e1a95abc60caf757293ca8eb4a68b","modified":1504351219842},{"_id":"themes/next/layout/_partials/footer.swig","hash":"fb02c81273d5897ebb98b50f4c10f7edc34f9240","modified":1504351219842},{"_id":"themes/next/layout/_partials/head.swig","hash":"2cbeae795c9929ec1966b8a1fb9c058a0b547fa9","modified":1504351219842},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1504351219842},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1504351219842},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1504351219842},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1504351219842},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1504351219842},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1504351219842},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1504351219842},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1504351219842},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1504351219842},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1504351219842},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1504351219842},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1504351219842},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1504351219842},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1504351219842},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1504351219842},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1504351219842},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"53d4f83b2b7fb4387dfc9fe81519abd56fbce4ae","modified":1504351219842},{"_id":"themes/next/layout/_macro/post.swig","hash":"767e1d5503ecce85f577c8fb673a3503b65484ce","modified":1504351219842},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5d5f70deb6074cb4dd0438463e14ccf89213c282","modified":1504351219842},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"604a091aac52b42a9518f6067c5ab45dae5d269c","modified":1508333279778},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1504351219842},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1504351219842},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1504351219842},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1504351219842},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1504351219842},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1504351219842},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1504351219842},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1504351219842},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1504351219842},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1504351219842},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1504351219846},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1504351219846},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1504351219846},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1504351219846},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1504351219846},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1504351219846},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1504351219846},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1504351219846},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1504351219846},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1504351219846},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1504351219846},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1504351219846},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1504351219846},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1504351219846},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1504351219846},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219842},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219842},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1504351219846},{"_id":"themes/next/.git/objects/02/5f1308d73285477af345b0a3052d938ae78b25","hash":"5586fe91b2f6e1650fe3947224bc1aeb631e18ca","modified":1507641326668},{"_id":"themes/next/.git/objects/0b/3e19b13ae846b384151365a12d68fe52b3185f","hash":"569a7901cffa8413cfaffb771f5566544f395312","modified":1514988833186},{"_id":"themes/next/.git/objects/2c/418dbf55824dde769ea6627dc557edddb004c7","hash":"2521ca23790350f683f0477cdc2bb99e00561469","modified":1514988833190},{"_id":"themes/next/.git/objects/29/2bd92759f2bf6744778e81b2fd1abe6a007f68","hash":"ea67f834ed803ad5777f0860ab6c5e062589ee14","modified":1507949518700},{"_id":"themes/next/.git/objects/31/c37a8fa54232f10fec03cd06c1d51c7a837c16","hash":"1a2a6c3b1f008760188ee60433b4f319f27d0699","modified":1507641326672},{"_id":"themes/next/.git/objects/30/d1ae8b285cf04275fc91defc1bd2b604c85de9","hash":"4e4d2d68204ddeaa0ff40012f49531e025b2768f","modified":1513415363909},{"_id":"themes/next/.git/objects/39/8b52f28d65257d36f04b191d1d05fe99d68a54","hash":"5fb774895e21bae4031ddf8cc55429c167627aa8","modified":1507641326668},{"_id":"themes/next/.git/objects/59/02b0c65687cf3b6df397326a21c2fc9b1f2b3c","hash":"3ad1552ccd0ece9710f8771d43d910dd91322ec5","modified":1510666811518},{"_id":"themes/next/.git/objects/50/0a4ad4d5750a228f218fd72512f34b93a50e7d","hash":"edf4eeeb91d85e25e91b5bfceb2f07d4fb3e6d37","modified":1507641326668},{"_id":"themes/next/.git/objects/72/6c00a7a650f93bce7abee170a5022d929437de","hash":"d3c75dd6d028ab27c454d0ed281669cf0e776de1","modified":1510666811518},{"_id":"themes/next/.git/objects/7c/3eb7ad7587b10fca1161f3776f0879e03b660e","hash":"939d90a843b1ce7d91e515716b7c700c343f6273","modified":1510666811518},{"_id":"themes/next/.git/objects/82/586b038c6d7c8aa3b65a28540b334a9502ffd3","hash":"50a7f0c320428e39474b8d30ac18d50e0ea9b80f","modified":1510666811522},{"_id":"themes/next/.git/objects/92/457adbbdc08c0d17bac52fb5e49ae4f145b111","hash":"7bfcfd9f41163ce48e5550770438113c8e351079","modified":1507949518700},{"_id":"themes/next/.git/objects/8d/55df349ac4e9ea81e8e66c8e235c485d207b10","hash":"2b81e5af0e7319d4e30202ac726ab7655af0c1c0","modified":1514988833186},{"_id":"themes/next/.git/objects/7b/6ccc77e8d11101d33eafb9ea8d282ad888747f","hash":"79e48a493400ba7f6896071b065c2651c593a37e","modified":1513511632087},{"_id":"themes/next/.git/objects/ca/63265ecd385480dbb1c86eb243c211df7835bb","hash":"c63495e3c85cf0f31b7934d4dfefdab9c78f1f9f","modified":1510666811518},{"_id":"themes/next/.git/objects/80/110db102eff313ca6786bc07bd8a60668cff71","hash":"b47ffcc1fbe451829d281e0f13b293c8f2646391","modified":1507949518696},{"_id":"themes/next/.git/objects/d0/1d5d386f882404d9c103150d2d6f436188f7c4","hash":"664f40742e988b2fb5fc54813adcda7380470c1c","modified":1510666811522},{"_id":"themes/next/.git/objects/b5/e41fc7fd3ff89e7ac77e6e18710a38e91a39c5","hash":"0ea24555febed564ef8f99ef40ceb2a79a45e0f9","modified":1513415363909},{"_id":"themes/next/.git/objects/d6/63af515cf90a8553917c52157f046d525178d6","hash":"c48d81ff1c9dd3ededd714e8362b7b6cc05bd513","modified":1513415363909},{"_id":"themes/next/.git/objects/d8/24ae36ce384ffa597e8bf17c889ce9b60bb435","hash":"4cf02b3d05c30790dcb4c6e1746a6a4b0e6ad8aa","modified":1513511611724},{"_id":"themes/next/.git/objects/d6/91fd73162dec1e781a4f4e3d37d40586c5eadc","hash":"f424d48cb15ce1e54ab0f18486050ec201218d81","modified":1514988833190},{"_id":"themes/next/.git/objects/e8/eadc02e5a16343a392ef8ca5123b2bb2e83509","hash":"0bbda04dd91c0d80112842ad26033546de11ed48","modified":1514988833186},{"_id":"themes/next/.git/objects/db/748740c8abc730822fec88b3f850d4afea7e81","hash":"2b4ce08bb0b497c16d67de50fecdef4850a1f6e2","modified":1507949518696},{"_id":"themes/next/.git/objects/e9/57ca4d53b12fa8b01074d02341b19ea50f08b8","hash":"adb8a650c335b611e51c8d8310fece12f1691d23","modified":1507641326672},{"_id":"themes/next/.git/objects/f7/9c4c5d1c22e813a298ee8b8ab801039b378707","hash":"86c71a7b3cb1bdbadafa8e92b5f541561e822767","modified":1514988833186},{"_id":"themes/next/.git/objects/ef/a17bd4976f7d8e122465ac84527cc2772bb456","hash":"f96f6a166910a449da3b3f400639d0be06969ba0","modified":1507641326668},{"_id":"themes/next/.git/objects/fc/dc1cddbea2e41a06b41dc09fb7f911eff4dc87","hash":"82d43c7a73c59dc6166126ced3ccbe7fcfb993d6","modified":1507949518696},{"_id":"themes/next/.git/refs/heads/master","hash":"6b0fe86301f3b15db92077806fca6fbbfbedc54e","modified":1514988833530},{"_id":"themes/next/.git/objects/e2/186ffd0c271a8a2b2d5474b98d59c728b9fc10","hash":"dc94631a51504d04d0f21fcd55b0042c40a8fd89","modified":1513511632087},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1504351219842},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1504351219842},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1504351219842},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1504351219842},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1504351219842},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1504351219842},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1504351219842},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1504351219842},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"fd65b0d38d4a8b8306de815c48caad20b84ba4cb","modified":1504351219842},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1504351219842},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1504351219842},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1504351219842},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"9f4ed36c73e890909b8ebbe601fb60e13d048288","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1504351219842},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"576e716893153a855eaf6d136fad7cb6d4065e09","modified":1504351219842},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1504351219842},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1504351219842},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1504351219842},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1504351219842},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1504351219846},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1504351219846},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1504351219846},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1504351219846},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1504351219846},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1504351219846},{"_id":"themes/next/source/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1504351219846},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1504351219846},{"_id":"themes/next/source/js/src/motion.js","hash":"da146caf488078a634d961debf2a71ce4106018c","modified":1504351219846},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1504351219846},{"_id":"themes/next/source/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1504351219846},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1504351219846},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1504351219846},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1504351219846},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1504351219846},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"6359c84aaa02c90be60b22abe638b737ddd69c9c","modified":1504351219846},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1504351219846},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cfee25d790e4f9b7d57f0dc7e2ea9c1649f08f11","modified":1504351219846},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d477196c5699c8261b08e993a77ef67054d86166","modified":1504351219846},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1504351219854},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1504351219854},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1504351219854},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1504351219850},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1504351219862},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1504351219866},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1504351219866},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1504351219862},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1504351219866},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1504351219866},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1504351219866},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1504351219866},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1504351219866},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1504351219866},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1504351219866},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1504351219866},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1504351219870},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1504351219870},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1504351219870},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1504351219870},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1504351219870},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1504351219862},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"afda003a066187a1e70d4525c8863021dde45f74","modified":1514988833530},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1504351219842},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1504351219838},{"_id":"themes/next/.git/refs/remotes/origin/master","hash":"6b0fe86301f3b15db92077806fca6fbbfbedc54e","modified":1514988833502},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d026c8489f66ab6c12ad04bd37f1d5b6f2f3f0d1","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1504351219842},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1504351219846},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"f2030fa436c47791d1a42358cc0ef6f9809f212c","modified":1504351219846},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"86b6fd7f1b1be3ae98f8af6b23a6b1299c670ce9","modified":1504351219846},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1504351219846},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1504351219846},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1504351219846},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bc8c388553bbcf95897459a466ba35bffd5ec5f0","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"0af5a9322156c4c21d3c7d38f5ee48de5286f523","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"f00d0a9ff02f6814011e0b613a2d9020911b5c58","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1504351219846},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1504351219846},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1504351219850},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1504351219850},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1504351219854},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1504351219850},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1504351219854},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1504351219854},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1504351219854},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1504351219870},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1504351219870},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1504351219854},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1504351219850},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1504351219862},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1504351219862},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1504351219870},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"c5477e51fc935404f44cf2273735ee55952595f4","modified":1504351219838},{"_id":"themes/next/.git/logs/refs/remotes/origin/master","hash":"faf3362b62203642ee9d9a8d659cdb93306892de","modified":1514988833506},{"_id":"themes/next/.git/objects/pack/pack-3eed718c95a3ecdf2c80c5de2a2e8765afb13a5c.idx","hash":"a677afcbbb3969843f2950e5f7ffd953cab24bab","modified":1504351219594},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ee554b1031ef0070a5916477939021800e3c9d27","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"67c357ddc16b31e7dfd8f956a77f984662c06fc2","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"65a64d5662637b66e2f039a5f58217afe7a6e800","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1504351219842},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"57d2c8a060f5e4e1a0aef9aae11a0016cf7ac5ba","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1504351219846},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1504351219846},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1504351219846},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1504351219846},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1504351219858},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1504351219858},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1504351219850},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1504351219862},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1504351219850},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1504351219850},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1504351219854},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1504351219854},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1504351219854},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1504351219870},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1504351219862},{"_id":"themes/next/.git/objects/pack/pack-3eed718c95a3ecdf2c80c5de2a2e8765afb13a5c.pack","hash":"13735fcb6830db352bd09c733c286a0ce726c778","modified":1504351219590},{"_id":"public/sitemap.xml","hash":"9165d0e6d38c917d26e8822bcd2889b7160ee55b","modified":1515682654029},{"_id":"public/about/index.html","hash":"6a5f01e6130c2735bc65d945fb32b655d4b4f95e","modified":1515682654112},{"_id":"public/categories/index.html","hash":"0f658da4979c1fcb0ddf93537920fb24e2cf372c","modified":1515682654112},{"_id":"public/tags/index.html","hash":"b89eafd9b2a29e962b26caa267f2b2205f597043","modified":1515682654112},{"_id":"public/archives/2017/07/index.html","hash":"984a9e99f5dbd555acbad6df51b27f4b9582341c","modified":1515682654112},{"_id":"public/archives/2017/08/index.html","hash":"6330d16ec78e9fc492fbc25f3040a9619445e91b","modified":1515682654112},{"_id":"public/archives/2017/09/index.html","hash":"2ceef0a8e82370e882acf9c3734025198a539722","modified":1515682654112},{"_id":"public/archives/2017/10/index.html","hash":"a63a07a5389582789f0859092bbd8888d976c4db","modified":1515682654113},{"_id":"public/archives/2017/11/index.html","hash":"e4628ca1f084be6ddde5bfec815dbb13acecdc9d","modified":1515682654113},{"_id":"public/archives/2018/index.html","hash":"be115ad1c3c0d4c6ddbd2c6775fca931dba8c626","modified":1515682654113},{"_id":"public/archives/2018/01/index.html","hash":"856f7b4671ef912eea77c7b0588ac9716274017c","modified":1515682654113},{"_id":"public/tags/moving-averages/index.html","hash":"73e916367cf4f7232aaf43238e0b9f9bd38cdb75","modified":1515682654113},{"_id":"public/tags/regularization/index.html","hash":"920c26dc71fbb6da4d5c487b37461ddbe1ec34f3","modified":1515682654113},{"_id":"public/tags/hyperparameter/index.html","hash":"ab55233ba848ac9712cc20b052dc912dd6ffa073","modified":1515682654113},{"_id":"public/tags/batch-norm/index.html","hash":"e0430fdc8d2270785ecf0b25f340a8fc8aa86020","modified":1515682654113},{"_id":"public/tags/covariate-shift/index.html","hash":"c7cd052a6260f31c7105990252fa22a9e9b9f92a","modified":1515682654113},{"_id":"public/tags/CNN/index.html","hash":"41bf03bdb6a6fc8ddd9ef7e88ef8597565c9008f","modified":1515682654113},{"_id":"public/tags/learning-strategy/index.html","hash":"4c9629ca6787d561494e22ee762bc28162b6355c","modified":1515682654113},{"_id":"public/tags/transfer-learning/index.html","hash":"d27f2a2932b04b8445af95899a9640fd39df0c06","modified":1515682654113},{"_id":"public/tags/multi-task-learning/index.html","hash":"a7235fbd24addf05dce89145f929765c95be1ea3","modified":1515682654113},{"_id":"public/tags/orthogonalization/index.html","hash":"ae237b078afa1849ad483f3a04e6013077100bf6","modified":1515682654113},{"_id":"public/tags/convex-optimization/index.html","hash":"3f411c1c3ea251b2b1385b768e5a26c45b31cf8b","modified":1515682654113},{"_id":"public/tags/newton-s-method/index.html","hash":"004cf368faf854c635e4c883e6b53a99cc820581","modified":1515682654113},{"_id":"public/tags/unconstrained-optimization/index.html","hash":"c0190f1738e8c5ca153ce26f62ab04f2e1b2dad3","modified":1515682654113},{"_id":"public/tags/imbalanced-data/index.html","hash":"d31cf19d17c799bc5fbd47061e8a084ed32af47a","modified":1515682654113},{"_id":"public/tags//index.html","hash":"95e37bc0572d48843a2a08ab0cdcd490641cb06a","modified":1515682654113},{"_id":"public/tags/MAP/index.html","hash":"a386a3b871265fef966df1a8acee2b276c4ce7a1","modified":1515682654113},{"_id":"public/tags/ridge-regression/index.html","hash":"f61328c88b41a986461d83e6377a96a7be147cfa","modified":1515682654113},{"_id":"public/tags/lasso-regression/index.html","hash":"4fcfdf9f787a5d14a6f7f2479773a42ddef09324","modified":1515682654113},{"_id":"public/tags/spark/index.html","hash":"c741bffc5cd657f9e5ef2e5f953857b9959a6fee","modified":1515682654113},{"_id":"public/tags/gbt/index.html","hash":"5c128e4d75f25789b2520a4a9fb09ee219061c22","modified":1515682654113},{"_id":"public/tags/logistic-regression/index.html","hash":"5baf45c420376f0aa8e0874708dd8af495f07b9e","modified":1515682654113},{"_id":"public/tags/activtion-function/index.html","hash":"8bba9cd2ce9e2fd2afce4fa3fa904fd720bdd21e","modified":1515682654113},{"_id":"public/tags/undersampling/index.html","hash":"0c09eddf4a7baf4acc5399471871328777040e14","modified":1515682654113},{"_id":"public/tags/bagging/index.html","hash":"1019650ade57c69eaff8c9a564bebc3314823d8c","modified":1515682654114},{"_id":"public/categories/machine-learning/index.html","hash":"37915afad71854768d6799c836c398ac64167bf0","modified":1515682654114},{"_id":"public/categories//index.html","hash":"82b3a7f8b6c666ab4250c068933ea2582c3cf1c6","modified":1515682654114},{"_id":"public/categories/spark/index.html","hash":"b38c294df633fb9d319c9c3c855ca08bedb4b301","modified":1515682654114},{"_id":"public/categories/reading-notes/index.html","hash":"7de9bb970e8fc1defc1b2c1241a2c671e123110a","modified":1515682654114},{"_id":"public/2018/01/07/spark-spark-workflow/index.html","hash":"3245188ad4daae14b6d1d712c0e239b560f5ac07","modified":1515682654114},{"_id":"public/2017/11/29/course-deep-learning-course4-week2/index.html","hash":"d7f57a03d7400aee10645d3c808d2a408b0d5aaa","modified":1515682654114},{"_id":"public/2017/11/26/course-deep-learning-course4-week1/index.html","hash":"7858253384a5d64d1582eb9c0228159d81765058","modified":1515682654114},{"_id":"public/2017/11/11/ml-imbalanced-data-solution/index.html","hash":"dcb4cc372229d6246d124a3522e14169f02b8c8d","modified":1515682654114},{"_id":"public/2017/10/22/paper-swish/index.html","hash":"7aec5e91833ef373f6e3bb6b23becb3af4e4f403","modified":1515682654114},{"_id":"public/2017/10/18/course-deep-learning-course3-week2/index.html","hash":"490df1ddda5d008bb3eaa0f8fcaca413ce9cfe22","modified":1515682654114},{"_id":"public/2017/10/12/course-deep-learning-course3-week1/index.html","hash":"c6b98b5fb8980915447475e98fc6cc57b0579f26","modified":1515682654114},{"_id":"public/2017/09/30/course-deep-learning-course2-week3/index.html","hash":"25391ff094faea8fcb9f6bed269f0f30259f8dfa","modified":1515682654114},{"_id":"public/2017/09/27/course-deep-learning-course2-week2/index.html","hash":"dbf85d8158992fdfd5d126c35fb809f20efabb1c","modified":1515682654114},{"_id":"public/2017/09/24/course-deep-learning-course2-week1/index.html","hash":"647eccddcc10599b9cb4a6dfc3bd09f93bc5da70","modified":1515682654114},{"_id":"public/2017/09/10/paper-imbalance/index.html","hash":"9f3d754b30b0f442ac6dca2d5e2a03932841bfb3","modified":1515682654114},{"_id":"public/2017/08/27/ml-ridge-lasso/index.html","hash":"4e8ef82ed59b973d67908a9774e97b2c3d27d569","modified":1515682654114},{"_id":"public/2017/08/23/paper-facebook/index.html","hash":"086fb9fd18eb3c24288d71db4a2bd906ad02a0fd","modified":1515682654114},{"_id":"public/2017/08/11/ml-gd-and-nm/index.html","hash":"ef782d5d7636f24c975505d40c842952064b7b65","modified":1515682654114},{"_id":"public/2017/08/02/ml-convex-opt/index.html","hash":"0c0d6bc9133ca03b3070719025dc5cae6f56d6c4","modified":1515682654114},{"_id":"public/2017/07/26/other-hello/index.html","hash":"974db5de3ae4c3937136ab9bb1f6980a3d74ea24","modified":1515682654114},{"_id":"public/archives/index.html","hash":"2cd3518b16aa27cf0120222765a7564d722f3a05","modified":1515682654114},{"_id":"public/archives/page/2/index.html","hash":"0579a1678a62c68fbc452782c273aefff3a07dbf","modified":1515682654114},{"_id":"public/archives/2017/index.html","hash":"bfa7e7667948d8d49893fe6f8c3956680135016e","modified":1515682654114},{"_id":"public/archives/2017/page/2/index.html","hash":"fa3e52b3a0c908b2c4abd461ddef49a037c3b356","modified":1515682654114},{"_id":"public/index.html","hash":"42a49c15fae4b27ea439c1e444df73390180df6a","modified":1515682654114},{"_id":"public/page/2/index.html","hash":"d334a594f8332878347b3e601388ae057416feaa","modified":1515682654114},{"_id":"public/tags/gradient-descent/index.html","hash":"c71346738c5a7a0dc53c73ec11251bd840443c38","modified":1515682654114},{"_id":"public/categories/learning-notes/index.html","hash":"02dd08d7d7702dc35617b635238feffd6e2e105a","modified":1515682654114},{"_id":"public/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1515682654122},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1515682654122},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1515682654122},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1515682654122},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1515682654122},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1515682654122},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1515682654122},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1515682654122},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1515682654122},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1515682654122},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515682654122},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1515682654122},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1515682654122},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1515682654122},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1515682654123},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1515682654123},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1515682654123},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1515682654123},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1515682654123},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1515682654123},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1515682654123},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1515682654123},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1515682654123},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1515682654123},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1515682654123},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1515682654123},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1515682654123},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1515682654123},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1515682654123},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1515682654778},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1515682654790},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1515682654803},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1515682654803},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1515682654803},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1515682654803},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1515682654803},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1515682654803},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1515682654803},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1515682654803},{"_id":"public/js/src/motion.js","hash":"da146caf488078a634d961debf2a71ce4106018c","modified":1515682654803},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1515682654803},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1515682654803},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1515682654803},{"_id":"public/lib/fastclick/README.html","hash":"c07b353b4efa132290ec4479102a55d80ac6d300","modified":1515682654803},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1515682654803},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1515682654803},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1515682654803},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1515682654803},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1515682654803},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1515682654803},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1515682654803},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1515682654804},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1515682654804},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1515682654804},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1515682654804},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1515682654804},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1515682654804},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1515682654804},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1515682654804},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1515682654804},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1515682654804},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1515682654804},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1515682654805},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1515682654805},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1515682654805},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1515682654805},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1515682654805},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1515682654805},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1515682654805},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1515682654805},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1515682654805},{"_id":"public/css/main.css","hash":"491275ea6f4db6ce09da71b1a0e3c2768e249344","modified":1515682654805},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1515682654805},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1515682654806},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1515682654806},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1515682654806},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1515682654806},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1515682654806},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1515682654806},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1515682654806},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1515682654806},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1515682654806},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1515682654806},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1515682654806},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1515682654806},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1515682654806},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1515682654806},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1515682654806},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1515682654806},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1515682654806},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1515682654806},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1515682654807},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1515682654807},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1515682654829}],"Category":[{"name":"learning notes","_id":"cjcambe0l0002wahqaz5qwl6v"},{"name":"machine learning","_id":"cjcambe820012wahqu4z2k13n"},{"name":"","_id":"cjcambe8k001kwahqhz3a7qdn"},{"name":"spark","_id":"cjcambe8n001owahq9nzt2sbm"},{"name":"reading notes","_id":"cjcambe8o001twahqu0oot2m3"}],"Data":[],"Page":[{"title":"About","date":"2017-07-25T23:25:29.000Z","type":"about","comments":0,"_content":"******Asir**\n\nsuperAsirJoeAsir**ASir**yeah\n\n7[****](http://www.buaa.edu.cn/)\n\n[**JD.com**](https://www.jd.com/)\n\n**Machine learning**\n\n**Arsenal****Rock&Roll**\n\nBTW**!!**\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2017-07-26 07:25:29\ntype: \"about\"\ncomments: false\n---\n******Asir**\n\nsuperAsirJoeAsir**ASir**yeah\n\n7[****](http://www.buaa.edu.cn/)\n\n[**JD.com**](https://www.jd.com/)\n\n**Machine learning**\n\n**Arsenal****Rock&Roll**\n\nBTW**!!**\n","updated":"2017-11-14T13:40:12.274Z","path":"about/index.html","layout":"page","_id":"cjcambe7i000owahqauvb74vr","content":"<p><strong></strong><strong>Asir</strong></p>\n<p>superAsirJoeAsir<strong>ASir</strong>yeah</p>\n<p>7<a href=\"http://www.buaa.edu.cn/\" target=\"_blank\" rel=\"external\"><strong></strong></a></p>\n<p><a href=\"https://www.jd.com/\" target=\"_blank\" rel=\"external\"><strong>JD.com</strong></a></p>\n<p><strong>Machine learning</strong></p>\n<p><strong>Arsenal</strong><strong>Rock&amp;Roll</strong></p>\n<p>BTW<strong>!!</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong></strong><strong>Asir</strong></p>\n<p>superAsirJoeAsir<strong>ASir</strong>yeah</p>\n<p>7<a href=\"http://www.buaa.edu.cn/\" target=\"_blank\" rel=\"external\"><strong></strong></a></p>\n<p><a href=\"https://www.jd.com/\" target=\"_blank\" rel=\"external\"><strong>JD.com</strong></a></p>\n<p><strong>Machine learning</strong></p>\n<p><strong>Arsenal</strong><strong>Rock&amp;Roll</strong></p>\n<p>BTW<strong>!!</strong></p>\n"},{"title":"Categories","date":"2017-07-25T23:25:10.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2017-07-26 07:25:10\ntype: categories\ncomments: false\n---\n","updated":"2017-11-14T13:40:12.274Z","path":"categories/index.html","layout":"page","_id":"cjcambe7n000qwahqxpx7utoo","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2017-07-25T23:25:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags \ndate: 2017-07-26 07:25:21\ntype: tags\ncomments: false\n---\n","updated":"2017-11-14T13:40:12.274Z","path":"tags/index.html","layout":"page","_id":"cjcambe7t000twahq4bl364o6","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Learning Notes-Deep Learning, course2, week2","date":"2017-09-27T13:38:35.000Z","_content":"neural networksrecap\n<!--more-->\n## Mini-batch gradient descent\nmini-batch gradient descentbatch gradientstochastic gradient descentbatch gradientconvergelocal minimumstochastic gradient descent epochgradient descent\n\nmini-batch gradient descentNgbatchmini-batchbatch gradient descent264,128,256,512batch size. batch sizeCPUGPU\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averagesmoving averagesNg\\\\(\\theta\\\\)\\\\(v\\\\)\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n\\\\(\\beta\\\\)moving averages\\\\(\\frac{1}{1- \\beta}\\\\)\\\\(\\theta\\\\)\\\\(\\beta = 0.9\\\\)10\n### Bias correction\nexponentially weighted averages\\\\(v_0=0\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png) \nbias correction\\\\(v_t\\\\)\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)bias correction\n\nNg\n* moving averages initial valuebias correction\n* Bias correctioninitial value\n\n## Gradient descent optimization\n### momentum\ngradient descent\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png) \niterationmoving averagesmomentum\n\nmomentum\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n\\\\(1- \\beta\\\\)Ngmomentum\n\nmomentumgradientgradientgradient descentmomentum0gradient descent\n\nNg\\\\(\\beta\\\\)0.910moving averagesNgmomenbias correction10gradient descentiteration10\n### RMSprop\nmomentumRMSprop(root means square prop)square\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)0item\\\\(10^{-8}\\\\)\n\ngradient descent\\\\(W\\\\)\\\\(dW\\\\)RMSprop\\\\(dW\\\\)\\\\( \\sqrt{S_{dW}}\\\\)\\\\(W\\\\)\n\ngradient descent\\\\(\\alpha\\\\)gradient descent.\n### Adam\nmomentumRMSpropAdammomentumRMSpropmomentum\\\\( \\beta\\\\)\\\\( \\beta\\_1\\\\)RMSprop\\\\( \\beta\\\\)\\\\( \\beta\\_2\\\\)\\\\(t\\\\)\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\nbias correction\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n\\\\(\\epsilon\\\\)0item\\\\(10^{-8}\\\\)\\\\(\\beta\\_1\\\\)0.9\\\\(\\beta\\_2\\\\)0.999\n## Learning rate decay\ngradient descentminimumlearning rateminimumlearning decay\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n\\\\(\\alpha\\\\)minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","source":"_posts/course-deep-learning-course2-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week2\ndate: 2017-09-27 21:38:35\ntags:\n\t- gradient descent\n\t- moving averages\ncategories: learning notes\n---\nneural networksrecap\n<!--more-->\n## Mini-batch gradient descent\nmini-batch gradient descentbatch gradientstochastic gradient descentbatch gradientconvergelocal minimumstochastic gradient descent epochgradient descent\n\nmini-batch gradient descentNgbatchmini-batchbatch gradient descent264,128,256,512batch size. batch sizeCPUGPU\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averagesmoving averagesNg\\\\(\\theta\\\\)\\\\(v\\\\)\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\n\\\\(\\beta\\\\)moving averages\\\\(\\frac{1}{1- \\beta}\\\\)\\\\(\\theta\\\\)\\\\(\\beta = 0.9\\\\)10\n### Bias correction\nexponentially weighted averages\\\\(v_0=0\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png) \nbias correction\\\\(v_t\\\\)\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)bias correction\n\nNg\n* moving averages initial valuebias correction\n* Bias correctioninitial value\n\n## Gradient descent optimization\n### momentum\ngradient descent\n![](http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png) \niterationmoving averagesmomentum\n\nmomentum\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\n\\\\(1- \\beta\\\\)Ngmomentum\n\nmomentumgradientgradientgradient descentmomentum0gradient descent\n\nNg\\\\(\\beta\\\\)0.910moving averagesNgmomenbias correction10gradient descentiteration10\n### RMSprop\nmomentumRMSprop(root means square prop)square\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)0item\\\\(10^{-8}\\\\)\n\ngradient descent\\\\(W\\\\)\\\\(dW\\\\)RMSprop\\\\(dW\\\\)\\\\( \\sqrt{S_{dW}}\\\\)\\\\(W\\\\)\n\ngradient descent\\\\(\\alpha\\\\)gradient descent.\n### Adam\nmomentumRMSpropAdammomentumRMSpropmomentum\\\\( \\beta\\\\)\\\\( \\beta\\_1\\\\)RMSprop\\\\( \\beta\\\\)\\\\( \\beta\\_2\\\\)\\\\(t\\\\)\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\nbias correction\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\n\\\\(\\epsilon\\\\)0item\\\\(10^{-8}\\\\)\\\\(\\beta\\_1\\\\)0.9\\\\(\\beta\\_2\\\\)0.999\n## Learning rate decay\ngradient descentminimumlearning rateminimumlearning decay\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\n\\\\(\\alpha\\\\)minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","slug":"course-deep-learning-course2-week2","published":1,"updated":"2017-10-22T09:54:51.259Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe090000wahqlgc28b3b","content":"<p>neural networksrecap<br><a id=\"more\"></a></p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>mini-batch gradient descentbatch gradientstochastic gradient descentbatch gradientconvergelocal minimumstochastic gradient descent epochgradient descent</p>\n<p>mini-batch gradient descentNgbatchmini-batchbatch gradient descent264,128,256,512batch size. batch sizeCPUGPU</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averagesmoving averagesNg\\(\\theta\\)\\(v\\)</p>\n<script type=\"math/tex; mode=display\">v_0 = 0</script><script type=\"math/tex; mode=display\">v_1 = \\beta v_0 + (1- \\beta) \\theta_1</script><script type=\"math/tex; mode=display\">v_2 = \\beta v_1 + (1- \\beta) \\theta_2</script><script type=\"math/tex; mode=display\">v_3= \\beta v_2 + (1- \\beta) \\theta_3</script><script type=\"math/tex; mode=display\">\\cdots</script><script type=\"math/tex; mode=display\">v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t</script><p>\\(\\beta\\)moving averages\\(\\frac{1}{1- \\beta}\\)\\(\\theta\\)\\(\\beta = 0.9\\)10</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>exponentially weighted averages\\(v_0=0\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png\" alt=\"\"><br>bias correction\\(v_t\\)\\( \\frac{v_t}{1- \\beta^{t}}\\)bias correction</p>\n<p>Ng</p>\n<ul>\n<li>moving averages initial valuebias correction</li>\n<li>Bias correctioninitial value</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>gradient descent<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png\" alt=\"\"><br>iterationmoving averagesmomentum</p>\n<p>momentum</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta v_{dW}+(1- \\beta)dW</script><script type=\"math/tex; mode=display\">v_{db}= \\beta v_{db}+(1- \\beta)db</script><script type=\"math/tex; mode=display\">W:=W- \\alpha v_{dW}</script><script type=\"math/tex; mode=display\">b:=b - \\alpha v_{db}</script><p>\\(1- \\beta\\)Ngmomentum</p>\n<p>momentumgradientgradientgradient descentmomentum0gradient descent</p>\n<p>Ng\\(\\beta\\)0.910moving averagesNgmomenbias correction10gradient descentiteration10</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>momentumRMSprop(root means square prop)square</p>\n<script type=\"math/tex; mode=display\">S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2</script><script type=\"math/tex; mode=display\">S_{db}= \\beta S_{db}+(1- \\beta)(db)^2</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}</script><p>\\(\\epsilon\\)0item\\(10^{-8}\\)</p>\n<p>gradient descent\\(W\\)\\(dW\\)RMSprop\\(dW\\)\\( \\sqrt{S_{dW}}\\)\\(W\\)</p>\n<p>gradient descent\\(\\alpha\\)gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>momentumRMSpropAdammomentumRMSpropmomentum\\( \\beta\\)\\( \\beta_1\\)RMSprop\\( \\beta\\)\\( \\beta_2\\)\\(t\\)</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db</script><script type=\"math/tex; mode=display\">s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2</script><p>bias correction</p>\n<script type=\"math/tex; mode=display\">v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}</script><script type=\"math/tex; mode=display\">s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}</script><p>\\(\\epsilon\\)0item\\(10^{-8}\\)\\(\\beta_1\\)0.9\\(\\beta_2\\)0.999</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>gradient descentminimumlearning rateminimumlearning decay</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = 0.95^{epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0</script><p>\\(\\alpha\\)minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"external\">An overview of gradient descent optimization algorithms </a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>neural networksrecap<br>","more":"</p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>mini-batch gradient descentbatch gradientstochastic gradient descentbatch gradientconvergelocal minimumstochastic gradient descent epochgradient descent</p>\n<p>mini-batch gradient descentNgbatchmini-batchbatch gradient descent264,128,256,512batch size. batch sizeCPUGPU</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averagesmoving averagesNg\\(\\theta\\)\\(v\\)</p>\n<script type=\"math/tex; mode=display\">v_0 = 0</script><script type=\"math/tex; mode=display\">v_1 = \\beta v_0 + (1- \\beta) \\theta_1</script><script type=\"math/tex; mode=display\">v_2 = \\beta v_1 + (1- \\beta) \\theta_2</script><script type=\"math/tex; mode=display\">v_3= \\beta v_2 + (1- \\beta) \\theta_3</script><script type=\"math/tex; mode=display\">\\cdots</script><script type=\"math/tex; mode=display\">v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t</script><p>\\(\\beta\\)moving averages\\(\\frac{1}{1- \\beta}\\)\\(\\theta\\)\\(\\beta = 0.9\\)10</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>exponentially weighted averages\\(v_0=0\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-1.png\" alt=\"\"><br>bias correction\\(v_t\\)\\( \\frac{v_t}{1- \\beta^{t}}\\)bias correction</p>\n<p>Ng</p>\n<ul>\n<li>moving averages initial valuebias correction</li>\n<li>Bias correctioninitial value</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>gradient descent<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/7/7-2.png\" alt=\"\"><br>iterationmoving averagesmomentum</p>\n<p>momentum</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta v_{dW}+(1- \\beta)dW</script><script type=\"math/tex; mode=display\">v_{db}= \\beta v_{db}+(1- \\beta)db</script><script type=\"math/tex; mode=display\">W:=W- \\alpha v_{dW}</script><script type=\"math/tex; mode=display\">b:=b - \\alpha v_{db}</script><p>\\(1- \\beta\\)Ngmomentum</p>\n<p>momentumgradientgradientgradient descentmomentum0gradient descent</p>\n<p>Ng\\(\\beta\\)0.910moving averagesNgmomenbias correction10gradient descentiteration10</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>momentumRMSprop(root means square prop)square</p>\n<script type=\"math/tex; mode=display\">S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2</script><script type=\"math/tex; mode=display\">S_{db}= \\beta S_{db}+(1- \\beta)(db)^2</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}</script><p>\\(\\epsilon\\)0item\\(10^{-8}\\)</p>\n<p>gradient descent\\(W\\)\\(dW\\)RMSprop\\(dW\\)\\( \\sqrt{S_{dW}}\\)\\(W\\)</p>\n<p>gradient descent\\(\\alpha\\)gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>momentumRMSpropAdammomentumRMSpropmomentum\\( \\beta\\)\\( \\beta_1\\)RMSprop\\( \\beta\\)\\( \\beta_2\\)\\(t\\)</p>\n<script type=\"math/tex; mode=display\">v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db</script><script type=\"math/tex; mode=display\">s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2</script><p>bias correction</p>\n<script type=\"math/tex; mode=display\">v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}</script><script type=\"math/tex; mode=display\">s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}</script><script type=\"math/tex; mode=display\">W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}</script><p>\\(\\epsilon\\)0item\\(10^{-8}\\)\\(\\beta_1\\)0.9\\(\\beta_2\\)0.999</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>gradient descentminimumlearning rateminimumlearning decay</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = 0.95^{epochNum} * \\alpha_0</script><script type=\"math/tex; mode=display\">\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0</script><p>\\(\\alpha\\)minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"external\">An overview of gradient descent optimization algorithms </a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week1","date":"2017-09-24T06:06:08.000Z","_content":"Andrew NgDeep learninglearning notesrecapmarkcourse1markcourse2weekmark.\n<!--more-->\n## Data set\nmachine learningdata setmachine learningdeep learningdata settrainingNgdata set\n* training set\n* dev/validation set\n* testing set\ndevtesting set\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n/99.5%/2.5%/2.5%/deep learningdeep learningtest setdev set\n## Bias and variance\n### biasvariance\nbias & variancemachine learning NgCS229\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png) \nhigh bias situationunder fittinghigh variance situationtraining setover fitting.\n### Solution\nbiasvarianceNg\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png) \nhigh bias training set  dev seterrortraining errordev errorhigh biastraining errordev errorhigh variancehigh biashigh variance\n\nhigh bias\nhigh varianceregularization\n## Regularization\n### L1&L2 regularization\nL1L2 regularization\n\nL1L2 regularizationcost function convergence **regularization item**convergenceregularization \n### Dropout\nDropoutneural networkregularization\n\nDropout****neural networklayerunitsepochunitsweightsbiasepochunitsweightsbias\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png) \nDropoutregularizationNg\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\ndropoutunitweightsbiasunitsepochwork onover fittingdropoutshrink weightsL2 regularizationregularization\n\ndropoutL2 regularizationregularization itemcost function convergence\n### Other methods\nL1L2 regularizationdropoutover fittingdata augmentation\n\nearly stoppingtraining epochtraining setover fittingearly stoppingover fitting\n## Exploding/vanishing gradient\n### exploding/vanishing gradient\ndeep learningexploding/vanishing gradientdeep learning\n ****neural network\\\\(l\\\\)weights\\\\(W^{[1]}\\\\)\\\\(W^{[l]}\\\\)bias\\\\(b^{[1]}\\\\)\\\\(b^{[l]}\\\\)bias0active function\\\\(g(z)=z\\\\)\\\\(y\\\\)\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n\n\n\\\\(l\\\\)\\\\(W\\\\)1exploding gradient\\\\(W\\\\)1vanishing gradient.\n### Solution\nweightsexploding or vanishing gradientactive function\\\\(g(z)\\\\)bias\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n\\\\(n\\\\)\\\\(w\\\\)\\\\(w\\\\)0mean1varanceGaussian distribution\\\\(w\\\\)\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\nNgactive functionsigmoid\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)reLu\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)tanh\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\nneural networkgradient checknetworkNggradient check\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png) \n\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","source":"_posts/course-deep-learning-course2-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week1\ndate: 2017-09-24 14:06:08\ntags: \n\t- regularization\n\t- gradient descent\ncategories: learning notes\n---\nAndrew NgDeep learninglearning notesrecapmarkcourse1markcourse2weekmark.\n<!--more-->\n## Data set\nmachine learningdata setmachine learningdeep learningdata settrainingNgdata set\n* training set\n* dev/validation set\n* testing set\ndevtesting set\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\n/99.5%/2.5%/2.5%/deep learningdeep learningtest setdev set\n## Bias and variance\n### biasvariance\nbias & variancemachine learning NgCS229\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png) \nhigh bias situationunder fittinghigh variance situationtraining setover fitting.\n### Solution\nbiasvarianceNg\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png) \nhigh bias training set  dev seterrortraining errordev errorhigh biastraining errordev errorhigh variancehigh biashigh variance\n\nhigh bias\nhigh varianceregularization\n## Regularization\n### L1&L2 regularization\nL1L2 regularization\n\nL1L2 regularizationcost function convergence **regularization item**convergenceregularization \n### Dropout\nDropoutneural networkregularization\n\nDropout****neural networklayerunitsepochunitsweightsbiasepochunitsweightsbias\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png) \nDropoutregularizationNg\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\ndropoutunitweightsbiasunitsepochwork onover fittingdropoutshrink weightsL2 regularizationregularization\n\ndropoutL2 regularizationregularization itemcost function convergence\n### Other methods\nL1L2 regularizationdropoutover fittingdata augmentation\n\nearly stoppingtraining epochtraining setover fittingearly stoppingover fitting\n## Exploding/vanishing gradient\n### exploding/vanishing gradient\ndeep learningexploding/vanishing gradientdeep learning\n ****neural network\\\\(l\\\\)weights\\\\(W^{[1]}\\\\)\\\\(W^{[l]}\\\\)bias\\\\(b^{[1]}\\\\)\\\\(b^{[l]}\\\\)bias0active function\\\\(g(z)=z\\\\)\\\\(y\\\\)\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\n\n\n\\\\(l\\\\)\\\\(W\\\\)1exploding gradient\\\\(W\\\\)1vanishing gradient.\n### Solution\nweightsexploding or vanishing gradientactive function\\\\(g(z)\\\\)bias\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\n\\\\(n\\\\)\\\\(w\\\\)\\\\(w\\\\)0mean1varanceGaussian distribution\\\\(w\\\\)\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\nNgactive functionsigmoid\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)reLu\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)tanh\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\nneural networkgradient checknetworkNggradient check\n![](http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png) \n\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","slug":"course-deep-learning-course2-week1","published":1,"updated":"2017-10-22T09:54:38.611Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe0h0001wahqn3shazvm","content":"<p>Andrew NgDeep learninglearning notesrecapmarkcourse1markcourse2weekmark.<br><a id=\"more\"></a></p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>machine learningdata setmachine learningdeep learningdata settrainingNgdata set</p>\n<ul>\n<li>training set</li>\n<li>dev/validation set</li>\n<li>testing set<br>devtesting set<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. Youre not adjusting the weights of the network with this data set, youre just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasnt trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then youre overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>/99.5%/2.5%/2.5%/deep learningdeep learningtest setdev set</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"biasvariance\"><a href=\"#biasvariance\" class=\"headerlink\" title=\"biasvariance\"></a>biasvariance</h3><p>bias &amp; variancemachine learning NgCS229<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png\" alt=\"\"><br>high bias situationunder fittinghigh variance situationtraining setover fitting.</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>biasvarianceNg<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png\" alt=\"\"><br>high bias training set  dev seterrortraining errordev errorhigh biastraining errordev errorhigh variancehigh biashigh variance</p>\n<p>high bias<br>high varianceregularization</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>L1L2 regularization</p>\n<p>L1L2 regularizationcost function convergence <strong>regularization item</strong>convergenceregularization </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropoutneural networkregularization</p>\n<p>Dropout<strong></strong>neural networklayerunitsepochunitsweightsbiasepochunitsweightsbias<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png\" alt=\"\"><br>DropoutregularizationNg</p>\n<blockquote>\n<p>Intuition:Cant rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>dropoutunitweightsbiasunitsepochwork onover fittingdropoutshrink weightsL2 regularizationregularization</p>\n<p>dropoutL2 regularizationregularization itemcost function convergence</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>L1L2 regularizationdropoutover fittingdata augmentation</p>\n<p>early stoppingtraining epochtraining setover fittingearly stoppingover fitting</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"exploding-vanishing-gradient\"><a href=\"#exploding-vanishing-gradient\" class=\"headerlink\" title=\"exploding/vanishing gradient\"></a>exploding/vanishing gradient</h3><p>deep learningexploding/vanishing gradientdeep learning<br> <strong></strong>neural network\\(l\\)weights\\(W^{[1]}\\)\\(W^{[l]}\\)bias\\(b^{[1]}\\)\\(b^{[l]}\\)bias0active function\\(g(z)=z\\)\\(y\\)</p>\n<script type=\"math/tex; mode=display\">\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X</script><p></p>\n<p>\\(l\\)\\(W\\)1exploding gradient\\(W\\)1vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>weightsexploding or vanishing gradientactive function\\(g(z)\\)bias</p>\n<script type=\"math/tex; mode=display\">z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n</script><p>\\(n\\)\\(w\\)\\(w\\)0mean1varanceGaussian distribution\\(w\\)\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>Ngactive functionsigmoid\\(var(w)= \\frac{1}{n^{l-1}}\\)reLu\\(var(w)= \\frac{2}{n^{l-1}}\\)tanh\\(var(w)= \\frac{1}{n^{l-1}}\\)\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>neural networkgradient checknetworkNggradient check<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png\" alt=\"\"><br></p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Dont use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosent wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"external\">Bias and variance</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Andrew NgDeep learninglearning notesrecapmarkcourse1markcourse2weekmark.<br>","more":"</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>machine learningdata setmachine learningdeep learningdata settrainingNgdata set</p>\n<ul>\n<li>training set</li>\n<li>dev/validation set</li>\n<li>testing set<br>devtesting set<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. Youre not adjusting the weights of the network with this data set, youre just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasnt trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then youre overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>/99.5%/2.5%/2.5%/deep learningdeep learningtest setdev set</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"biasvariance\"><a href=\"#biasvariance\" class=\"headerlink\" title=\"biasvariance\"></a>biasvariance</h3><p>bias &amp; variancemachine learning NgCS229<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-5.png\" alt=\"\"><br>high bias situationunder fittinghigh variance situationtraining setover fitting.</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>biasvarianceNg<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-2.png\" alt=\"\"><br>high bias training set  dev seterrortraining errordev errorhigh biastraining errordev errorhigh variancehigh biashigh variance</p>\n<p>high bias<br>high varianceregularization</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>L1L2 regularization</p>\n<p>L1L2 regularizationcost function convergence <strong>regularization item</strong>convergenceregularization </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropoutneural networkregularization</p>\n<p>Dropout<strong></strong>neural networklayerunitsepochunitsweightsbiasepochunitsweightsbias<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-3.png\" alt=\"\"><br>DropoutregularizationNg</p>\n<blockquote>\n<p>Intuition:Cant rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>dropoutunitweightsbiasunitsepochwork onover fittingdropoutshrink weightsL2 regularizationregularization</p>\n<p>dropoutL2 regularizationregularization itemcost function convergence</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>L1L2 regularizationdropoutover fittingdata augmentation</p>\n<p>early stoppingtraining epochtraining setover fittingearly stoppingover fitting</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"exploding-vanishing-gradient\"><a href=\"#exploding-vanishing-gradient\" class=\"headerlink\" title=\"exploding/vanishing gradient\"></a>exploding/vanishing gradient</h3><p>deep learningexploding/vanishing gradientdeep learning<br> <strong></strong>neural network\\(l\\)weights\\(W^{[1]}\\)\\(W^{[l]}\\)bias\\(b^{[1]}\\)\\(b^{[l]}\\)bias0active function\\(g(z)=z\\)\\(y\\)</p>\n<script type=\"math/tex; mode=display\">\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X</script><p></p>\n<p>\\(l\\)\\(W\\)1exploding gradient\\(W\\)1vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>weightsexploding or vanishing gradientactive function\\(g(z)\\)bias</p>\n<script type=\"math/tex; mode=display\">z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n</script><p>\\(n\\)\\(w\\)\\(w\\)0mean1varanceGaussian distribution\\(w\\)\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>Ngactive functionsigmoid\\(var(w)= \\frac{1}{n^{l-1}}\\)reLu\\(var(w)= \\frac{2}{n^{l-1}}\\)tanh\\(var(w)= \\frac{1}{n^{l-1}}\\)\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>neural networkgradient checknetworkNggradient check<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/6/6-6.png\" alt=\"\"><br></p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Dont use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosent wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"external\">Bias and variance</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week3","date":"2017-09-30T07:44:25.000Z","_content":"hyperparameter selectionbatch normal\n<!--more-->\n## Hyperparameter selection\nHyperparameter selectionmachine learninggradient descentlearning rate \\\\(\\alpha\\\\)hyperparameterNg\n* hyperparameterhyperparameter\n* log0.000110.00010.0010.010.11\n\nhyperparameter selectionNgbabysittingparallelhyperparameter\n\n\n## Batch norm\n### Normalization\nnormalizationgradient descentbatch normnormalization\n\nnormalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n01Gaussian distribution\n\nnormaliztionz-score\n### Batch norm\nnormalizationneural networksnerual networkslayernormalization\\\\(z^{(i)}\\\\)\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n\\\\(z^{N\\[l](i)}\\\\)\\\\(z^{\\[l](i)}\\\\) \\\\( \\gamma\\\\)\\\\(\\beta\\\\)parametergradient descentparameternormalizationGaussian distributionNormal distribution\\\\(\\epsilon\\\\)0\n\n\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)\\\\( \\beta = \\mu\\\\)\\\\(z^{N(i)}=z^(i)\\\\)normalization.\n### Batch norm on neural networks\nneural networks\\\\(X\\\\)parameter\\\\(w^{[1]}\\\\)\\\\(b^{[1]}\\\\)\\\\(z^{[1]}\\\\)\\\\(\\beta\\\\)\\\\(\\gamma\\\\)\\\\(z^{N[1]}\\\\)active function\\\\(a^{[1]}\\\\)\\\\(w^{[2]}\\\\)\\\\(b^{[2]}\\\\)\\\\(z^{[2]}\\\\)forward propagation.\n\nparameters\\\\(w^{[l]}\\\\)\\\\(b^{[l]}\\\\)\\\\( \\beta^{[l]}\\\\)\\\\( \\gamma^{[l]}\\\\)\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\nbatch normal\\\\(z^{[l]}\\\\)10Gaussian distribution\\\\(b^{[l]}\\\\)batch normalparameter\\\\(w^{[l]}\\\\)\\\\( \\beta^{[l]}\\\\)\\\\( \\gamma^{[l]}\\\\)\n\nbackforwardneural networks\\\\(db\\\\)\n### Solve covariate shift\ncovariate shiftNgtraining setcovariate shift. batch normneural networkssolve covariate shift\n\nOKneural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png) \nparameter\\\\(w^{[3]}\\\\)\\\\(b^{[3]}\\\\)neural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png) \nneural networks\\\\(a^{[2]}\\\\)\\\\(a^{[2]}\\\\)iteration\\\\(a^{[2]}\\\\)covariate shift\n\nbatch norm\\\\(a^{[2]}\\\\)\\\\(\\beta\\\\)\\\\(gamma\\\\)****covariate shift\n\nbatch normregularizationmini-batch gradient descentbatch normsampledropoutlayerregularizationbatch normregularization\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course2-week3.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week3\ndate: 2017-09-30 15:44:25\ntags: \n\t- hyperparameter\n\t- batch norm\n\t- covariate shift\ncategories: learning notes\n---\nhyperparameter selectionbatch normal\n<!--more-->\n## Hyperparameter selection\nHyperparameter selectionmachine learninggradient descentlearning rate \\\\(\\alpha\\\\)hyperparameterNg\n* hyperparameterhyperparameter\n* log0.000110.00010.0010.010.11\n\nhyperparameter selectionNgbabysittingparallelhyperparameter\n\n\n## Batch norm\n### Normalization\nnormalizationgradient descentbatch normnormalization\n\nnormalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\n01Gaussian distribution\n\nnormaliztionz-score\n### Batch norm\nnormalizationneural networksnerual networkslayernormalization\\\\(z^{(i)}\\\\)\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\n\\\\(z^{N\\[l](i)}\\\\)\\\\(z^{\\[l](i)}\\\\) \\\\( \\gamma\\\\)\\\\(\\beta\\\\)parametergradient descentparameternormalizationGaussian distributionNormal distribution\\\\(\\epsilon\\\\)0\n\n\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)\\\\( \\beta = \\mu\\\\)\\\\(z^{N(i)}=z^(i)\\\\)normalization.\n### Batch norm on neural networks\nneural networks\\\\(X\\\\)parameter\\\\(w^{[1]}\\\\)\\\\(b^{[1]}\\\\)\\\\(z^{[1]}\\\\)\\\\(\\beta\\\\)\\\\(\\gamma\\\\)\\\\(z^{N[1]}\\\\)active function\\\\(a^{[1]}\\\\)\\\\(w^{[2]}\\\\)\\\\(b^{[2]}\\\\)\\\\(z^{[2]}\\\\)forward propagation.\n\nparameters\\\\(w^{[l]}\\\\)\\\\(b^{[l]}\\\\)\\\\( \\beta^{[l]}\\\\)\\\\( \\gamma^{[l]}\\\\)\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\nbatch normal\\\\(z^{[l]}\\\\)10Gaussian distribution\\\\(b^{[l]}\\\\)batch normalparameter\\\\(w^{[l]}\\\\)\\\\( \\beta^{[l]}\\\\)\\\\( \\gamma^{[l]}\\\\)\n\nbackforwardneural networks\\\\(db\\\\)\n### Solve covariate shift\ncovariate shiftNgtraining setcovariate shift. batch normneural networkssolve covariate shift\n\nOKneural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png) \nparameter\\\\(w^{[3]}\\\\)\\\\(b^{[3]}\\\\)neural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png) \nneural networks\\\\(a^{[2]}\\\\)\\\\(a^{[2]}\\\\)iteration\\\\(a^{[2]}\\\\)covariate shift\n\nbatch norm\\\\(a^{[2]}\\\\)\\\\(\\beta\\\\)\\\\(gamma\\\\)****covariate shift\n\nbatch normregularizationmini-batch gradient descentbatch normsampledropoutlayerregularizationbatch normregularization\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course2-week3","published":1,"updated":"2017-10-22T09:55:01.371Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe0q0004wahqofnfxrif","content":"<p>hyperparameter selectionbatch normal<br><a id=\"more\"></a></p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selectionmachine learninggradient descentlearning rate \\(\\alpha\\)hyperparameterNg</p>\n<ul>\n<li>hyperparameterhyperparameter</li>\n<li>log0.000110.00010.0010.010.11</li>\n</ul>\n<p>hyperparameter selectionNgbabysittingparallelhyperparameter</p>\n<p></p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>normalizationgradient descentbatch normnormalization</p>\n<p>normalize</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i x^{(i)}</script><script type=\"math/tex; mode=display\">X = X- \\mu</script><script type=\"math/tex; mode=display\">\\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2</script><script type=\"math/tex; mode=display\">X = X/ \\sigma ^2</script><p>01Gaussian distribution</p>\n<p>normaliztionz-score</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>normalizationneural networksnerual networkslayernormalization\\(z^{(i)}\\)</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i z^{(i)}</script><script type=\"math/tex; mode=display\">\\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2</script><script type=\"math/tex; mode=display\">z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}</script><script type=\"math/tex; mode=display\">z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta</script><p>\\(z^{N[l](i)}\\)\\(z^{[l](i)}\\) \\( \\gamma\\)\\(\\beta\\)parametergradient descentparameternormalizationGaussian distributionNormal distribution\\(\\epsilon\\)0</p>\n<p>\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)\\( \\beta = \\mu\\)\\(z^{N(i)}=z^(i)\\)normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>neural networks\\(X\\)parameter\\(w^{[1]}\\)\\(b^{[1]}\\)\\(z^{[1]}\\)\\(\\beta\\)\\(\\gamma\\)\\(z^{N[1]}\\)active function\\(a^{[1]}\\)\\(w^{[2]}\\)\\(b^{[2]}\\)\\(z^{[2]}\\)forward propagation.</p>\n<p>parameters\\(w^{[l]}\\)\\(b^{[l]}\\)\\( \\beta^{[l]}\\)\\( \\gamma^{[l]}\\)</p>\n<script type=\"math/tex; mode=display\">z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}</script><p>batch normal\\(z^{[l]}\\)10Gaussian distribution\\(b^{[l]}\\)batch normalparameter\\(w^{[l]}\\)\\( \\beta^{[l]}\\)\\( \\gamma^{[l]}\\)</p>\n<p>backforwardneural networks\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>covariate shiftNgtraining setcovariate shift. batch normneural networkssolve covariate shift</p>\n<p>OKneural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png\" alt=\"\"><br>parameter\\(w^{[3]}\\)\\(b^{[3]}\\)neural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png\" alt=\"\"><br>neural networks\\(a^{[2]}\\)\\(a^{[2]}\\)iteration\\(a^{[2]}\\)covariate shift</p>\n<p>batch norm\\(a^{[2]}\\)\\(\\beta\\)\\(gamma\\)<strong></strong>covariate shift</p>\n<p>batch normregularizationmini-batch gradient descentbatch normsampledropoutlayerregularizationbatch normregularization</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>hyperparameter selectionbatch normal<br>","more":"</p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selectionmachine learninggradient descentlearning rate \\(\\alpha\\)hyperparameterNg</p>\n<ul>\n<li>hyperparameterhyperparameter</li>\n<li>log0.000110.00010.0010.010.11</li>\n</ul>\n<p>hyperparameter selectionNgbabysittingparallelhyperparameter</p>\n<p></p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>normalizationgradient descentbatch normnormalization</p>\n<p>normalize</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i x^{(i)}</script><script type=\"math/tex; mode=display\">X = X- \\mu</script><script type=\"math/tex; mode=display\">\\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2</script><script type=\"math/tex; mode=display\">X = X/ \\sigma ^2</script><p>01Gaussian distribution</p>\n<p>normaliztionz-score</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>normalizationneural networksnerual networkslayernormalization\\(z^{(i)}\\)</p>\n<script type=\"math/tex; mode=display\">\\mu = \\frac{1}{m} \\sum_i z^{(i)}</script><script type=\"math/tex; mode=display\">\\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2</script><script type=\"math/tex; mode=display\">z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}</script><script type=\"math/tex; mode=display\">z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta</script><p>\\(z^{N[l](i)}\\)\\(z^{[l](i)}\\) \\( \\gamma\\)\\(\\beta\\)parametergradient descentparameternormalizationGaussian distributionNormal distribution\\(\\epsilon\\)0</p>\n<p>\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)\\( \\beta = \\mu\\)\\(z^{N(i)}=z^(i)\\)normalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>neural networks\\(X\\)parameter\\(w^{[1]}\\)\\(b^{[1]}\\)\\(z^{[1]}\\)\\(\\beta\\)\\(\\gamma\\)\\(z^{N[1]}\\)active function\\(a^{[1]}\\)\\(w^{[2]}\\)\\(b^{[2]}\\)\\(z^{[2]}\\)forward propagation.</p>\n<p>parameters\\(w^{[l]}\\)\\(b^{[l]}\\)\\( \\beta^{[l]}\\)\\( \\gamma^{[l]}\\)</p>\n<script type=\"math/tex; mode=display\">z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}</script><p>batch normal\\(z^{[l]}\\)10Gaussian distribution\\(b^{[l]}\\)batch normalparameter\\(w^{[l]}\\)\\( \\beta^{[l]}\\)\\( \\gamma^{[l]}\\)</p>\n<p>backforwardneural networks\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>covariate shiftNgtraining setcovariate shift. batch normneural networkssolve covariate shift</p>\n<p>OKneural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-1.png\" alt=\"\"><br>parameter\\(w^{[3]}\\)\\(b^{[3]}\\)neural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/8/8-2.png\" alt=\"\"><br>neural networks\\(a^{[2]}\\)\\(a^{[2]}\\)iteration\\(a^{[2]}\\)covariate shift</p>\n<p>batch norm\\(a^{[2]}\\)\\(\\beta\\)\\(gamma\\)<strong></strong>covariate shift</p>\n<p>batch normregularizationmini-batch gradient descentbatch normsampledropoutlayerregularizationbatch normregularization</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week2","date":"2017-11-29T08:26:12.000Z","_content":"course4week2CNNCNNcomputer visionrecap\n<!--more-->\n## Classic Networks\nNg3CNNpaper\n### LeNet-5\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-1.png) \n[Lcun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-2.png) \n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-3.png) \n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\ncnnpaper\n\n## Residual Networks(ResNets)\nresidual networksresidual block\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-4.png) \nresidual block\\\\(a^{[l]}\\\\)\\\\(a^{[l+2]}\\\\)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\n\\\\(g\\\\)activation functionReLUshort circuitskip connectionresidual blockresidual networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-5.png) \nResidual networksnetworkslayertraining errorresidual networkslayertraining error\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 11 Convolutions\nfilterkernel matrix11filter\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-6.png) \n11filterinputchannel(depth)11filter\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\ninception network\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-7.png) \ninputfiltermax poolinghightwidthfilternetwork\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\nNginception network 11filter\nInception \n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-8.png) \npaper\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","source":"_posts/course-deep-learning-course4-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week2\ndate: 2017-11-29 16:26:12\ntags: CNN\ncategories: learning notes\n---\ncourse4week2CNNCNNcomputer visionrecap\n<!--more-->\n## Classic Networks\nNg3CNNpaper\n### LeNet-5\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-1.png) \n[Lcun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-2.png) \n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-3.png) \n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\ncnnpaper\n\n## Residual Networks(ResNets)\nresidual networksresidual block\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-4.png) \nresidual block\\\\(a^{[l]}\\\\)\\\\(a^{[l+2]}\\\\)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\n\\\\(g\\\\)activation functionReLUshort circuitskip connectionresidual blockresidual networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-5.png) \nResidual networksnetworkslayertraining errorresidual networkslayertraining error\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 11 Convolutions\nfilterkernel matrix11filter\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-6.png) \n11filterinputchannel(depth)11filter\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\ninception network\n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-7.png) \ninputfiltermax poolinghightwidthfilternetwork\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\nNginception network 11filter\nInception \n![](http://otmy7guvn.bkt.clouddn.com/blog/14/14-8.png) \npaper\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","slug":"course-deep-learning-course4-week2","published":1,"updated":"2017-12-11T14:41:39.792Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7g000nwahqhfzfp4fw","content":"<p>course4week2CNNCNNcomputer visionrecap<br><a id=\"more\"></a></p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ng3CNNpaper</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"external\">Lcun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"external\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>cnnpaper</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>residual networksresidual block<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-4.png\" alt=\"\"><br>residual block\\(a^{[l]}\\)\\(a^{[l+2]}\\)</p>\n<script type=\"math/tex; mode=display\">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})</script><p>\\(g\\)activation functionReLUshort circuitskip connectionresidual blockresidual networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-5.png\" alt=\"\"><br>Residual networksnetworkslayertraining errorresidual networkslayertraining error<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"external\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-11-Convolutions\"><a href=\"#Network-in-Network-and-11-Convolutions\" class=\"headerlink\" title=\"Network in Network and 11 Convolutions\"></a>Network in Network and 11 Convolutions</h2><p>filterkernel matrix11filter<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-6.png\" alt=\"\"><br>11filterinputchannel(depth)11filter<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"external\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>inception network<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-7.png\" alt=\"\"><br>inputfiltermax poolinghightwidthfilternetwork<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"external\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>Nginception network 11filter<br>Inception <br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-8.png\" alt=\"\"><br>paper</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>course4week2CNNCNNcomputer visionrecap<br>","more":"</p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ng3CNNpaper</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"external\">Lcun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"external\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>cnnpaper</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>residual networksresidual block<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-4.png\" alt=\"\"><br>residual block\\(a^{[l]}\\)\\(a^{[l+2]}\\)</p>\n<script type=\"math/tex; mode=display\">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})</script><p>\\(g\\)activation functionReLUshort circuitskip connectionresidual blockresidual networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-5.png\" alt=\"\"><br>Residual networksnetworkslayertraining errorresidual networkslayertraining error<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"external\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-11-Convolutions\"><a href=\"#Network-in-Network-and-11-Convolutions\" class=\"headerlink\" title=\"Network in Network and 11 Convolutions\"></a>Network in Network and 11 Convolutions</h2><p>filterkernel matrix11filter<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-6.png\" alt=\"\"><br>11filterinputchannel(depth)11filter<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"external\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>inception network<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-7.png\" alt=\"\"><br>inputfiltermax poolinghightwidthfilternetwork<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"external\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>Nginception network 11filter<br>Inception <br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/14/14-8.png\" alt=\"\"><br>paper</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week1","date":"2017-11-26T12:30:47.000Z","_content":"Hi, all. course4convolutional neural networks \n<!--more-->\n## Convolution\nNgedge detectionconvolutionimage processingNgmarkdownconvolution\n### Padding\nconvolutionimage\\\\(n \\*n\\\\)convolution filter\\\\(f \\*f\\\\)image\\\\( (n-f+1) \\*(n-f+1)\\\\)imagepadding\n\nPaddingimage\\\\(6 \\*6\\\\)image\\\\(3 \\*3\\\\)filter\\\\\\(p=1\\\\)padding\\\\(8 \\* 8\\\\)\\\\(6 \\*6\\\\)imageimagepaddingconvolution\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\nNgvalidsame convolutionsvalid convolutionpadding convolutionsame convolution\n\n### Stride\npaddingstridestridefilterconvolutionstride=1filterstride=2\n\nimage\\\\(n \\*n\\\\)convolution filter\\\\(f \\*f\\\\)padding\\\\(p\\\\)stride\\\\(s\\\\)image\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)convolution\n\n##Convolution over Volume\nRGBRGBchannelconvolution\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-1.png) \n663hightwidthchannel(depth)filter3channel(depth)44\n\nfilter\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-2.png) \nfilter333442channelfilterchannel\n\n## CNN\n### Convolution Layer\nCNNlayer\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-3.png) \n663RGB333filterimage\\\\(x\\\\)\\\\(a ^{[0]}\\\\)filter\\\\(w ^{[1]}\\\\)\\\\(w ^{[1]}a ^{[0]}\\\\)bias\\\\(b^{[1]}\\\\)liner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)non-liner functionReLU442outputCNNlayer.\n\nCNNDNNliner functionnon-liner functionnon-liner functionclassifydataCNNfilterfeaturefilterclassify data.\n\nfully connected DNNCNNparameters\n\n### Pooling\nPooling\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-4.png) \nmax poolingfilter\\\\(f=2\\\\)stride\\\\(s=2\\\\)filtermaxaverageaverage poolingpoolinghyperparameterfilter\\\\(f\\\\)stride\\\\(s\\\\)poolingno parameters to learn!\n\n### Fully Connected layer\nFully connected layerCNNinputDNNfully connected\n\nprameterpoolinglayerCNNCONV-POOL-CONV-POOL-FC-FC-Softmax5CNNCNN\n\n## Why Convolutions\nNg\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)","source":"_posts/course-deep-learning-course4-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week1\ndate: 2017-11-26 20:30:47\ntags: CNN\ncategories: learning notes\n---\nHi, all. course4convolutional neural networks \n<!--more-->\n## Convolution\nNgedge detectionconvolutionimage processingNgmarkdownconvolution\n### Padding\nconvolutionimage\\\\(n \\*n\\\\)convolution filter\\\\(f \\*f\\\\)image\\\\( (n-f+1) \\*(n-f+1)\\\\)imagepadding\n\nPaddingimage\\\\(6 \\*6\\\\)image\\\\(3 \\*3\\\\)filter\\\\\\(p=1\\\\)padding\\\\(8 \\* 8\\\\)\\\\(6 \\*6\\\\)imageimagepaddingconvolution\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\nNgvalidsame convolutionsvalid convolutionpadding convolutionsame convolution\n\n### Stride\npaddingstridestridefilterconvolutionstride=1filterstride=2\n\nimage\\\\(n \\*n\\\\)convolution filter\\\\(f \\*f\\\\)padding\\\\(p\\\\)stride\\\\(s\\\\)image\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)convolution\n\n##Convolution over Volume\nRGBRGBchannelconvolution\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-1.png) \n663hightwidthchannel(depth)filter3channel(depth)44\n\nfilter\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-2.png) \nfilter333442channelfilterchannel\n\n## CNN\n### Convolution Layer\nCNNlayer\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-3.png) \n663RGB333filterimage\\\\(x\\\\)\\\\(a ^{[0]}\\\\)filter\\\\(w ^{[1]}\\\\)\\\\(w ^{[1]}a ^{[0]}\\\\)bias\\\\(b^{[1]}\\\\)liner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)non-liner functionReLU442outputCNNlayer.\n\nCNNDNNliner functionnon-liner functionnon-liner functionclassifydataCNNfilterfeaturefilterclassify data.\n\nfully connected DNNCNNparameters\n\n### Pooling\nPooling\n![](http://otmy7guvn.bkt.clouddn.com/blog/13/13-4.png) \nmax poolingfilter\\\\(f=2\\\\)stride\\\\(s=2\\\\)filtermaxaverageaverage poolingpoolinghyperparameterfilter\\\\(f\\\\)stride\\\\(s\\\\)poolingno parameters to learn!\n\n### Fully Connected layer\nFully connected layerCNNinputDNNfully connected\n\nprameterpoolinglayerCNNCONV-POOL-CONV-POOL-FC-FC-Softmax5CNNCNN\n\n## Why Convolutions\nNg\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)","slug":"course-deep-learning-course4-week1","published":1,"updated":"2017-11-27T07:40:20.987Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7j000pwahqt6lr5gxg","content":"<p>Hi, all. course4convolutional neural networks <br><a id=\"more\"></a></p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>Ngedge detectionconvolutionimage processingNgmarkdownconvolution</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>convolutionimage\\(n *n\\)convolution filter\\(f *f\\)image\\( (n-f+1) *(n-f+1)\\)imagepadding</p>\n<p>Paddingimage\\(6 *6\\)image\\(3 *3\\)filter\\(p=1\\)padding\\(8 * 8\\)\\(6 *6\\)imageimagepaddingconvolution\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>Ngvalidsame convolutionsvalid convolutionpadding convolutionsame convolution</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>paddingstridestridefilterconvolutionstride=1filterstride=2</p>\n<p>image\\(n *n\\)convolution filter\\(f *f\\)padding\\(p\\)stride\\(s\\)image\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)convolution</p>\n<h2 id=\"Convolution-over-Volume\"><a href=\"#Convolution-over-Volume\" class=\"headerlink\" title=\"Convolution over Volume\"></a>Convolution over Volume</h2><p>RGBRGBchannelconvolution<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-1.png\" alt=\"\"><br>663hightwidthchannel(depth)filter3channel(depth)44</p>\n<p>filter<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-2.png\" alt=\"\"><br>filter333442channelfilterchannel</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>CNNlayer<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-3.png\" alt=\"\"><br>663RGB333filterimage\\(x\\)\\(a ^{[0]}\\)filter\\(w ^{[1]}\\)\\(w ^{[1]}a ^{[0]}\\)bias\\(b^{[1]}\\)liner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)non-liner functionReLU442outputCNNlayer.</p>\n<p>CNNDNNliner functionnon-liner functionnon-liner functionclassifydataCNNfilterfeaturefilterclassify data.</p>\n<p>fully connected DNNCNNparameters</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>Pooling<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-4.png\" alt=\"\"><br>max poolingfilter\\(f=2\\)stride\\(s=2\\)filtermaxaverageaverage poolingpoolinghyperparameterfilter\\(f\\)stride\\(s\\)poolingno parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layerCNNinputDNNfully connected</p>\n<p>prameterpoolinglayerCNNCONV-POOL-CONV-POOL-FC-FC-Softmax5CNNCNN</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>Ng</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) thats useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Hi, all. course4convolutional neural networks <br>","more":"</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>Ngedge detectionconvolutionimage processingNgmarkdownconvolution</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>convolutionimage\\(n *n\\)convolution filter\\(f *f\\)image\\( (n-f+1) *(n-f+1)\\)imagepadding</p>\n<p>Paddingimage\\(6 *6\\)image\\(3 *3\\)filter\\(p=1\\)padding\\(8 * 8\\)\\(6 *6\\)imageimagepaddingconvolution\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>Ngvalidsame convolutionsvalid convolutionpadding convolutionsame convolution</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>paddingstridestridefilterconvolutionstride=1filterstride=2</p>\n<p>image\\(n *n\\)convolution filter\\(f *f\\)padding\\(p\\)stride\\(s\\)image\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)convolution</p>\n<h2 id=\"Convolution-over-Volume\"><a href=\"#Convolution-over-Volume\" class=\"headerlink\" title=\"Convolution over Volume\"></a>Convolution over Volume</h2><p>RGBRGBchannelconvolution<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-1.png\" alt=\"\"><br>663hightwidthchannel(depth)filter3channel(depth)44</p>\n<p>filter<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-2.png\" alt=\"\"><br>filter333442channelfilterchannel</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>CNNlayer<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-3.png\" alt=\"\"><br>663RGB333filterimage\\(x\\)\\(a ^{[0]}\\)filter\\(w ^{[1]}\\)\\(w ^{[1]}a ^{[0]}\\)bias\\(b^{[1]}\\)liner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)non-liner functionReLU442outputCNNlayer.</p>\n<p>CNNDNNliner functionnon-liner functionnon-liner functionclassifydataCNNfilterfeaturefilterclassify data.</p>\n<p>fully connected DNNCNNparameters</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>Pooling<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/13/13-4.png\" alt=\"\"><br>max poolingfilter\\(f=2\\)stride\\(s=2\\)filtermaxaverageaverage poolingpoolinghyperparameterfilter\\(f\\)stride\\(s\\)poolingno parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layerCNNinputDNNfully connected</p>\n<p>prameterpoolinglayerCNNCONV-POOL-CONV-POOL-FC-FC-Softmax5CNNCNN</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>Ng</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) thats useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week2","date":"2017-10-18T13:28:12.000Z","_content":"Hi allcourse3week2learning strategyNg\n\n\n<!--more-->\n## Error analysis\n### Carry out error analysis\ntrainingdev setdev errortraining errorNgsolution\n\ncat recognitionsampledogcat\n* \n* \n* \n\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-1.png) \n\n### Clean up incorrectly labeled data\nlabellabeltraining\n\ntraining setincorrectly labeled dataNg\n\n> DL algorithms are quite robust to random errors in the training set\n\nDLrobusttraining setincorrectly labeled dataincorrectly labeled datasystematic errorscat recognition\n\ndev/test setincorrectly labeled dataincorrectly labeled datadev error\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-2.png) \nincorrectly labeled\nincorrectly labeled datadev errorfix it up.\n\ncorrecting incorrect dev/test set exampleNg\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\ndevtest setdistribution\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\nincorrect labeled \n\n> Tran and dev/test data may now come from slightly different distribution\n\nDLtrainingrobustincorrect labeled datatraining settraining set\n\n### Build up quickly and iterate\nNgspeech recognition\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\nNg\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\nguideline\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\ntraining/dev/test setdistributiontraining and testing on different distribution\n\napp10k200kdistribution\n\noption1210k205ktraining set2.5kdev2.5test set\n\ndev/test settargetappdata settask target\n\noption2200ktraining set10kapp5ktraining set2.5kdev2.5testdev/testtarget task targetoption2training/dev setdistribution\n### Bias & variance with mismatched data distribution\ntraining/dev/test setdistributiontraining errordev errorhigh variancetraining setdev setdistribution\n\ntraining settraining-dev settrainingtrainingsettraining errortraining-dev errordev errorerrorhuman errortraining errortraining-dev errorhigh biasvariancetraining-dev errordev errordata mismatch\n### Addressing data mismatch\naddressing data mismatchNgguideline\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\nanalysistraining setdev setdistributiontraining setdev set\n\noverfittingNgtraining setdev set1min1hoverfitting.\n## Transfer learning\ntransfer learningtransferABBApre-trainingneural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-3.png) \nneural networksimage recognitionoutputoutput\\\\(w\\\\)\\\\(b\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-4.png) \noutputoutputoutputtransferpre-trainingtransfer training\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\n444Multi-task learning.\n\n\\\\(y\\\\)m1m4multi-task\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learningDLlearningend2endfeature selectionimage processingDL\n\nend2end \nPros\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\nend2endend to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week2\ndate: 2017-10-18 21:28:12\ntags: \n\t- learning strategy\n\t- transfer learning\n\t- multi-task learning\t\ncategories: learning notes\n---\nHi allcourse3week2learning strategyNg\n\n\n<!--more-->\n## Error analysis\n### Carry out error analysis\ntrainingdev setdev errortraining errorNgsolution\n\ncat recognitionsampledogcat\n* \n* \n* \n\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-1.png) \n\n### Clean up incorrectly labeled data\nlabellabeltraining\n\ntraining setincorrectly labeled dataNg\n\n> DL algorithms are quite robust to random errors in the training set\n\nDLrobusttraining setincorrectly labeled dataincorrectly labeled datasystematic errorscat recognition\n\ndev/test setincorrectly labeled dataincorrectly labeled datadev error\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-2.png) \nincorrectly labeled\nincorrectly labeled datadev errorfix it up.\n\ncorrecting incorrect dev/test set exampleNg\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\ndevtest setdistribution\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\nincorrect labeled \n\n> Tran and dev/test data may now come from slightly different distribution\n\nDLtrainingrobustincorrect labeled datatraining settraining set\n\n### Build up quickly and iterate\nNgspeech recognition\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\nNg\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\nguideline\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\ntraining/dev/test setdistributiontraining and testing on different distribution\n\napp10k200kdistribution\n\noption1210k205ktraining set2.5kdev2.5test set\n\ndev/test settargetappdata settask target\n\noption2200ktraining set10kapp5ktraining set2.5kdev2.5testdev/testtarget task targetoption2training/dev setdistribution\n### Bias & variance with mismatched data distribution\ntraining/dev/test setdistributiontraining errordev errorhigh variancetraining setdev setdistribution\n\ntraining settraining-dev settrainingtrainingsettraining errortraining-dev errordev errorerrorhuman errortraining errortraining-dev errorhigh biasvariancetraining-dev errordev errordata mismatch\n### Addressing data mismatch\naddressing data mismatchNgguideline\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\nanalysistraining setdev setdistributiontraining setdev set\n\noverfittingNgtraining setdev set1min1hoverfitting.\n## Transfer learning\ntransfer learningtransferABBApre-trainingneural networks\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-3.png) \nneural networksimage recognitionoutputoutput\\\\(w\\\\)\\\\(b\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/10/10-4.png) \noutputoutputoutputtransferpre-trainingtransfer training\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\n444Multi-task learning.\n\n\\\\(y\\\\)m1m4multi-task\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learningDLlearningend2endfeature selectionimage processingDL\n\nend2end \nPros\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\nend2endend to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week2","published":1,"updated":"2017-10-22T09:55:22.354Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7s000swahqe5mjdav9","content":"<p>Hi allcourse3week2learning strategyNg</p>\n<p><br><a id=\"more\"></a></p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>trainingdev setdev errortraining errorNgsolution</p>\n<p>cat recognitionsampledogcat</p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-1.png\" alt=\"\"><br></p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>labellabeltraining</p>\n<p>training setincorrectly labeled dataNg</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DLrobusttraining setincorrectly labeled dataincorrectly labeled datasystematic errorscat recognition</p>\n<p>dev/test setincorrectly labeled dataincorrectly labeled datadev error<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-2.png\" alt=\"\"><br>incorrectly labeled<br>incorrectly labeled datadev errorfix it up.</p>\n<p>correcting incorrect dev/test set exampleNg</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>devtest setdistribution</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>incorrect labeled </p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>DLtrainingrobustincorrect labeled datatraining settraining set</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>Ngspeech recognition</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young childrens speech</li>\n</ul>\n<p><br>Ng</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>guideline</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>training/dev/test setdistributiontraining and testing on different distribution</p>\n<p>app10k200kdistribution</p>\n<p>option1210k205ktraining set2.5kdev2.5test set</p>\n<p>dev/test settargetappdata settask target</p>\n<p>option2200ktraining set10kapp5ktraining set2.5kdev2.5testdev/testtarget task targetoption2training/dev setdistribution</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>training/dev/test setdistributiontraining errordev errorhigh variancetraining setdev setdistribution</p>\n<p>training settraining-dev settrainingtrainingsettraining errortraining-dev errordev errorerrorhuman errortraining errortraining-dev errorhigh biasvariancetraining-dev errordev errordata mismatch</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>addressing data mismatchNgguideline</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>analysistraining setdev setdistributiontraining setdev set</p>\n<p>overfittingNgtraining setdev set1min1hoverfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>transfer learningtransferABBApre-trainingneural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-3.png\" alt=\"\"><br>neural networksimage recognitionoutputoutput\\(w\\)\\(b\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-4.png\" alt=\"\"><br>outputoutputoutputtransferpre-trainingtransfer training</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>444Multi-task learning.</p>\n<p>\\(y\\)m1m4multi-task</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learningDLlearningend2endfeature selectionimage processingDL</p>\n<p>end2end <br>Pros</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>end2endend to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Hi allcourse3week2learning strategyNg</p>\n<p><br>","more":"</p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>trainingdev setdev errortraining errorNgsolution</p>\n<p>cat recognitionsampledogcat</p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-1.png\" alt=\"\"><br></p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>labellabeltraining</p>\n<p>training setincorrectly labeled dataNg</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DLrobusttraining setincorrectly labeled dataincorrectly labeled datasystematic errorscat recognition</p>\n<p>dev/test setincorrectly labeled dataincorrectly labeled datadev error<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-2.png\" alt=\"\"><br>incorrectly labeled<br>incorrectly labeled datadev errorfix it up.</p>\n<p>correcting incorrect dev/test set exampleNg</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>devtest setdistribution</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>incorrect labeled </p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>DLtrainingrobustincorrect labeled datatraining settraining set</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>Ngspeech recognition</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young childrens speech</li>\n</ul>\n<p><br>Ng</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>guideline</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>training/dev/test setdistributiontraining and testing on different distribution</p>\n<p>app10k200kdistribution</p>\n<p>option1210k205ktraining set2.5kdev2.5test set</p>\n<p>dev/test settargetappdata settask target</p>\n<p>option2200ktraining set10kapp5ktraining set2.5kdev2.5testdev/testtarget task targetoption2training/dev setdistribution</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>training/dev/test setdistributiontraining errordev errorhigh variancetraining setdev setdistribution</p>\n<p>training settraining-dev settrainingtrainingsettraining errortraining-dev errordev errorerrorhuman errortraining errortraining-dev errorhigh biasvariancetraining-dev errordev errordata mismatch</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>addressing data mismatchNgguideline</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>analysistraining setdev setdistributiontraining setdev set</p>\n<p>overfittingNgtraining setdev set1min1hoverfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>transfer learningtransferABBApre-trainingneural networks<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-3.png\" alt=\"\"><br>neural networksimage recognitionoutputoutput\\(w\\)\\(b\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/10/10-4.png\" alt=\"\"><br>outputoutputoutputtransferpre-trainingtransfer training</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>444Multi-task learning.</p>\n<p>\\(y\\)m1m4multi-task</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learningDLlearningend2endfeature selectionimage processingDL</p>\n<p>end2end <br>Pros</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>end2endend to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week1","date":"2017-10-12T04:34:05.000Z","_content":"3deep learningstrategystrategy\n\nrecapweek1\n<!--more-->\n## Orthogonalization\nML taskorthogonalizationNg\n\northogonalizationNg4\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\n\n\ntraining set**high bias****under fitting**more complexbigger neural networkslonger training timetraining set.\n\ntraining setdevelopment setdevelopment set fit**high varience****over fitting**regularizationmore training data\n\ntest setdevelopment setdevelopment set\n\nreal worlddevelopment test setreal worlddata setreal world development test setreal world\n\northogonalization\n\n## Metric\n### Single number evaluation\nMetricML taskmetric\n\nmetricAprecisionBArecallBF1-scoresingle number evaluation metricmetrics\n### Satisficing   and optimizing\naccuracy\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-1.png) \naccuracyoptimizing metricaccuracyclassifier100ms running timesatisficing metric\\\\(N\\\\)metricsoptimizing metric\\\\(N-1\\\\)metrics satisficing metricsthreshold\n## Data set\n### Distributions\ntraining setdev settest setdatadata distributiontraining setdev setmetric.\n\ndatadatadistributionNgguideline\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\nML taskdataset\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-2.png) \nbig data\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-3.png) \nNgguideline\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \ndata setdistributiondevtest set metricreal worlddata setdev/test setreal worlddistribution\n### Change metric\nMLmetricmetric\n\nerror\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n\n## Improve model performance\nperformanceNg\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-4.png) \nhuman-levelBayes errorhuman-leveltraining errordev errorhuman-leveltraining errorbiasvariance\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week1\ndate: 2017-10-12 12:34:05\ntags: \n\t- learning strategy\n\t- orthogonalization\ncategories: learning notes\n---\n3deep learningstrategystrategy\n\nrecapweek1\n<!--more-->\n## Orthogonalization\nML taskorthogonalizationNg\n\northogonalizationNg4\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\n\n\ntraining set**high bias****under fitting**more complexbigger neural networkslonger training timetraining set.\n\ntraining setdevelopment setdevelopment set fit**high varience****over fitting**regularizationmore training data\n\ntest setdevelopment setdevelopment set\n\nreal worlddevelopment test setreal worlddata setreal world development test setreal world\n\northogonalization\n\n## Metric\n### Single number evaluation\nMetricML taskmetric\n\nmetricAprecisionBArecallBF1-scoresingle number evaluation metricmetrics\n### Satisficing   and optimizing\naccuracy\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-1.png) \naccuracyoptimizing metricaccuracyclassifier100ms running timesatisficing metric\\\\(N\\\\)metricsoptimizing metric\\\\(N-1\\\\)metrics satisficing metricsthreshold\n## Data set\n### Distributions\ntraining setdev settest setdatadata distributiontraining setdev setmetric.\n\ndatadatadistributionNgguideline\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\nML taskdataset\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-2.png) \nbig data\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-3.png) \nNgguideline\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \ndata setdistributiondevtest set metricreal worlddata setdev/test setreal worlddistribution\n### Change metric\nMLmetricmetric\n\nerror\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\n\n## Improve model performance\nperformanceNg\n![](http://otmy7guvn.bkt.clouddn.com/blog/9/9-4.png) \nhuman-levelBayes errorhuman-leveltraining errordev errorhuman-leveltraining errorbiasvariance\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning- Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week1","published":1,"updated":"2017-10-22T09:55:13.990Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7v000vwahq7ws5my2m","content":"<p>3deep learningstrategystrategy</p>\n<p>recapweek1<br><a id=\"more\"></a></p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>ML taskorthogonalizationNg</p>\n<p>orthogonalizationNg4</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p></p>\n<p>training set<strong>high bias</strong><strong>under fitting</strong>more complexbigger neural networkslonger training timetraining set.</p>\n<p>training setdevelopment setdevelopment set fit<strong>high varience</strong><strong>over fitting</strong>regularizationmore training data</p>\n<p>test setdevelopment setdevelopment set</p>\n<p>real worlddevelopment test setreal worlddata setreal world development test setreal world</p>\n<p>orthogonalization</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>MetricML taskmetric</p>\n<p>metricAprecisionBArecallBF1-scoresingle number evaluation metricmetrics</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>accuracy<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-1.png\" alt=\"\"><br>accuracyoptimizing metricaccuracyclassifier100ms running timesatisficing metric\\(N\\)metricsoptimizing metric\\(N-1\\)metrics satisficing metricsthreshold</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>training setdev settest setdatadata distributiontraining setdev setmetric.</p>\n<p>datadatadistributionNgguideline</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>ML taskdataset<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-2.png\" alt=\"\"><br>big data<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-3.png\" alt=\"\"><br>Ngguideline</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>data setdistributiondevtest set metricreal worlddata setdev/test setreal worlddistribution</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>MLmetricmetric</p>\n<p>error</p>\n<script type=\"math/tex; mode=display\">Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}</script><p></p>\n<script type=\"math/tex; mode=display\">w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.</script><script type=\"math/tex; mode=display\">Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}</script><p></p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>performanceNg<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-4.png\" alt=\"\"><br>human-levelBayes errorhuman-leveltraining errordev errorhuman-leveltraining errorbiasvariance</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>3deep learningstrategystrategy</p>\n<p>recapweek1<br>","more":"</p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>ML taskorthogonalizationNg</p>\n<p>orthogonalizationNg4</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p></p>\n<p>training set<strong>high bias</strong><strong>under fitting</strong>more complexbigger neural networkslonger training timetraining set.</p>\n<p>training setdevelopment setdevelopment set fit<strong>high varience</strong><strong>over fitting</strong>regularizationmore training data</p>\n<p>test setdevelopment setdevelopment set</p>\n<p>real worlddevelopment test setreal worlddata setreal world development test setreal world</p>\n<p>orthogonalization</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>MetricML taskmetric</p>\n<p>metricAprecisionBArecallBF1-scoresingle number evaluation metricmetrics</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>accuracy<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-1.png\" alt=\"\"><br>accuracyoptimizing metricaccuracyclassifier100ms running timesatisficing metric\\(N\\)metricsoptimizing metric\\(N-1\\)metrics satisficing metricsthreshold</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>training setdev settest setdatadata distributiontraining setdev setmetric.</p>\n<p>datadatadistributionNgguideline</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>ML taskdataset<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-2.png\" alt=\"\"><br>big data<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-3.png\" alt=\"\"><br>Ngguideline</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>data setdistributiondevtest set metricreal worlddata setdev/test setreal worlddistribution</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>MLmetricmetric</p>\n<p>error</p>\n<script type=\"math/tex; mode=display\">Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}</script><p></p>\n<script type=\"math/tex; mode=display\">w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.</script><script type=\"math/tex; mode=display\">Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}</script><p></p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>performanceNg<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/9/9-4.png\" alt=\"\"><br>human-levelBayes errorhuman-leveltraining errordev errorhuman-leveltraining errorbiasvariance</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"external\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"external\">Deep learning- Andrew Ng</a></li>\n</ul>"},{"title":"","date":"2017-08-02T07:53:55.000Z","_content":"logistic regression\n* logistic regressiongradient descent\n* logistic regressionnewton's method\n* Newton's methodHessian matrixpositive definitelog cost function\n\nblog\n<!--more-->\n\n## (Convex function)\namazing**convex function****concave function**.\n\nOKconvex function\n\n \\\\(f(x)\\\\) \\\\(a\\\\)\\\\(b\\\\) \\\\( 0 \\leq \\theta \\leq 1\\\\)convex function\n$$f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png) \n\n**strictly convex function**.\n\nConvex function\n\n### First order condition\n function \\\\(f\\\\)****\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n \\\\(f\\\\)convex function**** \\\\(x\\\\)  \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png) \n\nconvex function \\\\(f\\\\)\n\n### Second order condition\n function \\\\(f\\\\)**** \\\\(n\\\\) Hessian matrix\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\nHessian matrix positive semi-defnite\\\\(f\\\\) convex function****reference\n\n---\n \\\\(\\Bbb R\\\\) convex function\n* \\\\(f(x) = ax+b\\\\)\n* \\\\(f(x)=e^ {ax}\\\\)\n*  \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\nconcave function\n* \\\\(f(x) = ax+b\\\\)\n*  \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\nconvexconcavefirst order condition\n\nconvex functionconvex function. machine learningconvex functionoptimizeconvex functionliner regressionlogistic regressionconvex functiongradient descentnewton's method.\n\n## (Gradient descent)\ngradient descentliner regressionLiner regressionleast square methodcost function\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\nliner regression\\\\( J( \\theta) \\\\) **\\\\( \\theta\\\\)**minimumgradient descentgradient descentreference. \n\ncost functiongradient descent\n\n\\\\(J( \\theta)\\\\)convex function\\\\( \\theta\\\\)least square cost functionconvex function.\n\nleast square cost functionconvex functiongradient descentgradient descent\n\n> Gradien descentobjective functionconvex function\n\nobject functionnon-convex functionobjective functionconvex functiongradient descent\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png) \n\ngradien descentstochastic gradient descentnon-convex optimization\n\ngradient descentminimize liner regressionconvex optimization\n\n## (Newton's method)\nNewton's method logistic regressionlog cost function**label-1+1**cost function0,1\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n-1+1\\\\(J( \\omega)\\\\)logistic regressionSVM\n\nlikehood function \\\\(- \\frac{1}{m}\\\\) \\\\(J( \\omega)\\\\)convex functionconvex function\\\\(J( \\omega)\\\\)\\\\( \\omega\\\\)\n\nlog cost functionconvexconcaveconvex function\n\nOKlog cost functionconvex functiongradient descentnewton's method\n\nNewton's methodreferencelog cost functionconvex functionsecond order conditionHessian matrixpositive semi-definiteL2 regularizer**L2 regularizerstrict convex function**log cost functionstrict convex function\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\nlog cost function**Hessian matrixpositive definite**newton's method newton's methodlog cost function\n## Sum up\nOKblog\n* Machine learningobjective functiongradient descent\n* Gradient descentnewton's methodconvex optimizationconvex function\n* non-convex optimizationstochastic gradient descent\n\n\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLIblog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","source":"_posts/ml-convex-opt.md","raw":"---\ntitle: \ndate: 2017-08-02 15:53:55\ntags: \n    - convex optimization\n    - gradient descent\n    - newton's method\ncategories: machine learning\n---\nlogistic regression\n* logistic regressiongradient descent\n* logistic regressionnewton's method\n* Newton's methodHessian matrixpositive definitelog cost function\n\nblog\n<!--more-->\n\n## (Convex function)\namazing**convex function****concave function**.\n\nOKconvex function\n\n \\\\(f(x)\\\\) \\\\(a\\\\)\\\\(b\\\\) \\\\( 0 \\leq \\theta \\leq 1\\\\)convex function\n$$f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)$$\n\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png) \n\n**strictly convex function**.\n\nConvex function\n\n### First order condition\n function \\\\(f\\\\)****\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\n \\\\(f\\\\)convex function**** \\\\(x\\\\)  \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOK\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png) \n\nconvex function \\\\(f\\\\)\n\n### Second order condition\n function \\\\(f\\\\)**** \\\\(n\\\\) Hessian matrix\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\nHessian matrix positive semi-defnite\\\\(f\\\\) convex function****reference\n\n---\n \\\\(\\Bbb R\\\\) convex function\n* \\\\(f(x) = ax+b\\\\)\n* \\\\(f(x)=e^ {ax}\\\\)\n*  \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\nconcave function\n* \\\\(f(x) = ax+b\\\\)\n*  \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\nconvexconcavefirst order condition\n\nconvex functionconvex function. machine learningconvex functionoptimizeconvex functionliner regressionlogistic regressionconvex functiongradient descentnewton's method.\n\n## (Gradient descent)\ngradient descentliner regressionLiner regressionleast square methodcost function\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\nliner regression\\\\( J( \\theta) \\\\) **\\\\( \\theta\\\\)**minimumgradient descentgradient descentreference. \n\ncost functiongradient descent\n\n\\\\(J( \\theta)\\\\)convex function\\\\( \\theta\\\\)least square cost functionconvex function.\n\nleast square cost functionconvex functiongradient descentgradient descent\n\n> Gradien descentobjective functionconvex function\n\nobject functionnon-convex functionobjective functionconvex functiongradient descent\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png) \n\ngradien descentstochastic gradient descentnon-convex optimization\n\ngradient descentminimize liner regressionconvex optimization\n\n## (Newton's method)\nNewton's method logistic regressionlog cost function**label-1+1**cost function0,1\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\n-1+1\\\\(J( \\omega)\\\\)logistic regressionSVM\n\nlikehood function \\\\(- \\frac{1}{m}\\\\) \\\\(J( \\omega)\\\\)convex functionconvex function\\\\(J( \\omega)\\\\)\\\\( \\omega\\\\)\n\nlog cost functionconvexconcaveconvex function\n\nOKlog cost functionconvex functiongradient descentnewton's method\n\nNewton's methodreferencelog cost functionconvex functionsecond order conditionHessian matrixpositive semi-definiteL2 regularizer**L2 regularizerstrict convex function**log cost functionstrict convex function\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\nlog cost function**Hessian matrixpositive definite**newton's method newton's methodlog cost function\n## Sum up\nOKblog\n* Machine learningobjective functiongradient descent\n* Gradient descentnewton's methodconvex optimizationconvex function\n* non-convex optimizationstochastic gradient descent\n\n\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLIblog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","slug":"ml-convex-opt","published":1,"updated":"2017-09-02T11:17:42.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7x000xwahqfupeb2un","content":"<p>logistic regression</p>\n<ul>\n<li>logistic regressiongradient descent</li>\n<li>logistic regressionnewtons method</li>\n<li>Newtons methodHessian matrixpositive definitelog cost function</li>\n</ul>\n<p>blog<br><a id=\"more\"></a></p>\n<h2 id=\"-Convex-function\"><a href=\"#-Convex-function\" class=\"headerlink\" title=\"(Convex function)\"></a>(Convex function)</h2><p>amazing<strong>convex function</strong><strong>concave function</strong>.</p>\n<p>OKconvex function</p>\n<p> \\(f(x)\\) \\(a\\)\\(b\\) \\( 0 \\leq \\theta \\leq 1\\)convex function</p>\n<script type=\"math/tex; mode=display\">f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)</script><p></p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png\" alt=\"\"> </p>\n<p><strong>strictly convex function</strong>.</p>\n<p>Convex function</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p> function \\(f\\)<strong></strong></p>\n<script type=\"math/tex; mode=display\">\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})</script><p> \\(f\\)convex function<strong></strong> \\(x\\)  \\(y\\)</p>\n<script type=\"math/tex; mode=display\">f(y) \\geq f(x) + \\nabla f(x)^T (y - x)</script><p>OK</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png\" alt=\"\"> </p>\n<p>convex function \\(f\\)</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p> function \\(f\\)<strong></strong> \\(n\\) Hessian matrix</p>\n<script type=\"math/tex; mode=display\">\\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n</script><p>Hessian matrix positive semi-defnite\\(f\\) convex function<strong></strong>reference</p>\n<hr>\n<p> \\(\\Bbb R\\) convex function</p>\n<ul>\n<li>\\(f(x) = ax+b\\)</li>\n<li>\\(f(x)=e^ {ax}\\)</li>\n<li> \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>concave function</p>\n<ul>\n<li>\\(f(x) = ax+b\\)</li>\n<li> \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>convexconcavefirst order condition</p>\n<p>convex functionconvex function. machine learningconvex functionoptimizeconvex functionliner regressionlogistic regressionconvex functiongradient descentnewtons method.</p>\n<h2 id=\"-Gradient-descent\"><a href=\"#-Gradient-descent\" class=\"headerlink\" title=\"(Gradient descent)\"></a>(Gradient descent)</h2><p>gradient descentliner regressionLiner regressionleast square methodcost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx</script><p>liner regression\\( J( \\theta) \\) <strong>\\( \\theta\\)</strong>minimumgradient descentgradient descentreference. </p>\n<p>cost functiongradient descent</p>\n<p>\\(J( \\theta)\\)convex function\\( \\theta\\)least square cost functionconvex function.</p>\n<p>least square cost functionconvex functiongradient descentgradient descent</p>\n<blockquote>\n<p>Gradien descentobjective functionconvex function</p>\n</blockquote>\n<p>object functionnon-convex functionobjective functionconvex functiongradient descent</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png\" alt=\"\"> </p>\n<p>gradien descentstochastic gradient descentnon-convex optimization</p>\n<p>gradient descentminimize liner regressionconvex optimization</p>\n<h2 id=\"-Newtons-method\"><a href=\"#-Newtons-method\" class=\"headerlink\" title=\"(Newtons method)\"></a>(Newtons method)</h2><p>Newtons method logistic regressionlog cost function<strong>label-1+1</strong>cost function0,1</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})</script><p>-1+1\\(J( \\omega)\\)logistic regressionSVM</p>\n<p>likehood function \\(- \\frac{1}{m}\\) \\(J( \\omega)\\)convex functionconvex function\\(J( \\omega)\\)\\( \\omega\\)</p>\n<p>log cost functionconvexconcaveconvex function</p>\n<p>OKlog cost functionconvex functiongradient descentnewtons method</p>\n<p>Newtons methodreferencelog cost functionconvex functionsecond order conditionHessian matrixpositive semi-definiteL2 regularizer<strong>L2 regularizerstrict convex function</strong>log cost functionstrict convex function</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2</script><p>log cost function<strong>Hessian matrixpositive definite</strong>newtons method newtons methodlog cost function</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OKblog</p>\n<ul>\n<li>Machine learningobjective functiongradient descent</li>\n<li>Gradient descentnewtons methodconvex optimizationconvex function</li>\n<li>non-convex optimizationstochastic gradient descent</li>\n</ul>\n<p></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"external\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"external\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"external\">XinyiLIblog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"external\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"external\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"external\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"external\">Newtons method</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>logistic regression</p>\n<ul>\n<li>logistic regressiongradient descent</li>\n<li>logistic regressionnewtons method</li>\n<li>Newtons methodHessian matrixpositive definitelog cost function</li>\n</ul>\n<p>blog<br>","more":"</p>\n<h2 id=\"-Convex-function\"><a href=\"#-Convex-function\" class=\"headerlink\" title=\"(Convex function)\"></a>(Convex function)</h2><p>amazing<strong>convex function</strong><strong>concave function</strong>.</p>\n<p>OKconvex function</p>\n<p> \\(f(x)\\) \\(a\\)\\(b\\) \\( 0 \\leq \\theta \\leq 1\\)convex function</p>\n<script type=\"math/tex; mode=display\">f(\\theta a+(1-\\theta b)) \\leq \\theta f(a) + (1- \\theta)f(b)</script><p></p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-1.png\" alt=\"\"> </p>\n<p><strong>strictly convex function</strong>.</p>\n<p>Convex function</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p> function \\(f\\)<strong></strong></p>\n<script type=\"math/tex; mode=display\">\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})</script><p> \\(f\\)convex function<strong></strong> \\(x\\)  \\(y\\)</p>\n<script type=\"math/tex; mode=display\">f(y) \\geq f(x) + \\nabla f(x)^T (y - x)</script><p>OK</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-2.png\" alt=\"\"> </p>\n<p>convex function \\(f\\)</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p> function \\(f\\)<strong></strong> \\(n\\) Hessian matrix</p>\n<script type=\"math/tex; mode=display\">\\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n</script><p>Hessian matrix positive semi-defnite\\(f\\) convex function<strong></strong>reference</p>\n<hr>\n<p> \\(\\Bbb R\\) convex function</p>\n<ul>\n<li>\\(f(x) = ax+b\\)</li>\n<li>\\(f(x)=e^ {ax}\\)</li>\n<li> \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>concave function</p>\n<ul>\n<li>\\(f(x) = ax+b\\)</li>\n<li> \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>convexconcavefirst order condition</p>\n<p>convex functionconvex function. machine learningconvex functionoptimizeconvex functionliner regressionlogistic regressionconvex functiongradient descentnewtons method.</p>\n<h2 id=\"-Gradient-descent\"><a href=\"#-Gradient-descent\" class=\"headerlink\" title=\"(Gradient descent)\"></a>(Gradient descent)</h2><p>gradient descentliner regressionLiner regressionleast square methodcost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx</script><p>liner regression\\( J( \\theta) \\) <strong>\\( \\theta\\)</strong>minimumgradient descentgradient descentreference. </p>\n<p>cost functiongradient descent</p>\n<p>\\(J( \\theta)\\)convex function\\( \\theta\\)least square cost functionconvex function.</p>\n<p>least square cost functionconvex functiongradient descentgradient descent</p>\n<blockquote>\n<p>Gradien descentobjective functionconvex function</p>\n</blockquote>\n<p>object functionnon-convex functionobjective functionconvex functiongradient descent</p>\n<p><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/1/1-3.png\" alt=\"\"> </p>\n<p>gradien descentstochastic gradient descentnon-convex optimization</p>\n<p>gradient descentminimize liner regressionconvex optimization</p>\n<h2 id=\"-Newtons-method\"><a href=\"#-Newtons-method\" class=\"headerlink\" title=\"(Newtons method)\"></a>(Newtons method)</h2><p>Newtons method logistic regressionlog cost function<strong>label-1+1</strong>cost function0,1</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})</script><p>-1+1\\(J( \\omega)\\)logistic regressionSVM</p>\n<p>likehood function \\(- \\frac{1}{m}\\) \\(J( \\omega)\\)convex functionconvex function\\(J( \\omega)\\)\\( \\omega\\)</p>\n<p>log cost functionconvexconcaveconvex function</p>\n<p>OKlog cost functionconvex functiongradient descentnewtons method</p>\n<p>Newtons methodreferencelog cost functionconvex functionsecond order conditionHessian matrixpositive semi-definiteL2 regularizer<strong>L2 regularizerstrict convex function</strong>log cost functionstrict convex function</p>\n<script type=\"math/tex; mode=display\">J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2</script><p>log cost function<strong>Hessian matrixpositive definite</strong>newtons method newtons methodlog cost function</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OKblog</p>\n<ul>\n<li>Machine learningobjective functiongradient descent</li>\n<li>Gradient descentnewtons methodconvex optimizationconvex function</li>\n<li>non-convex optimizationstochastic gradient descent</li>\n</ul>\n<p></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"external\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"external\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"external\">XinyiLIblog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"external\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"external\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"external\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"external\">Newtons method</a></li>\n</ul>"},{"title":"","date":"2017-08-11T09:26:56.000Z","_content":"gradient descentnewton's methodgradient descentnewton's methodconvex optimizationconvex optimizationunconstrained optimization\n<!--more-->\n\noptimization taskobjective function \\\\(f(x)\\\\)\n* \\\\(f(x)\\\\)\\\\( \\nabla f(x)=0\\\\)**newton's method**\n* \\\\(f(x)\\\\)**gradient descent**\n\nOK\n\n## (Taylor series)\nTaylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\(n+1\\\\)\\\\( f(x)\\\\)\\\\(n\\\\)Taylor series\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\nTaylor series\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\nTaylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\( x_0\\\\)derivativesfunctionfunction\\\\(f(x)\\\\)Taylor series\\\\(f(x)\\\\)0Taylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\(x_0\\\\)\n\n1st order Taylor seriesgradient descent2nd order Taylor seriesnewton's method\n\nOK\n\n## 1st order Taylor series & gradient descent\n\\\\(x_k\\\\)kgradient descent\\\\(x\\\\)1st order Taylor series \n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n\\\\(x\\\\)gradient descent\\\\(f(x)\\\\)\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)**\\\\((x-x_k)\\\\)\\\\(\\vec g\\\\)\\\\( \\alpha\\\\)\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)} \\vec g)$$\n****\n\n>\n\n\\\\(\\vec g\\\\)\\\\( \\vec{\\nabla f(x_k)}\\\\)\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\ngradient descentyeah mateWe make it!\n## 2nd order Taylor series & newton's method\ngradient descent\\\\(x_k\\\\)\\\\(k\\\\)newton's method\\\\(x\\\\)2nd order Taylor series \n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n\\\\(x\\\\)\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\nnewton's method\\\\(\\nabla f(x)=0\\\\)**\\\\(x\\\\)newton's method\\\\(k+1\\\\)\\\\(x\\\\)**\\\\(\\nabla f(x_k)\\\\)\\\\(x_k\\\\)\\\\( \\nabla^2 f(x_k)\\\\)\\\\(x_k\\\\)Hessian\n\n\\\\(\\nabla f(x_k)=g\\\\)\\\\(\\nabla^2 f(x_k)=H\\\\)\n$$g+H(x-x_k)=0$$\n\n$$x=x_k-H^{-1}g$$\n\\\\(-g H^{-1} \\\\) \\\\(g\\\\)\\\\( g^T H^{-1} g > 0\\\\)positive definite**Hessianpositive definite**\n\nHessiannegative definite\\\\(g\\\\)newton's methodnewton's methodobjective functionnon-convex function\\\\(k\\\\)\\\\(x_k\\\\)Hessian matrix negative definitenewton's methodBFGS\n## Sum up\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png) \n\\\\(k\\\\)\\\\(x_k\\\\)objective function\\\\(x_k\\\\)Taylor series \\\\(f(x)\\\\)gradient descentfunctionnewton's methodfunction\n\nnew's method\\\\( \\nabla f(x)=0\\\\)\\\\(x \\_{k+1}\\\\)\\\\(x \\_{k+1}\\\\)function\n\nNewton's methodfunctionnewton's methodobjective function value(value)value(value)gradient descentfunction valuenewton's methodobjective functionnewton's method\n\nfunctionconvex functionunconstrained optimizationnon-convex optimizationnon-convex optimizationnewton's methodHessian matrixpositive definite\\\\(\\lambda\\\\)Hessian matrix negative definite\n$$x:=x- \\lambda H^{-1} g$$\nnon-convex functionconvex functionnewton's methodnewton's mtehodnon-convex\n\ngradient descentfunctiongradient descentnon-convex optimizationgradient descent\n\n\n* Gradient descent  newton's methodTaylor seriesobjective function\n* Gradient descent function newton's methodfunctionnewton's method\n* Newton's methodHessian matrixfeature\n* Newton's methodnon-convex optimizationgradient descent\n\n\n\n****\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","source":"_posts/ml-gd-and-nm.md","raw":"---\ntitle: \ndate: 2017-08-11 17:26:56\ntags: \n\t- unconstrained optimization\n\t- gradient descent\n\t- newton's method\ncategories: machine learning\n---\ngradient descentnewton's methodgradient descentnewton's methodconvex optimizationconvex optimizationunconstrained optimization\n<!--more-->\n\noptimization taskobjective function \\\\(f(x)\\\\)\n* \\\\(f(x)\\\\)\\\\( \\nabla f(x)=0\\\\)**newton's method**\n* \\\\(f(x)\\\\)**gradient descent**\n\nOK\n\n## (Taylor series)\nTaylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\(n+1\\\\)\\\\( f(x)\\\\)\\\\(n\\\\)Taylor series\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\nTaylor series\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\nTaylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\( x_0\\\\)derivativesfunctionfunction\\\\(f(x)\\\\)Taylor series\\\\(f(x)\\\\)0Taylor series\\\\( f(x)\\\\)\\\\( x_0\\\\)\\\\(x_0\\\\)\n\n1st order Taylor seriesgradient descent2nd order Taylor seriesnewton's method\n\nOK\n\n## 1st order Taylor series & gradient descent\n\\\\(x_k\\\\)kgradient descent\\\\(x\\\\)1st order Taylor series \n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\n\\\\(x\\\\)gradient descent\\\\(f(x)\\\\)\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\n**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)**\\\\((x-x_k)\\\\)\\\\(\\vec g\\\\)\\\\( \\alpha\\\\)\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)} \\vec g)$$\n****\n\n>\n\n\\\\(\\vec g\\\\)\\\\( \\vec{\\nabla f(x_k)}\\\\)\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\ngradient descentyeah mateWe make it!\n## 2nd order Taylor series & newton's method\ngradient descent\\\\(x_k\\\\)\\\\(k\\\\)newton's method\\\\(x\\\\)2nd order Taylor series \n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\n\\\\(x\\\\)\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\nnewton's method\\\\(\\nabla f(x)=0\\\\)**\\\\(x\\\\)newton's method\\\\(k+1\\\\)\\\\(x\\\\)**\\\\(\\nabla f(x_k)\\\\)\\\\(x_k\\\\)\\\\( \\nabla^2 f(x_k)\\\\)\\\\(x_k\\\\)Hessian\n\n\\\\(\\nabla f(x_k)=g\\\\)\\\\(\\nabla^2 f(x_k)=H\\\\)\n$$g+H(x-x_k)=0$$\n\n$$x=x_k-H^{-1}g$$\n\\\\(-g H^{-1} \\\\) \\\\(g\\\\)\\\\( g^T H^{-1} g > 0\\\\)positive definite**Hessianpositive definite**\n\nHessiannegative definite\\\\(g\\\\)newton's methodnewton's methodobjective functionnon-convex function\\\\(k\\\\)\\\\(x_k\\\\)Hessian matrix negative definitenewton's methodBFGS\n## Sum up\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png) \n\\\\(k\\\\)\\\\(x_k\\\\)objective function\\\\(x_k\\\\)Taylor series \\\\(f(x)\\\\)gradient descentfunctionnewton's methodfunction\n\nnew's method\\\\( \\nabla f(x)=0\\\\)\\\\(x \\_{k+1}\\\\)\\\\(x \\_{k+1}\\\\)function\n\nNewton's methodfunctionnewton's methodobjective function value(value)value(value)gradient descentfunction valuenewton's methodobjective functionnewton's method\n\nfunctionconvex functionunconstrained optimizationnon-convex optimizationnon-convex optimizationnewton's methodHessian matrixpositive definite\\\\(\\lambda\\\\)Hessian matrix negative definite\n$$x:=x- \\lambda H^{-1} g$$\nnon-convex functionconvex functionnewton's methodnewton's mtehodnon-convex\n\ngradient descentfunctiongradient descentnon-convex optimizationgradient descent\n\n\n* Gradient descent  newton's methodTaylor seriesobjective function\n* Gradient descent function newton's methodfunctionnewton's method\n* Newton's methodHessian matrixfeature\n* Newton's methodnon-convex optimizationgradient descent\n\n\n\n****\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","slug":"ml-gd-and-nm","published":1,"updated":"2017-09-27T13:33:44.063Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe7z0011wahqc7mz38b3","content":"<p>gradient descentnewtons methodgradient descentnewtons methodconvex optimizationconvex optimizationunconstrained optimization<br><a id=\"more\"></a></p>\n<p>optimization taskobjective function \\(f(x)\\)</p>\n<ul>\n<li>\\(f(x)\\)\\( \\nabla f(x)=0\\)<strong>newtons method</strong></li>\n<li>\\(f(x)\\)<strong>gradient descent</strong></li>\n</ul>\n<p>OK</p>\n<h2 id=\"-Taylor-series\"><a href=\"#-Taylor-series\" class=\"headerlink\" title=\"(Taylor series)\"></a>(Taylor series)</h2><p>Taylor series\\( f(x)\\)\\( x_0\\)\\(n+1\\)\\( f(x)\\)\\(n\\)Taylor series</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n</script><p>Taylor series</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point.</p>\n</blockquote>\n<p>Taylor series\\( f(x)\\)\\( x_0\\)\\( x_0\\)derivativesfunctionfunction\\(f(x)\\)Taylor series\\(f(x)\\)0Taylor series\\( f(x)\\)\\( x_0\\)\\(x_0\\)</p>\n<p>1st order Taylor seriesgradient descent2nd order Taylor seriesnewtons method</p>\n<p>OK</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>\\(x_k\\)kgradient descent\\(x\\)1st order Taylor series </p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)</script><p>\\(x\\)gradient descent\\(f(x)\\)</p>\n<script type=\"math/tex; mode=display\">f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)</script><p><strong>\\(- \\nabla f(x_k)(x-x_k)\\)</strong>\\((x-x_k)\\)\\(\\vec g\\)\\( \\alpha\\)</p>\n<script type=\"math/tex; mode=display\">\\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)} \\vec g)</script><p><strong></strong></p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>\\(\\vec g\\)\\( \\vec{\\nabla f(x_k)}\\)</p>\n<script type=\"math/tex; mode=display\">x-x_k=- \\alpha \\nabla f(x_k)</script><script type=\"math/tex; mode=display\">x:=x_k- \\alpha \\nabla f(x_k)</script><p>gradient descentyeah mateWe make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newtons-method\"><a href=\"#2nd-order-Taylor-series-amp-newtons-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newtons method\"></a>2nd order Taylor series &amp; newtons method</h2><p>gradient descent\\(x_k\\)\\(k\\)newtons method\\(x\\)2nd order Taylor series </p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k)</script><p>\\(x\\)</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0</script><p>newtons method\\(\\nabla f(x)=0\\)<strong>\\(x\\)newtons method\\(k+1\\)\\(x\\)</strong>\\(\\nabla f(x_k)\\)\\(x_k\\)\\( \\nabla^2 f(x_k)\\)\\(x_k\\)Hessian</p>\n<p>\\(\\nabla f(x_k)=g\\)\\(\\nabla^2 f(x_k)=H\\)</p>\n<script type=\"math/tex; mode=display\">g+H(x-x_k)=0</script><p></p>\n<script type=\"math/tex; mode=display\">x=x_k-H^{-1}g</script><p>\\(-g H^{-1} \\) \\(g\\)\\( g^T H^{-1} g &gt; 0\\)positive definite<strong>Hessianpositive definite</strong></p>\n<p>Hessiannegative definite\\(g\\)newtons methodnewtons methodobjective functionnon-convex function\\(k\\)\\(x_k\\)Hessian matrix negative definitenewtons methodBFGS</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png\" alt=\"\"><br>\\(k\\)\\(x_k\\)objective function\\(x_k\\)Taylor series \\(f(x)\\)gradient descentfunctionnewtons methodfunction</p>\n<p>news method\\( \\nabla f(x)=0\\)\\(x _{k+1}\\)\\(x _{k+1}\\)function</p>\n<p>Newtons methodfunctionnewtons methodobjective function value(value)value(value)gradient descentfunction valuenewtons methodobjective functionnewtons method</p>\n<p>functionconvex functionunconstrained optimizationnon-convex optimizationnon-convex optimizationnewtons methodHessian matrixpositive definite\\(\\lambda\\)Hessian matrix negative definite</p>\n<script type=\"math/tex; mode=display\">x:=x- \\lambda H^{-1} g</script><p>non-convex functionconvex functionnewtons methodnewtons mtehodnon-convex</p>\n<p>gradient descentfunctiongradient descentnon-convex optimizationgradient descent</p>\n<p></p>\n<ul>\n<li>Gradient descent  newtons methodTaylor seriesobjective function</li>\n<li>Gradient descent function newtons methodfunctionnewtons method</li>\n<li>Newtons methodHessian matrixfeature</li>\n<li>Newtons methodnon-convex optimizationgradient descent</li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"external\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"external\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">Taylor series</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>gradient descentnewtons methodgradient descentnewtons methodconvex optimizationconvex optimizationunconstrained optimization<br>","more":"</p>\n<p>optimization taskobjective function \\(f(x)\\)</p>\n<ul>\n<li>\\(f(x)\\)\\( \\nabla f(x)=0\\)<strong>newtons method</strong></li>\n<li>\\(f(x)\\)<strong>gradient descent</strong></li>\n</ul>\n<p>OK</p>\n<h2 id=\"-Taylor-series\"><a href=\"#-Taylor-series\" class=\"headerlink\" title=\"(Taylor series)\"></a>(Taylor series)</h2><p>Taylor series\\( f(x)\\)\\( x_0\\)\\(n+1\\)\\( f(x)\\)\\(n\\)Taylor series</p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n</script><p>Taylor series</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point.</p>\n</blockquote>\n<p>Taylor series\\( f(x)\\)\\( x_0\\)\\( x_0\\)derivativesfunctionfunction\\(f(x)\\)Taylor series\\(f(x)\\)0Taylor series\\( f(x)\\)\\( x_0\\)\\(x_0\\)</p>\n<p>1st order Taylor seriesgradient descent2nd order Taylor seriesnewtons method</p>\n<p>OK</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>\\(x_k\\)kgradient descent\\(x\\)1st order Taylor series </p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)</script><p>\\(x\\)gradient descent\\(f(x)\\)</p>\n<script type=\"math/tex; mode=display\">f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)</script><p><strong>\\(- \\nabla f(x_k)(x-x_k)\\)</strong>\\((x-x_k)\\)\\(\\vec g\\)\\( \\alpha\\)</p>\n<script type=\"math/tex; mode=display\">\\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)} \\vec g)</script><p><strong></strong></p>\n<blockquote>\n<p></p>\n</blockquote>\n<p>\\(\\vec g\\)\\( \\vec{\\nabla f(x_k)}\\)</p>\n<script type=\"math/tex; mode=display\">x-x_k=- \\alpha \\nabla f(x_k)</script><script type=\"math/tex; mode=display\">x:=x_k- \\alpha \\nabla f(x_k)</script><p>gradient descentyeah mateWe make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newtons-method\"><a href=\"#2nd-order-Taylor-series-amp-newtons-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newtons method\"></a>2nd order Taylor series &amp; newtons method</h2><p>gradient descent\\(x_k\\)\\(k\\)newtons method\\(x\\)2nd order Taylor series </p>\n<script type=\"math/tex; mode=display\">f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k)</script><p>\\(x\\)</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0</script><p>newtons method\\(\\nabla f(x)=0\\)<strong>\\(x\\)newtons method\\(k+1\\)\\(x\\)</strong>\\(\\nabla f(x_k)\\)\\(x_k\\)\\( \\nabla^2 f(x_k)\\)\\(x_k\\)Hessian</p>\n<p>\\(\\nabla f(x_k)=g\\)\\(\\nabla^2 f(x_k)=H\\)</p>\n<script type=\"math/tex; mode=display\">g+H(x-x_k)=0</script><p></p>\n<script type=\"math/tex; mode=display\">x=x_k-H^{-1}g</script><p>\\(-g H^{-1} \\) \\(g\\)\\( g^T H^{-1} g &gt; 0\\)positive definite<strong>Hessianpositive definite</strong></p>\n<p>Hessiannegative definite\\(g\\)newtons methodnewtons methodobjective functionnon-convex function\\(k\\)\\(x_k\\)Hessian matrix negative definitenewtons methodBFGS</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/2/2-1.png\" alt=\"\"><br>\\(k\\)\\(x_k\\)objective function\\(x_k\\)Taylor series \\(f(x)\\)gradient descentfunctionnewtons methodfunction</p>\n<p>news method\\( \\nabla f(x)=0\\)\\(x _{k+1}\\)\\(x _{k+1}\\)function</p>\n<p>Newtons methodfunctionnewtons methodobjective function value(value)value(value)gradient descentfunction valuenewtons methodobjective functionnewtons method</p>\n<p>functionconvex functionunconstrained optimizationnon-convex optimizationnon-convex optimizationnewtons methodHessian matrixpositive definite\\(\\lambda\\)Hessian matrix negative definite</p>\n<script type=\"math/tex; mode=display\">x:=x- \\lambda H^{-1} g</script><p>non-convex functionconvex functionnewtons methodnewtons mtehodnon-convex</p>\n<p>gradient descentfunctiongradient descentnon-convex optimizationgradient descent</p>\n<p></p>\n<ul>\n<li>Gradient descent  newtons methodTaylor seriesobjective function</li>\n<li>Gradient descent function newtons methodfunctionnewtons method</li>\n<li>Newtons methodHessian matrixfeature</li>\n<li>Newtons methodnon-convex optimizationgradient descent</li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"external\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"external\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">Taylor series</a></li>\n</ul>"},{"title":"Imbalanced data ","date":"2017-11-11T15:01:09.000Z","_content":"Hello[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)paperpaperimbalanced datasolutionimbalanced datapaper notespapersum up\n\n\n<!--more-->\n## Data level methods\ndata level methodsdataimbalanced data problem\n### Oversampling\nOversamplingsolutionsamples**sampleoverfitting**\n#### SMOTE\nSMOTEoversampling\\\\(m\\\\)\\\\(n\\\\)\\\\(x\\\\)\\\\(x'\\\\)\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\nSMOTEoversampling\n#### Cluster-base oversampling\nCluster-basedclusterclusteroversampling**between-class imbalanceclusterwithin-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\npaperimbalanced datak-means()clusteroversamplingmajority class\\\\(m\\\\)clusterminority\\\\(n\\\\)clusterclusterdata\\\\(k\\\\)majority classclusteroversampling\\\\(k\\\\)minorityclusteroversamplingcluster\\\\(m * k /n\\\\)between-class balancewithin-class balance.\n\n### Undersampling\noversamplingundersamplingundersamplingsamples\n#### One-sided selection\none-sided selection\n\n## Classifier level methods\nclassifier levelimbalanced data\n### Thresholding\nimbalanced dataprobabiltythresholdprior probability\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholdingtrainimbalanced datacost function.\n\nlearning ratecostsamplescost functionmisclassification costimbalanced data\n### One-class classification\nclassificationtasksamplessamples\n\nimbalanced data\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","source":"_posts/ml-imbalanced-data-solution.md","raw":"---\ntitle: Imbalanced data \ndate: 2017-11-11 23:01:09\ntags:\n\t- imbalanced data\ncategories: machine learning\n---\nHello[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)paperpaperimbalanced datasolutionimbalanced datapaper notespapersum up\n\n\n<!--more-->\n## Data level methods\ndata level methodsdataimbalanced data problem\n### Oversampling\nOversamplingsolutionsamples**sampleoverfitting**\n#### SMOTE\nSMOTEoversampling\\\\(m\\\\)\\\\(n\\\\)\\\\(x\\\\)\\\\(x'\\\\)\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\nSMOTEoversampling\n#### Cluster-base oversampling\nCluster-basedclusterclusteroversampling**between-class imbalanceclusterwithin-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\npaperimbalanced datak-means()clusteroversamplingmajority class\\\\(m\\\\)clusterminority\\\\(n\\\\)clusterclusterdata\\\\(k\\\\)majority classclusteroversampling\\\\(k\\\\)minorityclusteroversamplingcluster\\\\(m * k /n\\\\)between-class balancewithin-class balance.\n\n### Undersampling\noversamplingundersamplingundersamplingsamples\n#### One-sided selection\none-sided selection\n\n## Classifier level methods\nclassifier levelimbalanced data\n### Thresholding\nimbalanced dataprobabiltythresholdprior probability\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholdingtrainimbalanced datacost function.\n\nlearning ratecostsamplescost functionmisclassification costimbalanced data\n### One-class classification\nclassificationtasksamplessamples\n\nimbalanced data\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","slug":"ml-imbalanced-data-solution","published":1,"updated":"2017-12-19T15:13:18.554Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe820014wahquw40wa0r","content":"<p>Hello<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"external\">A systematic study of the class imbalance  problem in convolutional neural networks</a>paperpaperimbalanced datasolutionimbalanced datapaper notespapersum up</p>\n<p><br><a id=\"more\"></a></p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>data level methodsdataimbalanced data problem</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversamplingsolutionsamples<strong>sampleoverfitting</strong></p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTEoversampling\\(m\\)\\(n\\)\\(x\\)\\(x\\)</p>\n<script type=\"math/tex; mode=display\">x_{new}= x + rand(0,1) \\cdot |x-x'|</script><p>SMOTEoversampling</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-basedclusterclusteroversampling<strong>between-class imbalanceclusterwithin-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>paperimbalanced datak-means()clusteroversamplingmajority class\\(m\\)clusterminority\\(n\\)clusterclusterdata\\(k\\)majority classclusteroversampling\\(k\\)minorityclusteroversamplingcluster\\(m * k /n\\)between-class balancewithin-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>oversamplingundersamplingundersamplingsamples</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selection</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>classifier levelimbalanced data</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>imbalanced dataprobabiltythresholdprior probability</p>\n<script type=\"math/tex; mode=display\">y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}</script><h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholdingtrainimbalanced datacost function.</p>\n<p>learning ratecostsamplescost functionmisclassification costimbalanced data</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>classificationtasksamplessamples</p>\n<p>imbalanced data</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"external\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"external\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"external\">Chawla, Nitesh V., et al. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"external\">Jo, Taeho, and Nathalie Japkowicz. Class imbalances versus small disjuncts. ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"external\">Richard, Michael D., and Richard P. Lippmann. Neural network classifiers estimate Bayesian a posteriori probabilities. Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Hello<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"external\">A systematic study of the class imbalance  problem in convolutional neural networks</a>paperpaperimbalanced datasolutionimbalanced datapaper notespapersum up</p>\n<p><br>","more":"</p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>data level methodsdataimbalanced data problem</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversamplingsolutionsamples<strong>sampleoverfitting</strong></p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTEoversampling\\(m\\)\\(n\\)\\(x\\)\\(x\\)</p>\n<script type=\"math/tex; mode=display\">x_{new}= x + rand(0,1) \\cdot |x-x'|</script><p>SMOTEoversampling</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-basedclusterclusteroversampling<strong>between-class imbalanceclusterwithin-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>paperimbalanced datak-means()clusteroversamplingmajority class\\(m\\)clusterminority\\(n\\)clusterclusterdata\\(k\\)majority classclusteroversampling\\(k\\)minorityclusteroversamplingcluster\\(m * k /n\\)between-class balancewithin-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>oversamplingundersamplingundersamplingsamples</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selection</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>classifier levelimbalanced data</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>imbalanced dataprobabiltythresholdprior probability</p>\n<script type=\"math/tex; mode=display\">y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}</script><h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholdingtrainimbalanced datacost function.</p>\n<p>learning ratecostsamplescost functionmisclassification costimbalanced data</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>classificationtasksamplessamples</p>\n<p>imbalanced data</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"external\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"external\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"external\">Chawla, Nitesh V., et al. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"external\">Jo, Taeho, and Nathalie Japkowicz. Class imbalances versus small disjuncts. ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"external\">Richard, Michael D., and Richard P. Lippmann. Neural network classifiers estimate Bayesian a posteriori probabilities. Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>"},{"title":"Hello World","date":"2017-07-26T09:40:40.000Z","_content":"**blog!**\n\n**Asir** ,,,.,,,.\n\n,,,.,blog.\n<!--more-->\n***\n[****](https://www.unbelievable9.info/),!\n\n,coding man, ,\n\n**Hello world!!!**\n","source":"_posts/other-hello.md","raw":"---\ntitle: Hello World\ndate: 2017-07-26 17:40:40\ntags: \ncategories: \n---\n**blog!**\n\n**Asir** ,,,.,,,.\n\n,,,.,blog.\n<!--more-->\n***\n[****](https://www.unbelievable9.info/),!\n\n,coding man, ,\n\n**Hello world!!!**\n","slug":"other-hello","published":1,"updated":"2017-11-14T13:40:12.274Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe850017wahqlvdqxr7m","content":"<p><strong>blog!</strong></p>\n<p><strong>Asir</strong> ,,,.,,,.</p>\n<p>,,,.,blog.<br><a id=\"more\"></a></p>\n<hr>\n<p><a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"external\"><strong></strong></a>,!</p>\n<p>,coding man, ,</p>\n<p><strong>Hello world!!!</strong></p>\n","site":{"data":{}},"excerpt":"<p><strong>blog!</strong></p>\n<p><strong>Asir</strong> ,,,.,,,.</p>\n<p>,,,.,blog.<br>","more":"</p>\n<hr>\n<p><a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"external\"><strong></strong></a>,!</p>\n<p>,coding man, ,</p>\n<p><strong>Hello world!!!</strong></p>"},{"title":"","date":"2017-08-26T17:55:07.000Z","_content":"L1L2 regularizationregularization\n<!--more-->\n## MAP and regularization\ncost functionregularization**MLE**(Maximum likelihood estimation)Andrew NgCS229AIregularization**MAP**(Maximum a posteriori estimation)**priori distribution**MLE\n\npriori distributiondistributiondistribution\n\nL1 regularizationpriori distribution**Laplacian distribution**L2 regularizationpriorit distribution**Gaussian distribution**Gaussian distributionLaplacian distribution\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png) \ndistributionregularization\n### Lasso regression\nliner regression\\\\( \\theta\\\\)Laplacian distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\nLasso regression\n### Ridge regression\nliner regression\\\\( \\theta\\\\)Gaussian distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\nRidge regressionshrinkage\n## geometry of error surfaces\npriori distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png) \nobjective function\\\\( \\theta\\\\)\\\\(w\\\\)\\\\(l\\\\)losscost functionpriori distributionoptimization target.\n\npriori distributionoptimizationconvex optimization\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n\\\\( \\beta\\\\)ridgelasso\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png) \noptimizationregularization itemcost functionobjective functionregularization item.\n\ngradient descentoptimzationregularizationcost functiongradientgradient descentregularization\n\nlassolassoobjective function0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)","source":"_posts/ml-ridge-lasso.md","raw":"---\ntitle: \ndate: 2017-08-27 01:55:07\ntags: \n\t- regularization\n\t- MAP\n\t- ridge regression\n\t- lasso regression\ncategories: machine learning\n---\nL1L2 regularizationregularization\n<!--more-->\n## MAP and regularization\ncost functionregularization**MLE**(Maximum likelihood estimation)Andrew NgCS229AIregularization**MAP**(Maximum a posteriori estimation)**priori distribution**MLE\n\npriori distributiondistributiondistribution\n\nL1 regularizationpriori distribution**Laplacian distribution**L2 regularizationpriorit distribution**Gaussian distribution**Gaussian distributionLaplacian distribution\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png) \ndistributionregularization\n### Lasso regression\nliner regression\\\\( \\theta\\\\)Laplacian distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\nLasso regression\n### Ridge regression\nliner regression\\\\( \\theta\\\\)Gaussian distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\nRidge regressionshrinkage\n## geometry of error surfaces\npriori distributioncost function\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png) \nobjective function\\\\( \\theta\\\\)\\\\(w\\\\)\\\\(l\\\\)losscost functionpriori distributionoptimization target.\n\npriori distributionoptimizationconvex optimization\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\n\\\\( \\beta\\\\)ridgelasso\n\n![](http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png) \noptimizationregularization itemcost functionobjective functionregularization item.\n\ngradient descentoptimzationregularizationcost functiongradientgradient descentregularization\n\nlassolassoobjective function0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)","slug":"ml-ridge-lasso","published":1,"updated":"2017-09-02T11:17:42.936Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe890018wahqx8glneaw","content":"<p>L1L2 regularizationregularization<br><a id=\"more\"></a></p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>cost functionregularization<strong>MLE</strong>(Maximum likelihood estimation)Andrew NgCS229AIregularization<strong>MAP</strong>(Maximum a posteriori estimation)<strong>priori distribution</strong>MLE</p>\n<p>priori distributiondistributiondistribution</p>\n<p>L1 regularizationpriori distribution<strong>Laplacian distribution</strong>L2 regularizationpriorit distribution<strong>Gaussian distribution</strong>Gaussian distributionLaplacian distribution</p>\n<script type=\"math/tex; mode=display\">p(x;a)= \\frac{a}{2} e^{-a|x|}</script><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png\" alt=\"\"><br>distributionregularization</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>liner regression\\( \\theta\\)Laplacian distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|</script><p>Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>liner regression\\( \\theta\\)Gaussian distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2</script><p>Ridge regressionshrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>priori distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png\" alt=\"\"><br>objective function\\( \\theta\\)\\(w\\)\\(l\\)losscost functionpriori distributionoptimization target.</p>\n<p>priori distributionoptimizationconvex optimization</p>\n<script type=\"math/tex; mode=display\">\\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><script type=\"math/tex; mode=display\">s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta</script><p>\\( \\beta\\)ridgelasso<br><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png\" alt=\"\"><br>optimizationregularization itemcost functionobjective functionregularization item.</p>\n<p>gradient descentoptimzationregularizationcost functiongradientgradient descentregularization</p>\n<p>lassolassoobjective function0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"external\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"external\">STAT 897D</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>L1L2 regularizationregularization<br>","more":"</p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>cost functionregularization<strong>MLE</strong>(Maximum likelihood estimation)Andrew NgCS229AIregularization<strong>MAP</strong>(Maximum a posteriori estimation)<strong>priori distribution</strong>MLE</p>\n<p>priori distributiondistributiondistribution</p>\n<p>L1 regularizationpriori distribution<strong>Laplacian distribution</strong>L2 regularizationpriorit distribution<strong>Gaussian distribution</strong>Gaussian distributionLaplacian distribution</p>\n<script type=\"math/tex; mode=display\">p(x;a)= \\frac{a}{2} e^{-a|x|}</script><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-1.png\" alt=\"\"><br>distributionregularization</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>liner regression\\( \\theta\\)Laplacian distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|</script><p>Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>liner regression\\( \\theta\\)Gaussian distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2</script><p>Ridge regressionshrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>priori distributioncost function</p>\n<script type=\"math/tex; mode=display\">J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><p><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-2.png\" alt=\"\"><br>objective function\\( \\theta\\)\\(w\\)\\(l\\)losscost functionpriori distributionoptimization target.</p>\n<p>priori distributionoptimizationconvex optimization</p>\n<script type=\"math/tex; mode=display\">\\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2</script><script type=\"math/tex; mode=display\">s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta</script><p>\\( \\beta\\)ridgelasso<br><br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/4/4-3.png\" alt=\"\"><br>optimizationregularization itemcost functionobjective functionregularization item.</p>\n<p>gradient descentoptimzationregularizationcost functiongradientgradient descentregularization</p>\n<p>lassolassoobjective function0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"external\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"external\">STAT 897D</a></li>\n</ul>"},{"title":"spark","date":"2018-01-07T10:44:37.000Z","_content":"HelloblogSparkSparkspark\n<!--more-->\n## Work Flow\nhadoopsparkmaster-slaveSparkdrivertaskexecutorsdriverexecutorsspark applicationapplicationcluster managerSparkstandalone clusterSparkYARNMesosYARN cluster managerYARNspark\n\nsparkcluster manager\n![](http://otmy7guvn.bkt.clouddn.com/blog/15/15-1.png) \n\n## The Driver\nDriverapplicationapplicationmainapplicationdriverapplication\n\nSparkRDDtransformationactiondrivertasksuser programdriverDAG(directed acyclic graph)tasksdrivertasksexecutortasks\n\n## Executors\nExecutorsSpark applicationapplication**Spark jobexecutors**ExecutorstasksdriverRDD\n\n## Spark on Yarn-cluster\nSpark applicationdriverexecutorsyarn-clusterYARNSpark\n![](http://otmy7guvn.bkt.clouddn.com//blog/15/15-2.png) \nYARNYARNmaster-slaverCluster Manager YARNRM(ResourseManager)masterNM(NodeManager)workerYARNsparkYARN\n\nClientResourceManagerapplicationResourseManagerNodeManagerAppManagerAppManagerdriverdrivercontainersResourceManagerResourceManagerAppManagercontainercontainerspark executor\n\napplicationRDDtransformationactiondrivertasksexecutorsexecutorsdriverexecutorsyarn-clusterspark\n\nyarn-clusteryarn-clientdriverNodeManagerclientclientyarn-clusteryarn-client\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-clusterYarn-client](https://www.iteblog.com/archives/1223.html)\n","source":"_posts/spark-spark-workflow.md","raw":"---\ntitle: spark\ndate: 2018-01-07 18:44:37\ntags: spark\ncategories: spark\n---\nHelloblogSparkSparkspark\n<!--more-->\n## Work Flow\nhadoopsparkmaster-slaveSparkdrivertaskexecutorsdriverexecutorsspark applicationapplicationcluster managerSparkstandalone clusterSparkYARNMesosYARN cluster managerYARNspark\n\nsparkcluster manager\n![](http://otmy7guvn.bkt.clouddn.com/blog/15/15-1.png) \n\n## The Driver\nDriverapplicationapplicationmainapplicationdriverapplication\n\nSparkRDDtransformationactiondrivertasksuser programdriverDAG(directed acyclic graph)tasksdrivertasksexecutortasks\n\n## Executors\nExecutorsSpark applicationapplication**Spark jobexecutors**ExecutorstasksdriverRDD\n\n## Spark on Yarn-cluster\nSpark applicationdriverexecutorsyarn-clusterYARNSpark\n![](http://otmy7guvn.bkt.clouddn.com//blog/15/15-2.png) \nYARNYARNmaster-slaverCluster Manager YARNRM(ResourseManager)masterNM(NodeManager)workerYARNsparkYARN\n\nClientResourceManagerapplicationResourseManagerNodeManagerAppManagerAppManagerdriverdrivercontainersResourceManagerResourceManagerAppManagercontainercontainerspark executor\n\napplicationRDDtransformationactiondrivertasksexecutorsexecutorsdriverexecutorsyarn-clusterspark\n\nyarn-clusteryarn-clientdriverNodeManagerclientclientyarn-clusteryarn-client\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-clusterYarn-client](https://www.iteblog.com/archives/1223.html)\n","slug":"spark-spark-workflow","published":1,"updated":"2018-01-11T14:54:35.734Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe8b001bwahq8abm9zwr","content":"<p>HelloblogSparkSparkspark<br><a id=\"more\"></a></p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>hadoopsparkmaster-slaveSparkdrivertaskexecutorsdriverexecutorsspark applicationapplicationcluster managerSparkstandalone clusterSparkYARNMesosYARN cluster managerYARNspark</p>\n<p>sparkcluster manager<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/15/15-1.png\" alt=\"\"> </p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driverapplicationapplicationmainapplicationdriverapplication</p>\n<p>SparkRDDtransformationactiondrivertasksuser programdriverDAG(directed acyclic graph)tasksdrivertasksexecutortasks</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>ExecutorsSpark applicationapplication<strong>Spark jobexecutors</strong>ExecutorstasksdriverRDD</p>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>Spark applicationdriverexecutorsyarn-clusterYARNSpark<br><img src=\"http://otmy7guvn.bkt.clouddn.com//blog/15/15-2.png\" alt=\"\"><br>YARNYARNmaster-slaverCluster Manager YARNRM(ResourseManager)masterNM(NodeManager)workerYARNsparkYARN</p>\n<p>ClientResourceManagerapplicationResourseManagerNodeManagerAppManagerAppManagerdriverdrivercontainersResourceManagerResourceManagerAppManagercontainercontainerspark executor</p>\n<p>applicationRDDtransformationactiondrivertasksexecutorsexecutorsdriverexecutorsyarn-clusterspark</p>\n<p>yarn-clusteryarn-clientdriverNodeManagerclientclientyarn-clusteryarn-client</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"external\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis.  OReilly Media, Inc., 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"external\">Spark:Yarn-clusterYarn-client</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>HelloblogSparkSparkspark<br>","more":"</p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>hadoopsparkmaster-slaveSparkdrivertaskexecutorsdriverexecutorsspark applicationapplicationcluster managerSparkstandalone clusterSparkYARNMesosYARN cluster managerYARNspark</p>\n<p>sparkcluster manager<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/15/15-1.png\" alt=\"\"> </p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driverapplicationapplicationmainapplicationdriverapplication</p>\n<p>SparkRDDtransformationactiondrivertasksuser programdriverDAG(directed acyclic graph)tasksdrivertasksexecutortasks</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>ExecutorsSpark applicationapplication<strong>Spark jobexecutors</strong>ExecutorstasksdriverRDD</p>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>Spark applicationdriverexecutorsyarn-clusterYARNSpark<br><img src=\"http://otmy7guvn.bkt.clouddn.com//blog/15/15-2.png\" alt=\"\"><br>YARNYARNmaster-slaverCluster Manager YARNRM(ResourseManager)masterNM(NodeManager)workerYARNsparkYARN</p>\n<p>ClientResourceManagerapplicationResourseManagerNodeManagerAppManagerAppManagerdriverdrivercontainersResourceManagerResourceManagerAppManagercontainercontainerspark executor</p>\n<p>applicationRDDtransformationactiondrivertasksexecutorsexecutorsdriverexecutorsyarn-clusterspark</p>\n<p>yarn-clusteryarn-clientdriverNodeManagerclientclientyarn-clusteryarn-client</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"external\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis.  OReilly Media, Inc., 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"external\">Spark:Yarn-clusterYarn-client</a></li>\n</ul>"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","date":"2017-08-23T03:30:43.000Z","_content":"OKreviewpaperpaper3facebookgbtlrCTRpaper\n\npaperpoint.\n<!--more-->\n## Notes\nCTRlogistic regressionlrposterior probability\n\nlrlrlinerfeaturesfeature\n\npapergbtfeature transform\n![](http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png) \npaper\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\nfeaturegbttransformtree node10lr\n\nsamplegbttree1tree2tree1sampletree nodetree2sampletree nodetransformnew sample(1,0,0,0,1)\n\ngbttransformfeatureSVMkernelfeature0-1featurefeature\n\npapergetgbt+lrpythondemo[](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)folk\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","source":"_posts/paper-facebook.md","raw":"---\ntitle: Reading Notes-Practical lessons from predicting clicks on ads at facebook\ndate: 2017-08-23 11:30:43\ntags: \n\t- gbt\n\t- logistic regression\n\t- gradient descent\ncategories: reading notes\n---\nOKreviewpaperpaper3facebookgbtlrCTRpaper\n\npaperpoint.\n<!--more-->\n## Notes\nCTRlogistic regressionlrposterior probability\n\nlrlrlinerfeaturesfeature\n\npapergbtfeature transform\n![](http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png) \npaper\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\nfeaturegbttransformtree node10lr\n\nsamplegbttree1tree2tree1sampletree nodetree2sampletree nodetransformnew sample(1,0,0,0,1)\n\ngbttransformfeatureSVMkernelfeature0-1featurefeature\n\npapergetgbt+lrpythondemo[](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)folk\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","slug":"paper-facebook","published":1,"updated":"2017-09-09T16:23:01.646Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe8d001dwahqpj0m8pge","content":"<p>OKreviewpaperpaper3facebookgbtlrCTRpaper</p>\n<p>paperpoint.<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>CTRlogistic regressionlrposterior probability</p>\n<p>lrlrlinerfeaturesfeature</p>\n<p>papergbtfeature transform<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png\" alt=\"\"><br>paper</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>featuregbttransformtree node10lr</p>\n<p>samplegbttree1tree2tree1sampletree nodetree2sampletree nodetransformnew sample(1,0,0,0,1)</p>\n<p>gbttransformfeatureSVMkernelfeature0-1featurefeature</p>\n<p>papergetgbt+lrpythondemo<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"external\"></a>folk</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"external\">He, Xinran, et al. Practical lessons from predicting clicks on ads at facebook. Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>OKreviewpaperpaper3facebookgbtlrCTRpaper</p>\n<p>paperpoint.<br>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>CTRlogistic regressionlrposterior probability</p>\n<p>lrlrlinerfeaturesfeature</p>\n<p>papergbtfeature transform<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/3/3-1.png\" alt=\"\"><br>paper</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>featuregbttransformtree node10lr</p>\n<p>samplegbttree1tree2tree1sampletree nodetree2sampletree nodetransformnew sample(1,0,0,0,1)</p>\n<p>gbttransformfeatureSVMkernelfeature0-1featurefeature</p>\n<p>papergetgbt+lrpythondemo<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"external\"></a>folk</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"external\">He, Xinran, et al. Practical lessons from predicting clicks on ads at facebook. Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>"},{"title":"Reading Notes-SwishA Self-gated Activation Function","date":"2017-10-22T08:13:30.000Z","_content":"Hi allpaperactivation functionactivation functionsigmoidtanhReLUReLUactivation function\n<!--more-->\n## Notes\nswish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)\\\\(\\sigma(x)\\\\)sigmoid function\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functin\n![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-1.png) \nswish function1st and 2nd derivatives\n![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-2.png) \nswish function\n### Unbounded above\nUnbounded aboveactivation functionbounded valuesaturation. bounded above bounded valuefunction gradient0gradient descentconvergesigmoid tanh functionbounded below and aboveactivation functionfunctionlinerunbounded aboveReLU\n\n### Bounded below & non-monotonicity\nBounded belowactivation functioninputactivation value-1000-1author\n> make large negative input \"fogotten\"\n\nregularzationReLUswishnegative inputnegative valuenon-monotonicitygradient flow.\n\n### Smothness\nsmoothness\n ![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-3.png) \n \n swishactivationswish\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swisha Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","source":"_posts/paper-swish.md","raw":"---\ntitle: Reading Notes-SwishA Self-gated Activation Function\ndate: 2017-10-22 16:13:30\ntags: \n\t- activtion function\ncategories: reading notes\n---\nHi allpaperactivation functionactivation functionsigmoidtanhReLUReLUactivation function\n<!--more-->\n## Notes\nswish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)\\\\(\\sigma(x)\\\\)sigmoid function\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functin\n![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-1.png) \nswish function1st and 2nd derivatives\n![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-2.png) \nswish function\n### Unbounded above\nUnbounded aboveactivation functionbounded valuesaturation. bounded above bounded valuefunction gradient0gradient descentconvergesigmoid tanh functionbounded below and aboveactivation functionfunctionlinerunbounded aboveReLU\n\n### Bounded below & non-monotonicity\nBounded belowactivation functioninputactivation value-1000-1author\n> make large negative input \"fogotten\"\n\nregularzationReLUswishnegative inputnegative valuenon-monotonicitygradient flow.\n\n### Smothness\nsmoothness\n ![](http://otmy7guvn.bkt.clouddn.com/blog/11/11-3.png) \n \n swishactivationswish\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swisha Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","slug":"paper-swish","published":1,"updated":"2017-10-25T15:00:16.593Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe8g001hwahqerr0c534","content":"<p>Hi allpaperactivation functionactivation functionsigmoidtanhReLUReLUactivation function<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>swish activation function \\(f(x)=x \\cdot \\sigma (x)\\)\\(\\sigma(x)\\)sigmoid function\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functin<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-1.png\" alt=\"\"><br>swish function1st and 2nd derivatives<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-2.png\" alt=\"\"><br>swish function</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded aboveactivation functionbounded valuesaturation. bounded above bounded valuefunction gradient0gradient descentconvergesigmoid tanh functionbounded below and aboveactivation functionfunctionlinerunbounded aboveReLU</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded belowactivation functioninputactivation value-1000-1author</p>\n<blockquote>\n<p>make large negative input fogotten</p>\n</blockquote>\n<p>regularzationReLUswishnegative inputnegative valuenon-monotonicitygradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>smoothness<br> <img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-3.png\" alt=\"\"> </p>\n<p> swishactivationswish</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"external\">Ramachandran P, Zoph B, Le Q V. Swisha Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Hi allpaperactivation functionactivation functionsigmoidtanhReLUReLUactivation function<br>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>swish activation function \\(f(x)=x \\cdot \\sigma (x)\\)\\(\\sigma(x)\\)sigmoid function\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functin<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-1.png\" alt=\"\"><br>swish function1st and 2nd derivatives<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-2.png\" alt=\"\"><br>swish function</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded aboveactivation functionbounded valuesaturation. bounded above bounded valuefunction gradient0gradient descentconvergesigmoid tanh functionbounded below and aboveactivation functionfunctionlinerunbounded aboveReLU</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded belowactivation functioninputactivation value-1000-1author</p>\n<blockquote>\n<p>make large negative input fogotten</p>\n</blockquote>\n<p>regularzationReLUswishnegative inputnegative valuenon-monotonicitygradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>smoothness<br> <img src=\"http://otmy7guvn.bkt.clouddn.com/blog/11/11-3.png\" alt=\"\"> </p>\n<p> swishactivationswish</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"external\">Ramachandran P, Zoph B, Le Q V. Swisha Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>"},{"title":"Reading Notes-Class Imbalance, Redux","date":"2017-09-10T05:21:56.000Z","_content":"paper\n\nimbalanced dataimbalanced dataoversamplingundersamplingpaper\n<!--more-->\n## Notes\npositivenegativesamplepositive samples\\\\(P(x)\\\\)Guassiannegative samples\\\\(G(x)\\\\)Guassianpositive region\\\\(\\cal R^{+} \\_{w}\\\\)negative region\\\\(\\cal R^{-} \\_{w}\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png) \n\\\\(w^{ \\*}\\\\)\\\\(w^{ \\*}\\\\) loss\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\nlossfn(false negative)fp(false positive)minimunloss\\\\(w^{*}\\\\)error\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n\\\\(\\cal D \\\\)(paperpositive)\\\\(\\pi\\\\)(0.5)\\\\(\\pi\\\\)\\\\(\\cal D_{\\pi}\\\\)\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\nlossimbalanced data\\\\(\\pi\\\\)\n\nOKpaper\\\\(\\hat w\\\\)postiveskewed\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), \\\\(\\hat w\\\\)positive region\\\\(w^{\\*}\\\\)positive\n\nemail**email**\n\npaperundersamplingundersampling\\\\(\\pi\\\\)0.5\\\\(\\hat w\\\\)\\\\(w^{*}\\\\)\n\nbaggingundersamplingbagging\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png) \npaperWeighted Empirical Cost Minimization(weighted SVM)SMOTEbagging undersamplingSMOTEpaper\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png) \nSMOTEsamplesample\n\nOK\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)","source":"_posts/paper-imbalance.md","raw":"---\ntitle: Reading Notes-Class Imbalance, Redux\ndate: 2017-09-10 13:21:56\ntags: \n\t- imbalanced data\n\t- undersampling\n\t- bagging\ncategories: reading notes\n---\npaper\n\nimbalanced dataimbalanced dataoversamplingundersamplingpaper\n<!--more-->\n## Notes\npositivenegativesamplepositive samples\\\\(P(x)\\\\)Guassiannegative samples\\\\(G(x)\\\\)Guassianpositive region\\\\(\\cal R^{+} \\_{w}\\\\)negative region\\\\(\\cal R^{-} \\_{w}\\\\)\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png) \n\\\\(w^{ \\*}\\\\)\\\\(w^{ \\*}\\\\) loss\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\nlossfn(false negative)fp(false positive)minimunloss\\\\(w^{*}\\\\)error\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\n\\\\(\\cal D \\\\)(paperpositive)\\\\(\\pi\\\\)(0.5)\\\\(\\pi\\\\)\\\\(\\cal D_{\\pi}\\\\)\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\nlossimbalanced data\\\\(\\pi\\\\)\n\nOKpaper\\\\(\\hat w\\\\)postiveskewed\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), \\\\(\\hat w\\\\)positive region\\\\(w^{\\*}\\\\)positive\n\nemail**email**\n\npaperundersamplingundersampling\\\\(\\pi\\\\)0.5\\\\(\\hat w\\\\)\\\\(w^{*}\\\\)\n\nbaggingundersamplingbagging\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png) \npaperWeighted Empirical Cost Minimization(weighted SVM)SMOTEbagging undersamplingSMOTEpaper\n![](http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png) \nSMOTEsamplesample\n\nOK\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)","slug":"paper-imbalance","published":1,"updated":"2017-09-10T08:33:33.050Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcambe8j001jwahqfqy3ul3m","content":"<p>paper</p>\n<p>imbalanced dataimbalanced dataoversamplingundersamplingpaper<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>positivenegativesamplepositive samples\\(P(x)\\)Guassiannegative samples\\(G(x)\\)Guassianpositive region\\(\\cal R^{+} _{w}\\)negative region\\(\\cal R^{-} _{w}\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png\" alt=\"\"><br>\\(w^{ *}\\)\\(w^{ *}\\) loss</p>\n<script type=\"math/tex; mode=display\">w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)</script><p>lossfn(false negative)fp(false positive)minimunloss\\(w^{*}\\)error</p>\n<script type=\"math/tex; mode=display\">\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>\\(\\cal D \\)(paperpositive)\\(\\pi\\)(0.5)\\(\\pi\\)\\(\\cal D_{\\pi}\\)</p>\n<script type=\"math/tex; mode=display\">\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>lossimbalanced data\\(\\pi\\)</p>\n<p>OKpaper\\(\\hat w\\)postiveskewed\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), \\(\\hat w\\)positive region\\(w^{*}\\)positive</p>\n<p>email<strong>email</strong></p>\n<p>paperundersamplingundersampling\\(\\pi\\)0.5\\(\\hat w\\)\\(w^{*}\\)</p>\n<p>baggingundersamplingbagging<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png\" alt=\"\"><br>paperWeighted Empirical Cost Minimization(weighted SVM)SMOTEbagging undersamplingSMOTEpaper<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png\" alt=\"\"><br>SMOTEsamplesample</p>\n<p>OK</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"external\">Wallace, Byron C., et al. Class imbalance, redux. Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"external\">PPT-Class Imbalance, Redux</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>paper</p>\n<p>imbalanced dataimbalanced dataoversamplingundersamplingpaper<br>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>positivenegativesamplepositive samples\\(P(x)\\)Guassiannegative samples\\(G(x)\\)Guassianpositive region\\(\\cal R^{+} _{w}\\)negative region\\(\\cal R^{-} _{w}\\)<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-1.png\" alt=\"\"><br>\\(w^{ *}\\)\\(w^{ *}\\) loss</p>\n<script type=\"math/tex; mode=display\">w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)</script><p>lossfn(false negative)fp(false positive)minimunloss\\(w^{*}\\)error</p>\n<script type=\"math/tex; mode=display\">\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>\\(\\cal D \\)(paperpositive)\\(\\pi\\)(0.5)\\(\\pi\\)\\(\\cal D_{\\pi}\\)</p>\n<script type=\"math/tex; mode=display\">\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx</script><p>lossimbalanced data\\(\\pi\\)</p>\n<p>OKpaper\\(\\hat w\\)postiveskewed\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), \\(\\hat w\\)positive region\\(w^{*}\\)positive</p>\n<p>email<strong>email</strong></p>\n<p>paperundersamplingundersampling\\(\\pi\\)0.5\\(\\hat w\\)\\(w^{*}\\)</p>\n<p>baggingundersamplingbagging<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-2.png\" alt=\"\"><br>paperWeighted Empirical Cost Minimization(weighted SVM)SMOTEbagging undersamplingSMOTEpaper<br><img src=\"http://otmy7guvn.bkt.clouddn.com/blog/5/5-3.png\" alt=\"\"><br>SMOTEsamplesample</p>\n<p>OK</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"external\">Wallace, Byron C., et al. Class imbalance, redux. Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"external\">PPT-Class Imbalance, Redux</a></li>\n</ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjcambe090000wahqlgc28b3b","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe0x0007wahqi7jtrsmg"},{"post_id":"cjcambe0h0001wahqn3shazvm","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe10000bwahqq8a0lq2z"},{"post_id":"cjcambe0q0004wahqofnfxrif","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe11000ewahqzrcgzfpu"},{"post_id":"cjcambe7g000nwahqhfzfp4fw","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe7u000uwahq2jv02oob"},{"post_id":"cjcambe7j000pwahqt6lr5gxg","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe7w000wwahqu9bnzgf0"},{"post_id":"cjcambe7s000swahqe5mjdav9","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe7z000zwahqjew240pf"},{"post_id":"cjcambe7v000vwahq7ws5my2m","category_id":"cjcambe0l0002wahqaz5qwl6v","_id":"cjcambe820013wahq2x8uozk7"},{"post_id":"cjcambe7x000xwahqfupeb2un","category_id":"cjcambe820012wahqu4z2k13n","_id":"cjcambe8d001cwahqj4n3m4ji"},{"post_id":"cjcambe890018wahqx8glneaw","category_id":"cjcambe820012wahqu4z2k13n","_id":"cjcambe8g001gwahqaje77030"},{"post_id":"cjcambe7z0011wahqc7mz38b3","category_id":"cjcambe820012wahqu4z2k13n","_id":"cjcambe8i001iwahqncwm6aa5"},{"post_id":"cjcambe820014wahquw40wa0r","category_id":"cjcambe820012wahqu4z2k13n","_id":"cjcambe8n001nwahqmptz4lvn"},{"post_id":"cjcambe850017wahqlvdqxr7m","category_id":"cjcambe8k001kwahqhz3a7qdn","_id":"cjcambe8o001swahq2cbvesdk"},{"post_id":"cjcambe8b001bwahq8abm9zwr","category_id":"cjcambe8n001owahq9nzt2sbm","_id":"cjcambe8q001wwahqmrtdyiid"},{"post_id":"cjcambe8d001dwahqpj0m8pge","category_id":"cjcambe8o001twahqu0oot2m3","_id":"cjcambe8s0020wahq0ayq9v3a"},{"post_id":"cjcambe8g001hwahqerr0c534","category_id":"cjcambe8o001twahqu0oot2m3","_id":"cjcambe8t0024wahqrqmqsfz3"},{"post_id":"cjcambe8j001jwahqfqy3ul3m","category_id":"cjcambe8o001twahqu0oot2m3","_id":"cjcambe8u0028wahqhl3glfot"}],"PostTag":[{"post_id":"cjcambe090000wahqlgc28b3b","tag_id":"cjcambe0p0003wahq8to9xlxc","_id":"cjcambe10000awahq2payf4k8"},{"post_id":"cjcambe090000wahqlgc28b3b","tag_id":"cjcambe0t0006wahq6x0viai5","_id":"cjcambe10000cwahqyeanbulz"},{"post_id":"cjcambe0h0001wahqn3shazvm","tag_id":"cjcambe0z0009wahqtoce7vmd","_id":"cjcambe12000gwahq133k96lz"},{"post_id":"cjcambe0h0001wahqn3shazvm","tag_id":"cjcambe0p0003wahq8to9xlxc","_id":"cjcambe12000hwahqmu29o23p"},{"post_id":"cjcambe0q0004wahqofnfxrif","tag_id":"cjcambe11000fwahq405dj6zg","_id":"cjcambe13000kwahqddlkcx16"},{"post_id":"cjcambe0q0004wahqofnfxrif","tag_id":"cjcambe12000iwahqpaonpw19","_id":"cjcambe13000lwahq9cam4ur5"},{"post_id":"cjcambe0q0004wahqofnfxrif","tag_id":"cjcambe12000jwahqbfo2poli","_id":"cjcambe13000mwahqbkuoysua"},{"post_id":"cjcambe7g000nwahqhfzfp4fw","tag_id":"cjcambe7q000rwahqwmnrcjdz","_id":"cjcambe7z0010wahqa7ao1fqy"},{"post_id":"cjcambe7j000pwahqt6lr5gxg","tag_id":"cjcambe7q000rwahqwmnrcjdz","_id":"cjcambe850016wahq3hdx0j84"},{"post_id":"cjcambe7s000swahqe5mjdav9","tag_id":"cjcambe830015wahq7rdlojrx","_id":"cjcambe8n001mwahq2tunw8kk"},{"post_id":"cjcambe7s000swahqe5mjdav9","tag_id":"cjcambe8b001awahqkvpxjkb8","_id":"cjcambe8n001pwahq7u4648ex"},{"post_id":"cjcambe7s000swahqe5mjdav9","tag_id":"cjcambe8g001fwahq4t6khmzg","_id":"cjcambe8o001rwahqyxeevoo6"},{"post_id":"cjcambe7v000vwahq7ws5my2m","tag_id":"cjcambe830015wahq7rdlojrx","_id":"cjcambe8q001vwahq6c7zenra"},{"post_id":"cjcambe7v000vwahq7ws5my2m","tag_id":"cjcambe8n001qwahqqzend66w","_id":"cjcambe8r001ywahqneis0wvp"},{"post_id":"cjcambe7x000xwahqfupeb2un","tag_id":"cjcambe8p001uwahqto171aij","_id":"cjcambe8t0023wahqducm3s94"},{"post_id":"cjcambe7x000xwahqfupeb2un","tag_id":"cjcambe0p0003wahq8to9xlxc","_id":"cjcambe8t0025wahq95748scj"},{"post_id":"cjcambe7x000xwahqfupeb2un","tag_id":"cjcambe8r001zwahqn4aa5z3e","_id":"cjcambe8u0027wahqr5erm1iv"},{"post_id":"cjcambe7z0011wahqc7mz38b3","tag_id":"cjcambe8t0022wahqhc528gf6","_id":"cjcambe8v002awahqujdstk5c"},{"post_id":"cjcambe7z0011wahqc7mz38b3","tag_id":"cjcambe0p0003wahq8to9xlxc","_id":"cjcambe8v002bwahq8vh98997"},{"post_id":"cjcambe7z0011wahqc7mz38b3","tag_id":"cjcambe8r001zwahqn4aa5z3e","_id":"cjcambe8v002dwahq6iyizvpt"},{"post_id":"cjcambe820014wahquw40wa0r","tag_id":"cjcambe8u0029wahqu6dmmsi2","_id":"cjcambe8v002ewahq6g32icdc"},{"post_id":"cjcambe850017wahqlvdqxr7m","tag_id":"cjcambe8v002cwahqprc1j289","_id":"cjcambe8w002gwahqx2afuh4r"},{"post_id":"cjcambe890018wahqx8glneaw","tag_id":"cjcambe0z0009wahqtoce7vmd","_id":"cjcambe8x002kwahq0devjqpv"},{"post_id":"cjcambe890018wahqx8glneaw","tag_id":"cjcambe8v002fwahqk8uazcwt","_id":"cjcambe8x002lwahqs7dn4jrs"},{"post_id":"cjcambe890018wahqx8glneaw","tag_id":"cjcambe8w002hwahqmxoqkfg7","_id":"cjcambe8y002nwahqoh2gvdk3"},{"post_id":"cjcambe890018wahqx8glneaw","tag_id":"cjcambe8w002iwahqgdbn74ca","_id":"cjcambe8z002owahqzi392yef"},{"post_id":"cjcambe8b001bwahq8abm9zwr","tag_id":"cjcambe8x002jwahqm3zoy4jm","_id":"cjcambe8z002qwahqpsovwj92"},{"post_id":"cjcambe8d001dwahqpj0m8pge","tag_id":"cjcambe8x002mwahqhy955fd7","_id":"cjcambe90002swahqknvd89nt"},{"post_id":"cjcambe8d001dwahqpj0m8pge","tag_id":"cjcambe8z002pwahq833y1glv","_id":"cjcambe90002twahqtqlqrbog"},{"post_id":"cjcambe8d001dwahqpj0m8pge","tag_id":"cjcambe0p0003wahq8to9xlxc","_id":"cjcambe90002vwahqep126tvg"},{"post_id":"cjcambe8g001hwahqerr0c534","tag_id":"cjcambe8z002rwahqp4bgwj6q","_id":"cjcambe90002wwahqrexqc1e7"},{"post_id":"cjcambe8j001jwahqfqy3ul3m","tag_id":"cjcambe8u0029wahqu6dmmsi2","_id":"cjcambe92002zwahqbajrho7b"},{"post_id":"cjcambe8j001jwahqfqy3ul3m","tag_id":"cjcambe91002xwahqfqrol29o","_id":"cjcambe930030wahqpnadhfiv"},{"post_id":"cjcambe8j001jwahqfqy3ul3m","tag_id":"cjcambe91002ywahqswyr2pxx","_id":"cjcambe930031wahql8e1euo8"}],"Tag":[{"name":"gradient descent","_id":"cjcambe0p0003wahq8to9xlxc"},{"name":"moving averages","_id":"cjcambe0t0006wahq6x0viai5"},{"name":"regularization","_id":"cjcambe0z0009wahqtoce7vmd"},{"name":"hyperparameter","_id":"cjcambe11000fwahq405dj6zg"},{"name":"batch norm","_id":"cjcambe12000iwahqpaonpw19"},{"name":"covariate shift","_id":"cjcambe12000jwahqbfo2poli"},{"name":"CNN","_id":"cjcambe7q000rwahqwmnrcjdz"},{"name":"learning strategy","_id":"cjcambe830015wahq7rdlojrx"},{"name":"transfer learning","_id":"cjcambe8b001awahqkvpxjkb8"},{"name":"multi-task learning","_id":"cjcambe8g001fwahq4t6khmzg"},{"name":"orthogonalization","_id":"cjcambe8n001qwahqqzend66w"},{"name":"convex optimization","_id":"cjcambe8p001uwahqto171aij"},{"name":"newton's method","_id":"cjcambe8r001zwahqn4aa5z3e"},{"name":"unconstrained optimization","_id":"cjcambe8t0022wahqhc528gf6"},{"name":"imbalanced data","_id":"cjcambe8u0029wahqu6dmmsi2"},{"name":"","_id":"cjcambe8v002cwahqprc1j289"},{"name":"MAP","_id":"cjcambe8v002fwahqk8uazcwt"},{"name":"ridge regression","_id":"cjcambe8w002hwahqmxoqkfg7"},{"name":"lasso regression","_id":"cjcambe8w002iwahqgdbn74ca"},{"name":"spark","_id":"cjcambe8x002jwahqm3zoy4jm"},{"name":"gbt","_id":"cjcambe8x002mwahqhy955fd7"},{"name":"logistic regression","_id":"cjcambe8z002pwahq833y1glv"},{"name":"activtion function","_id":"cjcambe8z002rwahqp4bgwj6q"},{"name":"undersampling","_id":"cjcambe91002xwahqfqrol29o"},{"name":"bagging","_id":"cjcambe91002ywahqswyr2pxx"}]}}