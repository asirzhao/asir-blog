{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"themes/minos/source/css/insight.scss","path":"css/insight.scss","modified":1,"renderable":1},{"_id":"themes/minos/source/css/style.scss","path":"css/style.scss","modified":1,"renderable":1},{"_id":"themes/minos/source/images/check.svg","path":"images/check.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/exclamation.svg","path":"images/exclamation.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/info.svg","path":"images/info.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/logo.png","path":"images/logo.png","modified":1,"renderable":1},{"_id":"themes/minos/source/images/question.svg","path":"images/question.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/images/quote-left.svg","path":"images/quote-left.svg","modified":1,"renderable":1},{"_id":"themes/minos/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1},{"_id":"themes/minos/source/js/script.js","path":"js/script.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1540109493367},{"_id":"source/about/index.md","hash":"ffbc4488cbe9e131bcea2156946d8aefd5eda48e","modified":1542803094643},{"_id":"source/_posts/course-deep-learning-course2-week1.md","hash":"d6800156c26f9bba8da2f91ae2636be09931f768","modified":1542609433686},{"_id":"source/_posts/course-deep-learning-course2-week3.md","hash":"b72e8ee53bccc08d6fbe1fce659e7b700fe4805e","modified":1542609538491},{"_id":"source/_posts/course-deep-learning-course2-week2.md","hash":"ae283834c58e2122c02c378c20a2794dbd5a385f","modified":1542609478243},{"_id":"source/_posts/course-deep-learning-course3-week1.md","hash":"a2cff4f897c97f0207001c986510d93c17ffb687","modified":1542609666114},{"_id":"source/_posts/course-deep-learning-course3-week2.md","hash":"c2627a2a91fc54795eccd2bed06aa6bda780f485","modified":1542609819611},{"_id":"source/_posts/course-deep-learning-course4-week1.md","hash":"a27fb439d13ad9827e7ff2b43a4b0068ce874c8f","modified":1542609905266},{"_id":"source/_posts/course-deep-learning-course4-week2.md","hash":"be776844490a752e434e0c7884bb455317f680aa","modified":1542610043155},{"_id":"source/_posts/ml-convex-opt.md","hash":"2851eae3feaacd89c5dd0242c4d70b4c1bce150e","modified":1542609255106},{"_id":"source/_posts/ml-gd-and-nm.md","hash":"f966967debd4744d6435418cb054b2bca0918d9c","modified":1542608824265},{"_id":"source/_posts/ml-imbalanced-data-solution.md","hash":"6a3e85f66233c0534de708e81924a0c8fdd05def","modified":1542608773784},{"_id":"source/_posts/ml-ridge-lasso.md","hash":"9e9d106ddf90f0f578a6f368ce2e056c8680f5d7","modified":1542609071218},{"_id":"source/_posts/other-hello.md","hash":"e93c503e52dd939d6dacfaad2b4352bd3cf5979e","modified":1538812870985},{"_id":"source/_posts/paper-facebook.md","hash":"02fbeb86579cbfcf0a85f30aaec4e46bd0e3a452","modified":1542608419166},{"_id":"source/_posts/paper-imbalance.md","hash":"5d12080e1692a0c898cc79753d53f9cf17a696f2","modified":1542608356529},{"_id":"source/_posts/paper-swish.md","hash":"e999660a6a950168cddde524b6fadbbe4ec2d64a","modified":1542608212121},{"_id":"source/_posts/spark-catalyst-optimization.md","hash":"d78ef18e6dca4d294966ce4db5aa9a88386780eb","modified":1542684276205},{"_id":"source/_posts/spark-from-rdd-to-dataframe-dataset.md","hash":"ae3798c52aaf9ec55065d1603c47162f1104323f","modified":1542635378613},{"_id":"source/_posts/spark-second-generation-tungsten-in-spark.md","hash":"e044d136c50d2b392cc6649f51955b7d5ffc28f5","modified":1542689136619},{"_id":"source/_posts/spark-spark-tune.md","hash":"642f48855cc724fc19f0c0eae8457eccb5044fdf","modified":1542603038728},{"_id":"source/_posts/spark-spark-workflow.md","hash":"34b05704261de4ab0b883b42857bacd0c1f3f953","modified":1542603090519},{"_id":"source/_posts/spark-sumup-part-2.md","hash":"7d198f74ba09c7a5115224669948ac82398dc880","modified":1542602933715},{"_id":"source/_posts/spark-sumup-part-1.md","hash":"0c1d556517d3b08b50bd0c66b5a2c545845185ce","modified":1542635365169},{"_id":"source/_posts/spark-sumup-part-3.md","hash":"f744eaf569a8df1f40fae638b360798be069aa7b","modified":1553339801637},{"_id":"source/categories/index.md","hash":"4b365c6059e6bf92e31df59be1a2597c25bb4ba3","modified":1535340708796},{"_id":"source/tags/index.md","hash":"6069f89020880bad30d7801a8e1a4c2a77d7bc5b","modified":1535340708797},{"_id":"themes/minos/.git","hash":"18b991c2a70462f5eb0f58a5e9bfb706c98116ab","modified":1538818976519},{"_id":"themes/minos/.gitignore","hash":"8b02e7219e2dd9b50d198819fd7d8f74ebc9db2a","modified":1538818976531},{"_id":"themes/minos/LICENSE","hash":"ca01a2d52b59346e82f079c593df6cb26dd9a7a5","modified":1538818976531},{"_id":"themes/minos/README.md","hash":"ba6b4e134d718704cfd030e106bf24d6ef8b496d","modified":1538818976532},{"_id":"themes/minos/_config.yml","hash":"fb6b0bbee855eb73e7e67bd0d18993c97e870ee5","modified":1542802641261},{"_id":"themes/minos/_config.yml.example","hash":"28c6c5604380fb6cc5a99639fa445c40205764ba","modified":1538818976532},{"_id":"themes/minos/package-lock.json","hash":"e1fbecec56fb65379bf651f21fb485376e692b38","modified":1538818976538},{"_id":"themes/minos/package.json","hash":"f9d450db80149dea6c372990cdf51dfde901e5cc","modified":1538818976539},{"_id":"themes/minos/scripts/01_check.js","hash":"b26b19011a6eb61e61419331a7f9c5fdf553d830","modified":1538818976539},{"_id":"themes/minos/scripts/99_config.js","hash":"d41a5df0a442728fbc66514476fe043e416d7438","modified":1538818976540},{"_id":"themes/minos/scripts/99_tags.js","hash":"91369a4d8376bc981a9bad9c5dde4b3e775e5cb5","modified":1538818976540},{"_id":"themes/minos/scripts/99_content.js","hash":"5d19de210e9172a9acb61667a26810899fce917d","modified":1538818976540},{"_id":"themes/minos/scripts/10_i18n.js","hash":"346a09259e15913871e12c7418a639b8d65df570","modified":1538818976539},{"_id":"themes/minos/scripts/rfc5646.js","hash":"8ecf38d0ec7145720ea8e888da314131712770e8","modified":1538818976540},{"_id":"themes/minos/layout/archive.ejs","hash":"e3eefe819d61b4d0ee069bb705a9f5707a8bf3da","modified":1538818976533},{"_id":"themes/minos/layout/categories.ejs","hash":"fff6f911d0f548ee749292bc1942f8fbbb1fbfe7","modified":1538818976533},{"_id":"themes/minos/layout/category.ejs","hash":"403c646878834964883ac41e63952f7b1595c0ba","modified":1538818976533},{"_id":"themes/minos/layout/index.ejs","hash":"dff9e199d394f82c5416b814f9e644edbe4090f0","modified":1538818976536},{"_id":"themes/minos/layout/layout.ejs","hash":"45588aa46857cf9403fa79d738ab37a46ddcf773","modified":1538818976536},{"_id":"themes/minos/layout/post.ejs","hash":"68b84a717efc5ca59ee9eb6202ccf05c5a8abda5","modified":1538818976537},{"_id":"themes/minos/layout/tag.ejs","hash":"5593c7cf9618ef5650c779ed9d75424f057aa210","modified":1538818976538},{"_id":"themes/minos/layout/tags.ejs","hash":"e4a9909119294f131a45f10b2cb1058af5fb9be1","modified":1538818976538},{"_id":"themes/minos/languages/en.yml","hash":"ef98c8674fed78f2350598ee8b15fcd53fbd2ae5","modified":1538818976532},{"_id":"themes/minos/languages/es.yml","hash":"5c35950221411e34e7a9821d0b0671da9a458d8c","modified":1538818976532},{"_id":"themes/minos/languages/ko.yml","hash":"1acf3f959f1d2b4f7a77e7e82851821aa8635362","modified":1538818976532},{"_id":"themes/minos/languages/zh-cn.yml","hash":"9c5a489b11a056d1ea7b9d4a0e127aef9e192ee4","modified":1538818976533},{"_id":"themes/minos/languages/ru.yml","hash":"8e5a58176bf943432ba6e4f1981d9b98fdea36a4","modified":1538818976533},{"_id":"themes/minos/source/css/insight.scss","hash":"f785fc6574d2853c660be39b2e3149d4846b577f","modified":1538818976541},{"_id":"themes/minos/source/css/style.scss","hash":"6b6c06a88e57c366a2bb665462e6366461b392a1","modified":1538818976541},{"_id":"themes/minos/source/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1538818976542},{"_id":"themes/minos/source/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1538818976542},{"_id":"themes/minos/source/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1538818976542},{"_id":"themes/minos/source/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1538818976542},{"_id":"themes/minos/source/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1538818976542},{"_id":"themes/minos/source/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1538818976543},{"_id":"themes/minos/layout/comment/changyan.ejs","hash":"9ccc7ec354b968e60bdcfcd1dba451d38de61f12","modified":1538818976533},{"_id":"themes/minos/layout/comment/disqus.ejs","hash":"a2becdc02214a673c804af93488489807fa2c99c","modified":1538818976534},{"_id":"themes/minos/layout/comment/facebook.ejs","hash":"e73b6f93d98b27ba9068c1685874ecccfbac737b","modified":1538818976534},{"_id":"themes/minos/layout/comment/gitment.ejs","hash":"430416210933b7edcbfcc67ede4aa55539da2750","modified":1538818976534},{"_id":"themes/minos/layout/comment/isso.ejs","hash":"cc6a43bd24be764086f88ad7c5c97ff04df87e0b","modified":1538818976534},{"_id":"themes/minos/layout/comment/livere.ejs","hash":"12ff9a345f6bba2f732f592e39508c2afde89b00","modified":1538818976534},{"_id":"themes/minos/layout/comment/valine.ejs","hash":"350f28986dd610ebdfdeb16dc618d1d034312af1","modified":1538818976534},{"_id":"themes/minos/layout/comment/youyan.ejs","hash":"3d6cf9c523a7a5510ec2864bb29f861f9bb78af3","modified":1538818976534},{"_id":"themes/minos/layout/plugins/gallery.ejs","hash":"7c2becafdf6b60e677cdd5756b9d55eba2af4944","modified":1538818976536},{"_id":"themes/minos/layout/plugins/google-analytics.ejs","hash":"2a9d944a60aff7df27def5215bdc071e605c3c42","modified":1538818976537},{"_id":"themes/minos/layout/plugins/mathjax.ejs","hash":"b460310078d3506dce8dccc67310e3b9b3c124a9","modified":1538818976537},{"_id":"themes/minos/layout/search/google-cse.ejs","hash":"a6bf5c30339735126efa7efa684f9eb14dd6136a","modified":1538818976537},{"_id":"themes/minos/layout/search/insight.ejs","hash":"6fb7d27ef40145d8587b46b44a43516135b5a81a","modified":1538818976537},{"_id":"themes/minos/source/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1538818976543},{"_id":"themes/minos/source/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1538818976543},{"_id":"themes/minos/layout/share/addthis.ejs","hash":"f1c5f337333009d5f00dfbac4864a16ef8f9cb8d","modified":1538818976538},{"_id":"themes/minos/layout/share/sharethis.ejs","hash":"4f2c40f790f3be0a4e79db04f02ea41ba2f4d4c0","modified":1538818976538},{"_id":"themes/minos/layout/common/article.ejs","hash":"45d276fb6bfcee6690cfffa7cbdec18709cd8766","modified":1538818976535},{"_id":"themes/minos/layout/common/footer.ejs","hash":"367c5f2e69c66d4d6fbd8beeade0b60024ce9e6e","modified":1538818976535},{"_id":"themes/minos/layout/common/head.ejs","hash":"7565aeb729327b872f3c6ee6aec7951d2d9a6270","modified":1540103000917},{"_id":"themes/minos/layout/common/languages.ejs","hash":"89665c656a1ffebc9c97f03e7f9c12dd1d90702a","modified":1538818976535},{"_id":"themes/minos/layout/common/navbar.ejs","hash":"965e931095896445a30aa404c7ff5295c127408a","modified":1538818976535},{"_id":"themes/minos/layout/common/paginator.ejs","hash":"8f5060e4c8a86a3f4e58455c41c98e831e23e4a4","modified":1538818976536},{"_id":"themes/minos/layout/common/scripts.ejs","hash":"7a5a5271930423b95046836597e30e31fa708f66","modified":1538818976536},{"_id":"public/atom.xml","hash":"5fb537ff1b99aa6242bcb91cd55407de79e1abb6","modified":1553339816466},{"_id":"public/sitemap.xml","hash":"383b809924667e47e068cc4da2b42e31acf94a5f","modified":1553339816467},{"_id":"public/content.json","hash":"7107dd7833ea9cd9017bebc5e94044ff8b100b39","modified":1553339817214},{"_id":"public/about/index.html","hash":"51d6aec71a7c0e74dc27e1b48378880f506446f6","modified":1553339817238},{"_id":"public/tags/index.html","hash":"166196584d52b62a4491f300825454fabfb98336","modified":1553339817238},{"_id":"public/categories/index.html","hash":"3fb1dc806ab3b1b498aada349fd2de5057a3cc75","modified":1553339817238},{"_id":"public/2017/10/22/paper-swish/index.html","hash":"37602faf79d5dd9aa2ea5c9634c86563cb1aa742","modified":1553339817238},{"_id":"public/2017/09/10/paper-imbalance/index.html","hash":"5ca282504b4860030c7bf6bea9c44ca943e6f862","modified":1553339817238},{"_id":"public/2017/08/23/paper-facebook/index.html","hash":"c1dfacd72960f3833a81247e0b693d8fba8fd893","modified":1553339817238},{"_id":"public/2017/07/26/other-hello/index.html","hash":"a74bf2b49b6812360cc95a6bc21693a49d238d1c","modified":1553339817238},{"_id":"public/archives/index.html","hash":"0e0f01336a24feae33b8b9998bbeb46f22f26688","modified":1553339817239},{"_id":"public/archives/page/2/index.html","hash":"3e8638bdfa28ecd6be46813ae7078aa939297fd6","modified":1553339817239},{"_id":"public/archives/page/3/index.html","hash":"2659fabc3a9ac81cd9a055bfae5e45b212460212","modified":1553339817239},{"_id":"public/archives/2017/index.html","hash":"0949cb9c68fb03087546f72c353c6caa22ba7ff9","modified":1553339817239},{"_id":"public/archives/2017/page/2/index.html","hash":"187788a903793402ad3842477ed85062cb6957eb","modified":1553339817239},{"_id":"public/archives/2017/07/index.html","hash":"86723ca776565b71e06b959cffe5c1930d3a07c3","modified":1553339817239},{"_id":"public/archives/2017/08/index.html","hash":"3c743c4b31240d1d0d91eb299ef5e5f8631a76de","modified":1553339817239},{"_id":"public/archives/2017/09/index.html","hash":"65b703bd60ce569cf405372fb0cf460728fafe21","modified":1553339817239},{"_id":"public/archives/2017/10/index.html","hash":"aee2b93d56e8b897ca3a876db16ab5177b5845c7","modified":1553339817239},{"_id":"public/archives/2017/11/index.html","hash":"10fb6ebb4bb94b68123ec042880957cdd52d06de","modified":1553339817239},{"_id":"public/archives/2018/index.html","hash":"85ab7e133aa15e425a54eb825bf7338d582c8e1c","modified":1553339817239},{"_id":"public/archives/2018/01/index.html","hash":"c251f6259b6b01634106e4373a159e043e2bddd1","modified":1553339817239},{"_id":"public/archives/2018/02/index.html","hash":"c2361341b288d65db96b4bd131b6daded7cb4c23","modified":1553339817239},{"_id":"public/archives/2018/09/index.html","hash":"51739728ac1ced53520238e9e6fcba6db3f1523c","modified":1553339817239},{"_id":"public/archives/2018/10/index.html","hash":"c67427ecd705bac8e3157124853eea3d9d2a64ee","modified":1553339817239},{"_id":"public/archives/2018/11/index.html","hash":"ef252a9aab2750c1abb43e9badb0d6e780877f35","modified":1553339817239},{"_id":"public/archives/2019/index.html","hash":"58caac788fd077d1c9d0fd4ec32f89e5f087105a","modified":1553339817239},{"_id":"public/archives/2019/03/index.html","hash":"be9735ad120d1dc6498d9d707a1d134a8607cfe0","modified":1553339817239},{"_id":"public/categories/machine-learning/index.html","hash":"d70dbf6f4135e9ad732d61081cf055c5f37b3297","modified":1553339817239},{"_id":"public/categories/others/index.html","hash":"0611ebd0c26d4d6966cc484c2da5aa977974f469","modified":1553339817239},{"_id":"public/categories/reading-notes/index.html","hash":"31995b08a26ddc3e1c469eb6c513fce9320c4e66","modified":1553339817240},{"_id":"public/page/5/index.html","hash":"31e84069d565229b79c5b9fd01ff09e18cd23274","modified":1553339817240},{"_id":"public/tags/regularization/index.html","hash":"e41e74384b950d62947b2c0aa94caf5f0392f18e","modified":1553339817240},{"_id":"public/tags/hyperparameter/index.html","hash":"5e89207661c634b92e5eb3c527434489f751c18a","modified":1553339817240},{"_id":"public/tags/batch-norm/index.html","hash":"cc68f8a1664ed908f8cb29a102a4a4df67e3b08b","modified":1553339817240},{"_id":"public/tags/covariate-shift/index.html","hash":"f671036fa8a0444fae40bff2b783ddcd4bd4cd59","modified":1553339817240},{"_id":"public/tags/moving-averages/index.html","hash":"0968f505010edda9046860251a8b2256dd9b3036","modified":1553339817240},{"_id":"public/tags/learning-strategy/index.html","hash":"ccc4fc98f3c4a7dd742c912b0e4baf3bff07a95e","modified":1553339817240},{"_id":"public/tags/orthogonalization/index.html","hash":"6a26b5c29497e39ba6f6b51fb8553dd4b1c3d87f","modified":1553339817240},{"_id":"public/tags/transfer-learning/index.html","hash":"da8582178aabe812aaa8c5cd9127fc73c2bb9e6d","modified":1553339817240},{"_id":"public/tags/multi-task-learning/index.html","hash":"929789ebf222fc962e5a741d040314ceb79ec02c","modified":1553339817240},{"_id":"public/tags/CNN/index.html","hash":"e11905ac38826d4aaa4e6ba8baf1d7a7e26a54ce","modified":1553339817240},{"_id":"public/tags/convex-optimization/index.html","hash":"2b3d1c22acf00880e09d0058185c991c7807a622","modified":1553339817240},{"_id":"public/tags/newton-s-method/index.html","hash":"79678242500d9c41627c25f48cd82a9b7d7c5c8c","modified":1553339817240},{"_id":"public/tags/unconstrained-optimization/index.html","hash":"cf62e76dc262ce4565b9b50646ac07edcdee8262","modified":1553339817240},{"_id":"public/tags/MAP/index.html","hash":"8a1a92c3fbc44bc8a514144457f15eb96b1559d9","modified":1553339817240},{"_id":"public/tags/ridge-regression/index.html","hash":"c2ae774b08334796fac7dfdd3170e45e75d7f80f","modified":1553339817240},{"_id":"public/tags/lasso-regression/index.html","hash":"f332ab06e61a4cb1916ed36c01317f797f96aa43","modified":1553339817241},{"_id":"public/tags/life/index.html","hash":"85f4e75a76595fe8337d1be8580e9ec1b49bbfc0","modified":1553339817241},{"_id":"public/tags/imbalanced-data/index.html","hash":"3ff582589bc851224b0ad8beaec2576ca881d4f2","modified":1553339817241},{"_id":"public/tags/gbt/index.html","hash":"62e6b1900ea498f4b6f90dbfe6d7db8eac0d3d1f","modified":1553339817241},{"_id":"public/tags/logistic-regression/index.html","hash":"35fcf84ff46cc36f8b7b449c0b7dad0691d8bdc4","modified":1553339817241},{"_id":"public/tags/undersampling/index.html","hash":"34e91afd37f5cf2ee44fcd1137ee6785dcd9feb0","modified":1553339817241},{"_id":"public/tags/bagging/index.html","hash":"a98d60d86b88c3cc7888e68c4bca63b445653c97","modified":1553339817241},{"_id":"public/tags/activtion-function/index.html","hash":"9847c7d1c0dc5eafe935f9adfc56de9b306d2075","modified":1553339817241},{"_id":"public/2018/11/14/spark-second-generation-tungsten-in-spark/index.html","hash":"eed23c366aca1de04b1e2caab5c2aca389a3519f","modified":1553339817241},{"_id":"public/2019/03/06/spark-sumup-part-3/index.html","hash":"b2e248f06bd13071b8895b64e29f8024dd7920d0","modified":1553339817241},{"_id":"public/2018/10/13/spark-sumup-part-2/index.html","hash":"80f33eeecbbec7560e6402f9e0005eba81df7932","modified":1553339817241},{"_id":"public/2018/09/25/spark-catalyst-optimization/index.html","hash":"71e908f713f67a842ca5572f9cd347bbea088d4a","modified":1553339817241},{"_id":"public/2018/09/22/spark-from-rdd-to-dataframe-dataset/index.html","hash":"c6f958f3b90545f1365e9f45ab8c6e7113eae74b","modified":1553339817241},{"_id":"public/2018/09/15/spark-sumup-part-1/index.html","hash":"a500b9c53d0dfcb73af49d46cd864872746a6ec9","modified":1553339817241},{"_id":"public/2018/02/23/spark-spark-tune/index.html","hash":"ac2602bb0fce2aadad1045aff9460a766fe6c4b4","modified":1553339817241},{"_id":"public/2018/01/07/spark-spark-workflow/index.html","hash":"88ab50d5244bf323c89b66f65782db1b51066140","modified":1553339817241},{"_id":"public/2017/11/29/course-deep-learning-course4-week2/index.html","hash":"fd7802c0bfb4af5ee33d768bb2c3ea446c8b3fb4","modified":1553339817242},{"_id":"public/2017/11/26/course-deep-learning-course4-week1/index.html","hash":"7f2c3b9ca1c76ffbe0c3a7c84c02139b276e9050","modified":1553339817242},{"_id":"public/2017/10/18/course-deep-learning-course3-week2/index.html","hash":"95b779dda878177b3fac49ee8e0d8df592a6a70e","modified":1553339817242},{"_id":"public/2017/10/12/course-deep-learning-course3-week1/index.html","hash":"bec02dab3920586ca913283933d6d6f5ba251e4d","modified":1553339817242},{"_id":"public/2017/11/11/ml-imbalanced-data-solution/index.html","hash":"1bf3f1804986672168b66a6803122ec11dabb957","modified":1553339817242},{"_id":"public/2017/09/30/course-deep-learning-course2-week3/index.html","hash":"fe9c2c060a205bb0491a494b46415461fa1b0f7f","modified":1553339817242},{"_id":"public/2017/09/27/course-deep-learning-course2-week2/index.html","hash":"8b75efed46d6a87f8ad3991531253b850a1009d3","modified":1553339817242},{"_id":"public/2017/09/24/course-deep-learning-course2-week1/index.html","hash":"f3e17e254a72bbaaa1a7eeaf9bf4dc332f7ccc6c","modified":1553339817242},{"_id":"public/2017/08/27/ml-ridge-lasso/index.html","hash":"f3c8a3f01ea331fcb13d02e13c9637ca5987490a","modified":1553339817242},{"_id":"public/2017/08/11/ml-gd-and-nm/index.html","hash":"71f6d5fd839080b4031c20eb8582353f76fe41d1","modified":1553339817242},{"_id":"public/2017/08/02/ml-convex-opt/index.html","hash":"0abea30a0729e051de10860cbc0ea9617fcc22bf","modified":1553339817242},{"_id":"public/categories/learning-notes/index.html","hash":"7786983e53cb6715b43a3d2d86b6d7a78d1cab67","modified":1553339817242},{"_id":"public/categories/spark/index.html","hash":"907093b9d7f5a7fa56ad17619f268da7358bf18b","modified":1553339817242},{"_id":"public/page/2/index.html","hash":"3039b9071379041f1e87311134eebd38285c96f7","modified":1553339817242},{"_id":"public/index.html","hash":"e0715036944054119c6c364a2a69bac76a1c9a1c","modified":1553339817242},{"_id":"public/page/3/index.html","hash":"8f0f95b644e68dbf8e240a9c57ee4e220e463b8f","modified":1553339817242},{"_id":"public/page/4/index.html","hash":"696698c387221fdc4889cc6acedff75ac8b511fe","modified":1553339817243},{"_id":"public/tags/gradient-descent/index.html","hash":"db900d949ab1a9f7e45fb84799f7f37870998dca","modified":1553339817243},{"_id":"public/tags/spark/index.html","hash":"3b131ceb6ef192531cff72d7da8b8f91fbe6395b","modified":1553339817243},{"_id":"public/favicon.ico","hash":"7be62720671a143fb8b79a1a196730df57fee81d","modified":1553339817246},{"_id":"public/images/exclamation.svg","hash":"b2db56f2cc13fce73dbea46c7b446d9bcb3bf0fd","modified":1553339817246},{"_id":"public/images/check.svg","hash":"029b8b3523b7daa4005983b4463cd93408308aab","modified":1553339817246},{"_id":"public/images/info.svg","hash":"c8aa387e935ba9a7fa72c5dd000b7d46f2e030c4","modified":1553339817246},{"_id":"public/images/logo.png","hash":"4e012d9ba58cb8f87ee775262ef871c158ac5948","modified":1553339817246},{"_id":"public/images/question.svg","hash":"7153fa2a0c21e32da6a1f96a333d8b66a178569d","modified":1553339817246},{"_id":"public/images/quote-left.svg","hash":"d2561fa8d13e63ff196b71232a5968415ec6e372","modified":1553339817246},{"_id":"public/css/style.css","hash":"c784fe865fc3abf5455e419bfac380dbbdb36a7d","modified":1553339817263},{"_id":"public/css/insight.css","hash":"f376dcda6bb50b708f3206c15a49f7530b3c534d","modified":1553339817263},{"_id":"public/js/script.js","hash":"6b670ec4f90fb43b21a0bbd750a217af5d8aab6b","modified":1553339817263},{"_id":"public/js/insight.js","hash":"eb23c31141784eef7300f1d1c548950e77883f56","modified":1553339817263}],"Category":[{"name":"learning notes","_id":"cjtlef4ow0004tr8lo9flocbb"},{"name":"machine learning","_id":"cjtlef4pa000otr8lhr1q0vw5"},{"name":"others","_id":"cjtlef4pj0018tr8lotbkdqm2"},{"name":"reading notes","_id":"cjtlef4pl001ftr8lcybt42rw"},{"name":"spark","_id":"cjtlef4pr001vtr8lne0a1t2v"}],"Data":[],"Page":[{"title":"About","date":"2017-07-25T23:25:29.000Z","type":"about","comments":0,"_content":"Welome to **superAsir's Notes**. \nAll the cover images are downloaded from [PEXELS](https://www.pexels.com/).\n\n* ğŸ“– M.E. from [BUAA](http://ev.buaa.edu.cn/)\n* ğŸ’» Software Developer/Data Scientist\n* ğŸ‡¨ğŸ‡³ Beijing, China\n* ğŸ’¡ Machine learning, deep learning and distributed computing \n* ğŸ”´ COME ON YOU GOONERS!\n* ğŸ¸ Punk's not dead \n* ğŸ¥ƒ Prefer rum to vodka\n* ğŸº Prefer Tsingtao to Corona\n* â˜•ï¸ Could't love CaffÃ¨ Americano more\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2017-07-26 07:25:29\ntype: \"about\"\ncomments: false\n---\nWelome to **superAsir's Notes**. \nAll the cover images are downloaded from [PEXELS](https://www.pexels.com/).\n\n* ğŸ“– M.E. from [BUAA](http://ev.buaa.edu.cn/)\n* ğŸ’» Software Developer/Data Scientist\n* ğŸ‡¨ğŸ‡³ Beijing, China\n* ğŸ’¡ Machine learning, deep learning and distributed computing \n* ğŸ”´ COME ON YOU GOONERS!\n* ğŸ¸ Punk's not dead \n* ğŸ¥ƒ Prefer rum to vodka\n* ğŸº Prefer Tsingtao to Corona\n* â˜•ï¸ Could't love CaffÃ¨ Americano more\n","updated":"2018-11-21T12:24:54.643Z","path":"about/index.html","layout":"page","_id":"cjtlef4oq0000tr8luvn1xwxc","content":"<p>Welome to <strong>superAsirâ€™s Notes</strong>.<br>All the cover images are downloaded from <a href=\"https://www.pexels.com/\" target=\"_blank\" rel=\"noopener\">PEXELS</a>.</p>\n<ul>\n<li>ğŸ“– M.E. from <a href=\"http://ev.buaa.edu.cn/\" target=\"_blank\" rel=\"noopener\">BUAA</a></li>\n<li>ğŸ’» Software Developer/Data Scientist</li>\n<li>ğŸ‡¨ğŸ‡³ Beijing, China</li>\n<li>ğŸ’¡ Machine learning, deep learning and distributed computing </li>\n<li>ğŸ”´ COME ON YOU GOONERS!</li>\n<li>ğŸ¸ Punkâ€™s not dead </li>\n<li>ğŸ¥ƒ Prefer rum to vodka</li>\n<li>ğŸº Prefer Tsingtao to Corona</li>\n<li>â˜•ï¸ Couldâ€™t love CaffÃ¨ Americano more</li>\n</ul>\n","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":"<p>Welome to <strong>superAsirâ€™s Notes</strong>.<br>All the cover images are downloaded from <a href=\"https://www.pexels.com/\" target=\"_blank\" rel=\"noopener\">PEXELS</a>.</p>\n<ul>\n<li>ğŸ“– M.E. from <a href=\"http://ev.buaa.edu.cn/\" target=\"_blank\" rel=\"noopener\">BUAA</a></li>\n<li>ğŸ’» Software Developer/Data Scientist</li>\n<li>ğŸ‡¨ğŸ‡³ Beijing, China</li>\n<li>ğŸ’¡ Machine learning, deep learning and distributed computing </li>\n<li>ğŸ”´ COME ON YOU GOONERS!</li>\n<li>ğŸ¸ Punkâ€™s not dead </li>\n<li>ğŸ¥ƒ Prefer rum to vodka</li>\n<li>ğŸº Prefer Tsingtao to Corona</li>\n<li>â˜•ï¸ Couldâ€™t love CaffÃ¨ Americano more</li>\n</ul>\n"},{"title":"Tags","date":"2017-07-25T23:25:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags \ndate: 2017-07-26 07:25:21\ntype: tags\ncomments: false\n---\n","updated":"2018-08-27T03:31:48.797Z","path":"tags/index.html","layout":"page","_id":"cjtlef4ou0002tr8l4cus2wfd","content":"","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":""},{"title":"Categories","date":"2017-07-25T23:25:10.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2017-07-26 07:25:10\ntype: categories\ncomments: false\n---\n","updated":"2018-08-27T03:31:48.796Z","path":"categories/index.html","layout":"page","_id":"cjtlef4oy0006tr8lh0z6z7jj","content":"","site":{"data":{}},"_categories":[],"_tags":[],"excerpt":"","more":""}],"Post":[{"title":"Learning Notes-Deep Learning, course2, week1","date":"2017-09-24T06:06:08.000Z","_content":"å¤§å®¶å¥½ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Andrew Ngçš„Deep learningè¯¾ç¨‹ï¼Œäºæ˜¯å†³å®šå†™ä¸€äº›learning notesæ¥recapå’Œmarkä¸€ä¸‹å­¦åˆ°çš„çŸ¥è¯†ï¼Œé¿å…é—å¿˜ã€‚ç”±äºè¯¥è¯¾ç¨‹çš„course1æ¯”è¾ƒåŸºç¡€ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰markçš„å¿…è¦ï¼Œæ‰€ä»¥ä»course2å¼€å§‹ï¼ŒæŒ‰ç…§weekæ¥mark.\n<!--more-->\n## Data set\nåœ¨machine learningä¸­ï¼Œdata setå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿmachine learningï¼Œdeep learningä¸­çš„data setåˆ†å¸ƒæ›´ä¾§é‡äºtrainingï¼ŒNgå»ºè®®æˆ‘ä»¬è®²data setåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š\n* training setâ€”â€”è®­ç»ƒæ•°æ®é›†\n* dev/validation setâ€”â€”æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•\n* testing setâ€”â€”æ¨¡å‹æ•ˆæœæµ‹è¯•\nä¸€å®šæœ‰å¾ˆå¤šäººå¯¹äºdevå’Œtesting setæœ‰ä¸€äº›ç–‘é—®ï¼Œæœ€å¼€å§‹æˆ‘ä¹Ÿæ˜¯æ‡µé€¼çš„ï¼Œæ¥çœ‹çœ‹ä¸‹é¢è¿™æ®µè¯\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\nè¿™ä¸‰è€…çš„æ¯”ä¾‹åˆ™æ˜¯/99.5%/2.5%/2.5%/ï¼Œè¿™æ ·çš„åŸå› æ˜¯å› ä¸ºdeep learningä¸­ï¼Œæ•°æ®é‡è¶³å¤Ÿå¤§è€Œä¸”deep learningçš„å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œå¤§å®¶ä¸€å®šæ³¨æ„è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœå®åœ¨æ²¡æœ‰test setï¼Œä½†æ˜¯æœ‰dev setä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚\n## Bias and variance\n### ä»€ä¹ˆæ˜¯biaså’Œvariance\nbias & varianceæ˜¯machine learning é¢†åŸŸä¸€ä¸ªç»å…¸çš„è¾©è¯é—®é¢˜ï¼Œåœ¨Ngç»å…¸çš„CS229ä¸­å°±é‡ç‚¹çš„è®²è¿°è¿‡ï¼Œå…·ä½“çš„å®šä¹‰æˆ‘ä¸å¤ªæƒ³ç»™å‡ºäº†ï¼Œåç»­æœ‰æ—¶é—´å¯ä»¥ä¸“é—¨å†™ä¸€ç¯‡ï¼Œåé¢ä¼šç»™å‡ºä¸€äº›èµ„æ–™é“¾æ¥ã€‚æˆ‘ä»¬ç®€å•çš„çœ‹ä¸€å¹…å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png)\nå·¦å›¾å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„high bias situationï¼Œæ¨¡å‹æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„under fittingï¼Œå³å›¾åˆ™æ˜¯å…¸å‹çš„high variance situationï¼Œæ¨¡å‹è¿‡åˆ†çš„æ‹Ÿåˆäº†training setï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æœ€éœ€è¦é˜²èŒƒçš„over fitting.å½“ç„¶ï¼Œä¸­é—´çš„åˆ™æ˜¯æ¯”è¾ƒç†æƒ³çš„çŠ¶å†µã€‚\n### Solution\nåœ¨å®é™…çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåˆ†æè‡ªå·±æ¨¡å‹çš„biaså’Œvarianceæƒ…å†µå‘¢ï¼ŒNgç»™äº†æˆ‘ä»¬ä¸€ä¸ªæµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png)\né¦–å…ˆæ£€éªŒæ˜¯å¦å­˜åœ¨high bias æƒ…å†µï¼Œå…·ä½“æ–¹æ³•æ˜¯åœ¨training set å’Œ dev setä¸Šè®¡ç®—errorï¼Œå¯¹æ¯”training errorå’Œdev errorï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆé«˜ï¼Œé‚£ä¹ˆå°±æ˜¯high biasï¼Œå¦‚æœtraining errorå¾ˆå°è€Œdev errorå¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯high varianceï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆå¤§ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å·®çš„æƒ…å†µäº†æ—¢high biasåˆhigh variance\n\nå¯¹äºhigh biasï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ›´å¼ºçš„ç½‘ç»œç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›\nå¯¹äºhigh varianceï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„æ•°æ®ï¼Œregularizationçš„æ–¹æ³•æ¥è§£å†³ã€‚\n## Regularization\n### L1&L2 regularization\nè¿™éƒ¨åˆ†å†…å®¹æˆ‘å°±ä¸å¤šè¯´äº†ï¼Œæˆ‘ä¹‹å‰ä¸“é—¨è¯¦ç»†æ·±å…¥çš„è®²è¿°è¿‡L1å’ŒL2 regularizationï¼Œå¤§å®¶å¯ä»¥å»çœ‹ä¸€çœ‹ã€‚\n\nå”¯ä¸€éœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨åŠ å…¥L1æˆ–è€…L2 regularizationä¹‹åï¼Œåœ¨è§‚æµ‹cost function convergence çš„æ—¶å€™ï¼Œ**ä¸€å®šè¦å¸¦ä¸Šregularization item**ï¼Œå¦åˆ™ç»“æœæ˜¯å¾ˆéš¾çœ‹åˆ°convergenceçš„ï¼Œè¿™å’Œregularizationæ€§è´¨æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚ \n### Dropout\nDropoutæ˜¯neural networkä¸­ä¸€ç§ç»å…¸çš„regularizationæ–¹æ³•ï¼Œç»å…¸åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Œæˆ‘å½“å¹´æ¯•è®¾è¯¾é¢˜ä¸­éƒ½ç”¨åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœè¶…èµ\n\nDropoutæ–¹æ³•çš„å®è´¨æ˜¯**æŒ‰æ¯”ä¾‹éšæœºéšè—**æ‰neural networkä¸­layeré‡Œçš„æŸäº›unitsï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†ä¸€æ¬¡epochä¸­ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çš„unitså¯¹åº”çš„weightså’Œbiasä¼šå¾—åˆ°æ›´æ–°ï¼Œè€Œä¸‹ä¸€æ¬¡epochä¸­ï¼Œåˆ™æ˜¯å¦ä¸€éƒ¨åˆ†unitså¯¹åº”çš„weightså’Œbiaså¾—åˆ°æ›´æ–°ï¼Œå¦‚ä¸‹å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png)\né‚£ä¹ˆä¸ºä»€ä¹ˆDropoutå¯ä»¥å®ç°regularizationæ•ˆæœå‘¢ï¼ŒNgå‘Šè¯‰æˆ‘ä»¬ï¼š\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\nå¦‚ä½•ç†è§£å‘¢ï¼ŸåŠ å…¥dropoutåï¼Œæ¯ä¸ªunitå¯¹åº”çš„weightså’Œbiasä¸èƒ½å®Œå…¨ä¾èµ–ä¸Šå±‚unitsï¼Œå› ä¸ºä»–å¹¶ä¸æ˜¯æ¯ä¸€æ¬¡epochéƒ½å¯ä»¥work onï¼Œå› æ­¤åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œè§ç¬‘äº†over fittingçš„é£é™©ã€‚å®é™…ä¸Šï¼Œdropoutå¯ä»¥äº§ç”Ÿshrink weightsçš„æ•ˆæœï¼Œå’ŒL2 regularizationç›¸ä¼¼ï¼Œå› æ­¤ä¹Ÿæ˜¯ä¸€ç§regularizationæ–¹æ³•ã€‚\n\nä½†æ˜¯ï¼Œdropoutå’ŒL2 regularizationå”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–å¾ˆéš¾ç»™å‡ºä¸€ä¸ªregularization itemï¼Œæ‰€ä»¥ä½ æ²¡æœ‰åŠæ³•ç”»å‡ºcost function convergenceçš„è½¨è¿¹ã€‚\n### Other methods\né™¤äº†ç»å…¸çš„L1ã€L2 regularizationå’Œdropoutæ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›é˜²æ­¢over fittingçš„æ–¹æ³•ï¼Œä¾‹å¦‚å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨data augmentationï¼Œæ—‹è½¬ï¼Œç¿»è½¬ï¼ŒåŠ å™ªå£°ç­‰æ–¹æ³•ã€‚\n\nè¿˜æœ‰ä¸€ä¸ªearly stoppingæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œéšç€training çš„epochå¢å¤šï¼Œæ¨¡å‹å¯¹training setæ‹Ÿåˆä¼šè¶Šæ¥è¶Šå¥½ï¼Œéšä¹‹å¸¦æ¥çš„é—®é¢˜å°±æ˜¯å¯èƒ½over fittingï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡early stoppingï¼Œè®©æ¨¡å‹åœ¨æ²¡æœ‰äº§ç”Ÿover fittingçš„æ—¶å€™åœä¸‹æ¥ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚\n## Exploding/vanishing gradient\n### ä»€ä¹ˆæ˜¯exploding/vanishing gradient\nå¯¹äºdeep learningï¼Œæ›¾ç»æœ€ä¸ºæ£˜æ‰‹çš„é—®é¢˜å°±æ˜¯exploding/vanishing gradientï¼Œç”šè‡³æ˜¯é™åˆ¶deep learningå‘å±•çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚\n å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª**æ¯”è¾ƒæ·±**çš„neural networkï¼Œå‡è®¾ä¸€å…±æœ‰\\\\(l\\\\)å±‚ï¼Œå¯¹åº”çš„weightsæ˜¯\\\\(W^{[1]}\\\\)åˆ°\\\\(W^{[l]}\\\\)ï¼Œbiasæ˜¯\\\\(b^{[1]}\\\\)åˆ°\\\\(b^{[l]}\\\\)ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œå‡è®¾biaså‡ä¸º0ï¼Œactive functionä¸º\\\\(g(z)=z\\\\)ï¼Œé‚£ä¹ˆï¼Œ\\\\(y\\\\)å°±ç­‰äº\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\nå¤§å®¶æ„Ÿå…´è¶£çš„è¯å¯ä»¥éªŒè¯ä¸€ä¸‹ï¼Œå¾ˆç®€å•çš„ã€‚\n\né‚£ä¹ˆç°åœ¨é—®é¢˜æ¥äº†ï¼Œå½“\\\\(l\\\\)å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ\\\\(W\\\\)å…ƒç´ éƒ½å¤§äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šéå¸¸éå¸¸å¤§ï¼Œç”šè‡³åˆ°æ— é™å¤§ï¼Œè¿™ç§æƒ…å†µå«exploding gradientï¼›ç›¸åº”çš„ï¼Œå¦‚æœ\\\\(W\\\\)å…ƒç´ éƒ½å°äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šç‰¹åˆ«å°ï¼Œç”šè‡³ä¸ºé›¶ï¼Œè¿™å°±æ˜¯vanishing gradient.\n### Solution\nå¯¹äºä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨weightsåˆå§‹åŒ–çš„æ—¶å€™åšä¸€äº›å·¥ä½œæ¥è§£å†³å¯èƒ½å‡ºç°çš„exploding or vanishing gradientã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå¯¹äºactive function\\\\(g(z)\\\\)ï¼Œå‡è®¾biasä¸ºé›¶ï¼Œ\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\næˆ‘ä»¬è¦æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\\\(n\\\\)çš„å¢å¤§ï¼Œ\\\\(w\\\\)ä¼šå˜å°ï¼Œæˆ‘ä»¬è®©\\\\(w\\\\)å§‹ç»ˆä¿æŒä»¥0ä¸ºmeanï¼Œ1ä¸ºvaranceçš„Gaussian distributionä¸‹ï¼Œå°±å¯ä»¥å¾ˆå¥½çš„æ§åˆ¶\\\\(w\\\\)çš„å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\nåœ¨Ngçš„å»ºè®®ä¸­ï¼Œå¦‚æœactive functionæ˜¯sigmoidï¼Œæˆ‘ä»¬ä¸€èˆ¬å–\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)ï¼Œå¦‚æœæ˜¯reLuï¼Œæˆ‘ä»¬å–\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)ï¼Œå¯¹äºtanhï¼Œ\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)æˆ–è€…\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¥½çš„é¿å…exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\nåœ¨è°ƒè¯•neural networkçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åšgradient checkçš„å·¥ä½œï¼Œä»¥ç¡®å®šæ•´ä¸ªnetworkæ­£å¸¸çš„è¿è¡Œï¼ŒNgåœ¨è¿™é‡Œå»ºè®®æˆ‘ä»¬ä½¿ç”¨åŒè¾¹é€¼è¿‘çš„æ–¹æ³•å»åšgradient checkï¼Œè¿™é‡Œæˆ‘ä¸åšå¤ªå¤šæè¿°ï¼Œä¸»è¦ä¸Šä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png)\né€šå¸¸æ¥è¯´ï¼ŒåŒè¾¹é€¼è¿‘çš„æ–¹æ³•è·å¾—ç»“æœæ›´åŠ å‡†ç¡®ã€‚\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","source":"_posts/course-deep-learning-course2-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week1\ndate: 2017-09-24 14:06:08\ntags: \n\t- regularization\n\t- gradient descent\ncategories: learning notes\n---\nå¤§å®¶å¥½ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Andrew Ngçš„Deep learningè¯¾ç¨‹ï¼Œäºæ˜¯å†³å®šå†™ä¸€äº›learning notesæ¥recapå’Œmarkä¸€ä¸‹å­¦åˆ°çš„çŸ¥è¯†ï¼Œé¿å…é—å¿˜ã€‚ç”±äºè¯¥è¯¾ç¨‹çš„course1æ¯”è¾ƒåŸºç¡€ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰markçš„å¿…è¦ï¼Œæ‰€ä»¥ä»course2å¼€å§‹ï¼ŒæŒ‰ç…§weekæ¥mark.\n<!--more-->\n## Data set\nåœ¨machine learningä¸­ï¼Œdata setå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿmachine learningï¼Œdeep learningä¸­çš„data setåˆ†å¸ƒæ›´ä¾§é‡äºtrainingï¼ŒNgå»ºè®®æˆ‘ä»¬è®²data setåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š\n* training setâ€”â€”è®­ç»ƒæ•°æ®é›†\n* dev/validation setâ€”â€”æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•\n* testing setâ€”â€”æ¨¡å‹æ•ˆæœæµ‹è¯•\nä¸€å®šæœ‰å¾ˆå¤šäººå¯¹äºdevå’Œtesting setæœ‰ä¸€äº›ç–‘é—®ï¼Œæœ€å¼€å§‹æˆ‘ä¹Ÿæ˜¯æ‡µé€¼çš„ï¼Œæ¥çœ‹çœ‹ä¸‹é¢è¿™æ®µè¯\n> Dev/Validation Set: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\nTesting Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.\n\nè¿™ä¸‰è€…çš„æ¯”ä¾‹åˆ™æ˜¯/99.5%/2.5%/2.5%/ï¼Œè¿™æ ·çš„åŸå› æ˜¯å› ä¸ºdeep learningä¸­ï¼Œæ•°æ®é‡è¶³å¤Ÿå¤§è€Œä¸”deep learningçš„å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œå¤§å®¶ä¸€å®šæ³¨æ„è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœå®åœ¨æ²¡æœ‰test setï¼Œä½†æ˜¯æœ‰dev setä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚\n## Bias and variance\n### ä»€ä¹ˆæ˜¯biaså’Œvariance\nbias & varianceæ˜¯machine learning é¢†åŸŸä¸€ä¸ªç»å…¸çš„è¾©è¯é—®é¢˜ï¼Œåœ¨Ngç»å…¸çš„CS229ä¸­å°±é‡ç‚¹çš„è®²è¿°è¿‡ï¼Œå…·ä½“çš„å®šä¹‰æˆ‘ä¸å¤ªæƒ³ç»™å‡ºäº†ï¼Œåç»­æœ‰æ—¶é—´å¯ä»¥ä¸“é—¨å†™ä¸€ç¯‡ï¼Œåé¢ä¼šç»™å‡ºä¸€äº›èµ„æ–™é“¾æ¥ã€‚æˆ‘ä»¬ç®€å•çš„çœ‹ä¸€å¹…å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png)\nå·¦å›¾å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„high bias situationï¼Œæ¨¡å‹æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„under fittingï¼Œå³å›¾åˆ™æ˜¯å…¸å‹çš„high variance situationï¼Œæ¨¡å‹è¿‡åˆ†çš„æ‹Ÿåˆäº†training setï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æœ€éœ€è¦é˜²èŒƒçš„over fitting.å½“ç„¶ï¼Œä¸­é—´çš„åˆ™æ˜¯æ¯”è¾ƒç†æƒ³çš„çŠ¶å†µã€‚\n### Solution\nåœ¨å®é™…çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåˆ†æè‡ªå·±æ¨¡å‹çš„biaså’Œvarianceæƒ…å†µå‘¢ï¼ŒNgç»™äº†æˆ‘ä»¬ä¸€ä¸ªæµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png)\né¦–å…ˆæ£€éªŒæ˜¯å¦å­˜åœ¨high bias æƒ…å†µï¼Œå…·ä½“æ–¹æ³•æ˜¯åœ¨training set å’Œ dev setä¸Šè®¡ç®—errorï¼Œå¯¹æ¯”training errorå’Œdev errorï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆé«˜ï¼Œé‚£ä¹ˆå°±æ˜¯high biasï¼Œå¦‚æœtraining errorå¾ˆå°è€Œdev errorå¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯high varianceï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆå¤§ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å·®çš„æƒ…å†µäº†æ—¢high biasåˆhigh variance\n\nå¯¹äºhigh biasï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ›´å¼ºçš„ç½‘ç»œç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›\nå¯¹äºhigh varianceï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„æ•°æ®ï¼Œregularizationçš„æ–¹æ³•æ¥è§£å†³ã€‚\n## Regularization\n### L1&L2 regularization\nè¿™éƒ¨åˆ†å†…å®¹æˆ‘å°±ä¸å¤šè¯´äº†ï¼Œæˆ‘ä¹‹å‰ä¸“é—¨è¯¦ç»†æ·±å…¥çš„è®²è¿°è¿‡L1å’ŒL2 regularizationï¼Œå¤§å®¶å¯ä»¥å»çœ‹ä¸€çœ‹ã€‚\n\nå”¯ä¸€éœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨åŠ å…¥L1æˆ–è€…L2 regularizationä¹‹åï¼Œåœ¨è§‚æµ‹cost function convergence çš„æ—¶å€™ï¼Œ**ä¸€å®šè¦å¸¦ä¸Šregularization item**ï¼Œå¦åˆ™ç»“æœæ˜¯å¾ˆéš¾çœ‹åˆ°convergenceçš„ï¼Œè¿™å’Œregularizationæ€§è´¨æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚ \n### Dropout\nDropoutæ˜¯neural networkä¸­ä¸€ç§ç»å…¸çš„regularizationæ–¹æ³•ï¼Œç»å…¸åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Œæˆ‘å½“å¹´æ¯•è®¾è¯¾é¢˜ä¸­éƒ½ç”¨åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœè¶…èµ\n\nDropoutæ–¹æ³•çš„å®è´¨æ˜¯**æŒ‰æ¯”ä¾‹éšæœºéšè—**æ‰neural networkä¸­layeré‡Œçš„æŸäº›unitsï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†ä¸€æ¬¡epochä¸­ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çš„unitså¯¹åº”çš„weightså’Œbiasä¼šå¾—åˆ°æ›´æ–°ï¼Œè€Œä¸‹ä¸€æ¬¡epochä¸­ï¼Œåˆ™æ˜¯å¦ä¸€éƒ¨åˆ†unitså¯¹åº”çš„weightså’Œbiaså¾—åˆ°æ›´æ–°ï¼Œå¦‚ä¸‹å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png)\né‚£ä¹ˆä¸ºä»€ä¹ˆDropoutå¯ä»¥å®ç°regularizationæ•ˆæœå‘¢ï¼ŒNgå‘Šè¯‰æˆ‘ä»¬ï¼š\n> Intuition:Can't rely on any one feature, so have to spread out weights\n\nå¦‚ä½•ç†è§£å‘¢ï¼ŸåŠ å…¥dropoutåï¼Œæ¯ä¸ªunitå¯¹åº”çš„weightså’Œbiasä¸èƒ½å®Œå…¨ä¾èµ–ä¸Šå±‚unitsï¼Œå› ä¸ºä»–å¹¶ä¸æ˜¯æ¯ä¸€æ¬¡epochéƒ½å¯ä»¥work onï¼Œå› æ­¤åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œè§ç¬‘äº†over fittingçš„é£é™©ã€‚å®é™…ä¸Šï¼Œdropoutå¯ä»¥äº§ç”Ÿshrink weightsçš„æ•ˆæœï¼Œå’ŒL2 regularizationç›¸ä¼¼ï¼Œå› æ­¤ä¹Ÿæ˜¯ä¸€ç§regularizationæ–¹æ³•ã€‚\n\nä½†æ˜¯ï¼Œdropoutå’ŒL2 regularizationå”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–å¾ˆéš¾ç»™å‡ºä¸€ä¸ªregularization itemï¼Œæ‰€ä»¥ä½ æ²¡æœ‰åŠæ³•ç”»å‡ºcost function convergenceçš„è½¨è¿¹ã€‚\n### Other methods\né™¤äº†ç»å…¸çš„L1ã€L2 regularizationå’Œdropoutæ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›é˜²æ­¢over fittingçš„æ–¹æ³•ï¼Œä¾‹å¦‚å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨data augmentationï¼Œæ—‹è½¬ï¼Œç¿»è½¬ï¼ŒåŠ å™ªå£°ç­‰æ–¹æ³•ã€‚\n\nè¿˜æœ‰ä¸€ä¸ªearly stoppingæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œéšç€training çš„epochå¢å¤šï¼Œæ¨¡å‹å¯¹training setæ‹Ÿåˆä¼šè¶Šæ¥è¶Šå¥½ï¼Œéšä¹‹å¸¦æ¥çš„é—®é¢˜å°±æ˜¯å¯èƒ½over fittingï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡early stoppingï¼Œè®©æ¨¡å‹åœ¨æ²¡æœ‰äº§ç”Ÿover fittingçš„æ—¶å€™åœä¸‹æ¥ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚\n## Exploding/vanishing gradient\n### ä»€ä¹ˆæ˜¯exploding/vanishing gradient\nå¯¹äºdeep learningï¼Œæ›¾ç»æœ€ä¸ºæ£˜æ‰‹çš„é—®é¢˜å°±æ˜¯exploding/vanishing gradientï¼Œç”šè‡³æ˜¯é™åˆ¶deep learningå‘å±•çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚\n å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª**æ¯”è¾ƒæ·±**çš„neural networkï¼Œå‡è®¾ä¸€å…±æœ‰\\\\(l\\\\)å±‚ï¼Œå¯¹åº”çš„weightsæ˜¯\\\\(W^{[1]}\\\\)åˆ°\\\\(W^{[l]}\\\\)ï¼Œbiasæ˜¯\\\\(b^{[1]}\\\\)åˆ°\\\\(b^{[l]}\\\\)ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œå‡è®¾biaså‡ä¸º0ï¼Œactive functionä¸º\\\\(g(z)=z\\\\)ï¼Œé‚£ä¹ˆï¼Œ\\\\(y\\\\)å°±ç­‰äº\n$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$\nå¤§å®¶æ„Ÿå…´è¶£çš„è¯å¯ä»¥éªŒè¯ä¸€ä¸‹ï¼Œå¾ˆç®€å•çš„ã€‚\n\né‚£ä¹ˆç°åœ¨é—®é¢˜æ¥äº†ï¼Œå½“\\\\(l\\\\)å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ\\\\(W\\\\)å…ƒç´ éƒ½å¤§äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šéå¸¸éå¸¸å¤§ï¼Œç”šè‡³åˆ°æ— é™å¤§ï¼Œè¿™ç§æƒ…å†µå«exploding gradientï¼›ç›¸åº”çš„ï¼Œå¦‚æœ\\\\(W\\\\)å…ƒç´ éƒ½å°äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šç‰¹åˆ«å°ï¼Œç”šè‡³ä¸ºé›¶ï¼Œè¿™å°±æ˜¯vanishing gradient.\n### Solution\nå¯¹äºä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨weightsåˆå§‹åŒ–çš„æ—¶å€™åšä¸€äº›å·¥ä½œæ¥è§£å†³å¯èƒ½å‡ºç°çš„exploding or vanishing gradientã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå¯¹äºactive function\\\\(g(z)\\\\)ï¼Œå‡è®¾biasä¸ºé›¶ï¼Œ\n$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$\næˆ‘ä»¬è¦æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\\\(n\\\\)çš„å¢å¤§ï¼Œ\\\\(w\\\\)ä¼šå˜å°ï¼Œæˆ‘ä»¬è®©\\\\(w\\\\)å§‹ç»ˆä¿æŒä»¥0ä¸ºmeanï¼Œ1ä¸ºvaranceçš„Gaussian distributionä¸‹ï¼Œå°±å¯ä»¥å¾ˆå¥½çš„æ§åˆ¶\\\\(w\\\\)çš„å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)\n\nåœ¨Ngçš„å»ºè®®ä¸­ï¼Œå¦‚æœactive functionæ˜¯sigmoidï¼Œæˆ‘ä»¬ä¸€èˆ¬å–\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)ï¼Œå¦‚æœæ˜¯reLuï¼Œæˆ‘ä»¬å–\\\\(var(w)= \\frac{2}{n^{l-1}}\\\\)ï¼Œå¯¹äºtanhï¼Œ\\\\(var(w)= \\frac{1}{n^{l-1}}\\\\)æˆ–è€…\\\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\\\)ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¥½çš„é¿å…exploding or vanishing gradient.\n## Gradient checking\n### Gradient approximation\nåœ¨è°ƒè¯•neural networkçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åšgradient checkçš„å·¥ä½œï¼Œä»¥ç¡®å®šæ•´ä¸ªnetworkæ­£å¸¸çš„è¿è¡Œï¼ŒNgåœ¨è¿™é‡Œå»ºè®®æˆ‘ä»¬ä½¿ç”¨åŒè¾¹é€¼è¿‘çš„æ–¹æ³•å»åšgradient checkï¼Œè¿™é‡Œæˆ‘ä¸åšå¤ªå¤šæè¿°ï¼Œä¸»è¦ä¸Šä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png)\né€šå¸¸æ¥è¯´ï¼ŒåŒè¾¹é€¼è¿‘çš„æ–¹æ³•è·å¾—ç»“æœæ›´åŠ å‡†ç¡®ã€‚\n### Gradient checking notes\n> 1. Don't use in training-only to debug(too slow)\n2. If algorithm fails grad check, look at components to try to identify bug\n3. Remeber regularization\n4. Dosen't wrok with dropout\n5. Run at random initialzation; perhaps again after some training.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [Bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n","slug":"course-deep-learning-course2-week1","published":1,"updated":"2018-11-19T06:37:13.686Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4os0001tr8lamqrp5a4","content":"<p>å¤§å®¶å¥½ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Andrew Ngçš„Deep learningè¯¾ç¨‹ï¼Œäºæ˜¯å†³å®šå†™ä¸€äº›learning notesæ¥recapå’Œmarkä¸€ä¸‹å­¦åˆ°çš„çŸ¥è¯†ï¼Œé¿å…é—å¿˜ã€‚ç”±äºè¯¥è¯¾ç¨‹çš„course1æ¯”è¾ƒåŸºç¡€ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰markçš„å¿…è¦ï¼Œæ‰€ä»¥ä»course2å¼€å§‹ï¼ŒæŒ‰ç…§weekæ¥mark.<br><a id=\"more\"></a></p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>åœ¨machine learningä¸­ï¼Œdata setå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿmachine learningï¼Œdeep learningä¸­çš„data setåˆ†å¸ƒæ›´ä¾§é‡äºtrainingï¼ŒNgå»ºè®®æˆ‘ä»¬è®²data setåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š</p>\n<ul>\n<li>training setâ€”â€”è®­ç»ƒæ•°æ®é›†</li>\n<li>dev/validation setâ€”â€”æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•</li>\n<li>testing setâ€”â€”æ¨¡å‹æ•ˆæœæµ‹è¯•<br>ä¸€å®šæœ‰å¾ˆå¤šäººå¯¹äºdevå’Œtesting setæœ‰ä¸€äº›ç–‘é—®ï¼Œæœ€å¼€å§‹æˆ‘ä¹Ÿæ˜¯æ‡µé€¼çš„ï¼Œæ¥çœ‹çœ‹ä¸‹é¢è¿™æ®µè¯<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. Youâ€™re not adjusting the weights of the network with this data set, youâ€™re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasnâ€™t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then youâ€™re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>è¿™ä¸‰è€…çš„æ¯”ä¾‹åˆ™æ˜¯/99.5%/2.5%/2.5%/ï¼Œè¿™æ ·çš„åŸå› æ˜¯å› ä¸ºdeep learningä¸­ï¼Œæ•°æ®é‡è¶³å¤Ÿå¤§è€Œä¸”deep learningçš„å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œå¤§å®¶ä¸€å®šæ³¨æ„è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœå®åœ¨æ²¡æœ‰test setï¼Œä½†æ˜¯æœ‰dev setä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"ä»€ä¹ˆæ˜¯biaså’Œvariance\"><a href=\"#ä»€ä¹ˆæ˜¯biaså’Œvariance\" class=\"headerlink\" title=\"ä»€ä¹ˆæ˜¯biaså’Œvariance\"></a>ä»€ä¹ˆæ˜¯biaså’Œvariance</h3><p>bias &amp; varianceæ˜¯machine learning é¢†åŸŸä¸€ä¸ªç»å…¸çš„è¾©è¯é—®é¢˜ï¼Œåœ¨Ngç»å…¸çš„CS229ä¸­å°±é‡ç‚¹çš„è®²è¿°è¿‡ï¼Œå…·ä½“çš„å®šä¹‰æˆ‘ä¸å¤ªæƒ³ç»™å‡ºäº†ï¼Œåç»­æœ‰æ—¶é—´å¯ä»¥ä¸“é—¨å†™ä¸€ç¯‡ï¼Œåé¢ä¼šç»™å‡ºä¸€äº›èµ„æ–™é“¾æ¥ã€‚æˆ‘ä»¬ç®€å•çš„çœ‹ä¸€å¹…å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png\" alt=\"\"><br>å·¦å›¾å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„high bias situationï¼Œæ¨¡å‹æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„under fittingï¼Œå³å›¾åˆ™æ˜¯å…¸å‹çš„high variance situationï¼Œæ¨¡å‹è¿‡åˆ†çš„æ‹Ÿåˆäº†training setï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æœ€éœ€è¦é˜²èŒƒçš„over fitting.å½“ç„¶ï¼Œä¸­é—´çš„åˆ™æ˜¯æ¯”è¾ƒç†æƒ³çš„çŠ¶å†µã€‚</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>åœ¨å®é™…çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåˆ†æè‡ªå·±æ¨¡å‹çš„biaså’Œvarianceæƒ…å†µå‘¢ï¼ŒNgç»™äº†æˆ‘ä»¬ä¸€ä¸ªæµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png\" alt=\"\"><br>é¦–å…ˆæ£€éªŒæ˜¯å¦å­˜åœ¨high bias æƒ…å†µï¼Œå…·ä½“æ–¹æ³•æ˜¯åœ¨training set å’Œ dev setä¸Šè®¡ç®—errorï¼Œå¯¹æ¯”training errorå’Œdev errorï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆé«˜ï¼Œé‚£ä¹ˆå°±æ˜¯high biasï¼Œå¦‚æœtraining errorå¾ˆå°è€Œdev errorå¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯high varianceï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆå¤§ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å·®çš„æƒ…å†µäº†æ—¢high biasåˆhigh variance</p>\n<p>å¯¹äºhigh biasï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ›´å¼ºçš„ç½‘ç»œç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›<br>å¯¹äºhigh varianceï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„æ•°æ®ï¼Œregularizationçš„æ–¹æ³•æ¥è§£å†³ã€‚</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>è¿™éƒ¨åˆ†å†…å®¹æˆ‘å°±ä¸å¤šè¯´äº†ï¼Œæˆ‘ä¹‹å‰ä¸“é—¨è¯¦ç»†æ·±å…¥çš„è®²è¿°è¿‡L1å’ŒL2 regularizationï¼Œå¤§å®¶å¯ä»¥å»çœ‹ä¸€çœ‹ã€‚</p>\n<p>å”¯ä¸€éœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨åŠ å…¥L1æˆ–è€…L2 regularizationä¹‹åï¼Œåœ¨è§‚æµ‹cost function convergence çš„æ—¶å€™ï¼Œ<strong>ä¸€å®šè¦å¸¦ä¸Šregularization item</strong>ï¼Œå¦åˆ™ç»“æœæ˜¯å¾ˆéš¾çœ‹åˆ°convergenceçš„ï¼Œè¿™å’Œregularizationæ€§è´¨æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚ </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropoutæ˜¯neural networkä¸­ä¸€ç§ç»å…¸çš„regularizationæ–¹æ³•ï¼Œç»å…¸åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Œæˆ‘å½“å¹´æ¯•è®¾è¯¾é¢˜ä¸­éƒ½ç”¨åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœè¶…èµ</p>\n<p>Dropoutæ–¹æ³•çš„å®è´¨æ˜¯<strong>æŒ‰æ¯”ä¾‹éšæœºéšè—</strong>æ‰neural networkä¸­layeré‡Œçš„æŸäº›unitsï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†ä¸€æ¬¡epochä¸­ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çš„unitså¯¹åº”çš„weightså’Œbiasä¼šå¾—åˆ°æ›´æ–°ï¼Œè€Œä¸‹ä¸€æ¬¡epochä¸­ï¼Œåˆ™æ˜¯å¦ä¸€éƒ¨åˆ†unitså¯¹åº”çš„weightså’Œbiaså¾—åˆ°æ›´æ–°ï¼Œå¦‚ä¸‹å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png\" alt=\"\"><br>é‚£ä¹ˆä¸ºä»€ä¹ˆDropoutå¯ä»¥å®ç°regularizationæ•ˆæœå‘¢ï¼ŒNgå‘Šè¯‰æˆ‘ä»¬ï¼š</p>\n<blockquote>\n<p>Intuition:Canâ€™t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>å¦‚ä½•ç†è§£å‘¢ï¼ŸåŠ å…¥dropoutåï¼Œæ¯ä¸ªunitå¯¹åº”çš„weightså’Œbiasä¸èƒ½å®Œå…¨ä¾èµ–ä¸Šå±‚unitsï¼Œå› ä¸ºä»–å¹¶ä¸æ˜¯æ¯ä¸€æ¬¡epochéƒ½å¯ä»¥work onï¼Œå› æ­¤åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œè§ç¬‘äº†over fittingçš„é£é™©ã€‚å®é™…ä¸Šï¼Œdropoutå¯ä»¥äº§ç”Ÿshrink weightsçš„æ•ˆæœï¼Œå’ŒL2 regularizationç›¸ä¼¼ï¼Œå› æ­¤ä¹Ÿæ˜¯ä¸€ç§regularizationæ–¹æ³•ã€‚</p>\n<p>ä½†æ˜¯ï¼Œdropoutå’ŒL2 regularizationå”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–å¾ˆéš¾ç»™å‡ºä¸€ä¸ªregularization itemï¼Œæ‰€ä»¥ä½ æ²¡æœ‰åŠæ³•ç”»å‡ºcost function convergenceçš„è½¨è¿¹ã€‚</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>é™¤äº†ç»å…¸çš„L1ã€L2 regularizationå’Œdropoutæ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›é˜²æ­¢over fittingçš„æ–¹æ³•ï¼Œä¾‹å¦‚å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨data augmentationï¼Œæ—‹è½¬ï¼Œç¿»è½¬ï¼ŒåŠ å™ªå£°ç­‰æ–¹æ³•ã€‚</p>\n<p>è¿˜æœ‰ä¸€ä¸ªearly stoppingæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œéšç€training çš„epochå¢å¤šï¼Œæ¨¡å‹å¯¹training setæ‹Ÿåˆä¼šè¶Šæ¥è¶Šå¥½ï¼Œéšä¹‹å¸¦æ¥çš„é—®é¢˜å°±æ˜¯å¯èƒ½over fittingï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡early stoppingï¼Œè®©æ¨¡å‹åœ¨æ²¡æœ‰äº§ç”Ÿover fittingçš„æ—¶å€™åœä¸‹æ¥ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"ä»€ä¹ˆæ˜¯exploding-vanishing-gradient\"><a href=\"#ä»€ä¹ˆæ˜¯exploding-vanishing-gradient\" class=\"headerlink\" title=\"ä»€ä¹ˆæ˜¯exploding/vanishing gradient\"></a>ä»€ä¹ˆæ˜¯exploding/vanishing gradient</h3><p>å¯¹äºdeep learningï¼Œæ›¾ç»æœ€ä¸ºæ£˜æ‰‹çš„é—®é¢˜å°±æ˜¯exploding/vanishing gradientï¼Œç”šè‡³æ˜¯é™åˆ¶deep learningå‘å±•çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚<br> å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª<strong>æ¯”è¾ƒæ·±</strong>çš„neural networkï¼Œå‡è®¾ä¸€å…±æœ‰\\(l\\)å±‚ï¼Œå¯¹åº”çš„weightsæ˜¯\\(W^{[1]}\\)åˆ°\\(W^{[l]}\\)ï¼Œbiasæ˜¯\\(b^{[1]}\\)åˆ°\\(b^{[l]}\\)ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œå‡è®¾biaså‡ä¸º0ï¼Œactive functionä¸º\\(g(z)=z\\)ï¼Œé‚£ä¹ˆï¼Œ\\(y\\)å°±ç­‰äº<br>$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$<br>å¤§å®¶æ„Ÿå…´è¶£çš„è¯å¯ä»¥éªŒè¯ä¸€ä¸‹ï¼Œå¾ˆç®€å•çš„ã€‚</p>\n<p>é‚£ä¹ˆç°åœ¨é—®é¢˜æ¥äº†ï¼Œå½“\\(l\\)å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å¤§äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šéå¸¸éå¸¸å¤§ï¼Œç”šè‡³åˆ°æ— é™å¤§ï¼Œè¿™ç§æƒ…å†µå«exploding gradientï¼›ç›¸åº”çš„ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å°äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šç‰¹åˆ«å°ï¼Œç”šè‡³ä¸ºé›¶ï¼Œè¿™å°±æ˜¯vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>å¯¹äºä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨weightsåˆå§‹åŒ–çš„æ—¶å€™åšä¸€äº›å·¥ä½œæ¥è§£å†³å¯èƒ½å‡ºç°çš„exploding or vanishing gradientã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå¯¹äºactive function\\(g(z)\\)ï¼Œå‡è®¾biasä¸ºé›¶ï¼Œ<br>$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$<br>æˆ‘ä»¬è¦æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(n\\)çš„å¢å¤§ï¼Œ\\(w\\)ä¼šå˜å°ï¼Œæˆ‘ä»¬è®©\\(w\\)å§‹ç»ˆä¿æŒä»¥0ä¸ºmeanï¼Œ1ä¸ºvaranceçš„Gaussian distributionä¸‹ï¼Œå°±å¯ä»¥å¾ˆå¥½çš„æ§åˆ¶\\(w\\)çš„å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>åœ¨Ngçš„å»ºè®®ä¸­ï¼Œå¦‚æœactive functionæ˜¯sigmoidï¼Œæˆ‘ä»¬ä¸€èˆ¬å–\\(var(w)= \\frac{1}{n^{l-1}}\\)ï¼Œå¦‚æœæ˜¯reLuï¼Œæˆ‘ä»¬å–\\(var(w)= \\frac{2}{n^{l-1}}\\)ï¼Œå¯¹äºtanhï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\)æˆ–è€…\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¥½çš„é¿å…exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>åœ¨è°ƒè¯•neural networkçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åšgradient checkçš„å·¥ä½œï¼Œä»¥ç¡®å®šæ•´ä¸ªnetworkæ­£å¸¸çš„è¿è¡Œï¼ŒNgåœ¨è¿™é‡Œå»ºè®®æˆ‘ä»¬ä½¿ç”¨åŒè¾¹é€¼è¿‘çš„æ–¹æ³•å»åšgradient checkï¼Œè¿™é‡Œæˆ‘ä¸åšå¤ªå¤šæè¿°ï¼Œä¸»è¦ä¸Šä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png\" alt=\"\"><br>é€šå¸¸æ¥è¯´ï¼ŒåŒè¾¹é€¼è¿‘çš„æ–¹æ³•è·å¾—ç»“æœæ›´åŠ å‡†ç¡®ã€‚</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Donâ€™t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosenâ€™t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">Bias and variance</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"regularization","path":"tags/regularization/"},{"name":"gradient descent","path":"tags/gradient-descent/"}],"excerpt":"<p>å¤§å®¶å¥½ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Andrew Ngçš„Deep learningè¯¾ç¨‹ï¼Œäºæ˜¯å†³å®šå†™ä¸€äº›learning notesæ¥recapå’Œmarkä¸€ä¸‹å­¦åˆ°çš„çŸ¥è¯†ï¼Œé¿å…é—å¿˜ã€‚ç”±äºè¯¥è¯¾ç¨‹çš„course1æ¯”è¾ƒåŸºç¡€ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰markçš„å¿…è¦ï¼Œæ‰€ä»¥ä»course2å¼€å§‹ï¼ŒæŒ‰ç…§weekæ¥mark.<br></p>","more":"</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><p>åœ¨machine learningä¸­ï¼Œdata setå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿmachine learningï¼Œdeep learningä¸­çš„data setåˆ†å¸ƒæ›´ä¾§é‡äºtrainingï¼ŒNgå»ºè®®æˆ‘ä»¬è®²data setåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š</p>\n<ul>\n<li>training setâ€”â€”è®­ç»ƒæ•°æ®é›†</li>\n<li>dev/validation setâ€”â€”æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–èƒ½åŠ›æµ‹è¯•</li>\n<li>testing setâ€”â€”æ¨¡å‹æ•ˆæœæµ‹è¯•<br>ä¸€å®šæœ‰å¾ˆå¤šäººå¯¹äºdevå’Œtesting setæœ‰ä¸€äº›ç–‘é—®ï¼Œæœ€å¼€å§‹æˆ‘ä¹Ÿæ˜¯æ‡µé€¼çš„ï¼Œæ¥çœ‹çœ‹ä¸‹é¢è¿™æ®µè¯<blockquote>\n<p>Dev/Validation Set: this data set is used to minimize overfitting. Youâ€™re not adjusting the weights of the network with this data set, youâ€™re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasnâ€™t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then youâ€™re overfitting your neural network and you should stop training.<br>Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.</p>\n</blockquote>\n</li>\n</ul>\n<p>è¿™ä¸‰è€…çš„æ¯”ä¾‹åˆ™æ˜¯/99.5%/2.5%/2.5%/ï¼Œè¿™æ ·çš„åŸå› æ˜¯å› ä¸ºdeep learningä¸­ï¼Œæ•°æ®é‡è¶³å¤Ÿå¤§è€Œä¸”deep learningçš„å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œå¤§å®¶ä¸€å®šæ³¨æ„è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœå®åœ¨æ²¡æœ‰test setï¼Œä½†æ˜¯æœ‰dev setä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚</p>\n<h2 id=\"Bias-and-variance\"><a href=\"#Bias-and-variance\" class=\"headerlink\" title=\"Bias and variance\"></a>Bias and variance</h2><h3 id=\"ä»€ä¹ˆæ˜¯biaså’Œvariance\"><a href=\"#ä»€ä¹ˆæ˜¯biaså’Œvariance\" class=\"headerlink\" title=\"ä»€ä¹ˆæ˜¯biaså’Œvariance\"></a>ä»€ä¹ˆæ˜¯biaså’Œvariance</h3><p>bias &amp; varianceæ˜¯machine learning é¢†åŸŸä¸€ä¸ªç»å…¸çš„è¾©è¯é—®é¢˜ï¼Œåœ¨Ngç»å…¸çš„CS229ä¸­å°±é‡ç‚¹çš„è®²è¿°è¿‡ï¼Œå…·ä½“çš„å®šä¹‰æˆ‘ä¸å¤ªæƒ³ç»™å‡ºäº†ï¼Œåç»­æœ‰æ—¶é—´å¯ä»¥ä¸“é—¨å†™ä¸€ç¯‡ï¼Œåé¢ä¼šç»™å‡ºä¸€äº›èµ„æ–™é“¾æ¥ã€‚æˆ‘ä»¬ç®€å•çš„çœ‹ä¸€å¹…å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-5.png\" alt=\"\"><br>å·¦å›¾å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„high bias situationï¼Œæ¨¡å‹æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„under fittingï¼Œå³å›¾åˆ™æ˜¯å…¸å‹çš„high variance situationï¼Œæ¨¡å‹è¿‡åˆ†çš„æ‹Ÿåˆäº†training setï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æœ€éœ€è¦é˜²èŒƒçš„over fitting.å½“ç„¶ï¼Œä¸­é—´çš„åˆ™æ˜¯æ¯”è¾ƒç†æƒ³çš„çŠ¶å†µã€‚</p>\n<h3 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>åœ¨å®é™…çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåˆ†æè‡ªå·±æ¨¡å‹çš„biaså’Œvarianceæƒ…å†µå‘¢ï¼ŒNgç»™äº†æˆ‘ä»¬ä¸€ä¸ªæµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-2.png\" alt=\"\"><br>é¦–å…ˆæ£€éªŒæ˜¯å¦å­˜åœ¨high bias æƒ…å†µï¼Œå…·ä½“æ–¹æ³•æ˜¯åœ¨training set å’Œ dev setä¸Šè®¡ç®—errorï¼Œå¯¹æ¯”training errorå’Œdev errorï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆé«˜ï¼Œé‚£ä¹ˆå°±æ˜¯high biasï¼Œå¦‚æœtraining errorå¾ˆå°è€Œdev errorå¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯high varianceï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆå¤§ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å·®çš„æƒ…å†µäº†æ—¢high biasåˆhigh variance</p>\n<p>å¯¹äºhigh biasï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ›´å¼ºçš„ç½‘ç»œç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›<br>å¯¹äºhigh varianceï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„æ•°æ®ï¼Œregularizationçš„æ–¹æ³•æ¥è§£å†³ã€‚</p>\n<h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><h3 id=\"L1-amp-L2-regularization\"><a href=\"#L1-amp-L2-regularization\" class=\"headerlink\" title=\"L1&amp;L2 regularization\"></a>L1&amp;L2 regularization</h3><p>è¿™éƒ¨åˆ†å†…å®¹æˆ‘å°±ä¸å¤šè¯´äº†ï¼Œæˆ‘ä¹‹å‰ä¸“é—¨è¯¦ç»†æ·±å…¥çš„è®²è¿°è¿‡L1å’ŒL2 regularizationï¼Œå¤§å®¶å¯ä»¥å»çœ‹ä¸€çœ‹ã€‚</p>\n<p>å”¯ä¸€éœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨åŠ å…¥L1æˆ–è€…L2 regularizationä¹‹åï¼Œåœ¨è§‚æµ‹cost function convergence çš„æ—¶å€™ï¼Œ<strong>ä¸€å®šè¦å¸¦ä¸Šregularization item</strong>ï¼Œå¦åˆ™ç»“æœæ˜¯å¾ˆéš¾çœ‹åˆ°convergenceçš„ï¼Œè¿™å’Œregularizationæ€§è´¨æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚ </p>\n<h3 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h3><p>Dropoutæ˜¯neural networkä¸­ä¸€ç§ç»å…¸çš„regularizationæ–¹æ³•ï¼Œç»å…¸åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Œæˆ‘å½“å¹´æ¯•è®¾è¯¾é¢˜ä¸­éƒ½ç”¨åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœè¶…èµ</p>\n<p>Dropoutæ–¹æ³•çš„å®è´¨æ˜¯<strong>æŒ‰æ¯”ä¾‹éšæœºéšè—</strong>æ‰neural networkä¸­layeré‡Œçš„æŸäº›unitsï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†ä¸€æ¬¡epochä¸­ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çš„unitså¯¹åº”çš„weightså’Œbiasä¼šå¾—åˆ°æ›´æ–°ï¼Œè€Œä¸‹ä¸€æ¬¡epochä¸­ï¼Œåˆ™æ˜¯å¦ä¸€éƒ¨åˆ†unitså¯¹åº”çš„weightså’Œbiaså¾—åˆ°æ›´æ–°ï¼Œå¦‚ä¸‹å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-3.png\" alt=\"\"><br>é‚£ä¹ˆä¸ºä»€ä¹ˆDropoutå¯ä»¥å®ç°regularizationæ•ˆæœå‘¢ï¼ŒNgå‘Šè¯‰æˆ‘ä»¬ï¼š</p>\n<blockquote>\n<p>Intuition:Canâ€™t rely on any one feature, so have to spread out weights</p>\n</blockquote>\n<p>å¦‚ä½•ç†è§£å‘¢ï¼ŸåŠ å…¥dropoutåï¼Œæ¯ä¸ªunitå¯¹åº”çš„weightså’Œbiasä¸èƒ½å®Œå…¨ä¾èµ–ä¸Šå±‚unitsï¼Œå› ä¸ºä»–å¹¶ä¸æ˜¯æ¯ä¸€æ¬¡epochéƒ½å¯ä»¥work onï¼Œå› æ­¤åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œè§ç¬‘äº†over fittingçš„é£é™©ã€‚å®é™…ä¸Šï¼Œdropoutå¯ä»¥äº§ç”Ÿshrink weightsçš„æ•ˆæœï¼Œå’ŒL2 regularizationç›¸ä¼¼ï¼Œå› æ­¤ä¹Ÿæ˜¯ä¸€ç§regularizationæ–¹æ³•ã€‚</p>\n<p>ä½†æ˜¯ï¼Œdropoutå’ŒL2 regularizationå”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–å¾ˆéš¾ç»™å‡ºä¸€ä¸ªregularization itemï¼Œæ‰€ä»¥ä½ æ²¡æœ‰åŠæ³•ç”»å‡ºcost function convergenceçš„è½¨è¿¹ã€‚</p>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h3><p>é™¤äº†ç»å…¸çš„L1ã€L2 regularizationå’Œdropoutæ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›é˜²æ­¢over fittingçš„æ–¹æ³•ï¼Œä¾‹å¦‚å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨data augmentationï¼Œæ—‹è½¬ï¼Œç¿»è½¬ï¼ŒåŠ å™ªå£°ç­‰æ–¹æ³•ã€‚</p>\n<p>è¿˜æœ‰ä¸€ä¸ªearly stoppingæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œéšç€training çš„epochå¢å¤šï¼Œæ¨¡å‹å¯¹training setæ‹Ÿåˆä¼šè¶Šæ¥è¶Šå¥½ï¼Œéšä¹‹å¸¦æ¥çš„é—®é¢˜å°±æ˜¯å¯èƒ½over fittingï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡early stoppingï¼Œè®©æ¨¡å‹åœ¨æ²¡æœ‰äº§ç”Ÿover fittingçš„æ—¶å€™åœä¸‹æ¥ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚</p>\n<h2 id=\"Exploding-vanishing-gradient\"><a href=\"#Exploding-vanishing-gradient\" class=\"headerlink\" title=\"Exploding/vanishing gradient\"></a>Exploding/vanishing gradient</h2><h3 id=\"ä»€ä¹ˆæ˜¯exploding-vanishing-gradient\"><a href=\"#ä»€ä¹ˆæ˜¯exploding-vanishing-gradient\" class=\"headerlink\" title=\"ä»€ä¹ˆæ˜¯exploding/vanishing gradient\"></a>ä»€ä¹ˆæ˜¯exploding/vanishing gradient</h3><p>å¯¹äºdeep learningï¼Œæ›¾ç»æœ€ä¸ºæ£˜æ‰‹çš„é—®é¢˜å°±æ˜¯exploding/vanishing gradientï¼Œç”šè‡³æ˜¯é™åˆ¶deep learningå‘å±•çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚<br> å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª<strong>æ¯”è¾ƒæ·±</strong>çš„neural networkï¼Œå‡è®¾ä¸€å…±æœ‰\\(l\\)å±‚ï¼Œå¯¹åº”çš„weightsæ˜¯\\(W^{[1]}\\)åˆ°\\(W^{[l]}\\)ï¼Œbiasæ˜¯\\(b^{[1]}\\)åˆ°\\(b^{[l]}\\)ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œå‡è®¾biaså‡ä¸º0ï¼Œactive functionä¸º\\(g(z)=z\\)ï¼Œé‚£ä¹ˆï¼Œ\\(y\\)å°±ç­‰äº<br>$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$<br>å¤§å®¶æ„Ÿå…´è¶£çš„è¯å¯ä»¥éªŒè¯ä¸€ä¸‹ï¼Œå¾ˆç®€å•çš„ã€‚</p>\n<p>é‚£ä¹ˆç°åœ¨é—®é¢˜æ¥äº†ï¼Œå½“\\(l\\)å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å¤§äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šéå¸¸éå¸¸å¤§ï¼Œç”šè‡³åˆ°æ— é™å¤§ï¼Œè¿™ç§æƒ…å†µå«exploding gradientï¼›ç›¸åº”çš„ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å°äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šç‰¹åˆ«å°ï¼Œç”šè‡³ä¸ºé›¶ï¼Œè¿™å°±æ˜¯vanishing gradient.</p>\n<h3 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h3><p>å¯¹äºä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨weightsåˆå§‹åŒ–çš„æ—¶å€™åšä¸€äº›å·¥ä½œæ¥è§£å†³å¯èƒ½å‡ºç°çš„exploding or vanishing gradientã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå¯¹äºactive function\\(g(z)\\)ï¼Œå‡è®¾biasä¸ºé›¶ï¼Œ<br>$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$<br>æˆ‘ä»¬è¦æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(n\\)çš„å¢å¤§ï¼Œ\\(w\\)ä¼šå˜å°ï¼Œæˆ‘ä»¬è®©\\(w\\)å§‹ç»ˆä¿æŒä»¥0ä¸ºmeanï¼Œ1ä¸ºvaranceçš„Gaussian distributionä¸‹ï¼Œå°±å¯ä»¥å¾ˆå¥½çš„æ§åˆ¶\\(w\\)çš„å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\)</p>\n<p>åœ¨Ngçš„å»ºè®®ä¸­ï¼Œå¦‚æœactive functionæ˜¯sigmoidï¼Œæˆ‘ä»¬ä¸€èˆ¬å–\\(var(w)= \\frac{1}{n^{l-1}}\\)ï¼Œå¦‚æœæ˜¯reLuï¼Œæˆ‘ä»¬å–\\(var(w)= \\frac{2}{n^{l-1}}\\)ï¼Œå¯¹äºtanhï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\)æˆ–è€…\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¥½çš„é¿å…exploding or vanishing gradient.</p>\n<h2 id=\"Gradient-checking\"><a href=\"#Gradient-checking\" class=\"headerlink\" title=\"Gradient checking\"></a>Gradient checking</h2><h3 id=\"Gradient-approximation\"><a href=\"#Gradient-approximation\" class=\"headerlink\" title=\"Gradient approximation\"></a>Gradient approximation</h3><p>åœ¨è°ƒè¯•neural networkçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åšgradient checkçš„å·¥ä½œï¼Œä»¥ç¡®å®šæ•´ä¸ªnetworkæ­£å¸¸çš„è¿è¡Œï¼ŒNgåœ¨è¿™é‡Œå»ºè®®æˆ‘ä»¬ä½¿ç”¨åŒè¾¹é€¼è¿‘çš„æ–¹æ³•å»åšgradient checkï¼Œè¿™é‡Œæˆ‘ä¸åšå¤ªå¤šæè¿°ï¼Œä¸»è¦ä¸Šä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/6/6-6.png\" alt=\"\"><br>é€šå¸¸æ¥è¯´ï¼ŒåŒè¾¹é€¼è¿‘çš„æ–¹æ³•è·å¾—ç»“æœæ›´åŠ å‡†ç¡®ã€‚</p>\n<h3 id=\"Gradient-checking-notes\"><a href=\"#Gradient-checking-notes\" class=\"headerlink\" title=\"Gradient checking notes\"></a>Gradient checking notes</h3><blockquote>\n<ol>\n<li>Donâ€™t use in training-only to debug(too slow)</li>\n<li>If algorithm fails grad check, look at components to try to identify bug</li>\n<li>Remeber regularization</li>\n<li>Dosenâ€™t wrok with dropout</li>\n<li>Run at random initialzation; perhaps again after some training.</li>\n</ol>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">Bias and variance</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week3","date":"2017-09-30T07:44:25.000Z","_content":"ä¸çŸ¥ä¸è§‰æ¥åˆ°ç¬¬ä¸‰å‘¨çš„è¯¾ç¨‹äº†ï¼Œå¤§å®¶åŠ æ²¹ï¼è¿™å‘¨çš„ä¸»è¦å†…å®¹æ˜¯hyperparameter selectionå’Œbatch normalçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸€å‘¨çš„å†…å®¹ï¼\n<!--more-->\n## Hyperparameter selection\nHyperparameter selectionåœ¨machine learningä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚gradient descentä¸­çš„learning rate \\\\(\\alpha\\\\)å°±æ˜¯å…³ä¹ç®—æ³•ç»“æœçš„é‡è¦hyperparameterï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»é€‰æ‹©å‘¢ï¼ŸNgç»™å‡ºäº†ä¸¤ä¸ªå»ºè®®ï¼š\n* æ„å»ºå¤šä¸ªhyperparameteräº¤å‰ï¼Œéšæœºé€‰æ‹©å¤§å°ï¼Œé€‰æ‹©æ•ˆæœè¾ƒå¥½çš„èŒƒå›´ï¼Œç»§ç»­éšæœºé€‰æ‹©hyperparameterå¤§å°ï¼Œè§‚å¯Ÿç»“æœã€‚\n* é€‰æ‹©å‚æ•°çš„æ—¶å€™åˆ†æ®µé€‰æ‹©ï¼Œå¹¶ä¸”ä½¿ç”¨logåˆ†æ®µï¼Œä¾‹å¦‚åœ¨0.0001åˆ°1ä¹‹é—´é€‰æ‹©ï¼Œå°†æ•°è½´åˆ†æˆ0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1å’Œ1ï¼Œè¿™æ ·é€‰æ‹©å‡ºçš„ç»“æœæ›´å¥½\n\nå¯¹äºæ•´ä½“æ¨¡å‹çš„hyperparameter selectionï¼ŒNgä¹Ÿå‡ºäº†å»ºè®®ï¼Œé‚£å°±æ˜¯babysittingå’Œparallelæ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹ä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒæ•´ï¼Œä¸€ç§æ˜¯åŒæ—¶å¯åŠ¨å¤šä¸ªä¸åŒhyperparameterçš„æ¨¡å‹ï¼Œæœ€åå–æ•ˆæœæœ€å¥½çš„ã€‚\n\nä¸¤ç§æ–¹æ³•æ®Šé€”åŒå½’ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…·ä½“æƒ…å†µåšå‡ºé€‰æ‹©ã€‚\n## Batch norm\n### Normalization\nç›¸ä¿¡å¤§å®¶éƒ½å¬è¯´è¿‡å¤§åé¼é¼çš„normalizationå§ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¾ˆå¥½çš„æå‡æ•°æ®å¤„ç†ï¼ˆä¾‹å¦‚gradient descentï¼‰çš„é€Ÿåº¦å’Œæ•ˆæœï¼Œåœ¨å¼•å…¥batch normä¹‹å‰ï¼Œæˆ‘ä¹Ÿç¨å¾®æä¸€ä¸‹normalizationï¼Œä¸‹é¢ä¸Šå…¬å¼ï¼š\n\nå¯¹äºè¾“å…¥æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹æ³•æ¥normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\nè¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠè¾“å…¥æ•°æ®è½¬åŒ–æˆäº†ç¬¦åˆæœŸæœ›ä¸º0ï¼Œæ–¹å·®ä¸º1çš„Gaussian distributionçš„æ•°æ®ã€‚\n\nå½“ç„¶ï¼Œè¿™åªæ˜¯normaliztionä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¢«ç§°ä½œz-scoreæ–¹æ³•ã€‚\n### Batch norm\nä¸Šé¢è¯´çš„normalizationæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°neural networksä¸­ï¼Œå¯¹äºnerual networksä¸­çš„æŸä¸€ä¸ªlayeræ¥è¯´ï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå­¤ç«‹çš„è®¡ç®—è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥normalizationï¼Œå¯¹äº\\\\(z^{(i)}\\\\)æ¥è¯´ï¼š\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\nç„¶åæˆ‘ä»¬ç”¨æœ€ç»ˆçš„\\\\(z^{N\\[l](i)}\\\\)æ¥æ›¿æ¢\\\\(z^{\\[l](i)}\\\\) å°±å¯ä»¥ï¼Œå…¶ä¸­\\\\( \\gamma\\\\)å’Œ\\\\(\\beta\\\\)æ˜¯ä¸¤ä¸ªparameterï¼Œå¯ä»¥é€šè¿‡gradient descentæ¥æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªparameterå­˜åœ¨çš„æ„ä¹‰ï¼Œå°±æ˜¯å¯ä»¥è°ƒæ•´normalizationæ˜ å°„çš„Gaussian distributionï¼Œè€Œä¸æ˜¯ç»Ÿç»Ÿæ˜ å°„åˆ°Normal distributionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\\\(\\epsilon\\\\)æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨æ¥é¿å…åˆ†æ¯åˆ†0çš„æƒ…å†µã€‚\n\nå¦‚æœ\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)ä¸”\\\\( \\beta = \\mu\\\\)çš„è¯ï¼Œé‚£ä¹ˆå…¶å®\\\\(z^{N(i)}=z^(i)\\\\)çš„ï¼Œå¤§å®¶å¯ä»¥ç®—ç®—ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œå°±æ˜¯ç›¸å½“äºæ²¡åšnormalization.\n### Batch norm on neural networks\nå¯¹äºneural networksï¼Œè¾“å…¥\\\\(X\\\\)é€šè¿‡parameter\\\\(w^{[1]}\\\\)å’Œ\\\\(b^{[1]}\\\\)å¾—åˆ°\\\\(z^{[1]}\\\\)ï¼Œé€šè¿‡\\\\(\\beta\\\\)å’Œ\\\\(\\gamma\\\\)è·å¾—\\\\(z^{N[1]}\\\\)ï¼Œç»è¿‡active functionåè·å¾—\\\\(a^{[1]}\\\\)ï¼Œé€šè¿‡\\\\(w^{[2]}\\\\)å’Œ\\\\(b^{[2]}\\\\)è·å¾—\\\\(z^{[2]}\\\\)ï¼Œå¦‚æ­¤ä¸‹å»ï¼Œä¸€ç›´åˆ°æœ€åçš„è¾“å‡ºå±‚ï¼Œå®Œæˆforward propagation.\n\nåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±æœ‰å››ä¸ªparametersï¼Œåˆ†åˆ«æ˜¯\\\\(w^{[l]}\\\\)ï¼Œ\\\\(b^{[l]}\\\\)ï¼Œ\\\\( \\beta^{[l]}\\\\)ï¼Œ\\\\( \\gamma^{[l]}\\\\)ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼š\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\nä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨åšbatch normalçš„æ—¶å€™ï¼Œé¦–å…ˆä¼šæŠŠ\\\\(z^{[l]}\\\\)æ˜ å°„åˆ°æœŸæœ›ä¸º1æ–¹å·®ä¸º0çš„Gaussian distributionä¸Šï¼Œè¿™å°±æ„å‘³ç€\\\\(b^{[l]}\\\\)æ˜¯å¯ä»¥å¿½ç•¥æ‰çš„ï¼Œå› ä¸ºå³ä½¿ä¿ç•™ï¼Œåœ¨batch normalçš„æ—¶å€™ä¹Ÿä¼šè¢«å‡å»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„parameteråªæœ‰ä¸‰ä¸ªï¼Œå³ï¼š\\\\(w^{[l]}\\\\)ï¼Œ\\\\( \\beta^{[l]}\\\\)ï¼Œ\\\\( \\gamma^{[l]}\\\\)\n\nåœ¨backforwardçš„æ—¶å€™ï¼Œæˆ‘ä»¬å’Œæ™®é€šçš„neural networksä¸€æ ·ï¼Œåªæ˜¯å¯ä»¥ä¸ç”¨å†å»è®¡ç®—\\\\(db\\\\)\n### Solve covariate shift\nä»€ä¹ˆæ˜¯covariate shiftï¼Ÿç®€å•çš„ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦éšç€æ ·æœ¬çš„å˜åŒ–è€Œå˜åŒ–ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆç›´è§‚ï¼Œåœ¨çŒ«è„¸è¯•éªŒä¸­ï¼Œå‡è®¾training seté‡Œéƒ½æ˜¯é»‘çŒ«ï¼Œè¿™æ ·è·å¾—çš„æ¨¡å‹ï¼Œå¯¹äºèŠ±çŒ«è¯†åˆ«å°±æ˜¯ä¸é€‚ç”¨çš„ï¼Œè¿™å°±å«covariate shift. å…¶å®ï¼Œbatch normå¯ä»¥æ”¹å–„neural networksæ•ˆæœçš„åŸå› ï¼Œå°±å¯ä»¥ç†è§£ä¸ºsolve covariate shiftçš„è¿‡ç¨‹ã€‚\n\nOKï¼Œæˆ‘ä»¬æ¥è¯¦ç»†çœ‹çœ‹åŸå› ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¦‚å›¾çš„neural networksï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png)\nåœ¨æ ‡ç¤ºå‡ºçš„ä½ç½®ï¼Œæœ‰parameter\\\\(w^{[3]}\\\\)å’Œ\\\\(b^{[3]}\\\\)ï¼Œå¦‚æœæˆ‘ä»¬ç›–ä½å‰é¢çš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è·å¾—å¦‚å›¾çš„neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png)\nå¯¹äºneural networksæ¥è¯´ï¼Œç›¸å½“äºè·å¾—äº†é»‘ç®±è¾“å‡ºçš„\\\\(a^{[2]}\\\\)ï¼Œè€Œ\\\\(a^{[2]}\\\\)çš„å€¼å…¶å®å¹¶ä¸æ˜¯æ˜¯å›ºå®šçš„ï¼Œæ¯ä¸€æ¬¡iterationåéƒ½æœ‰ä¸ä¸€æ ·çš„\\\\(a^{[2]}\\\\)ï¼Œè¿™å°±äº§ç”Ÿäº†covariate shifté—®é¢˜ã€‚\n\nä½†æ˜¯ï¼Œbatch normå¯ä»¥å°†\\\\(a^{[2]}\\\\)çš„æœŸæœ›å’Œæ–¹å·®é™åˆ¶åˆ°\\\\(\\beta\\\\)å’Œ\\\\(gamma\\\\)æ§åˆ¶çš„èŒƒå›´å†…ï¼Œä»¥æ­¤**æå¤§é™åº¦**çš„ç¼“è§£äº†covariate shiftç°è±¡ã€‚\n\nå¦å¤–ï¼Œbatch normè¿˜å¯ä»¥æœ‰ä¸€äº›regularizationçš„ä½œç”¨ï¼Œç”±äºæ¯æ¬¡mini-batch gradient descentä¸­batch normä½œç”¨çš„sampleä¸ä¸€æ ·ï¼Œç±»ä¼¼äºdropoutçš„æ•ˆæœï¼Œä¼šç»™å¯¹åº”layeråŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»¥æ­¤äº§ç”Ÿä¸€äº›regularizationçš„æ•ˆæœã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šæŠŠbatch normåˆ—å…¥regularizationèŒƒç•´å†…ã€‚\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course2-week3.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week3\ndate: 2017-09-30 15:44:25\ntags: \n\t- hyperparameter\n\t- batch norm\n\t- covariate shift\ncategories: learning notes\n---\nä¸çŸ¥ä¸è§‰æ¥åˆ°ç¬¬ä¸‰å‘¨çš„è¯¾ç¨‹äº†ï¼Œå¤§å®¶åŠ æ²¹ï¼è¿™å‘¨çš„ä¸»è¦å†…å®¹æ˜¯hyperparameter selectionå’Œbatch normalçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸€å‘¨çš„å†…å®¹ï¼\n<!--more-->\n## Hyperparameter selection\nHyperparameter selectionåœ¨machine learningä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚gradient descentä¸­çš„learning rate \\\\(\\alpha\\\\)å°±æ˜¯å…³ä¹ç®—æ³•ç»“æœçš„é‡è¦hyperparameterï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»é€‰æ‹©å‘¢ï¼ŸNgç»™å‡ºäº†ä¸¤ä¸ªå»ºè®®ï¼š\n* æ„å»ºå¤šä¸ªhyperparameteräº¤å‰ï¼Œéšæœºé€‰æ‹©å¤§å°ï¼Œé€‰æ‹©æ•ˆæœè¾ƒå¥½çš„èŒƒå›´ï¼Œç»§ç»­éšæœºé€‰æ‹©hyperparameterå¤§å°ï¼Œè§‚å¯Ÿç»“æœã€‚\n* é€‰æ‹©å‚æ•°çš„æ—¶å€™åˆ†æ®µé€‰æ‹©ï¼Œå¹¶ä¸”ä½¿ç”¨logåˆ†æ®µï¼Œä¾‹å¦‚åœ¨0.0001åˆ°1ä¹‹é—´é€‰æ‹©ï¼Œå°†æ•°è½´åˆ†æˆ0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1å’Œ1ï¼Œè¿™æ ·é€‰æ‹©å‡ºçš„ç»“æœæ›´å¥½\n\nå¯¹äºæ•´ä½“æ¨¡å‹çš„hyperparameter selectionï¼ŒNgä¹Ÿå‡ºäº†å»ºè®®ï¼Œé‚£å°±æ˜¯babysittingå’Œparallelæ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹ä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒæ•´ï¼Œä¸€ç§æ˜¯åŒæ—¶å¯åŠ¨å¤šä¸ªä¸åŒhyperparameterçš„æ¨¡å‹ï¼Œæœ€åå–æ•ˆæœæœ€å¥½çš„ã€‚\n\nä¸¤ç§æ–¹æ³•æ®Šé€”åŒå½’ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…·ä½“æƒ…å†µåšå‡ºé€‰æ‹©ã€‚\n## Batch norm\n### Normalization\nç›¸ä¿¡å¤§å®¶éƒ½å¬è¯´è¿‡å¤§åé¼é¼çš„normalizationå§ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¾ˆå¥½çš„æå‡æ•°æ®å¤„ç†ï¼ˆä¾‹å¦‚gradient descentï¼‰çš„é€Ÿåº¦å’Œæ•ˆæœï¼Œåœ¨å¼•å…¥batch normä¹‹å‰ï¼Œæˆ‘ä¹Ÿç¨å¾®æä¸€ä¸‹normalizationï¼Œä¸‹é¢ä¸Šå…¬å¼ï¼š\n\nå¯¹äºè¾“å…¥æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹æ³•æ¥normalize\n$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$\n$$X = X- \\mu$$\n$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$\n$$ X = X/ \\sigma ^2$$\nè¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠè¾“å…¥æ•°æ®è½¬åŒ–æˆäº†ç¬¦åˆæœŸæœ›ä¸º0ï¼Œæ–¹å·®ä¸º1çš„Gaussian distributionçš„æ•°æ®ã€‚\n\nå½“ç„¶ï¼Œè¿™åªæ˜¯normaliztionä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¢«ç§°ä½œz-scoreæ–¹æ³•ã€‚\n### Batch norm\nä¸Šé¢è¯´çš„normalizationæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°neural networksä¸­ï¼Œå¯¹äºnerual networksä¸­çš„æŸä¸€ä¸ªlayeræ¥è¯´ï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå­¤ç«‹çš„è®¡ç®—è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥normalizationï¼Œå¯¹äº\\\\(z^{(i)}\\\\)æ¥è¯´ï¼š\n$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$\n$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$\n$$z^{(i)}_{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$\n$$z^{N(i)}= \\gamma z^{(i)}_{norm} + \\beta$$\nç„¶åæˆ‘ä»¬ç”¨æœ€ç»ˆçš„\\\\(z^{N\\[l](i)}\\\\)æ¥æ›¿æ¢\\\\(z^{\\[l](i)}\\\\) å°±å¯ä»¥ï¼Œå…¶ä¸­\\\\( \\gamma\\\\)å’Œ\\\\(\\beta\\\\)æ˜¯ä¸¤ä¸ªparameterï¼Œå¯ä»¥é€šè¿‡gradient descentæ¥æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªparameterå­˜åœ¨çš„æ„ä¹‰ï¼Œå°±æ˜¯å¯ä»¥è°ƒæ•´normalizationæ˜ å°„çš„Gaussian distributionï¼Œè€Œä¸æ˜¯ç»Ÿç»Ÿæ˜ å°„åˆ°Normal distributionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\\\(\\epsilon\\\\)æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨æ¥é¿å…åˆ†æ¯åˆ†0çš„æƒ…å†µã€‚\n\nå¦‚æœ\\\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\\\)ä¸”\\\\( \\beta = \\mu\\\\)çš„è¯ï¼Œé‚£ä¹ˆå…¶å®\\\\(z^{N(i)}=z^(i)\\\\)çš„ï¼Œå¤§å®¶å¯ä»¥ç®—ç®—ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œå°±æ˜¯ç›¸å½“äºæ²¡åšnormalization.\n### Batch norm on neural networks\nå¯¹äºneural networksï¼Œè¾“å…¥\\\\(X\\\\)é€šè¿‡parameter\\\\(w^{[1]}\\\\)å’Œ\\\\(b^{[1]}\\\\)å¾—åˆ°\\\\(z^{[1]}\\\\)ï¼Œé€šè¿‡\\\\(\\beta\\\\)å’Œ\\\\(\\gamma\\\\)è·å¾—\\\\(z^{N[1]}\\\\)ï¼Œç»è¿‡active functionåè·å¾—\\\\(a^{[1]}\\\\)ï¼Œé€šè¿‡\\\\(w^{[2]}\\\\)å’Œ\\\\(b^{[2]}\\\\)è·å¾—\\\\(z^{[2]}\\\\)ï¼Œå¦‚æ­¤ä¸‹å»ï¼Œä¸€ç›´åˆ°æœ€åçš„è¾“å‡ºå±‚ï¼Œå®Œæˆforward propagation.\n\nåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±æœ‰å››ä¸ªparametersï¼Œåˆ†åˆ«æ˜¯\\\\(w^{[l]}\\\\)ï¼Œ\\\\(b^{[l]}\\\\)ï¼Œ\\\\( \\beta^{[l]}\\\\)ï¼Œ\\\\( \\gamma^{[l]}\\\\)ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼š\n$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$\nä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨åšbatch normalçš„æ—¶å€™ï¼Œé¦–å…ˆä¼šæŠŠ\\\\(z^{[l]}\\\\)æ˜ å°„åˆ°æœŸæœ›ä¸º1æ–¹å·®ä¸º0çš„Gaussian distributionä¸Šï¼Œè¿™å°±æ„å‘³ç€\\\\(b^{[l]}\\\\)æ˜¯å¯ä»¥å¿½ç•¥æ‰çš„ï¼Œå› ä¸ºå³ä½¿ä¿ç•™ï¼Œåœ¨batch normalçš„æ—¶å€™ä¹Ÿä¼šè¢«å‡å»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„parameteråªæœ‰ä¸‰ä¸ªï¼Œå³ï¼š\\\\(w^{[l]}\\\\)ï¼Œ\\\\( \\beta^{[l]}\\\\)ï¼Œ\\\\( \\gamma^{[l]}\\\\)\n\nåœ¨backforwardçš„æ—¶å€™ï¼Œæˆ‘ä»¬å’Œæ™®é€šçš„neural networksä¸€æ ·ï¼Œåªæ˜¯å¯ä»¥ä¸ç”¨å†å»è®¡ç®—\\\\(db\\\\)\n### Solve covariate shift\nä»€ä¹ˆæ˜¯covariate shiftï¼Ÿç®€å•çš„ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦éšç€æ ·æœ¬çš„å˜åŒ–è€Œå˜åŒ–ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆç›´è§‚ï¼Œåœ¨çŒ«è„¸è¯•éªŒä¸­ï¼Œå‡è®¾training seté‡Œéƒ½æ˜¯é»‘çŒ«ï¼Œè¿™æ ·è·å¾—çš„æ¨¡å‹ï¼Œå¯¹äºèŠ±çŒ«è¯†åˆ«å°±æ˜¯ä¸é€‚ç”¨çš„ï¼Œè¿™å°±å«covariate shift. å…¶å®ï¼Œbatch normå¯ä»¥æ”¹å–„neural networksæ•ˆæœçš„åŸå› ï¼Œå°±å¯ä»¥ç†è§£ä¸ºsolve covariate shiftçš„è¿‡ç¨‹ã€‚\n\nOKï¼Œæˆ‘ä»¬æ¥è¯¦ç»†çœ‹çœ‹åŸå› ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¦‚å›¾çš„neural networksï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png)\nåœ¨æ ‡ç¤ºå‡ºçš„ä½ç½®ï¼Œæœ‰parameter\\\\(w^{[3]}\\\\)å’Œ\\\\(b^{[3]}\\\\)ï¼Œå¦‚æœæˆ‘ä»¬ç›–ä½å‰é¢çš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è·å¾—å¦‚å›¾çš„neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png)\nå¯¹äºneural networksæ¥è¯´ï¼Œç›¸å½“äºè·å¾—äº†é»‘ç®±è¾“å‡ºçš„\\\\(a^{[2]}\\\\)ï¼Œè€Œ\\\\(a^{[2]}\\\\)çš„å€¼å…¶å®å¹¶ä¸æ˜¯æ˜¯å›ºå®šçš„ï¼Œæ¯ä¸€æ¬¡iterationåéƒ½æœ‰ä¸ä¸€æ ·çš„\\\\(a^{[2]}\\\\)ï¼Œè¿™å°±äº§ç”Ÿäº†covariate shifté—®é¢˜ã€‚\n\nä½†æ˜¯ï¼Œbatch normå¯ä»¥å°†\\\\(a^{[2]}\\\\)çš„æœŸæœ›å’Œæ–¹å·®é™åˆ¶åˆ°\\\\(\\beta\\\\)å’Œ\\\\(gamma\\\\)æ§åˆ¶çš„èŒƒå›´å†…ï¼Œä»¥æ­¤**æå¤§é™åº¦**çš„ç¼“è§£äº†covariate shiftç°è±¡ã€‚\n\nå¦å¤–ï¼Œbatch normè¿˜å¯ä»¥æœ‰ä¸€äº›regularizationçš„ä½œç”¨ï¼Œç”±äºæ¯æ¬¡mini-batch gradient descentä¸­batch normä½œç”¨çš„sampleä¸ä¸€æ ·ï¼Œç±»ä¼¼äºdropoutçš„æ•ˆæœï¼Œä¼šç»™å¯¹åº”layeråŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»¥æ­¤äº§ç”Ÿä¸€äº›regularizationçš„æ•ˆæœã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šæŠŠbatch normåˆ—å…¥regularizationèŒƒç•´å†…ã€‚\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course2-week3","published":1,"updated":"2018-11-19T06:38:58.491Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4ov0003tr8lwnny342t","content":"<p>ä¸çŸ¥ä¸è§‰æ¥åˆ°ç¬¬ä¸‰å‘¨çš„è¯¾ç¨‹äº†ï¼Œå¤§å®¶åŠ æ²¹ï¼è¿™å‘¨çš„ä¸»è¦å†…å®¹æ˜¯hyperparameter selectionå’Œbatch normalçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸€å‘¨çš„å†…å®¹ï¼<br><a id=\"more\"></a></p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selectionåœ¨machine learningä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚gradient descentä¸­çš„learning rate \\(\\alpha\\)å°±æ˜¯å…³ä¹ç®—æ³•ç»“æœçš„é‡è¦hyperparameterï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»é€‰æ‹©å‘¢ï¼ŸNgç»™å‡ºäº†ä¸¤ä¸ªå»ºè®®ï¼š</p>\n<ul>\n<li>æ„å»ºå¤šä¸ªhyperparameteräº¤å‰ï¼Œéšæœºé€‰æ‹©å¤§å°ï¼Œé€‰æ‹©æ•ˆæœè¾ƒå¥½çš„èŒƒå›´ï¼Œç»§ç»­éšæœºé€‰æ‹©hyperparameterå¤§å°ï¼Œè§‚å¯Ÿç»“æœã€‚</li>\n<li>é€‰æ‹©å‚æ•°çš„æ—¶å€™åˆ†æ®µé€‰æ‹©ï¼Œå¹¶ä¸”ä½¿ç”¨logåˆ†æ®µï¼Œä¾‹å¦‚åœ¨0.0001åˆ°1ä¹‹é—´é€‰æ‹©ï¼Œå°†æ•°è½´åˆ†æˆ0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1å’Œ1ï¼Œè¿™æ ·é€‰æ‹©å‡ºçš„ç»“æœæ›´å¥½</li>\n</ul>\n<p>å¯¹äºæ•´ä½“æ¨¡å‹çš„hyperparameter selectionï¼ŒNgä¹Ÿå‡ºäº†å»ºè®®ï¼Œé‚£å°±æ˜¯babysittingå’Œparallelæ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹ä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒæ•´ï¼Œä¸€ç§æ˜¯åŒæ—¶å¯åŠ¨å¤šä¸ªä¸åŒhyperparameterçš„æ¨¡å‹ï¼Œæœ€åå–æ•ˆæœæœ€å¥½çš„ã€‚</p>\n<p>ä¸¤ç§æ–¹æ³•æ®Šé€”åŒå½’ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…·ä½“æƒ…å†µåšå‡ºé€‰æ‹©ã€‚</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>ç›¸ä¿¡å¤§å®¶éƒ½å¬è¯´è¿‡å¤§åé¼é¼çš„normalizationå§ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¾ˆå¥½çš„æå‡æ•°æ®å¤„ç†ï¼ˆä¾‹å¦‚gradient descentï¼‰çš„é€Ÿåº¦å’Œæ•ˆæœï¼Œåœ¨å¼•å…¥batch normä¹‹å‰ï¼Œæˆ‘ä¹Ÿç¨å¾®æä¸€ä¸‹normalizationï¼Œä¸‹é¢ä¸Šå…¬å¼ï¼š</p>\n<p>å¯¹äºè¾“å…¥æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹æ³•æ¥normalize<br>$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$<br>$$X = X- \\mu$$<br>$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$<br>$$ X = X/ \\sigma ^2$$<br>è¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠè¾“å…¥æ•°æ®è½¬åŒ–æˆäº†ç¬¦åˆæœŸæœ›ä¸º0ï¼Œæ–¹å·®ä¸º1çš„Gaussian distributionçš„æ•°æ®ã€‚</p>\n<p>å½“ç„¶ï¼Œè¿™åªæ˜¯normaliztionä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¢«ç§°ä½œz-scoreæ–¹æ³•ã€‚</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>ä¸Šé¢è¯´çš„normalizationæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°neural networksä¸­ï¼Œå¯¹äºnerual networksä¸­çš„æŸä¸€ä¸ªlayeræ¥è¯´ï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå­¤ç«‹çš„è®¡ç®—è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥normalizationï¼Œå¯¹äº\\(z^{(i)}\\)æ¥è¯´ï¼š<br>$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$<br>$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$<br>$$z^{(i)}<em>{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$<br>$$z^{N(i)}= \\gamma z^{(i)}</em>{norm} + \\beta$$<br>ç„¶åæˆ‘ä»¬ç”¨æœ€ç»ˆçš„\\(z^{N[l](i)}\\)æ¥æ›¿æ¢\\(z^{[l](i)}\\) å°±å¯ä»¥ï¼Œå…¶ä¸­\\( \\gamma\\)å’Œ\\(\\beta\\)æ˜¯ä¸¤ä¸ªparameterï¼Œå¯ä»¥é€šè¿‡gradient descentæ¥æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªparameterå­˜åœ¨çš„æ„ä¹‰ï¼Œå°±æ˜¯å¯ä»¥è°ƒæ•´normalizationæ˜ å°„çš„Gaussian distributionï¼Œè€Œä¸æ˜¯ç»Ÿç»Ÿæ˜ å°„åˆ°Normal distributionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(\\epsilon\\)æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨æ¥é¿å…åˆ†æ¯åˆ†0çš„æƒ…å†µã€‚</p>\n<p>å¦‚æœ\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)ä¸”\\( \\beta = \\mu\\)çš„è¯ï¼Œé‚£ä¹ˆå…¶å®\\(z^{N(i)}=z^(i)\\)çš„ï¼Œå¤§å®¶å¯ä»¥ç®—ç®—ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œå°±æ˜¯ç›¸å½“äºæ²¡åšnormalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>å¯¹äºneural networksï¼Œè¾“å…¥\\(X\\)é€šè¿‡parameter\\(w^{[1]}\\)å’Œ\\(b^{[1]}\\)å¾—åˆ°\\(z^{[1]}\\)ï¼Œé€šè¿‡\\(\\beta\\)å’Œ\\(\\gamma\\)è·å¾—\\(z^{N[1]}\\)ï¼Œç»è¿‡active functionåè·å¾—\\(a^{[1]}\\)ï¼Œé€šè¿‡\\(w^{[2]}\\)å’Œ\\(b^{[2]}\\)è·å¾—\\(z^{[2]}\\)ï¼Œå¦‚æ­¤ä¸‹å»ï¼Œä¸€ç›´åˆ°æœ€åçš„è¾“å‡ºå±‚ï¼Œå®Œæˆforward propagation.</p>\n<p>åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±æœ‰å››ä¸ªparametersï¼Œåˆ†åˆ«æ˜¯\\(w^{[l]}\\)ï¼Œ\\(b^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\)ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼š<br>$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$<br>ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨åšbatch normalçš„æ—¶å€™ï¼Œé¦–å…ˆä¼šæŠŠ\\(z^{[l]}\\)æ˜ å°„åˆ°æœŸæœ›ä¸º1æ–¹å·®ä¸º0çš„Gaussian distributionä¸Šï¼Œè¿™å°±æ„å‘³ç€\\(b^{[l]}\\)æ˜¯å¯ä»¥å¿½ç•¥æ‰çš„ï¼Œå› ä¸ºå³ä½¿ä¿ç•™ï¼Œåœ¨batch normalçš„æ—¶å€™ä¹Ÿä¼šè¢«å‡å»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„parameteråªæœ‰ä¸‰ä¸ªï¼Œå³ï¼š\\(w^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\)</p>\n<p>åœ¨backforwardçš„æ—¶å€™ï¼Œæˆ‘ä»¬å’Œæ™®é€šçš„neural networksä¸€æ ·ï¼Œåªæ˜¯å¯ä»¥ä¸ç”¨å†å»è®¡ç®—\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>ä»€ä¹ˆæ˜¯covariate shiftï¼Ÿç®€å•çš„ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦éšç€æ ·æœ¬çš„å˜åŒ–è€Œå˜åŒ–ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆç›´è§‚ï¼Œåœ¨çŒ«è„¸è¯•éªŒä¸­ï¼Œå‡è®¾training seté‡Œéƒ½æ˜¯é»‘çŒ«ï¼Œè¿™æ ·è·å¾—çš„æ¨¡å‹ï¼Œå¯¹äºèŠ±çŒ«è¯†åˆ«å°±æ˜¯ä¸é€‚ç”¨çš„ï¼Œè¿™å°±å«covariate shift. å…¶å®ï¼Œbatch normå¯ä»¥æ”¹å–„neural networksæ•ˆæœçš„åŸå› ï¼Œå°±å¯ä»¥ç†è§£ä¸ºsolve covariate shiftçš„è¿‡ç¨‹ã€‚</p>\n<p>OKï¼Œæˆ‘ä»¬æ¥è¯¦ç»†çœ‹çœ‹åŸå› ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¦‚å›¾çš„neural networksï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png\" alt=\"\"><br>åœ¨æ ‡ç¤ºå‡ºçš„ä½ç½®ï¼Œæœ‰parameter\\(w^{[3]}\\)å’Œ\\(b^{[3]}\\)ï¼Œå¦‚æœæˆ‘ä»¬ç›–ä½å‰é¢çš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è·å¾—å¦‚å›¾çš„neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png\" alt=\"\"><br>å¯¹äºneural networksæ¥è¯´ï¼Œç›¸å½“äºè·å¾—äº†é»‘ç®±è¾“å‡ºçš„\\(a^{[2]}\\)ï¼Œè€Œ\\(a^{[2]}\\)çš„å€¼å…¶å®å¹¶ä¸æ˜¯æ˜¯å›ºå®šçš„ï¼Œæ¯ä¸€æ¬¡iterationåéƒ½æœ‰ä¸ä¸€æ ·çš„\\(a^{[2]}\\)ï¼Œè¿™å°±äº§ç”Ÿäº†covariate shifté—®é¢˜ã€‚</p>\n<p>ä½†æ˜¯ï¼Œbatch normå¯ä»¥å°†\\(a^{[2]}\\)çš„æœŸæœ›å’Œæ–¹å·®é™åˆ¶åˆ°\\(\\beta\\)å’Œ\\(gamma\\)æ§åˆ¶çš„èŒƒå›´å†…ï¼Œä»¥æ­¤<strong>æå¤§é™åº¦</strong>çš„ç¼“è§£äº†covariate shiftç°è±¡ã€‚</p>\n<p>å¦å¤–ï¼Œbatch normè¿˜å¯ä»¥æœ‰ä¸€äº›regularizationçš„ä½œç”¨ï¼Œç”±äºæ¯æ¬¡mini-batch gradient descentä¸­batch normä½œç”¨çš„sampleä¸ä¸€æ ·ï¼Œç±»ä¼¼äºdropoutçš„æ•ˆæœï¼Œä¼šç»™å¯¹åº”layeråŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»¥æ­¤äº§ç”Ÿä¸€äº›regularizationçš„æ•ˆæœã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šæŠŠbatch normåˆ—å…¥regularizationèŒƒç•´å†…ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"hyperparameter","path":"tags/hyperparameter/"},{"name":"batch norm","path":"tags/batch-norm/"},{"name":"covariate shift","path":"tags/covariate-shift/"}],"excerpt":"<p>ä¸çŸ¥ä¸è§‰æ¥åˆ°ç¬¬ä¸‰å‘¨çš„è¯¾ç¨‹äº†ï¼Œå¤§å®¶åŠ æ²¹ï¼è¿™å‘¨çš„ä¸»è¦å†…å®¹æ˜¯hyperparameter selectionå’Œbatch normalçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸€å‘¨çš„å†…å®¹ï¼<br></p>","more":"</p>\n<h2 id=\"Hyperparameter-selection\"><a href=\"#Hyperparameter-selection\" class=\"headerlink\" title=\"Hyperparameter selection\"></a>Hyperparameter selection</h2><p>Hyperparameter selectionåœ¨machine learningä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚gradient descentä¸­çš„learning rate \\(\\alpha\\)å°±æ˜¯å…³ä¹ç®—æ³•ç»“æœçš„é‡è¦hyperparameterï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»é€‰æ‹©å‘¢ï¼ŸNgç»™å‡ºäº†ä¸¤ä¸ªå»ºè®®ï¼š</p>\n<ul>\n<li>æ„å»ºå¤šä¸ªhyperparameteräº¤å‰ï¼Œéšæœºé€‰æ‹©å¤§å°ï¼Œé€‰æ‹©æ•ˆæœè¾ƒå¥½çš„èŒƒå›´ï¼Œç»§ç»­éšæœºé€‰æ‹©hyperparameterå¤§å°ï¼Œè§‚å¯Ÿç»“æœã€‚</li>\n<li>é€‰æ‹©å‚æ•°çš„æ—¶å€™åˆ†æ®µé€‰æ‹©ï¼Œå¹¶ä¸”ä½¿ç”¨logåˆ†æ®µï¼Œä¾‹å¦‚åœ¨0.0001åˆ°1ä¹‹é—´é€‰æ‹©ï¼Œå°†æ•°è½´åˆ†æˆ0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1å’Œ1ï¼Œè¿™æ ·é€‰æ‹©å‡ºçš„ç»“æœæ›´å¥½</li>\n</ul>\n<p>å¯¹äºæ•´ä½“æ¨¡å‹çš„hyperparameter selectionï¼ŒNgä¹Ÿå‡ºäº†å»ºè®®ï¼Œé‚£å°±æ˜¯babysittingå’Œparallelæ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹ä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒæ•´ï¼Œä¸€ç§æ˜¯åŒæ—¶å¯åŠ¨å¤šä¸ªä¸åŒhyperparameterçš„æ¨¡å‹ï¼Œæœ€åå–æ•ˆæœæœ€å¥½çš„ã€‚</p>\n<p>ä¸¤ç§æ–¹æ³•æ®Šé€”åŒå½’ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…·ä½“æƒ…å†µåšå‡ºé€‰æ‹©ã€‚</p>\n<h2 id=\"Batch-norm\"><a href=\"#Batch-norm\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h2><h3 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h3><p>ç›¸ä¿¡å¤§å®¶éƒ½å¬è¯´è¿‡å¤§åé¼é¼çš„normalizationå§ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¾ˆå¥½çš„æå‡æ•°æ®å¤„ç†ï¼ˆä¾‹å¦‚gradient descentï¼‰çš„é€Ÿåº¦å’Œæ•ˆæœï¼Œåœ¨å¼•å…¥batch normä¹‹å‰ï¼Œæˆ‘ä¹Ÿç¨å¾®æä¸€ä¸‹normalizationï¼Œä¸‹é¢ä¸Šå…¬å¼ï¼š</p>\n<p>å¯¹äºè¾“å…¥æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹æ³•æ¥normalize<br>$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$<br>$$X = X- \\mu$$<br>$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$<br>$$ X = X/ \\sigma ^2$$<br>è¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠè¾“å…¥æ•°æ®è½¬åŒ–æˆäº†ç¬¦åˆæœŸæœ›ä¸º0ï¼Œæ–¹å·®ä¸º1çš„Gaussian distributionçš„æ•°æ®ã€‚</p>\n<p>å½“ç„¶ï¼Œè¿™åªæ˜¯normaliztionä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¢«ç§°ä½œz-scoreæ–¹æ³•ã€‚</p>\n<h3 id=\"Batch-norm-1\"><a href=\"#Batch-norm-1\" class=\"headerlink\" title=\"Batch norm\"></a>Batch norm</h3><p>ä¸Šé¢è¯´çš„normalizationæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°neural networksä¸­ï¼Œå¯¹äºnerual networksä¸­çš„æŸä¸€ä¸ªlayeræ¥è¯´ï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå­¤ç«‹çš„è®¡ç®—è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥normalizationï¼Œå¯¹äº\\(z^{(i)}\\)æ¥è¯´ï¼š<br>$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$<br>$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$<br>$$z^{(i)}<em>{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$<br>$$z^{N(i)}= \\gamma z^{(i)}</em>{norm} + \\beta$$<br>ç„¶åæˆ‘ä»¬ç”¨æœ€ç»ˆçš„\\(z^{N[l](i)}\\)æ¥æ›¿æ¢\\(z^{[l](i)}\\) å°±å¯ä»¥ï¼Œå…¶ä¸­\\( \\gamma\\)å’Œ\\(\\beta\\)æ˜¯ä¸¤ä¸ªparameterï¼Œå¯ä»¥é€šè¿‡gradient descentæ¥æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªparameterå­˜åœ¨çš„æ„ä¹‰ï¼Œå°±æ˜¯å¯ä»¥è°ƒæ•´normalizationæ˜ å°„çš„Gaussian distributionï¼Œè€Œä¸æ˜¯ç»Ÿç»Ÿæ˜ å°„åˆ°Normal distributionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(\\epsilon\\)æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨æ¥é¿å…åˆ†æ¯åˆ†0çš„æƒ…å†µã€‚</p>\n<p>å¦‚æœ\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)ä¸”\\( \\beta = \\mu\\)çš„è¯ï¼Œé‚£ä¹ˆå…¶å®\\(z^{N(i)}=z^(i)\\)çš„ï¼Œå¤§å®¶å¯ä»¥ç®—ç®—ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œå°±æ˜¯ç›¸å½“äºæ²¡åšnormalization.</p>\n<h3 id=\"Batch-norm-on-neural-networks\"><a href=\"#Batch-norm-on-neural-networks\" class=\"headerlink\" title=\"Batch norm on neural networks\"></a>Batch norm on neural networks</h3><p>å¯¹äºneural networksï¼Œè¾“å…¥\\(X\\)é€šè¿‡parameter\\(w^{[1]}\\)å’Œ\\(b^{[1]}\\)å¾—åˆ°\\(z^{[1]}\\)ï¼Œé€šè¿‡\\(\\beta\\)å’Œ\\(\\gamma\\)è·å¾—\\(z^{N[1]}\\)ï¼Œç»è¿‡active functionåè·å¾—\\(a^{[1]}\\)ï¼Œé€šè¿‡\\(w^{[2]}\\)å’Œ\\(b^{[2]}\\)è·å¾—\\(z^{[2]}\\)ï¼Œå¦‚æ­¤ä¸‹å»ï¼Œä¸€ç›´åˆ°æœ€åçš„è¾“å‡ºå±‚ï¼Œå®Œæˆforward propagation.</p>\n<p>åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±æœ‰å››ä¸ªparametersï¼Œåˆ†åˆ«æ˜¯\\(w^{[l]}\\)ï¼Œ\\(b^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\)ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼š<br>$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$<br>ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨åšbatch normalçš„æ—¶å€™ï¼Œé¦–å…ˆä¼šæŠŠ\\(z^{[l]}\\)æ˜ å°„åˆ°æœŸæœ›ä¸º1æ–¹å·®ä¸º0çš„Gaussian distributionä¸Šï¼Œè¿™å°±æ„å‘³ç€\\(b^{[l]}\\)æ˜¯å¯ä»¥å¿½ç•¥æ‰çš„ï¼Œå› ä¸ºå³ä½¿ä¿ç•™ï¼Œåœ¨batch normalçš„æ—¶å€™ä¹Ÿä¼šè¢«å‡å»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„parameteråªæœ‰ä¸‰ä¸ªï¼Œå³ï¼š\\(w^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\)</p>\n<p>åœ¨backforwardçš„æ—¶å€™ï¼Œæˆ‘ä»¬å’Œæ™®é€šçš„neural networksä¸€æ ·ï¼Œåªæ˜¯å¯ä»¥ä¸ç”¨å†å»è®¡ç®—\\(db\\)</p>\n<h3 id=\"Solve-covariate-shift\"><a href=\"#Solve-covariate-shift\" class=\"headerlink\" title=\"Solve covariate shift\"></a>Solve covariate shift</h3><p>ä»€ä¹ˆæ˜¯covariate shiftï¼Ÿç®€å•çš„ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦éšç€æ ·æœ¬çš„å˜åŒ–è€Œå˜åŒ–ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆç›´è§‚ï¼Œåœ¨çŒ«è„¸è¯•éªŒä¸­ï¼Œå‡è®¾training seté‡Œéƒ½æ˜¯é»‘çŒ«ï¼Œè¿™æ ·è·å¾—çš„æ¨¡å‹ï¼Œå¯¹äºèŠ±çŒ«è¯†åˆ«å°±æ˜¯ä¸é€‚ç”¨çš„ï¼Œè¿™å°±å«covariate shift. å…¶å®ï¼Œbatch normå¯ä»¥æ”¹å–„neural networksæ•ˆæœçš„åŸå› ï¼Œå°±å¯ä»¥ç†è§£ä¸ºsolve covariate shiftçš„è¿‡ç¨‹ã€‚</p>\n<p>OKï¼Œæˆ‘ä»¬æ¥è¯¦ç»†çœ‹çœ‹åŸå› ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¦‚å›¾çš„neural networksï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-1.png\" alt=\"\"><br>åœ¨æ ‡ç¤ºå‡ºçš„ä½ç½®ï¼Œæœ‰parameter\\(w^{[3]}\\)å’Œ\\(b^{[3]}\\)ï¼Œå¦‚æœæˆ‘ä»¬ç›–ä½å‰é¢çš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è·å¾—å¦‚å›¾çš„neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/8/8-2.png\" alt=\"\"><br>å¯¹äºneural networksæ¥è¯´ï¼Œç›¸å½“äºè·å¾—äº†é»‘ç®±è¾“å‡ºçš„\\(a^{[2]}\\)ï¼Œè€Œ\\(a^{[2]}\\)çš„å€¼å…¶å®å¹¶ä¸æ˜¯æ˜¯å›ºå®šçš„ï¼Œæ¯ä¸€æ¬¡iterationåéƒ½æœ‰ä¸ä¸€æ ·çš„\\(a^{[2]}\\)ï¼Œè¿™å°±äº§ç”Ÿäº†covariate shifté—®é¢˜ã€‚</p>\n<p>ä½†æ˜¯ï¼Œbatch normå¯ä»¥å°†\\(a^{[2]}\\)çš„æœŸæœ›å’Œæ–¹å·®é™åˆ¶åˆ°\\(\\beta\\)å’Œ\\(gamma\\)æ§åˆ¶çš„èŒƒå›´å†…ï¼Œä»¥æ­¤<strong>æå¤§é™åº¦</strong>çš„ç¼“è§£äº†covariate shiftç°è±¡ã€‚</p>\n<p>å¦å¤–ï¼Œbatch normè¿˜å¯ä»¥æœ‰ä¸€äº›regularizationçš„ä½œç”¨ï¼Œç”±äºæ¯æ¬¡mini-batch gradient descentä¸­batch normä½œç”¨çš„sampleä¸ä¸€æ ·ï¼Œç±»ä¼¼äºdropoutçš„æ•ˆæœï¼Œä¼šç»™å¯¹åº”layeråŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»¥æ­¤äº§ç”Ÿä¸€äº›regularizationçš„æ•ˆæœã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šæŠŠbatch normåˆ—å…¥regularizationèŒƒç•´å†…ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course2, week2","date":"2017-09-27T13:38:35.000Z","_content":"å¤§å®¶å¥½ï¼Œè¯¾ç¨‹æ¥åˆ°äº†ç¬¬äºŒå‘¨ï¼Œè¿™å‘¨ä¸»è¦æ˜¯ä¸€äº›ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿å¾—æ•´ä¸ªneural networkså¯ä»¥æ›´å¿«æ›´å¥½çš„å·¥ä½œï¼Œæˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹ã€‚\n<!--more-->\n## Mini-batch gradient descent\nè¿™ä¸€èŠ‚æˆ‘å°±ä¸æ‰“ç®—å†™äº†ï¼Œæ¯”è¾ƒåŸºç¡€ï¼Œå…¶å®mini-batch gradient descentæ˜¯batch gradientå’Œstochastic gradient descentç»¼åˆåçš„ç»“æœï¼Œä¸€æ–¹é¢è§£å†³äº†batch gradientè®¡ç®—é‡å¤§ï¼Œå®¹æ˜“convergeåˆ°local minimumçš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†stochastic gradient descent epochå¤ªå¤šçš„å¼Šç«¯ï¼Œæ˜¯ç°åœ¨gradient descentä½¿ç”¨æœ€å¹¿æ³›çš„æ–¹æ³•ã€‚\n\nå¯¹äºmini-batch gradient descentï¼ŒNgå»ºè®®batchå¤§å°å–å†³äºæ•°æ®é‡å¤šå°‘ï¼Œåœ¨å°æ•°æ®é‡ä¸Šå®Œå…¨æ²¡æœ‰å¿…è¦åšmini-batchï¼Œç›´æ¥ä½¿ç”¨batch gradient descentå°±å¯ä»¥ï¼Œå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼Œæœ€å¥½é€‰æ‹©2çš„ä¹˜æ–¹ï¼Œå¦‚64,128,256,512æ¥ä½œä¸ºbatch size. å½“ç„¶ï¼Œbatch sizeä¹Ÿè¦æ»¡è¶³CPUå’ŒGPUçš„å†…å­˜å¤§å°ã€‚\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averagesï¼Œä¹Ÿè¢«ç§°ä¸ºmoving averagesï¼Œæ˜¯ä¸€ç§ç»¼åˆå†å²æ•°æ®çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œè¯¾ç¨‹ä¸­Ngç”¨äº†ä¼¦æ•¦ä¸€å¹´çš„æ°”æ¸©å˜åŒ–æ›²çº¿ä½œä¸ºä¾‹å­ï¼Œå¯¹äºå›ºæœ‰å˜é‡\\\\(\\theta\\\\)æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ±‚çš„å¹³å‡å€¼\\\\(v\\\\)åº”è¯¥æ˜¯\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\nå…¶ä¸­\\\\(\\beta\\\\)æ˜¯ä¸€ä¸ªå› å­ï¼Œå®ƒå†³å®šäº†moving averageså¤§çº¦å‘å‰å¹³å‡äº†\\\\(\\frac{1}{1- \\beta}\\\\)ä¸ª\\\\(\\theta\\\\)å€¼ï¼Œä¾‹å¦‚\\\\(\\beta = 0.9\\\\)ï¼Œé‚£ä¹ˆå¤§çº¦å‘å‰å¹³å‡äº†10ä¸ªå€¼ï¼Œå¹¶ä¸”æ˜¯å‘å‰æŒ‰æŒ‡æ•°è¡°å‡åŠ æƒè·å¾—çš„å¹³å‡å€¼ã€‚\n### Bias correction\nåœ¨exponentially weighted averagesä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¾ˆå°–é”ï¼Œé‚£å°±æ˜¯åœ¨æœ€åˆçš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œç”±äº\\\\(v_0=0\\\\)ï¼Œå¯¼è‡´å‰é¢çš„æ•°å­—ç»“æœè·ç¦»æ­£ç¡®ç»“æœè¾ƒå°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²æ›²çº¿æ˜¯è·å¾—çš„ç»“æœï¼Œè€Œåœ¨èµ·å§‹ä½ç½®çš„å€¼æ˜æ˜¾æ˜¯åå°çš„ã€‚\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png)\næ­¤æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥bias correctionï¼ŒåŸç†ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯æ­¤å¤„æˆ‘ä»¬ä¸ä½¿ç”¨\\\\(v_t\\\\)ä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œè€Œæ˜¯ä½¿ç”¨\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)ä½œä¸ºæœ€åçš„ç»“æœï¼Œé€šè¿‡bias correctionï¼Œæˆ‘ä»¬ä¼šè·å¾—ç»¿è‰²çš„æ›²çº¿ã€‚\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèµ·å§‹ç»¿è‰²é’±å’Œç´«è‰²æ›²çº¿åœ¨æœ€ååŸºæœ¬æ²¡æœ‰å·®åˆ«ï¼Œå‡ ä¹é‡åˆï¼Œä½†æ˜¯åœ¨æ›²çº¿å¼€å§‹çš„æ—¶å€™ï¼Œç»¿è‰²æ›²çº¿æ¯”ç´«è‰²æ›²çº¿æ›´åŠ é€¼è¿‘çœŸå®æƒ…å†µï¼Œå› æ­¤ï¼ŒNgç»™æˆ‘ä»¬ä»¥ä¸‹å»ºè®®ï¼š\n* å½“æˆ‘ä»¬ä¸å…³æ³¨moving averages initial valueå¤§å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ä½¿ç”¨bias correction\n* Bias correctionå¯¹äºinitial valueæ•ˆæœæ›´å¥½\n\n## Gradient descent optimization\n### momentum\nåœ¨gradient descentä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ç§æƒ…å†µï¼Œå¦‚å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png)\nåœ¨æ°´å¹³æ–¹å‘ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¿«çš„ä¸‹é™ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ä¸Šæˆ‘ä»¬å¸Œæœ›æ›´å°çš„ä¸‹é™é€Ÿç‡ï¼Œä»¥é¿å…è¿‡å¤šçš„iterationï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è®²moving averagesçš„æ€æƒ³å¸¦å…¥è¿›æ¥ï¼Œè¿™å°±æ˜¯momentumæ–¹æ³•ã€‚\n\nåœ¨momentumä¸­ï¼Œæˆ‘ä»¬çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼š\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\nåœ¨å¾ˆå¤šçš„æ–‡çŒ®ä¸­ï¼Œä¸Šé¢çš„\\\\(1- \\beta\\\\)é¡¹è¢«çœç•¥æ‰äº†ï¼Œè¿™æ ·åšåªæ˜¯å°†ç­‰å¼ç­‰é‡åšäº†ç¼©æ”¾ï¼Œå¹¶ä¸å½±å“å®é™…çš„æ•ˆæœï¼ŒNgè¡¨ç¤ºï¼Œä¸¤ç§æ–¹å¼çš„momentumå‡ ä¹æ²¡æœ‰å·®åˆ«ï¼Œå¤§å®¶å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚\n\nå®é™…ä¸Šï¼Œmomentumå¯ä»¥ç†è§£ä¸ºå°†æ•°æ¬¡ä¹‹å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„gradientå˜åŒ–ä¹Ÿå¸¦å…¥åˆ°äº†è¿™æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºå°±æ˜¯å¸¦å…¥äº†ä¸€ç§gradientå˜åŒ–çš„è¶‹åŠ¿ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„æ§åˆ¶gradient descentçš„æ–¹å‘å’Œå¤§å°ã€‚ä¾‹å¦‚ä¸Šå›¾ä¸­çš„çºµå‘æ–¹å‘ä¸­ï¼ŒåŠ å…¥momentumåå¯ä»¥è½»æ¾çš„å°†çºµå‘æ¢¯åº¦æ­£è´ŸæŠµæ¶ˆåˆ°è¿‘ä¼¼0ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘åœ¨gradient descentåœ¨çºµå‘çš„åå¤è¿­ä»£ï¼Œå› ä¸ºï¼Œé‚£æ—¢æ˜¯æ— ç”¨åŠŸï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„æƒ…å†µã€‚\n\nå¦å¤–ï¼ŒNgç»™æˆ‘ä»¬äº†ä¸€ä¸ª\\\\(\\beta\\\\)çš„ç†æƒ³å–å€¼ï¼Œæ—¢0.9ï¼Œè¿™ä¸ªå€¼å¤§çº¦å–äº†å‰10æ¬¡è¿­ä»£ç»“æœçš„moving averagesã€‚å¦å¤–ï¼ŒNgè¡¨ç¤ºï¼Œåœ¨momenä¸­å¾ˆå°‘ä½¿ç”¨bias correctionï¼Œå› ä¸º10æ¬¡è¿­ä»£ä¹‹åï¼Œè¿™ç§é—®é¢˜å¾ˆå¿«å°±ä¼šæ¶ˆé™¤ï¼Œè€Œä¸€èˆ¬çš„gradient descentï¼Œiterationæ¬¡æ•°è¿œè¿œå¤§äº10æ¬¡ã€‚\n### RMSprop\né™¤äº†momentumï¼Œè¿˜æœ‰ä¸€äº›ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤§åé¼é¼çš„RMSprop(root means square prop)ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦ä½“ç°äº†squareçš„åº”ç”¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨æ¯æ¬¡çš„è¿­ä»£ä¸­ï¼š\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\\\(10^{-8}\\\\)\n\nå‡è®¾åœ¨gradient descentä¸­ï¼Œ\\\\(W\\\\)ä¸‹é™é€Ÿç‡å¤ªä½ï¼Œä¹Ÿå°±æ˜¯\\\\(dW\\\\)å¤ªå°ï¼Œé‚£ä¹ˆåœ¨RMSpropçš„è¿­ä»£ä¸­ï¼Œ\\\\(dW\\\\)å°†ä¼šé™¤ä»¥ä¸€ä¸ªå¾ˆå°çš„å€¼\\\\( \\sqrt{S_{dW}}\\\\)ï¼Œä¹Ÿå°±æ˜¯\\\\(W\\\\)å°†ä¼šå‡å»ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ç¼“è§£äº†ä¸Šå›¾æ‰€ç¤ºçš„æƒ…å†µï¼Œæ”¹å–„äº†gradient descentçš„åˆç†æ€§ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„\\\\(\\alpha\\\\)å»å®ç°æ›´å¿«çš„gradient descent.\n### Adam\næˆ‘ä»¬çœ‹åˆ°äº†momentumå’ŒRMSpropä¼˜åŒ–æ–¹æ³•çš„å‰å®³ä¹‹å¤„ï¼Œç°åœ¨Adamæ–¹æ³•æ¨ªç©ºå‡ºä¸–ï¼Œä»–èåˆäº†momentumå’ŒRMSpropï¼Œä»–æ˜¯å¦‚ä½•èåˆçš„å‘¢ï¼Œæˆ‘ä»¬æŠŠmomentumä¸­çš„\\\\( \\beta\\\\)å‘½åä¸º\\\\( \\beta\\_1\\\\)ï¼ŒæŠŠRMSpropä¸­çš„\\\\( \\beta\\\\)å‘½åä¸º\\\\( \\beta\\_2\\\\)ï¼Œåœ¨ç¬¬\\\\(t\\\\)æ¬¡è¿­ä»£ä¸­ï¼š\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\nåŠ ä¸Šbias correctionå\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\nå…¶ä¸­ï¼Œ\\\\(\\epsilon\\\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\\\(10^{-8}\\\\)ï¼Œ\\\\(\\beta\\_1\\\\)å»ºè®®å–å€¼0.9ï¼Œ\\\\(\\beta\\_2\\\\)å»ºè®®å–å€¼0.999ã€‚\n## Learning rate decay\nåœ¨gradient descentä¸­ï¼Œéšç€è¿­ä»£çš„æ·±åº¦ï¼Œè¶Šæ¥è¶Šé è¿‘minimumï¼Œæˆ‘ä»¬éœ€è¦æ›´å°çš„learning rateï¼Œä»¥é¿å…è¶Šè¿‡minimumï¼Œå¸¸ç”¨çš„learning decayæ–¹æ³•æœ‰ï¼š\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\nè¿™äº›æ–¹æ³•éƒ½å¯ä»¥è®©\\\\(\\alpha\\\\)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œæ…¢æ…¢å˜å°ï¼Œå¯ä»¥æ›´å¥½çš„é€¼è¿‘minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","source":"_posts/course-deep-learning-course2-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course2, week2\ndate: 2017-09-27 21:38:35\ntags:\n\t- gradient descent\n\t- moving averages\ncategories: learning notes\n---\nå¤§å®¶å¥½ï¼Œè¯¾ç¨‹æ¥åˆ°äº†ç¬¬äºŒå‘¨ï¼Œè¿™å‘¨ä¸»è¦æ˜¯ä¸€äº›ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿å¾—æ•´ä¸ªneural networkså¯ä»¥æ›´å¿«æ›´å¥½çš„å·¥ä½œï¼Œæˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹ã€‚\n<!--more-->\n## Mini-batch gradient descent\nè¿™ä¸€èŠ‚æˆ‘å°±ä¸æ‰“ç®—å†™äº†ï¼Œæ¯”è¾ƒåŸºç¡€ï¼Œå…¶å®mini-batch gradient descentæ˜¯batch gradientå’Œstochastic gradient descentç»¼åˆåçš„ç»“æœï¼Œä¸€æ–¹é¢è§£å†³äº†batch gradientè®¡ç®—é‡å¤§ï¼Œå®¹æ˜“convergeåˆ°local minimumçš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†stochastic gradient descent epochå¤ªå¤šçš„å¼Šç«¯ï¼Œæ˜¯ç°åœ¨gradient descentä½¿ç”¨æœ€å¹¿æ³›çš„æ–¹æ³•ã€‚\n\nå¯¹äºmini-batch gradient descentï¼ŒNgå»ºè®®batchå¤§å°å–å†³äºæ•°æ®é‡å¤šå°‘ï¼Œåœ¨å°æ•°æ®é‡ä¸Šå®Œå…¨æ²¡æœ‰å¿…è¦åšmini-batchï¼Œç›´æ¥ä½¿ç”¨batch gradient descentå°±å¯ä»¥ï¼Œå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼Œæœ€å¥½é€‰æ‹©2çš„ä¹˜æ–¹ï¼Œå¦‚64,128,256,512æ¥ä½œä¸ºbatch size. å½“ç„¶ï¼Œbatch sizeä¹Ÿè¦æ»¡è¶³CPUå’ŒGPUçš„å†…å­˜å¤§å°ã€‚\n## Exponentially weighted averages\n### Exponentially weighted averages\nExponentially weighted averagesï¼Œä¹Ÿè¢«ç§°ä¸ºmoving averagesï¼Œæ˜¯ä¸€ç§ç»¼åˆå†å²æ•°æ®çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œè¯¾ç¨‹ä¸­Ngç”¨äº†ä¼¦æ•¦ä¸€å¹´çš„æ°”æ¸©å˜åŒ–æ›²çº¿ä½œä¸ºä¾‹å­ï¼Œå¯¹äºå›ºæœ‰å˜é‡\\\\(\\theta\\\\)æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ±‚çš„å¹³å‡å€¼\\\\(v\\\\)åº”è¯¥æ˜¯\n$$v_0 = 0$$ \n$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$\n$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$\n$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$\n$$\\cdots$$\n$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$\nå…¶ä¸­\\\\(\\beta\\\\)æ˜¯ä¸€ä¸ªå› å­ï¼Œå®ƒå†³å®šäº†moving averageså¤§çº¦å‘å‰å¹³å‡äº†\\\\(\\frac{1}{1- \\beta}\\\\)ä¸ª\\\\(\\theta\\\\)å€¼ï¼Œä¾‹å¦‚\\\\(\\beta = 0.9\\\\)ï¼Œé‚£ä¹ˆå¤§çº¦å‘å‰å¹³å‡äº†10ä¸ªå€¼ï¼Œå¹¶ä¸”æ˜¯å‘å‰æŒ‰æŒ‡æ•°è¡°å‡åŠ æƒè·å¾—çš„å¹³å‡å€¼ã€‚\n### Bias correction\nåœ¨exponentially weighted averagesä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¾ˆå°–é”ï¼Œé‚£å°±æ˜¯åœ¨æœ€åˆçš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œç”±äº\\\\(v_0=0\\\\)ï¼Œå¯¼è‡´å‰é¢çš„æ•°å­—ç»“æœè·ç¦»æ­£ç¡®ç»“æœè¾ƒå°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²æ›²çº¿æ˜¯è·å¾—çš„ç»“æœï¼Œè€Œåœ¨èµ·å§‹ä½ç½®çš„å€¼æ˜æ˜¾æ˜¯åå°çš„ã€‚\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png)\næ­¤æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥bias correctionï¼ŒåŸç†ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯æ­¤å¤„æˆ‘ä»¬ä¸ä½¿ç”¨\\\\(v_t\\\\)ä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œè€Œæ˜¯ä½¿ç”¨\\\\( \\frac{v_t}{1- \\beta^{t}}\\\\)ä½œä¸ºæœ€åçš„ç»“æœï¼Œé€šè¿‡bias correctionï¼Œæˆ‘ä»¬ä¼šè·å¾—ç»¿è‰²çš„æ›²çº¿ã€‚\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèµ·å§‹ç»¿è‰²é’±å’Œç´«è‰²æ›²çº¿åœ¨æœ€ååŸºæœ¬æ²¡æœ‰å·®åˆ«ï¼Œå‡ ä¹é‡åˆï¼Œä½†æ˜¯åœ¨æ›²çº¿å¼€å§‹çš„æ—¶å€™ï¼Œç»¿è‰²æ›²çº¿æ¯”ç´«è‰²æ›²çº¿æ›´åŠ é€¼è¿‘çœŸå®æƒ…å†µï¼Œå› æ­¤ï¼ŒNgç»™æˆ‘ä»¬ä»¥ä¸‹å»ºè®®ï¼š\n* å½“æˆ‘ä»¬ä¸å…³æ³¨moving averages initial valueå¤§å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ä½¿ç”¨bias correction\n* Bias correctionå¯¹äºinitial valueæ•ˆæœæ›´å¥½\n\n## Gradient descent optimization\n### momentum\nåœ¨gradient descentä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ç§æƒ…å†µï¼Œå¦‚å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png)\nåœ¨æ°´å¹³æ–¹å‘ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¿«çš„ä¸‹é™ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ä¸Šæˆ‘ä»¬å¸Œæœ›æ›´å°çš„ä¸‹é™é€Ÿç‡ï¼Œä»¥é¿å…è¿‡å¤šçš„iterationï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è®²moving averagesçš„æ€æƒ³å¸¦å…¥è¿›æ¥ï¼Œè¿™å°±æ˜¯momentumæ–¹æ³•ã€‚\n\nåœ¨momentumä¸­ï¼Œæˆ‘ä»¬çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼š\n$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$\n$$v_{db}= \\beta v_{db}+(1- \\beta)db$$\n$$W:=W- \\alpha v_{dW}$$\n$$b:=b - \\alpha v_{db}$$\nåœ¨å¾ˆå¤šçš„æ–‡çŒ®ä¸­ï¼Œä¸Šé¢çš„\\\\(1- \\beta\\\\)é¡¹è¢«çœç•¥æ‰äº†ï¼Œè¿™æ ·åšåªæ˜¯å°†ç­‰å¼ç­‰é‡åšäº†ç¼©æ”¾ï¼Œå¹¶ä¸å½±å“å®é™…çš„æ•ˆæœï¼ŒNgè¡¨ç¤ºï¼Œä¸¤ç§æ–¹å¼çš„momentumå‡ ä¹æ²¡æœ‰å·®åˆ«ï¼Œå¤§å®¶å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚\n\nå®é™…ä¸Šï¼Œmomentumå¯ä»¥ç†è§£ä¸ºå°†æ•°æ¬¡ä¹‹å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„gradientå˜åŒ–ä¹Ÿå¸¦å…¥åˆ°äº†è¿™æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºå°±æ˜¯å¸¦å…¥äº†ä¸€ç§gradientå˜åŒ–çš„è¶‹åŠ¿ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„æ§åˆ¶gradient descentçš„æ–¹å‘å’Œå¤§å°ã€‚ä¾‹å¦‚ä¸Šå›¾ä¸­çš„çºµå‘æ–¹å‘ä¸­ï¼ŒåŠ å…¥momentumåå¯ä»¥è½»æ¾çš„å°†çºµå‘æ¢¯åº¦æ­£è´ŸæŠµæ¶ˆåˆ°è¿‘ä¼¼0ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘åœ¨gradient descentåœ¨çºµå‘çš„åå¤è¿­ä»£ï¼Œå› ä¸ºï¼Œé‚£æ—¢æ˜¯æ— ç”¨åŠŸï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„æƒ…å†µã€‚\n\nå¦å¤–ï¼ŒNgç»™æˆ‘ä»¬äº†ä¸€ä¸ª\\\\(\\beta\\\\)çš„ç†æƒ³å–å€¼ï¼Œæ—¢0.9ï¼Œè¿™ä¸ªå€¼å¤§çº¦å–äº†å‰10æ¬¡è¿­ä»£ç»“æœçš„moving averagesã€‚å¦å¤–ï¼ŒNgè¡¨ç¤ºï¼Œåœ¨momenä¸­å¾ˆå°‘ä½¿ç”¨bias correctionï¼Œå› ä¸º10æ¬¡è¿­ä»£ä¹‹åï¼Œè¿™ç§é—®é¢˜å¾ˆå¿«å°±ä¼šæ¶ˆé™¤ï¼Œè€Œä¸€èˆ¬çš„gradient descentï¼Œiterationæ¬¡æ•°è¿œè¿œå¤§äº10æ¬¡ã€‚\n### RMSprop\né™¤äº†momentumï¼Œè¿˜æœ‰ä¸€äº›ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤§åé¼é¼çš„RMSprop(root means square prop)ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦ä½“ç°äº†squareçš„åº”ç”¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨æ¯æ¬¡çš„è¿­ä»£ä¸­ï¼š\n$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$\n$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$\n$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$\n$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\n\\\\(\\epsilon\\\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\\\(10^{-8}\\\\)\n\nå‡è®¾åœ¨gradient descentä¸­ï¼Œ\\\\(W\\\\)ä¸‹é™é€Ÿç‡å¤ªä½ï¼Œä¹Ÿå°±æ˜¯\\\\(dW\\\\)å¤ªå°ï¼Œé‚£ä¹ˆåœ¨RMSpropçš„è¿­ä»£ä¸­ï¼Œ\\\\(dW\\\\)å°†ä¼šé™¤ä»¥ä¸€ä¸ªå¾ˆå°çš„å€¼\\\\( \\sqrt{S_{dW}}\\\\)ï¼Œä¹Ÿå°±æ˜¯\\\\(W\\\\)å°†ä¼šå‡å»ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ç¼“è§£äº†ä¸Šå›¾æ‰€ç¤ºçš„æƒ…å†µï¼Œæ”¹å–„äº†gradient descentçš„åˆç†æ€§ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„\\\\(\\alpha\\\\)å»å®ç°æ›´å¿«çš„gradient descent.\n### Adam\næˆ‘ä»¬çœ‹åˆ°äº†momentumå’ŒRMSpropä¼˜åŒ–æ–¹æ³•çš„å‰å®³ä¹‹å¤„ï¼Œç°åœ¨Adamæ–¹æ³•æ¨ªç©ºå‡ºä¸–ï¼Œä»–èåˆäº†momentumå’ŒRMSpropï¼Œä»–æ˜¯å¦‚ä½•èåˆçš„å‘¢ï¼Œæˆ‘ä»¬æŠŠmomentumä¸­çš„\\\\( \\beta\\\\)å‘½åä¸º\\\\( \\beta\\_1\\\\)ï¼ŒæŠŠRMSpropä¸­çš„\\\\( \\beta\\\\)å‘½åä¸º\\\\( \\beta\\_2\\\\)ï¼Œåœ¨ç¬¬\\\\(t\\\\)æ¬¡è¿­ä»£ä¸­ï¼š\n$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$\n$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$\nåŠ ä¸Šbias correctionå\n$$v^{corrected}_{dW}= \\frac{v_{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}_{db}= \\frac{v_{db}}{1- \\beta^t_1}$$\n$$s^{corrected}_{dW}= \\frac{s_{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}_{db}= \\frac{s_{db}}{1- \\beta^t_2}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{dW}}{ \\sqrt{s^{corrected}_{dW}}+ \\epsilon}$$\n$$W:=W- \\alpha \\frac{v^{corrected}_{db}}{ \\sqrt{s^{corrected}_{db}}+ \\epsilon}$$\nå…¶ä¸­ï¼Œ\\\\(\\epsilon\\\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\\\(10^{-8}\\\\)ï¼Œ\\\\(\\beta\\_1\\\\)å»ºè®®å–å€¼0.9ï¼Œ\\\\(\\beta\\_2\\\\)å»ºè®®å–å€¼0.999ã€‚\n## Learning rate decay\nåœ¨gradient descentä¸­ï¼Œéšç€è¿­ä»£çš„æ·±åº¦ï¼Œè¶Šæ¥è¶Šé è¿‘minimumï¼Œæˆ‘ä»¬éœ€è¦æ›´å°çš„learning rateï¼Œä»¥é¿å…è¶Šè¿‡minimumï¼Œå¸¸ç”¨çš„learning decayæ–¹æ³•æœ‰ï¼š\n$$\\alpha = \\frac{1}{1+decayRate*epochNum} * \\alpha_0$$\n$$\\alpha = 0.95^{epochNum} * \\alpha_0$$\n$$\\alpha = \\frac{k}{\\sqrt{epochNum}}* \\alpha_0$$\nè¿™äº›æ–¹æ³•éƒ½å¯ä»¥è®©\\\\(\\alpha\\\\)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œæ…¢æ…¢å˜å°ï¼Œå¯ä»¥æ›´å¥½çš„é€¼è¿‘minimum.\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n* [An overview of gradient descent optimization algorithms ](http://ruder.io/optimizing-gradient-descent/)\n","slug":"course-deep-learning-course2-week2","published":1,"updated":"2018-11-19T06:37:58.243Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4oz0007tr8lafvbj814","content":"<p>å¤§å®¶å¥½ï¼Œè¯¾ç¨‹æ¥åˆ°äº†ç¬¬äºŒå‘¨ï¼Œè¿™å‘¨ä¸»è¦æ˜¯ä¸€äº›ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿å¾—æ•´ä¸ªneural networkså¯ä»¥æ›´å¿«æ›´å¥½çš„å·¥ä½œï¼Œæˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>è¿™ä¸€èŠ‚æˆ‘å°±ä¸æ‰“ç®—å†™äº†ï¼Œæ¯”è¾ƒåŸºç¡€ï¼Œå…¶å®mini-batch gradient descentæ˜¯batch gradientå’Œstochastic gradient descentç»¼åˆåçš„ç»“æœï¼Œä¸€æ–¹é¢è§£å†³äº†batch gradientè®¡ç®—é‡å¤§ï¼Œå®¹æ˜“convergeåˆ°local minimumçš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†stochastic gradient descent epochå¤ªå¤šçš„å¼Šç«¯ï¼Œæ˜¯ç°åœ¨gradient descentä½¿ç”¨æœ€å¹¿æ³›çš„æ–¹æ³•ã€‚</p>\n<p>å¯¹äºmini-batch gradient descentï¼ŒNgå»ºè®®batchå¤§å°å–å†³äºæ•°æ®é‡å¤šå°‘ï¼Œåœ¨å°æ•°æ®é‡ä¸Šå®Œå…¨æ²¡æœ‰å¿…è¦åšmini-batchï¼Œç›´æ¥ä½¿ç”¨batch gradient descentå°±å¯ä»¥ï¼Œå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼Œæœ€å¥½é€‰æ‹©2çš„ä¹˜æ–¹ï¼Œå¦‚64,128,256,512æ¥ä½œä¸ºbatch size. å½“ç„¶ï¼Œbatch sizeä¹Ÿè¦æ»¡è¶³CPUå’ŒGPUçš„å†…å­˜å¤§å°ã€‚</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averagesï¼Œä¹Ÿè¢«ç§°ä¸ºmoving averagesï¼Œæ˜¯ä¸€ç§ç»¼åˆå†å²æ•°æ®çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œè¯¾ç¨‹ä¸­Ngç”¨äº†ä¼¦æ•¦ä¸€å¹´çš„æ°”æ¸©å˜åŒ–æ›²çº¿ä½œä¸ºä¾‹å­ï¼Œå¯¹äºå›ºæœ‰å˜é‡\\(\\theta\\)æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ±‚çš„å¹³å‡å€¼\\(v\\)åº”è¯¥æ˜¯<br>$$v_0 = 0$$<br>$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$<br>$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$<br>$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$<br>$$\\cdots$$<br>$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$<br>å…¶ä¸­\\(\\beta\\)æ˜¯ä¸€ä¸ªå› å­ï¼Œå®ƒå†³å®šäº†moving averageså¤§çº¦å‘å‰å¹³å‡äº†\\(\\frac{1}{1- \\beta}\\)ä¸ª\\(\\theta\\)å€¼ï¼Œä¾‹å¦‚\\(\\beta = 0.9\\)ï¼Œé‚£ä¹ˆå¤§çº¦å‘å‰å¹³å‡äº†10ä¸ªå€¼ï¼Œå¹¶ä¸”æ˜¯å‘å‰æŒ‰æŒ‡æ•°è¡°å‡åŠ æƒè·å¾—çš„å¹³å‡å€¼ã€‚</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>åœ¨exponentially weighted averagesä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¾ˆå°–é”ï¼Œé‚£å°±æ˜¯åœ¨æœ€åˆçš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œç”±äº\\(v_0=0\\)ï¼Œå¯¼è‡´å‰é¢çš„æ•°å­—ç»“æœè·ç¦»æ­£ç¡®ç»“æœè¾ƒå°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²æ›²çº¿æ˜¯è·å¾—çš„ç»“æœï¼Œè€Œåœ¨èµ·å§‹ä½ç½®çš„å€¼æ˜æ˜¾æ˜¯åå°çš„ã€‚<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png\" alt=\"\"><br>æ­¤æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥bias correctionï¼ŒåŸç†ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯æ­¤å¤„æˆ‘ä»¬ä¸ä½¿ç”¨\\(v_t\\)ä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œè€Œæ˜¯ä½¿ç”¨\\( \\frac{v_t}{1- \\beta^{t}}\\)ä½œä¸ºæœ€åçš„ç»“æœï¼Œé€šè¿‡bias correctionï¼Œæˆ‘ä»¬ä¼šè·å¾—ç»¿è‰²çš„æ›²çº¿ã€‚</p>\n<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèµ·å§‹ç»¿è‰²é’±å’Œç´«è‰²æ›²çº¿åœ¨æœ€ååŸºæœ¬æ²¡æœ‰å·®åˆ«ï¼Œå‡ ä¹é‡åˆï¼Œä½†æ˜¯åœ¨æ›²çº¿å¼€å§‹çš„æ—¶å€™ï¼Œç»¿è‰²æ›²çº¿æ¯”ç´«è‰²æ›²çº¿æ›´åŠ é€¼è¿‘çœŸå®æƒ…å†µï¼Œå› æ­¤ï¼ŒNgç»™æˆ‘ä»¬ä»¥ä¸‹å»ºè®®ï¼š</p>\n<ul>\n<li>å½“æˆ‘ä»¬ä¸å…³æ³¨moving averages initial valueå¤§å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ä½¿ç”¨bias correction</li>\n<li>Bias correctionå¯¹äºinitial valueæ•ˆæœæ›´å¥½</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>åœ¨gradient descentä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ç§æƒ…å†µï¼Œå¦‚å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png\" alt=\"\"><br>åœ¨æ°´å¹³æ–¹å‘ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¿«çš„ä¸‹é™ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ä¸Šæˆ‘ä»¬å¸Œæœ›æ›´å°çš„ä¸‹é™é€Ÿç‡ï¼Œä»¥é¿å…è¿‡å¤šçš„iterationï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è®²moving averagesçš„æ€æƒ³å¸¦å…¥è¿›æ¥ï¼Œè¿™å°±æ˜¯momentumæ–¹æ³•ã€‚</p>\n<p>åœ¨momentumä¸­ï¼Œæˆ‘ä»¬çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼š<br>$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$<br>$$v_{db}= \\beta v_{db}+(1- \\beta)db$$<br>$$W:=W- \\alpha v_{dW}$$<br>$$b:=b - \\alpha v_{db}$$<br>åœ¨å¾ˆå¤šçš„æ–‡çŒ®ä¸­ï¼Œä¸Šé¢çš„\\(1- \\beta\\)é¡¹è¢«çœç•¥æ‰äº†ï¼Œè¿™æ ·åšåªæ˜¯å°†ç­‰å¼ç­‰é‡åšäº†ç¼©æ”¾ï¼Œå¹¶ä¸å½±å“å®é™…çš„æ•ˆæœï¼ŒNgè¡¨ç¤ºï¼Œä¸¤ç§æ–¹å¼çš„momentumå‡ ä¹æ²¡æœ‰å·®åˆ«ï¼Œå¤§å®¶å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚</p>\n<p>å®é™…ä¸Šï¼Œmomentumå¯ä»¥ç†è§£ä¸ºå°†æ•°æ¬¡ä¹‹å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„gradientå˜åŒ–ä¹Ÿå¸¦å…¥åˆ°äº†è¿™æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºå°±æ˜¯å¸¦å…¥äº†ä¸€ç§gradientå˜åŒ–çš„è¶‹åŠ¿ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„æ§åˆ¶gradient descentçš„æ–¹å‘å’Œå¤§å°ã€‚ä¾‹å¦‚ä¸Šå›¾ä¸­çš„çºµå‘æ–¹å‘ä¸­ï¼ŒåŠ å…¥momentumåå¯ä»¥è½»æ¾çš„å°†çºµå‘æ¢¯åº¦æ­£è´ŸæŠµæ¶ˆåˆ°è¿‘ä¼¼0ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘åœ¨gradient descentåœ¨çºµå‘çš„åå¤è¿­ä»£ï¼Œå› ä¸ºï¼Œé‚£æ—¢æ˜¯æ— ç”¨åŠŸï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„æƒ…å†µã€‚</p>\n<p>å¦å¤–ï¼ŒNgç»™æˆ‘ä»¬äº†ä¸€ä¸ª\\(\\beta\\)çš„ç†æƒ³å–å€¼ï¼Œæ—¢0.9ï¼Œè¿™ä¸ªå€¼å¤§çº¦å–äº†å‰10æ¬¡è¿­ä»£ç»“æœçš„moving averagesã€‚å¦å¤–ï¼ŒNgè¡¨ç¤ºï¼Œåœ¨momenä¸­å¾ˆå°‘ä½¿ç”¨bias correctionï¼Œå› ä¸º10æ¬¡è¿­ä»£ä¹‹åï¼Œè¿™ç§é—®é¢˜å¾ˆå¿«å°±ä¼šæ¶ˆé™¤ï¼Œè€Œä¸€èˆ¬çš„gradient descentï¼Œiterationæ¬¡æ•°è¿œè¿œå¤§äº10æ¬¡ã€‚</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>é™¤äº†momentumï¼Œè¿˜æœ‰ä¸€äº›ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤§åé¼é¼çš„RMSprop(root means square prop)ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦ä½“ç°äº†squareçš„åº”ç”¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨æ¯æ¬¡çš„è¿­ä»£ä¸­ï¼š<br>$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$<br>$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$<br>$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$<br>$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$<br>\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\)</p>\n<p>å‡è®¾åœ¨gradient descentä¸­ï¼Œ\\(W\\)ä¸‹é™é€Ÿç‡å¤ªä½ï¼Œä¹Ÿå°±æ˜¯\\(dW\\)å¤ªå°ï¼Œé‚£ä¹ˆåœ¨RMSpropçš„è¿­ä»£ä¸­ï¼Œ\\(dW\\)å°†ä¼šé™¤ä»¥ä¸€ä¸ªå¾ˆå°çš„å€¼\\( \\sqrt{S_{dW}}\\)ï¼Œä¹Ÿå°±æ˜¯\\(W\\)å°†ä¼šå‡å»ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚</p>\n<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ç¼“è§£äº†ä¸Šå›¾æ‰€ç¤ºçš„æƒ…å†µï¼Œæ”¹å–„äº†gradient descentçš„åˆç†æ€§ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„\\(\\alpha\\)å»å®ç°æ›´å¿«çš„gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>æˆ‘ä»¬çœ‹åˆ°äº†momentumå’ŒRMSpropä¼˜åŒ–æ–¹æ³•çš„å‰å®³ä¹‹å¤„ï¼Œç°åœ¨Adamæ–¹æ³•æ¨ªç©ºå‡ºä¸–ï¼Œä»–èåˆäº†momentumå’ŒRMSpropï¼Œä»–æ˜¯å¦‚ä½•èåˆçš„å‘¢ï¼Œæˆ‘ä»¬æŠŠmomentumä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_1\\)ï¼ŒæŠŠRMSpropä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_2\\)ï¼Œåœ¨ç¬¬\\(t\\)æ¬¡è¿­ä»£ä¸­ï¼š<br>$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$<br>$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$<br>åŠ ä¸Šbias correctionå<br>$$v^{corrected}<em>{dW}= \\frac{v</em>{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}<em>{db}= \\frac{v</em>{db}}{1- \\beta^t_1}$$<br>$$s^{corrected}<em>{dW}= \\frac{s</em>{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}<em>{db}= \\frac{s</em>{db}}{1- \\beta^t_2}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{dW}}{ \\sqrt{s^{corrected}</em>{dW}}+ \\epsilon}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{db}}{ \\sqrt{s^{corrected}</em>{db}}+ \\epsilon}$$<br>å…¶ä¸­ï¼Œ\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\)ï¼Œ\\(\\beta_1\\)å»ºè®®å–å€¼0.9ï¼Œ\\(\\beta_2\\)å»ºè®®å–å€¼0.999ã€‚</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>åœ¨gradient descentä¸­ï¼Œéšç€è¿­ä»£çš„æ·±åº¦ï¼Œè¶Šæ¥è¶Šé è¿‘minimumï¼Œæˆ‘ä»¬éœ€è¦æ›´å°çš„learning rateï¼Œä»¥é¿å…è¶Šè¿‡minimumï¼Œå¸¸ç”¨çš„learning decayæ–¹æ³•æœ‰ï¼š<br>$$\\alpha = \\frac{1}{1+decayRate<em>epochNum} </em> \\alpha_0$$<br>$$\\alpha = 0.95^{epochNum} <em> \\alpha_0$$<br>$$\\alpha = \\frac{k}{\\sqrt{epochNum}}</em> \\alpha_0$$<br>è¿™äº›æ–¹æ³•éƒ½å¯ä»¥è®©\\(\\alpha\\)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œæ…¢æ…¢å˜å°ï¼Œå¯ä»¥æ›´å¥½çš„é€¼è¿‘minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms </a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"moving averages","path":"tags/moving-averages/"}],"excerpt":"<p>å¤§å®¶å¥½ï¼Œè¯¾ç¨‹æ¥åˆ°äº†ç¬¬äºŒå‘¨ï¼Œè¿™å‘¨ä¸»è¦æ˜¯ä¸€äº›ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿å¾—æ•´ä¸ªneural networkså¯ä»¥æ›´å¿«æ›´å¥½çš„å·¥ä½œï¼Œæˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹ã€‚<br></p>","more":"</p>\n<h2 id=\"Mini-batch-gradient-descent\"><a href=\"#Mini-batch-gradient-descent\" class=\"headerlink\" title=\"Mini-batch gradient descent\"></a>Mini-batch gradient descent</h2><p>è¿™ä¸€èŠ‚æˆ‘å°±ä¸æ‰“ç®—å†™äº†ï¼Œæ¯”è¾ƒåŸºç¡€ï¼Œå…¶å®mini-batch gradient descentæ˜¯batch gradientå’Œstochastic gradient descentç»¼åˆåçš„ç»“æœï¼Œä¸€æ–¹é¢è§£å†³äº†batch gradientè®¡ç®—é‡å¤§ï¼Œå®¹æ˜“convergeåˆ°local minimumçš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†stochastic gradient descent epochå¤ªå¤šçš„å¼Šç«¯ï¼Œæ˜¯ç°åœ¨gradient descentä½¿ç”¨æœ€å¹¿æ³›çš„æ–¹æ³•ã€‚</p>\n<p>å¯¹äºmini-batch gradient descentï¼ŒNgå»ºè®®batchå¤§å°å–å†³äºæ•°æ®é‡å¤šå°‘ï¼Œåœ¨å°æ•°æ®é‡ä¸Šå®Œå…¨æ²¡æœ‰å¿…è¦åšmini-batchï¼Œç›´æ¥ä½¿ç”¨batch gradient descentå°±å¯ä»¥ï¼Œå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼Œæœ€å¥½é€‰æ‹©2çš„ä¹˜æ–¹ï¼Œå¦‚64,128,256,512æ¥ä½œä¸ºbatch size. å½“ç„¶ï¼Œbatch sizeä¹Ÿè¦æ»¡è¶³CPUå’ŒGPUçš„å†…å­˜å¤§å°ã€‚</p>\n<h2 id=\"Exponentially-weighted-averages\"><a href=\"#Exponentially-weighted-averages\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h2><h3 id=\"Exponentially-weighted-averages-1\"><a href=\"#Exponentially-weighted-averages-1\" class=\"headerlink\" title=\"Exponentially weighted averages\"></a>Exponentially weighted averages</h3><p>Exponentially weighted averagesï¼Œä¹Ÿè¢«ç§°ä¸ºmoving averagesï¼Œæ˜¯ä¸€ç§ç»¼åˆå†å²æ•°æ®çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œè¯¾ç¨‹ä¸­Ngç”¨äº†ä¼¦æ•¦ä¸€å¹´çš„æ°”æ¸©å˜åŒ–æ›²çº¿ä½œä¸ºä¾‹å­ï¼Œå¯¹äºå›ºæœ‰å˜é‡\\(\\theta\\)æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ±‚çš„å¹³å‡å€¼\\(v\\)åº”è¯¥æ˜¯<br>$$v_0 = 0$$<br>$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$<br>$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$<br>$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$<br>$$\\cdots$$<br>$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$<br>å…¶ä¸­\\(\\beta\\)æ˜¯ä¸€ä¸ªå› å­ï¼Œå®ƒå†³å®šäº†moving averageså¤§çº¦å‘å‰å¹³å‡äº†\\(\\frac{1}{1- \\beta}\\)ä¸ª\\(\\theta\\)å€¼ï¼Œä¾‹å¦‚\\(\\beta = 0.9\\)ï¼Œé‚£ä¹ˆå¤§çº¦å‘å‰å¹³å‡äº†10ä¸ªå€¼ï¼Œå¹¶ä¸”æ˜¯å‘å‰æŒ‰æŒ‡æ•°è¡°å‡åŠ æƒè·å¾—çš„å¹³å‡å€¼ã€‚</p>\n<h3 id=\"Bias-correction\"><a href=\"#Bias-correction\" class=\"headerlink\" title=\"Bias correction\"></a>Bias correction</h3><p>åœ¨exponentially weighted averagesä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¾ˆå°–é”ï¼Œé‚£å°±æ˜¯åœ¨æœ€åˆçš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œç”±äº\\(v_0=0\\)ï¼Œå¯¼è‡´å‰é¢çš„æ•°å­—ç»“æœè·ç¦»æ­£ç¡®ç»“æœè¾ƒå°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²æ›²çº¿æ˜¯è·å¾—çš„ç»“æœï¼Œè€Œåœ¨èµ·å§‹ä½ç½®çš„å€¼æ˜æ˜¾æ˜¯åå°çš„ã€‚<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-1.png\" alt=\"\"><br>æ­¤æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥bias correctionï¼ŒåŸç†ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯æ­¤å¤„æˆ‘ä»¬ä¸ä½¿ç”¨\\(v_t\\)ä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œè€Œæ˜¯ä½¿ç”¨\\( \\frac{v_t}{1- \\beta^{t}}\\)ä½œä¸ºæœ€åçš„ç»“æœï¼Œé€šè¿‡bias correctionï¼Œæˆ‘ä»¬ä¼šè·å¾—ç»¿è‰²çš„æ›²çº¿ã€‚</p>\n<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèµ·å§‹ç»¿è‰²é’±å’Œç´«è‰²æ›²çº¿åœ¨æœ€ååŸºæœ¬æ²¡æœ‰å·®åˆ«ï¼Œå‡ ä¹é‡åˆï¼Œä½†æ˜¯åœ¨æ›²çº¿å¼€å§‹çš„æ—¶å€™ï¼Œç»¿è‰²æ›²çº¿æ¯”ç´«è‰²æ›²çº¿æ›´åŠ é€¼è¿‘çœŸå®æƒ…å†µï¼Œå› æ­¤ï¼ŒNgç»™æˆ‘ä»¬ä»¥ä¸‹å»ºè®®ï¼š</p>\n<ul>\n<li>å½“æˆ‘ä»¬ä¸å…³æ³¨moving averages initial valueå¤§å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ä½¿ç”¨bias correction</li>\n<li>Bias correctionå¯¹äºinitial valueæ•ˆæœæ›´å¥½</li>\n</ul>\n<h2 id=\"Gradient-descent-optimization\"><a href=\"#Gradient-descent-optimization\" class=\"headerlink\" title=\"Gradient descent optimization\"></a>Gradient descent optimization</h2><h3 id=\"momentum\"><a href=\"#momentum\" class=\"headerlink\" title=\"momentum\"></a>momentum</h3><p>åœ¨gradient descentä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ç§æƒ…å†µï¼Œå¦‚å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/7/7-2.png\" alt=\"\"><br>åœ¨æ°´å¹³æ–¹å‘ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¿«çš„ä¸‹é™ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ä¸Šæˆ‘ä»¬å¸Œæœ›æ›´å°çš„ä¸‹é™é€Ÿç‡ï¼Œä»¥é¿å…è¿‡å¤šçš„iterationï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è®²moving averagesçš„æ€æƒ³å¸¦å…¥è¿›æ¥ï¼Œè¿™å°±æ˜¯momentumæ–¹æ³•ã€‚</p>\n<p>åœ¨momentumä¸­ï¼Œæˆ‘ä»¬çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼š<br>$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$<br>$$v_{db}= \\beta v_{db}+(1- \\beta)db$$<br>$$W:=W- \\alpha v_{dW}$$<br>$$b:=b - \\alpha v_{db}$$<br>åœ¨å¾ˆå¤šçš„æ–‡çŒ®ä¸­ï¼Œä¸Šé¢çš„\\(1- \\beta\\)é¡¹è¢«çœç•¥æ‰äº†ï¼Œè¿™æ ·åšåªæ˜¯å°†ç­‰å¼ç­‰é‡åšäº†ç¼©æ”¾ï¼Œå¹¶ä¸å½±å“å®é™…çš„æ•ˆæœï¼ŒNgè¡¨ç¤ºï¼Œä¸¤ç§æ–¹å¼çš„momentumå‡ ä¹æ²¡æœ‰å·®åˆ«ï¼Œå¤§å®¶å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚</p>\n<p>å®é™…ä¸Šï¼Œmomentumå¯ä»¥ç†è§£ä¸ºå°†æ•°æ¬¡ä¹‹å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„gradientå˜åŒ–ä¹Ÿå¸¦å…¥åˆ°äº†è¿™æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºå°±æ˜¯å¸¦å…¥äº†ä¸€ç§gradientå˜åŒ–çš„è¶‹åŠ¿ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„æ§åˆ¶gradient descentçš„æ–¹å‘å’Œå¤§å°ã€‚ä¾‹å¦‚ä¸Šå›¾ä¸­çš„çºµå‘æ–¹å‘ä¸­ï¼ŒåŠ å…¥momentumåå¯ä»¥è½»æ¾çš„å°†çºµå‘æ¢¯åº¦æ­£è´ŸæŠµæ¶ˆåˆ°è¿‘ä¼¼0ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘åœ¨gradient descentåœ¨çºµå‘çš„åå¤è¿­ä»£ï¼Œå› ä¸ºï¼Œé‚£æ—¢æ˜¯æ— ç”¨åŠŸï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„æƒ…å†µã€‚</p>\n<p>å¦å¤–ï¼ŒNgç»™æˆ‘ä»¬äº†ä¸€ä¸ª\\(\\beta\\)çš„ç†æƒ³å–å€¼ï¼Œæ—¢0.9ï¼Œè¿™ä¸ªå€¼å¤§çº¦å–äº†å‰10æ¬¡è¿­ä»£ç»“æœçš„moving averagesã€‚å¦å¤–ï¼ŒNgè¡¨ç¤ºï¼Œåœ¨momenä¸­å¾ˆå°‘ä½¿ç”¨bias correctionï¼Œå› ä¸º10æ¬¡è¿­ä»£ä¹‹åï¼Œè¿™ç§é—®é¢˜å¾ˆå¿«å°±ä¼šæ¶ˆé™¤ï¼Œè€Œä¸€èˆ¬çš„gradient descentï¼Œiterationæ¬¡æ•°è¿œè¿œå¤§äº10æ¬¡ã€‚</p>\n<h3 id=\"RMSprop\"><a href=\"#RMSprop\" class=\"headerlink\" title=\"RMSprop\"></a>RMSprop</h3><p>é™¤äº†momentumï¼Œè¿˜æœ‰ä¸€äº›ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤§åé¼é¼çš„RMSprop(root means square prop)ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦ä½“ç°äº†squareçš„åº”ç”¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨æ¯æ¬¡çš„è¿­ä»£ä¸­ï¼š<br>$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$<br>$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$<br>$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$<br>$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$<br>\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\)</p>\n<p>å‡è®¾åœ¨gradient descentä¸­ï¼Œ\\(W\\)ä¸‹é™é€Ÿç‡å¤ªä½ï¼Œä¹Ÿå°±æ˜¯\\(dW\\)å¤ªå°ï¼Œé‚£ä¹ˆåœ¨RMSpropçš„è¿­ä»£ä¸­ï¼Œ\\(dW\\)å°†ä¼šé™¤ä»¥ä¸€ä¸ªå¾ˆå°çš„å€¼\\( \\sqrt{S_{dW}}\\)ï¼Œä¹Ÿå°±æ˜¯\\(W\\)å°†ä¼šå‡å»ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚</p>\n<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ç¼“è§£äº†ä¸Šå›¾æ‰€ç¤ºçš„æƒ…å†µï¼Œæ”¹å–„äº†gradient descentçš„åˆç†æ€§ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„\\(\\alpha\\)å»å®ç°æ›´å¿«çš„gradient descent.</p>\n<h3 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h3><p>æˆ‘ä»¬çœ‹åˆ°äº†momentumå’ŒRMSpropä¼˜åŒ–æ–¹æ³•çš„å‰å®³ä¹‹å¤„ï¼Œç°åœ¨Adamæ–¹æ³•æ¨ªç©ºå‡ºä¸–ï¼Œä»–èåˆäº†momentumå’ŒRMSpropï¼Œä»–æ˜¯å¦‚ä½•èåˆçš„å‘¢ï¼Œæˆ‘ä»¬æŠŠmomentumä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_1\\)ï¼ŒæŠŠRMSpropä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_2\\)ï¼Œåœ¨ç¬¬\\(t\\)æ¬¡è¿­ä»£ä¸­ï¼š<br>$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$<br>$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$<br>åŠ ä¸Šbias correctionå<br>$$v^{corrected}<em>{dW}= \\frac{v</em>{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}<em>{db}= \\frac{v</em>{db}}{1- \\beta^t_1}$$<br>$$s^{corrected}<em>{dW}= \\frac{s</em>{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}<em>{db}= \\frac{s</em>{db}}{1- \\beta^t_2}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{dW}}{ \\sqrt{s^{corrected}</em>{dW}}+ \\epsilon}$$<br>$$W:=W- \\alpha \\frac{v^{corrected}<em>{db}}{ \\sqrt{s^{corrected}</em>{db}}+ \\epsilon}$$<br>å…¶ä¸­ï¼Œ\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\)ï¼Œ\\(\\beta_1\\)å»ºè®®å–å€¼0.9ï¼Œ\\(\\beta_2\\)å»ºè®®å–å€¼0.999ã€‚</p>\n<h2 id=\"Learning-rate-decay\"><a href=\"#Learning-rate-decay\" class=\"headerlink\" title=\"Learning rate decay\"></a>Learning rate decay</h2><p>åœ¨gradient descentä¸­ï¼Œéšç€è¿­ä»£çš„æ·±åº¦ï¼Œè¶Šæ¥è¶Šé è¿‘minimumï¼Œæˆ‘ä»¬éœ€è¦æ›´å°çš„learning rateï¼Œä»¥é¿å…è¶Šè¿‡minimumï¼Œå¸¸ç”¨çš„learning decayæ–¹æ³•æœ‰ï¼š<br>$$\\alpha = \\frac{1}{1+decayRate<em>epochNum} </em> \\alpha_0$$<br>$$\\alpha = 0.95^{epochNum} <em> \\alpha_0$$<br>$$\\alpha = \\frac{k}{\\sqrt{epochNum}}</em> \\alpha_0$$<br>è¿™äº›æ–¹æ³•éƒ½å¯ä»¥è®©\\(\\alpha\\)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œæ…¢æ…¢å˜å°ï¼Œå¯ä»¥æ›´å¥½çš„é€¼è¿‘minimum.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n<li><a href=\"http://ruder.io/optimizing-gradient-descent/\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms </a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week1","date":"2017-10-12T04:34:05.000Z","_content":"è¯¾ç¨‹3ä¸»è¦è®²çš„æ˜¯deep learningä¸­çš„ä¸€äº›strategyï¼Œè¿™äº›strategyå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿçš„åˆ†ææ¨¡å‹æ‰€å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æœ‰åå·®è€Œå¯¼è‡´çš„äººåŠ›ä»¥åŠæ—¶é—´çš„æµªè´¹ï¼Œè¿™ä¸€ç‚¹å¯¹äºå›¢é˜Ÿå°¤å…¶é‡è¦ã€‚\n\næˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹week1çš„è¯¾ç¨‹\n<!--more-->\n## Orthogonalization\nå¯¹äºML taskæ¥è¯´ï¼Œæœ‰ä¼—å¤šå› ç´ å½±å“æœ€ç»ˆçš„æ•ˆæœï¼Œè¿™äº›å› ç´ ç›¸äº’çŠ¬ç‰™äº¤é”™ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æå‡æ¨¡å‹æ•ˆæœçš„æ—¶å€™ï¼Œä¸€å®šè¦æŠŠæ‰€æœ‰çš„å› ç´ orthogonalizationä¸€ä¸‹ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆå½¢è±¡ï¼Œå°±åƒæ˜¾ç¤ºå™¨çš„æ§åˆ¶æŒ‰é’®ä¸€æ ·ï¼Œæ¯ä¸ªæŒ‰é’®å„å¸å…¶èŒï¼Œä¸€ä¸ªæ§åˆ¶é«˜åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å®½åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å¤§å°ï¼Œä¸€ä¸ªæ§åˆ¶æ¢¯åº¦ï¼Œé€šè¿‡å„è‡ªè°ƒæ•´æ¯ä¸€ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„å®Œæˆç”»é¢è°ƒæ•´ã€‚\n\nå¯¹äºorthogonalizationä¼˜åŒ–æ¨¡å‹ï¼ŒNgç»™å‡ºäº†4æ–¹é¢çš„å»ºè®®ï¼š\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\næˆ‘ä»¬è¯¦ç»†æ¥çœ‹çœ‹è¿™å››æ¡ï¼š\n\nå¯¹äºç¬¬ä¸€æ¡ï¼Œé¦–å…ˆæ¨¡å‹å¿…é¡»è¦å¯¹äºtraining setæœ‰è‰¯å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œå¦‚æœè¿™ç‚¹è¾¾ä¸åˆ°çš„è¯ï¼Œæ¨¡å‹ä¸€å®šæ˜¯**high bias**ï¼Œä¹Ÿå°±æ˜¯**under fitting**äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å°è¯•é€šè¿‡more complexçš„æ¨¡å‹ï¼Œbigger neural networksæˆ–è€…æ˜¯longer training timeå»æ›´å……åˆ†çš„æ‹Ÿåˆtraining set.\n\nå¯¹äºç¬¬äºŒæ¡ï¼Œåœ¨å¾ˆå¥½çš„æ‹Ÿåˆtraining setçš„å‰æä¸‹ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹development setçš„æ•ˆæœäº†ï¼Œå¦‚æœå¯¹äºdevelopment set fitæ•ˆæœä¸å¥½çš„è¯ï¼Œé‚£åŸºæœ¬ä¸Šå°±æ˜¯**high varience**ï¼Œä¹Ÿå°±æ˜¯**over fitting**çš„é—®é¢˜äº†ï¼Œè¿™æ—¶å€™ï¼Œregularizationæˆ–è€…more training dataå¯ä»¥è§£å†³è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\nå¯¹äºç¬¬ä¸‰æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸¤æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨test setä¸Šè¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤§çš„development setå»æ¶µç›–æ›´å¤šçš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ‰©å……åçš„development seté‡å¤ç¬¬äºŒæ¡çš„æ£€éªŒ\n\nå¯¹äºæœ€åä¸€æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸‰æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨real worldä¸­è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºæˆ‘ä»¬çš„development å’Œtest setä¸real worldç›¸å·®æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚çŒ«è„¸æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬çš„data setéƒ½æ˜¯é«˜æ¸…çš„å›¾åƒï¼Œä½†æ˜¯real world ä¸­ï¼Œéƒ½æ˜¯åƒç´ å¾ˆä½çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©development å’Œtest setæ›´æ¥è¿‘ä¸real worldï¼Œç„¶åé‡å¤ä¸Šé¢çš„æ­¥éª¤ã€‚\n\né€šè¿‡è¿™å››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡orthogonalizationæ¥å®Œæˆæ¨¡å‹çš„è°ƒæ•´\n\n## Metric\n### Single number evaluation\nMetricæ— ç–‘æ˜¯ML taskä¸­å¾ˆé‡è¦çš„ç¯èŠ‚ï¼Œé€šè¿‡metricï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„ä¼˜è‰¯å·®å¼‚ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©å‡ºæœ€ç†æƒ³çš„æ¨¡å‹ã€‚\n\nä½†æ˜¯metricæŒ‡æ ‡ç³éƒæ»¡ç›®ï¼Œä¾‹å¦‚å¯¹äºä¸¤ä¸ªæ¨¡å‹ï¼Œæ¨¡å‹Açš„precisioné«˜äºBçš„ï¼Œä½†æ˜¯Açš„recallåˆä½äºBï¼Œè¿™æ—¶å€™å°±ä¸å¤ªå¥½è¯„ä»·ä¸¤ä¸ªæ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨å•ä¸€çš„æ•°å­—è¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥ç”¨F1-scoreæ¥è¿›è¡Œè¯„ä¼°ï¼Œsingle number evaluation metricæ˜¯æˆ‘ä»¬åšmetricsæ—¶ä¸€å®šè¦æ³¨æ„çš„\n### Satisficing   and optimizing\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚æˆ‘ä»¬ä¸ä»…ä»…è¦æ±‚æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿˜å¯¹å…¶ä»–çš„ï¼Œä¾‹å¦‚æ¨¡å‹æ—¶é—´ä¼šæœ‰è¦æ±‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰å¾ˆé«˜çš„æ¨¡å‹accuracyï¼Œä½†æ˜¯å´å¾ˆè€—è´¹æ—¶é—´ï¼Œé‚£æ˜¯æˆ‘ä»¬ä¸èƒ½æ¥å—çš„ï¼Œå¦‚ä¸‹å›¾ä¾‹å­ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png)\nå›¾ä¸­çš„accuracyæ˜¯optimizing metricï¼Œé€šå¸¸æ›´é«˜çš„accuracyå°±ä»£è¡¨è¿™classifieræ›´åŠ çš„ä¼˜ç§€ï¼›ä½†æ˜¯ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå¿…é¡»ä½äº100ms çš„running timeä½œä¸ºsatisficing metricï¼Œé€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰\\\\(N\\\\)ä¸ªmetricsï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„optimizing metricå¿…é¡»åªæœ‰ä¸€ä¸ªï¼Œå‰©ä¸‹çš„\\\\(N-1\\\\)metrics éƒ½æ˜¯satisficing metricsï¼Œåªè¦ä»¥thresholdå½¢å¼è¿›è¡Œé™å®šå°±å¯ä»¥äº†ã€‚\n## Data set\n### Distributions\nå¯¹äºtraining setï¼Œdev setå’Œtest setæ¥è¯´ï¼Œæ‰€æœ‰dataä¸€å®šè¦ä¿è¯æœä»åŒä¸€ä¸ªdata distributionï¼Œä¾‹å¦‚çŒ«è„¸å®éªŒï¼Œå¦‚æœtraining setæ˜¯é«˜æ¸…å¤§å›¾è€Œdev setæ˜¯æ¨¡ç³Šå›¾åƒï¼Œé‚£ä¹ˆæœ€ç»ˆä¸€å®šå¾ˆéš¾è·å¾—ç†æƒ³çš„metric.\n\næœ€é‡è¦çš„æ˜¯ï¼Œä½ æ‰€æ„å»ºæ¨¡å‹çš„dataï¼Œä¸€å®šå’Œæ¨¡å‹åº”ç”¨åœºæ™¯çš„dataåœ¨åŒæ ·çš„distributionä¸‹ï¼ŒNgç»™å‡ºçš„guidelineæ˜¯\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\nä¼ ç»Ÿçš„ML taskä¸­ï¼Œdatasetçš„åˆ†å¸ƒä¸€èˆ¬å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png)\nä½†æ˜¯åœ¨big dataæ—¶ä»£ï¼Œä¸€èˆ¬é‡‡ç”¨ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png)\nNgåŒæ ·ç»™å‡ºäº†guidelineï¼š\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \nè¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå…¶å®å°±æ˜¯data setä¸åœ¨åŒä¸€distributionä¸‹çš„é—®é¢˜ï¼Œå¦‚æœæ¨¡å‹åœ¨devå’Œtest set ä¸Šéƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°å’Œmetricï¼Œä½†æ˜¯åœ¨real worldä¸­æ•ˆæœå¹¶ä¸å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä¸€å®šå°±æ˜¯æ”¹å˜data setï¼Œè®©dev/test setä¸real worldåœ¨åŒä¸€distributionä¸‹\n### Change metric\næˆ‘ä»¬ä½¿ç”¨MLæ¥è§£å†³ç°å®ä¸­çš„é—®é¢˜æ—¶ï¼Œmetricä¹Ÿä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æƒ…å†µåšå‡ºä¸€äº›æ”¹å˜ï¼Œä¾‹å¦‚ï¼Œè‰²æƒ…å›¾åƒè¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬è¯¯è®²éè‰²æƒ…è¯†åˆ«ç»´è‰²æƒ…å›¾åƒï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯å°†è‰²æƒ…å›¾åƒè¯†åˆ«æˆéè‰²æƒ…å›¾åƒåˆ™æ˜¯ä¸å¯æ¥å—çš„ï¼Œç›¸ä¼¼çš„ï¼Œé£æ§ç³»ç»Ÿä¸­ï¼Œå°†é£é™©ç”¨æˆ·åˆ†ç±»ä¸ºæ­£å¸¸ç”¨æˆ·çš„é”™è¯¯ï¼Œæ¯”æŠŠæ­£å¸¸ç”¨æˆ·åˆ†ç±»ä¸ºé£é™©ç”¨æˆ·çš„é”™è¯¯è¦ä¸¥é‡å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨ç±»ä¼¼çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„metricéœ€è¦éšç€ä¸šåŠ¡åœºæ™¯åšå‡ºä¸€äº›æ”¹å˜ã€‚\n\næ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ¨¡å‹erroræ˜¯ï¼š\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\nä½†æ˜¯åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªæƒé‡ï¼Œæ¥å¯¹ä¸¤ç§ä¸åŒé”™è¯¯åŠ ä»¥åŒºåˆ†\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\nè¿™æ ·å°±æŠŠä¸¤ç§ä¸åŒçš„é—®é¢˜åŒºåˆ†å¼€äº†ã€‚\n## Improve model performance\næ¨¡å‹performanceçš„æå‡æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šæ¨¡å‹è°ƒæ•´çš„å¤§ä½“æ–¹å‘ï¼ŒNgç»™å‡ºäº†å¦‚ä¸‹çš„å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png)\næˆ‘ä»¬é»˜è®¤human-levelæ˜¯å¾ˆæ¥è¿‘ç†è®ºè¯¯å·®ï¼Œä¹Ÿå°±æ˜¯Bayes errorï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒhuman-levelï¼Œtraining errorå’Œdev errorè¿™ä¸‰è€…ä¹‹é—´çš„å…³ç³»ï¼Œhuman-levelå’Œtraining errorä¹‹é—´çš„å·®å€¼æ›´å¤§çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å»å‡å°biasï¼Œåä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å»å‡å°‘varianceï¼Œå…·ä½“çš„æ–¹æ³•ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¹‹å‰çš„è€å¥—è·¯ã€‚\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week1\ndate: 2017-10-12 12:34:05\ntags: \n\t- learning strategy\n\t- orthogonalization\ncategories: learning notes\n---\nè¯¾ç¨‹3ä¸»è¦è®²çš„æ˜¯deep learningä¸­çš„ä¸€äº›strategyï¼Œè¿™äº›strategyå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿçš„åˆ†ææ¨¡å‹æ‰€å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æœ‰åå·®è€Œå¯¼è‡´çš„äººåŠ›ä»¥åŠæ—¶é—´çš„æµªè´¹ï¼Œè¿™ä¸€ç‚¹å¯¹äºå›¢é˜Ÿå°¤å…¶é‡è¦ã€‚\n\næˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹week1çš„è¯¾ç¨‹\n<!--more-->\n## Orthogonalization\nå¯¹äºML taskæ¥è¯´ï¼Œæœ‰ä¼—å¤šå› ç´ å½±å“æœ€ç»ˆçš„æ•ˆæœï¼Œè¿™äº›å› ç´ ç›¸äº’çŠ¬ç‰™äº¤é”™ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æå‡æ¨¡å‹æ•ˆæœçš„æ—¶å€™ï¼Œä¸€å®šè¦æŠŠæ‰€æœ‰çš„å› ç´ orthogonalizationä¸€ä¸‹ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆå½¢è±¡ï¼Œå°±åƒæ˜¾ç¤ºå™¨çš„æ§åˆ¶æŒ‰é’®ä¸€æ ·ï¼Œæ¯ä¸ªæŒ‰é’®å„å¸å…¶èŒï¼Œä¸€ä¸ªæ§åˆ¶é«˜åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å®½åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å¤§å°ï¼Œä¸€ä¸ªæ§åˆ¶æ¢¯åº¦ï¼Œé€šè¿‡å„è‡ªè°ƒæ•´æ¯ä¸€ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„å®Œæˆç”»é¢è°ƒæ•´ã€‚\n\nå¯¹äºorthogonalizationä¼˜åŒ–æ¨¡å‹ï¼ŒNgç»™å‡ºäº†4æ–¹é¢çš„å»ºè®®ï¼š\n* Fit training set well in cost function\n* Fit development set well on cost function\n* Fit test set well on cost function\n* Performs well in real world\n\næˆ‘ä»¬è¯¦ç»†æ¥çœ‹çœ‹è¿™å››æ¡ï¼š\n\nå¯¹äºç¬¬ä¸€æ¡ï¼Œé¦–å…ˆæ¨¡å‹å¿…é¡»è¦å¯¹äºtraining setæœ‰è‰¯å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œå¦‚æœè¿™ç‚¹è¾¾ä¸åˆ°çš„è¯ï¼Œæ¨¡å‹ä¸€å®šæ˜¯**high bias**ï¼Œä¹Ÿå°±æ˜¯**under fitting**äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å°è¯•é€šè¿‡more complexçš„æ¨¡å‹ï¼Œbigger neural networksæˆ–è€…æ˜¯longer training timeå»æ›´å……åˆ†çš„æ‹Ÿåˆtraining set.\n\nå¯¹äºç¬¬äºŒæ¡ï¼Œåœ¨å¾ˆå¥½çš„æ‹Ÿåˆtraining setçš„å‰æä¸‹ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹development setçš„æ•ˆæœäº†ï¼Œå¦‚æœå¯¹äºdevelopment set fitæ•ˆæœä¸å¥½çš„è¯ï¼Œé‚£åŸºæœ¬ä¸Šå°±æ˜¯**high varience**ï¼Œä¹Ÿå°±æ˜¯**over fitting**çš„é—®é¢˜äº†ï¼Œè¿™æ—¶å€™ï¼Œregularizationæˆ–è€…more training dataå¯ä»¥è§£å†³è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\nå¯¹äºç¬¬ä¸‰æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸¤æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨test setä¸Šè¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤§çš„development setå»æ¶µç›–æ›´å¤šçš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ‰©å……åçš„development seté‡å¤ç¬¬äºŒæ¡çš„æ£€éªŒ\n\nå¯¹äºæœ€åä¸€æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸‰æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨real worldä¸­è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºæˆ‘ä»¬çš„development å’Œtest setä¸real worldç›¸å·®æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚çŒ«è„¸æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬çš„data setéƒ½æ˜¯é«˜æ¸…çš„å›¾åƒï¼Œä½†æ˜¯real world ä¸­ï¼Œéƒ½æ˜¯åƒç´ å¾ˆä½çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©development å’Œtest setæ›´æ¥è¿‘ä¸real worldï¼Œç„¶åé‡å¤ä¸Šé¢çš„æ­¥éª¤ã€‚\n\né€šè¿‡è¿™å››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡orthogonalizationæ¥å®Œæˆæ¨¡å‹çš„è°ƒæ•´\n\n## Metric\n### Single number evaluation\nMetricæ— ç–‘æ˜¯ML taskä¸­å¾ˆé‡è¦çš„ç¯èŠ‚ï¼Œé€šè¿‡metricï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„ä¼˜è‰¯å·®å¼‚ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©å‡ºæœ€ç†æƒ³çš„æ¨¡å‹ã€‚\n\nä½†æ˜¯metricæŒ‡æ ‡ç³éƒæ»¡ç›®ï¼Œä¾‹å¦‚å¯¹äºä¸¤ä¸ªæ¨¡å‹ï¼Œæ¨¡å‹Açš„precisioné«˜äºBçš„ï¼Œä½†æ˜¯Açš„recallåˆä½äºBï¼Œè¿™æ—¶å€™å°±ä¸å¤ªå¥½è¯„ä»·ä¸¤ä¸ªæ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨å•ä¸€çš„æ•°å­—è¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥ç”¨F1-scoreæ¥è¿›è¡Œè¯„ä¼°ï¼Œsingle number evaluation metricæ˜¯æˆ‘ä»¬åšmetricsæ—¶ä¸€å®šè¦æ³¨æ„çš„\n### Satisficing   and optimizing\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚æˆ‘ä»¬ä¸ä»…ä»…è¦æ±‚æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿˜å¯¹å…¶ä»–çš„ï¼Œä¾‹å¦‚æ¨¡å‹æ—¶é—´ä¼šæœ‰è¦æ±‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰å¾ˆé«˜çš„æ¨¡å‹accuracyï¼Œä½†æ˜¯å´å¾ˆè€—è´¹æ—¶é—´ï¼Œé‚£æ˜¯æˆ‘ä»¬ä¸èƒ½æ¥å—çš„ï¼Œå¦‚ä¸‹å›¾ä¾‹å­ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png)\nå›¾ä¸­çš„accuracyæ˜¯optimizing metricï¼Œé€šå¸¸æ›´é«˜çš„accuracyå°±ä»£è¡¨è¿™classifieræ›´åŠ çš„ä¼˜ç§€ï¼›ä½†æ˜¯ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå¿…é¡»ä½äº100ms çš„running timeä½œä¸ºsatisficing metricï¼Œé€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰\\\\(N\\\\)ä¸ªmetricsï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„optimizing metricå¿…é¡»åªæœ‰ä¸€ä¸ªï¼Œå‰©ä¸‹çš„\\\\(N-1\\\\)metrics éƒ½æ˜¯satisficing metricsï¼Œåªè¦ä»¥thresholdå½¢å¼è¿›è¡Œé™å®šå°±å¯ä»¥äº†ã€‚\n## Data set\n### Distributions\nå¯¹äºtraining setï¼Œdev setå’Œtest setæ¥è¯´ï¼Œæ‰€æœ‰dataä¸€å®šè¦ä¿è¯æœä»åŒä¸€ä¸ªdata distributionï¼Œä¾‹å¦‚çŒ«è„¸å®éªŒï¼Œå¦‚æœtraining setæ˜¯é«˜æ¸…å¤§å›¾è€Œdev setæ˜¯æ¨¡ç³Šå›¾åƒï¼Œé‚£ä¹ˆæœ€ç»ˆä¸€å®šå¾ˆéš¾è·å¾—ç†æƒ³çš„metric.\n\næœ€é‡è¦çš„æ˜¯ï¼Œä½ æ‰€æ„å»ºæ¨¡å‹çš„dataï¼Œä¸€å®šå’Œæ¨¡å‹åº”ç”¨åœºæ™¯çš„dataåœ¨åŒæ ·çš„distributionä¸‹ï¼ŒNgç»™å‡ºçš„guidelineæ˜¯\n> Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.\n\n### Size of data set\nä¼ ç»Ÿçš„ML taskä¸­ï¼Œdatasetçš„åˆ†å¸ƒä¸€èˆ¬å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png)\nä½†æ˜¯åœ¨big dataæ—¶ä»£ï¼Œä¸€èˆ¬é‡‡ç”¨ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png)\nNgåŒæ ·ç»™å‡ºäº†guidelineï¼š\n* Set uop the size of test set to give a high confidence in the overall performance of the system.\n* Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set\n* The development set has to be big enough to evaluate different ideas\n\n## Change data set and metric\n### Change data set \nè¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå…¶å®å°±æ˜¯data setä¸åœ¨åŒä¸€distributionä¸‹çš„é—®é¢˜ï¼Œå¦‚æœæ¨¡å‹åœ¨devå’Œtest set ä¸Šéƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°å’Œmetricï¼Œä½†æ˜¯åœ¨real worldä¸­æ•ˆæœå¹¶ä¸å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä¸€å®šå°±æ˜¯æ”¹å˜data setï¼Œè®©dev/test setä¸real worldåœ¨åŒä¸€distributionä¸‹\n### Change metric\næˆ‘ä»¬ä½¿ç”¨MLæ¥è§£å†³ç°å®ä¸­çš„é—®é¢˜æ—¶ï¼Œmetricä¹Ÿä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æƒ…å†µåšå‡ºä¸€äº›æ”¹å˜ï¼Œä¾‹å¦‚ï¼Œè‰²æƒ…å›¾åƒè¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬è¯¯è®²éè‰²æƒ…è¯†åˆ«ç»´è‰²æƒ…å›¾åƒï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯å°†è‰²æƒ…å›¾åƒè¯†åˆ«æˆéè‰²æƒ…å›¾åƒåˆ™æ˜¯ä¸å¯æ¥å—çš„ï¼Œç›¸ä¼¼çš„ï¼Œé£æ§ç³»ç»Ÿä¸­ï¼Œå°†é£é™©ç”¨æˆ·åˆ†ç±»ä¸ºæ­£å¸¸ç”¨æˆ·çš„é”™è¯¯ï¼Œæ¯”æŠŠæ­£å¸¸ç”¨æˆ·åˆ†ç±»ä¸ºé£é™©ç”¨æˆ·çš„é”™è¯¯è¦ä¸¥é‡å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨ç±»ä¼¼çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„metricéœ€è¦éšç€ä¸šåŠ¡åœºæ™¯åšå‡ºä¸€äº›æ”¹å˜ã€‚\n\næ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ¨¡å‹erroræ˜¯ï¼š\n$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}_{i=1} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\nä½†æ˜¯åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªæƒé‡ï¼Œæ¥å¯¹ä¸¤ç§ä¸åŒé”™è¯¯åŠ ä»¥åŒºåˆ†\n$$ w^{(i)}=\\left\\{\n\\begin{aligned}\n1 \\quad x^{(i)}notpron \\\\\n10 \\quad x^{(i)} pron \\\\\n\\end{aligned}\n\\right.\n$$\n$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m_{dev}}_{i=1} w^{(i)} \\mathcal \\{ \\hat{y}^{(i)} \\neq y^{(i)} \\}$$\nè¿™æ ·å°±æŠŠä¸¤ç§ä¸åŒçš„é—®é¢˜åŒºåˆ†å¼€äº†ã€‚\n## Improve model performance\næ¨¡å‹performanceçš„æå‡æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šæ¨¡å‹è°ƒæ•´çš„å¤§ä½“æ–¹å‘ï¼ŒNgç»™å‡ºäº†å¦‚ä¸‹çš„å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png)\næˆ‘ä»¬é»˜è®¤human-levelæ˜¯å¾ˆæ¥è¿‘ç†è®ºè¯¯å·®ï¼Œä¹Ÿå°±æ˜¯Bayes errorï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒhuman-levelï¼Œtraining errorå’Œdev errorè¿™ä¸‰è€…ä¹‹é—´çš„å…³ç³»ï¼Œhuman-levelå’Œtraining errorä¹‹é—´çš„å·®å€¼æ›´å¤§çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å»å‡å°biasï¼Œåä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å»å‡å°‘varianceï¼Œå…·ä½“çš„æ–¹æ³•ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¹‹å‰çš„è€å¥—è·¯ã€‚\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week1","published":1,"updated":"2018-11-19T06:41:06.114Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p00008tr8l0klebrfx","content":"<p>è¯¾ç¨‹3ä¸»è¦è®²çš„æ˜¯deep learningä¸­çš„ä¸€äº›strategyï¼Œè¿™äº›strategyå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿçš„åˆ†ææ¨¡å‹æ‰€å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æœ‰åå·®è€Œå¯¼è‡´çš„äººåŠ›ä»¥åŠæ—¶é—´çš„æµªè´¹ï¼Œè¿™ä¸€ç‚¹å¯¹äºå›¢é˜Ÿå°¤å…¶é‡è¦ã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹week1çš„è¯¾ç¨‹<br><a id=\"more\"></a></p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>å¯¹äºML taskæ¥è¯´ï¼Œæœ‰ä¼—å¤šå› ç´ å½±å“æœ€ç»ˆçš„æ•ˆæœï¼Œè¿™äº›å› ç´ ç›¸äº’çŠ¬ç‰™äº¤é”™ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æå‡æ¨¡å‹æ•ˆæœçš„æ—¶å€™ï¼Œä¸€å®šè¦æŠŠæ‰€æœ‰çš„å› ç´ orthogonalizationä¸€ä¸‹ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆå½¢è±¡ï¼Œå°±åƒæ˜¾ç¤ºå™¨çš„æ§åˆ¶æŒ‰é’®ä¸€æ ·ï¼Œæ¯ä¸ªæŒ‰é’®å„å¸å…¶èŒï¼Œä¸€ä¸ªæ§åˆ¶é«˜åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å®½åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å¤§å°ï¼Œä¸€ä¸ªæ§åˆ¶æ¢¯åº¦ï¼Œé€šè¿‡å„è‡ªè°ƒæ•´æ¯ä¸€ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„å®Œæˆç”»é¢è°ƒæ•´ã€‚</p>\n<p>å¯¹äºorthogonalizationä¼˜åŒ–æ¨¡å‹ï¼ŒNgç»™å‡ºäº†4æ–¹é¢çš„å»ºè®®ï¼š</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p>æˆ‘ä»¬è¯¦ç»†æ¥çœ‹çœ‹è¿™å››æ¡ï¼š</p>\n<p>å¯¹äºç¬¬ä¸€æ¡ï¼Œé¦–å…ˆæ¨¡å‹å¿…é¡»è¦å¯¹äºtraining setæœ‰è‰¯å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œå¦‚æœè¿™ç‚¹è¾¾ä¸åˆ°çš„è¯ï¼Œæ¨¡å‹ä¸€å®šæ˜¯<strong>high bias</strong>ï¼Œä¹Ÿå°±æ˜¯<strong>under fitting</strong>äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å°è¯•é€šè¿‡more complexçš„æ¨¡å‹ï¼Œbigger neural networksæˆ–è€…æ˜¯longer training timeå»æ›´å……åˆ†çš„æ‹Ÿåˆtraining set.</p>\n<p>å¯¹äºç¬¬äºŒæ¡ï¼Œåœ¨å¾ˆå¥½çš„æ‹Ÿåˆtraining setçš„å‰æä¸‹ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹development setçš„æ•ˆæœäº†ï¼Œå¦‚æœå¯¹äºdevelopment set fitæ•ˆæœä¸å¥½çš„è¯ï¼Œé‚£åŸºæœ¬ä¸Šå°±æ˜¯<strong>high varience</strong>ï¼Œä¹Ÿå°±æ˜¯<strong>over fitting</strong>çš„é—®é¢˜äº†ï¼Œè¿™æ—¶å€™ï¼Œregularizationæˆ–è€…more training dataå¯ä»¥è§£å†³è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p>å¯¹äºç¬¬ä¸‰æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸¤æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨test setä¸Šè¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤§çš„development setå»æ¶µç›–æ›´å¤šçš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ‰©å……åçš„development seté‡å¤ç¬¬äºŒæ¡çš„æ£€éªŒ</p>\n<p>å¯¹äºæœ€åä¸€æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸‰æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨real worldä¸­è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºæˆ‘ä»¬çš„development å’Œtest setä¸real worldç›¸å·®æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚çŒ«è„¸æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬çš„data setéƒ½æ˜¯é«˜æ¸…çš„å›¾åƒï¼Œä½†æ˜¯real world ä¸­ï¼Œéƒ½æ˜¯åƒç´ å¾ˆä½çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©development å’Œtest setæ›´æ¥è¿‘ä¸real worldï¼Œç„¶åé‡å¤ä¸Šé¢çš„æ­¥éª¤ã€‚</p>\n<p>é€šè¿‡è¿™å››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡orthogonalizationæ¥å®Œæˆæ¨¡å‹çš„è°ƒæ•´</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>Metricæ— ç–‘æ˜¯ML taskä¸­å¾ˆé‡è¦çš„ç¯èŠ‚ï¼Œé€šè¿‡metricï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„ä¼˜è‰¯å·®å¼‚ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©å‡ºæœ€ç†æƒ³çš„æ¨¡å‹ã€‚</p>\n<p>ä½†æ˜¯metricæŒ‡æ ‡ç³éƒæ»¡ç›®ï¼Œä¾‹å¦‚å¯¹äºä¸¤ä¸ªæ¨¡å‹ï¼Œæ¨¡å‹Açš„precisioné«˜äºBçš„ï¼Œä½†æ˜¯Açš„recallåˆä½äºBï¼Œè¿™æ—¶å€™å°±ä¸å¤ªå¥½è¯„ä»·ä¸¤ä¸ªæ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨å•ä¸€çš„æ•°å­—è¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥ç”¨F1-scoreæ¥è¿›è¡Œè¯„ä¼°ï¼Œsingle number evaluation metricæ˜¯æˆ‘ä»¬åšmetricsæ—¶ä¸€å®šè¦æ³¨æ„çš„</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚æˆ‘ä»¬ä¸ä»…ä»…è¦æ±‚æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿˜å¯¹å…¶ä»–çš„ï¼Œä¾‹å¦‚æ¨¡å‹æ—¶é—´ä¼šæœ‰è¦æ±‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰å¾ˆé«˜çš„æ¨¡å‹accuracyï¼Œä½†æ˜¯å´å¾ˆè€—è´¹æ—¶é—´ï¼Œé‚£æ˜¯æˆ‘ä»¬ä¸èƒ½æ¥å—çš„ï¼Œå¦‚ä¸‹å›¾ä¾‹å­ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png\" alt=\"\"><br>å›¾ä¸­çš„accuracyæ˜¯optimizing metricï¼Œé€šå¸¸æ›´é«˜çš„accuracyå°±ä»£è¡¨è¿™classifieræ›´åŠ çš„ä¼˜ç§€ï¼›ä½†æ˜¯ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå¿…é¡»ä½äº100ms çš„running timeä½œä¸ºsatisficing metricï¼Œé€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰\\(N\\)ä¸ªmetricsï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„optimizing metricå¿…é¡»åªæœ‰ä¸€ä¸ªï¼Œå‰©ä¸‹çš„\\(N-1\\)metrics éƒ½æ˜¯satisficing metricsï¼Œåªè¦ä»¥thresholdå½¢å¼è¿›è¡Œé™å®šå°±å¯ä»¥äº†ã€‚</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>å¯¹äºtraining setï¼Œdev setå’Œtest setæ¥è¯´ï¼Œæ‰€æœ‰dataä¸€å®šè¦ä¿è¯æœä»åŒä¸€ä¸ªdata distributionï¼Œä¾‹å¦‚çŒ«è„¸å®éªŒï¼Œå¦‚æœtraining setæ˜¯é«˜æ¸…å¤§å›¾è€Œdev setæ˜¯æ¨¡ç³Šå›¾åƒï¼Œé‚£ä¹ˆæœ€ç»ˆä¸€å®šå¾ˆéš¾è·å¾—ç†æƒ³çš„metric.</p>\n<p>æœ€é‡è¦çš„æ˜¯ï¼Œä½ æ‰€æ„å»ºæ¨¡å‹çš„dataï¼Œä¸€å®šå’Œæ¨¡å‹åº”ç”¨åœºæ™¯çš„dataåœ¨åŒæ ·çš„distributionä¸‹ï¼ŒNgç»™å‡ºçš„guidelineæ˜¯</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>ä¼ ç»Ÿçš„ML taskä¸­ï¼Œdatasetçš„åˆ†å¸ƒä¸€èˆ¬å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png\" alt=\"\"><br>ä½†æ˜¯åœ¨big dataæ—¶ä»£ï¼Œä¸€èˆ¬é‡‡ç”¨ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png\" alt=\"\"><br>NgåŒæ ·ç»™å‡ºäº†guidelineï¼š</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>è¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå…¶å®å°±æ˜¯data setä¸åœ¨åŒä¸€distributionä¸‹çš„é—®é¢˜ï¼Œå¦‚æœæ¨¡å‹åœ¨devå’Œtest set ä¸Šéƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°å’Œmetricï¼Œä½†æ˜¯åœ¨real worldä¸­æ•ˆæœå¹¶ä¸å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä¸€å®šå°±æ˜¯æ”¹å˜data setï¼Œè®©dev/test setä¸real worldåœ¨åŒä¸€distributionä¸‹</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>æˆ‘ä»¬ä½¿ç”¨MLæ¥è§£å†³ç°å®ä¸­çš„é—®é¢˜æ—¶ï¼Œmetricä¹Ÿä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æƒ…å†µåšå‡ºä¸€äº›æ”¹å˜ï¼Œä¾‹å¦‚ï¼Œè‰²æƒ…å›¾åƒè¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬è¯¯è®²éè‰²æƒ…è¯†åˆ«ç»´è‰²æƒ…å›¾åƒï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯å°†è‰²æƒ…å›¾åƒè¯†åˆ«æˆéè‰²æƒ…å›¾åƒåˆ™æ˜¯ä¸å¯æ¥å—çš„ï¼Œç›¸ä¼¼çš„ï¼Œé£æ§ç³»ç»Ÿä¸­ï¼Œå°†é£é™©ç”¨æˆ·åˆ†ç±»ä¸ºæ­£å¸¸ç”¨æˆ·çš„é”™è¯¯ï¼Œæ¯”æŠŠæ­£å¸¸ç”¨æˆ·åˆ†ç±»ä¸ºé£é™©ç”¨æˆ·çš„é”™è¯¯è¦ä¸¥é‡å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨ç±»ä¼¼çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„metricéœ€è¦éšç€ä¸šåŠ¡åœºæ™¯åšå‡ºä¸€äº›æ”¹å˜ã€‚</p>\n<p>æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ¨¡å‹erroræ˜¯ï¼š<br>$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}<em>{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>ä½†æ˜¯åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªæƒé‡ï¼Œæ¥å¯¹ä¸¤ç§ä¸åŒé”™è¯¯åŠ ä»¥åŒºåˆ†<br>$$ w^{(i)}=\\left{<br>\\begin{aligned}<br>1 \\quad x^{(i)}notpron \\<br>10 \\quad x^{(i)} pron \\<br>\\end{aligned}<br>\\right.<br>$$<br>$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m</em>{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>è¿™æ ·å°±æŠŠä¸¤ç§ä¸åŒçš„é—®é¢˜åŒºåˆ†å¼€äº†ã€‚</p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>æ¨¡å‹performanceçš„æå‡æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šæ¨¡å‹è°ƒæ•´çš„å¤§ä½“æ–¹å‘ï¼ŒNgç»™å‡ºäº†å¦‚ä¸‹çš„å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png\" alt=\"\"><br>æˆ‘ä»¬é»˜è®¤human-levelæ˜¯å¾ˆæ¥è¿‘ç†è®ºè¯¯å·®ï¼Œä¹Ÿå°±æ˜¯Bayes errorï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒhuman-levelï¼Œtraining errorå’Œdev errorè¿™ä¸‰è€…ä¹‹é—´çš„å…³ç³»ï¼Œhuman-levelå’Œtraining errorä¹‹é—´çš„å·®å€¼æ›´å¤§çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å»å‡å°biasï¼Œåä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å»å‡å°‘varianceï¼Œå…·ä½“çš„æ–¹æ³•ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¹‹å‰çš„è€å¥—è·¯ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"learning strategy","path":"tags/learning-strategy/"},{"name":"orthogonalization","path":"tags/orthogonalization/"}],"excerpt":"<p>è¯¾ç¨‹3ä¸»è¦è®²çš„æ˜¯deep learningä¸­çš„ä¸€äº›strategyï¼Œè¿™äº›strategyå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿçš„åˆ†ææ¨¡å‹æ‰€å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æœ‰åå·®è€Œå¯¼è‡´çš„äººåŠ›ä»¥åŠæ—¶é—´çš„æµªè´¹ï¼Œè¿™ä¸€ç‚¹å¯¹äºå›¢é˜Ÿå°¤å…¶é‡è¦ã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹week1çš„è¯¾ç¨‹<br></p>","more":"</p>\n<h2 id=\"Orthogonalization\"><a href=\"#Orthogonalization\" class=\"headerlink\" title=\"Orthogonalization\"></a>Orthogonalization</h2><p>å¯¹äºML taskæ¥è¯´ï¼Œæœ‰ä¼—å¤šå› ç´ å½±å“æœ€ç»ˆçš„æ•ˆæœï¼Œè¿™äº›å› ç´ ç›¸äº’çŠ¬ç‰™äº¤é”™ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æå‡æ¨¡å‹æ•ˆæœçš„æ—¶å€™ï¼Œä¸€å®šè¦æŠŠæ‰€æœ‰çš„å› ç´ orthogonalizationä¸€ä¸‹ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆå½¢è±¡ï¼Œå°±åƒæ˜¾ç¤ºå™¨çš„æ§åˆ¶æŒ‰é’®ä¸€æ ·ï¼Œæ¯ä¸ªæŒ‰é’®å„å¸å…¶èŒï¼Œä¸€ä¸ªæ§åˆ¶é«˜åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å®½åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å¤§å°ï¼Œä¸€ä¸ªæ§åˆ¶æ¢¯åº¦ï¼Œé€šè¿‡å„è‡ªè°ƒæ•´æ¯ä¸€ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„å®Œæˆç”»é¢è°ƒæ•´ã€‚</p>\n<p>å¯¹äºorthogonalizationä¼˜åŒ–æ¨¡å‹ï¼ŒNgç»™å‡ºäº†4æ–¹é¢çš„å»ºè®®ï¼š</p>\n<ul>\n<li>Fit training set well in cost function</li>\n<li>Fit development set well on cost function</li>\n<li>Fit test set well on cost function</li>\n<li>Performs well in real world</li>\n</ul>\n<p>æˆ‘ä»¬è¯¦ç»†æ¥çœ‹çœ‹è¿™å››æ¡ï¼š</p>\n<p>å¯¹äºç¬¬ä¸€æ¡ï¼Œé¦–å…ˆæ¨¡å‹å¿…é¡»è¦å¯¹äºtraining setæœ‰è‰¯å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œå¦‚æœè¿™ç‚¹è¾¾ä¸åˆ°çš„è¯ï¼Œæ¨¡å‹ä¸€å®šæ˜¯<strong>high bias</strong>ï¼Œä¹Ÿå°±æ˜¯<strong>under fitting</strong>äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å°è¯•é€šè¿‡more complexçš„æ¨¡å‹ï¼Œbigger neural networksæˆ–è€…æ˜¯longer training timeå»æ›´å……åˆ†çš„æ‹Ÿåˆtraining set.</p>\n<p>å¯¹äºç¬¬äºŒæ¡ï¼Œåœ¨å¾ˆå¥½çš„æ‹Ÿåˆtraining setçš„å‰æä¸‹ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹development setçš„æ•ˆæœäº†ï¼Œå¦‚æœå¯¹äºdevelopment set fitæ•ˆæœä¸å¥½çš„è¯ï¼Œé‚£åŸºæœ¬ä¸Šå°±æ˜¯<strong>high varience</strong>ï¼Œä¹Ÿå°±æ˜¯<strong>over fitting</strong>çš„é—®é¢˜äº†ï¼Œè¿™æ—¶å€™ï¼Œregularizationæˆ–è€…more training dataå¯ä»¥è§£å†³è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>\n<p>å¯¹äºç¬¬ä¸‰æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸¤æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨test setä¸Šè¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤§çš„development setå»æ¶µç›–æ›´å¤šçš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ‰©å……åçš„development seté‡å¤ç¬¬äºŒæ¡çš„æ£€éªŒ</p>\n<p>å¯¹äºæœ€åä¸€æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸‰æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨real worldä¸­è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºæˆ‘ä»¬çš„development å’Œtest setä¸real worldç›¸å·®æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚çŒ«è„¸æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬çš„data setéƒ½æ˜¯é«˜æ¸…çš„å›¾åƒï¼Œä½†æ˜¯real world ä¸­ï¼Œéƒ½æ˜¯åƒç´ å¾ˆä½çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©development å’Œtest setæ›´æ¥è¿‘ä¸real worldï¼Œç„¶åé‡å¤ä¸Šé¢çš„æ­¥éª¤ã€‚</p>\n<p>é€šè¿‡è¿™å››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡orthogonalizationæ¥å®Œæˆæ¨¡å‹çš„è°ƒæ•´</p>\n<h2 id=\"Metric\"><a href=\"#Metric\" class=\"headerlink\" title=\"Metric\"></a>Metric</h2><h3 id=\"Single-number-evaluation\"><a href=\"#Single-number-evaluation\" class=\"headerlink\" title=\"Single number evaluation\"></a>Single number evaluation</h3><p>Metricæ— ç–‘æ˜¯ML taskä¸­å¾ˆé‡è¦çš„ç¯èŠ‚ï¼Œé€šè¿‡metricï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„ä¼˜è‰¯å·®å¼‚ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©å‡ºæœ€ç†æƒ³çš„æ¨¡å‹ã€‚</p>\n<p>ä½†æ˜¯metricæŒ‡æ ‡ç³éƒæ»¡ç›®ï¼Œä¾‹å¦‚å¯¹äºä¸¤ä¸ªæ¨¡å‹ï¼Œæ¨¡å‹Açš„precisioné«˜äºBçš„ï¼Œä½†æ˜¯Açš„recallåˆä½äºBï¼Œè¿™æ—¶å€™å°±ä¸å¤ªå¥½è¯„ä»·ä¸¤ä¸ªæ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨å•ä¸€çš„æ•°å­—è¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥ç”¨F1-scoreæ¥è¿›è¡Œè¯„ä¼°ï¼Œsingle number evaluation metricæ˜¯æˆ‘ä»¬åšmetricsæ—¶ä¸€å®šè¦æ³¨æ„çš„</p>\n<h3 id=\"Satisficing-and-optimizing\"><a href=\"#Satisficing-and-optimizing\" class=\"headerlink\" title=\"Satisficing   and optimizing\"></a>Satisficing   and optimizing</h3><p>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚æˆ‘ä»¬ä¸ä»…ä»…è¦æ±‚æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿˜å¯¹å…¶ä»–çš„ï¼Œä¾‹å¦‚æ¨¡å‹æ—¶é—´ä¼šæœ‰è¦æ±‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰å¾ˆé«˜çš„æ¨¡å‹accuracyï¼Œä½†æ˜¯å´å¾ˆè€—è´¹æ—¶é—´ï¼Œé‚£æ˜¯æˆ‘ä»¬ä¸èƒ½æ¥å—çš„ï¼Œå¦‚ä¸‹å›¾ä¾‹å­ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-1.png\" alt=\"\"><br>å›¾ä¸­çš„accuracyæ˜¯optimizing metricï¼Œé€šå¸¸æ›´é«˜çš„accuracyå°±ä»£è¡¨è¿™classifieræ›´åŠ çš„ä¼˜ç§€ï¼›ä½†æ˜¯ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå¿…é¡»ä½äº100ms çš„running timeä½œä¸ºsatisficing metricï¼Œé€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰\\(N\\)ä¸ªmetricsï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„optimizing metricå¿…é¡»åªæœ‰ä¸€ä¸ªï¼Œå‰©ä¸‹çš„\\(N-1\\)metrics éƒ½æ˜¯satisficing metricsï¼Œåªè¦ä»¥thresholdå½¢å¼è¿›è¡Œé™å®šå°±å¯ä»¥äº†ã€‚</p>\n<h2 id=\"Data-set\"><a href=\"#Data-set\" class=\"headerlink\" title=\"Data set\"></a>Data set</h2><h3 id=\"Distributions\"><a href=\"#Distributions\" class=\"headerlink\" title=\"Distributions\"></a>Distributions</h3><p>å¯¹äºtraining setï¼Œdev setå’Œtest setæ¥è¯´ï¼Œæ‰€æœ‰dataä¸€å®šè¦ä¿è¯æœä»åŒä¸€ä¸ªdata distributionï¼Œä¾‹å¦‚çŒ«è„¸å®éªŒï¼Œå¦‚æœtraining setæ˜¯é«˜æ¸…å¤§å›¾è€Œdev setæ˜¯æ¨¡ç³Šå›¾åƒï¼Œé‚£ä¹ˆæœ€ç»ˆä¸€å®šå¾ˆéš¾è·å¾—ç†æƒ³çš„metric.</p>\n<p>æœ€é‡è¦çš„æ˜¯ï¼Œä½ æ‰€æ„å»ºæ¨¡å‹çš„dataï¼Œä¸€å®šå’Œæ¨¡å‹åº”ç”¨åœºæ™¯çš„dataåœ¨åŒæ ·çš„distributionä¸‹ï¼ŒNgç»™å‡ºçš„guidelineæ˜¯</p>\n<blockquote>\n<p>Choose a development set and test set to reflect data you expect to get in  the future and consider important to do well.</p>\n</blockquote>\n<h3 id=\"Size-of-data-set\"><a href=\"#Size-of-data-set\" class=\"headerlink\" title=\"Size of data set\"></a>Size of data set</h3><p>ä¼ ç»Ÿçš„ML taskä¸­ï¼Œdatasetçš„åˆ†å¸ƒä¸€èˆ¬å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-2.png\" alt=\"\"><br>ä½†æ˜¯åœ¨big dataæ—¶ä»£ï¼Œä¸€èˆ¬é‡‡ç”¨ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-3.png\" alt=\"\"><br>NgåŒæ ·ç»™å‡ºäº†guidelineï¼š</p>\n<ul>\n<li>Set uop the size of test set to give a high confidence in the overall performance of the system.</li>\n<li>Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set</li>\n<li>The development set has to be big enough to evaluate different ideas</li>\n</ul>\n<h2 id=\"Change-data-set-and-metric\"><a href=\"#Change-data-set-and-metric\" class=\"headerlink\" title=\"Change data set and metric\"></a>Change data set and metric</h2><h3 id=\"Change-data-set\"><a href=\"#Change-data-set\" class=\"headerlink\" title=\"Change data set\"></a>Change data set</h3><p>è¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå…¶å®å°±æ˜¯data setä¸åœ¨åŒä¸€distributionä¸‹çš„é—®é¢˜ï¼Œå¦‚æœæ¨¡å‹åœ¨devå’Œtest set ä¸Šéƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°å’Œmetricï¼Œä½†æ˜¯åœ¨real worldä¸­æ•ˆæœå¹¶ä¸å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä¸€å®šå°±æ˜¯æ”¹å˜data setï¼Œè®©dev/test setä¸real worldåœ¨åŒä¸€distributionä¸‹</p>\n<h3 id=\"Change-metric\"><a href=\"#Change-metric\" class=\"headerlink\" title=\"Change metric\"></a>Change metric</h3><p>æˆ‘ä»¬ä½¿ç”¨MLæ¥è§£å†³ç°å®ä¸­çš„é—®é¢˜æ—¶ï¼Œmetricä¹Ÿä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æƒ…å†µåšå‡ºä¸€äº›æ”¹å˜ï¼Œä¾‹å¦‚ï¼Œè‰²æƒ…å›¾åƒè¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬è¯¯è®²éè‰²æƒ…è¯†åˆ«ç»´è‰²æƒ…å›¾åƒï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯å°†è‰²æƒ…å›¾åƒè¯†åˆ«æˆéè‰²æƒ…å›¾åƒåˆ™æ˜¯ä¸å¯æ¥å—çš„ï¼Œç›¸ä¼¼çš„ï¼Œé£æ§ç³»ç»Ÿä¸­ï¼Œå°†é£é™©ç”¨æˆ·åˆ†ç±»ä¸ºæ­£å¸¸ç”¨æˆ·çš„é”™è¯¯ï¼Œæ¯”æŠŠæ­£å¸¸ç”¨æˆ·åˆ†ç±»ä¸ºé£é™©ç”¨æˆ·çš„é”™è¯¯è¦ä¸¥é‡å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨ç±»ä¼¼çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„metricéœ€è¦éšç€ä¸šåŠ¡åœºæ™¯åšå‡ºä¸€äº›æ”¹å˜ã€‚</p>\n<p>æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ¨¡å‹erroræ˜¯ï¼š<br>$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}<em>{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>ä½†æ˜¯åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªæƒé‡ï¼Œæ¥å¯¹ä¸¤ç§ä¸åŒé”™è¯¯åŠ ä»¥åŒºåˆ†<br>$$ w^{(i)}=\\left{<br>\\begin{aligned}<br>1 \\quad x^{(i)}notpron \\<br>10 \\quad x^{(i)} pron \\<br>\\end{aligned}<br>\\right.<br>$$<br>$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m</em>{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$<br>è¿™æ ·å°±æŠŠä¸¤ç§ä¸åŒçš„é—®é¢˜åŒºåˆ†å¼€äº†ã€‚</p>\n<h2 id=\"Improve-model-performance\"><a href=\"#Improve-model-performance\" class=\"headerlink\" title=\"Improve model performance\"></a>Improve model performance</h2><p>æ¨¡å‹performanceçš„æå‡æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šæ¨¡å‹è°ƒæ•´çš„å¤§ä½“æ–¹å‘ï¼ŒNgç»™å‡ºäº†å¦‚ä¸‹çš„å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/9/9-4.png\" alt=\"\"><br>æˆ‘ä»¬é»˜è®¤human-levelæ˜¯å¾ˆæ¥è¿‘ç†è®ºè¯¯å·®ï¼Œä¹Ÿå°±æ˜¯Bayes errorï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒhuman-levelï¼Œtraining errorå’Œdev errorè¿™ä¸‰è€…ä¹‹é—´çš„å…³ç³»ï¼Œhuman-levelå’Œtraining errorä¹‹é—´çš„å·®å€¼æ›´å¤§çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å»å‡å°biasï¼Œåä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å»å‡å°‘varianceï¼Œå…·ä½“çš„æ–¹æ³•ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¹‹å‰çš„è€å¥—è·¯ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course3, week2","date":"2017-10-18T13:28:12.000Z","_content":"Hi allï¼Œcourse3æ¥åˆ°äº†week2ï¼Œæœ¬å‘¨çš„è¯¾ç¨‹ä¾ç„¶ä¸»è¦æ˜¯å…³äºä¸€äº›learning strategyï¼Œè¿™äº›æ–¹æ³•ç›¸å½“å®ç”¨ã€‚è™½ç„¶ä¸æ˜¯ä»€ä¹ˆå…·ä½“çš„ç®—æ³•ï¼Œä½†éƒ½éƒ½æ˜¯Ngåœ¨ç§‘ç ”å’Œå·¥ä½œä¸­ç§¯ç´¯ä¸‹æ¥çš„å®è´µç»éªŒï¼Œå¯¹äºå®é™…é—®é¢˜ååˆ†æœ‰æ•ˆã€‚\n\næˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ã€‚\n<!--more-->\n## Error analysis\n### Carry out error analysis\næŒ‰ç…§é€šå¸¸çš„æµç¨‹ï¼Œåœ¨è¿›è¡Œtrainingè¿‡ç¨‹åï¼Œæˆ‘ä»¬åœ¨dev setä¼šè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ï¼Œå¦‚æœdev erroræ¯”training errorå¤§å¾ˆå¤šçš„è¯ï¼Œæˆ‘ä»¬åº”è¯¥å»æ’æŸ¥é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨å‘¢ï¼ŸNgç»™å‡ºäº†solution\n\nä¾‹å¦‚åœ¨cat recognitionä¸­ï¼Œæˆ‘ä»¬å‘ç°é”™åˆ†çš„sampleæœ‰å¾ˆå¤šdogå›¾åƒï¼Œè¿˜æœ‰å¾ˆå¤šçŒ«ç§‘åŠ¨ç‰©çš„å›¾åƒï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ¨¡ç³Šçš„catå›¾åƒã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶è€Œç„¶çš„æƒ³åˆ°ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š\n* è§£å†³ç‹—é”™åˆ†ä¸ºçŒ«çš„é—®é¢˜\n* è§£å†³çŒ«ç§‘åŠ¨ç‰©è¢«é”™åˆ†æˆçŒ«çš„é—®é¢˜\n* æå‡æ¨¡ç³Šå›¾åƒè¢«è¯¯åˆ†çš„é—®é¢˜\n\nå¯æ˜¯ç”±äºæˆ‘ä»¬ç²¾åŠ›å’Œæ—¶é—´éƒ½æœ‰é™ï¼Œéœ€è¦æ‰¾å‡ºè¯¯åˆ†æœ€ä¸»è¦çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯æŠŠæ‰€æœ‰é”™åˆ†çš„å›¾åƒç½—åˆ—å‡ºæ¥ï¼Œæˆ–è€…éšæœºæŠ½æ ·ä¸€å®šçš„å›¾åƒï¼Œåˆ†ææ¯ç§é”™è¯¯å®ƒæœ‰å¤šå°‘ï¼Œå é”™åˆ†å›¾åƒå¤šå°‘æ¯”ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹æˆªå›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png)\næ¯ä¸€ä¸ªé”™åˆ†çš„å›¾åƒéƒ½ä¼šè¿›è¡Œæ ‡ç­¾åŒ–çš„ç»Ÿè®¡ï¼Œæœ€åé€šè¿‡ç»Ÿè®¡æ¯ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰¾å‡ºå½±å“é”™åˆ†æœ€ä¸¥é‡çš„å› ç´ ï¼Œä½œä¸ºæˆ‘ä»¬çš„æ”¹è¿›æ–¹å‘ã€‚\n### Clean up incorrectly labeled data\nåœ¨å¸¸è§çš„é”™è¯¯ä¸­ï¼Œé”™è¯¯çš„labelæ˜¯ä¸€ç§å¾ˆå¸¸è§çš„é—®é¢˜ï¼Œè¿™ç§é—®é¢˜å¾€å¾€æ¥è‡ªäºæ ‡æ³¨æ—¶å€™ï¼Œé”™è¯¯çš„labelä¼šå¯¹trainingé€ æˆè¯¯å¯¼ã€‚\n\né¦–å…ˆï¼Œå¯¹äºtraining setï¼Œæ¥è¯´ï¼Œincorrectly labeled dataåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿé¦–å…ˆï¼ŒNgå‘Šè¯‰äº†æˆ‘ä»¬ä¸€ä¸ªæ€§è´¨ï¼š\n\n> DL algorithms are quite robust to random errors in the training set\n\nDLå› ä¸ºå…¶è‡ªèº«çš„robustæ€§è´¨ï¼Œå½“training setä¸­æœ‰å°‘è®¸çš„ï¼Œéšæœºäº§ç”Ÿçš„incorrectly labeled dataæ—¶ï¼Œæ•ˆæœå¹¶ä¸ä¼šæœ‰å¤šå·®ï¼Œæˆ‘ä»¬å®Œå…¨ä¸éœ€è¦å»ç®¡ä»–ã€‚ä½†æ˜¯ï¼Œå½“è¿™incorrectly labeled dataå¾ˆå¤šæ—¶å°±ä¸è¡Œäº†ï¼Œå› ä¸ºå®ƒä»¬å¸¦æ¥çš„æ˜¯systematic errorsï¼Œæç«¯çš„æƒ³ï¼Œå¦‚æœæŠŠæ‰€æœ‰çš„ç™½ç‹—éƒ½é”™è¯¯çš„æ ‡æ³¨æˆäº†çŒ«ï¼Œé‚£ä¹ˆè¿™ä¸ªcat recognitionç³»ç»Ÿä¸€å®šä¸ä¼šå¥½ï¼Œå› ä¸ºå®ƒä¸€å®šä¼šæŠŠç™½è‰²çš„ç‹—åˆ¤æ–­æˆä¸ºçŒ«ã€‚\n\nå†æ¥çœ‹çœ‹dev/test setä¸­çš„incorrectly labeled dataï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼Œè¯„ä¼°incorrectly labeled dataå¯¹dev errorå¸¦æ¥äº†å¤šå°‘è´¡çŒ®ï¼Œè§£å†³çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¥çœ‹æˆªå›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png)\næˆ‘ä»¬æŠŠincorrectly labeledä¹Ÿä½œä¸ºä¸€ä¸ªè¦ç´ æˆ–æ ‡ç­¾ï¼Œæ”¾åœ¨é”™åˆ†å›¾åƒåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œ\nçœ‹çœ‹æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœï¼Œå†å†³å®šincorrectly labeled dataæ˜¯ä¸æ˜¯å½±å“dev errorçš„ä¸»è¦åŸå› ï¼Œæ˜¯å¦å€¼å¾—æˆ‘ä»¬å»fix it up.\n\næœ€åï¼Œå…³äºcorrecting incorrect dev/test set exampleï¼ŒNgç»™å‡ºäº†ä¸€äº›å»ºè®®ï¼š\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\nåœ¨ä¿®æ­£çš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦ä¿è¯devå’Œtest setåŒæ—¶è¢«ä¿®æ­£ï¼Œå¦‚æœä»–ä»¬ä¸å†ç¬¦åˆåŒä¸€distributionï¼Œé‚£ä¹ˆä¼šå¯¹äºåç»­çš„è¯„ä»·å¸¦æ¥ä¸€äº›é—®é¢˜ã€‚\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\næˆ‘ä»¬åœ¨æ›´æ­£çš„æ—¶å€™ï¼Œä¸èƒ½åªæ˜¯çœ‹è¢«é”™åˆ†çš„å›¾åƒï¼Œå¯¹äºè¢«æ­£ç¡®åˆ†ç±»çš„ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨incorrect labeled çš„æƒ…å†µã€‚\n\n> Tran and dev/test data may now come from slightly different distribution\n\næ­£å¦‚åˆšæ‰è®²çš„ï¼ŒDLå¯¹äºtrainingæœ‰ä¸€å®šç¨‹åº¦çš„robustæ€§ï¼Œincorrect labeled dataå¯èƒ½ä¸ä¼šå¯¹training setå¸¦æ¥è¿™äº›é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç”¨å»æ›´æ­£training setï¼Œè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥æ¥å—çš„ã€‚\n\n### Build up quickly and iterate\næœ€åNgç”¨ä¸€ä¸ªspeech recognitionä½œä¸ºä¾‹å­ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åˆ†æå‡ºå¯èƒ½å½±å“æ•ˆæœçš„ä¸€äº›å› ç´ ï¼š\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\né’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•æ„é€ æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼ŒNgç»™å‡ºäº†å»ºè®®\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\næ€»è€Œè¨€ä¹‹ï¼Œguidelineæ˜¯\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\nä¹‹å‰æˆ‘ä»¬å†ä¸‰å¼ºè°ƒè¿‡ä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯training/dev/test setä¸€å®šè¦åœ¨åŒä¸€ä¸ªdistributionä¸‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ„¿æœ›æ€»æ˜¯ç¾å¥½çš„è€Œç°å®å¾ˆæ®‹é…·ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šé¢å¯¹ä¸€äº›training and testing on different distributioné—®é¢˜ã€‚\n\nä¾‹å¦‚åœ¨çŒ«è¯†åˆ«çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹éƒ¨ç½²åˆ°æ‰‹æœºappä¸Šï¼Œæˆ‘ä»¬æ‰‹ä¸Šçš„æ•°æ®åªæœ‰10kæ˜¯ä»æ‰‹æœºæ‹æ‘„è·å¾—çš„ï¼Œè€Œæœ‰200kçš„æ•°æ®æ˜¯ä»ç½‘ç»œä¸Šè·å¾—çš„ï¼Œè¿™ä¸¤ç§å›¾åƒæ˜¾ç„¶ä¸å±äºåŒä¸€distributionï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ\n\né¦–å…ˆæ¥çœ‹option1ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„210kæ•°æ®å……åˆ†æ··åˆåœ¨ä¸€èµ·ï¼Œå…¶ä¸­205kä½œä¸ºtraining setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtest setã€‚è¿™æ ·çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼Œä½†æ˜¯ï¼Œç¡®å®å¾ˆä¸å¥½çš„ä¸€ä¸ªæ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ\n\nåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œdev/test setå…¶å®æ‰®æ¼”äº†ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬çš„targetï¼Œä¹Ÿå°±æ˜¯æ•´ä½“çš„ä¼˜åŒ–æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦ä¼˜åŒ–çš„æ–¹å‘æ˜¯appä¸Šçš„å›¾åƒï¼Œè€Œè¿™ç§data setåˆ†å‰²æ–¹æ³•å’Œæˆ‘ä»¬çš„task targetå¹¶ä¸ç¬¦åˆï¼Œå› æ­¤å¹¶ä¸ä¼˜ç§€ã€‚\n\næˆ‘ä»¬å†æ¥çœ‹option2ï¼Œæˆ‘ä»¬å°†200kçš„æ¥è‡ªç½‘ç»œçš„å›¾ç‰‡å…¨éƒ¨æ”¾å…¥training setï¼Œç„¶åå°†10kçš„appæ•°æ®ï¼Œ5kæ”¾å…¥training setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtestï¼Œè¿™æ ·åšçš„è¯ï¼Œdev/testå†³å®šçš„target å’Œæˆ‘ä»¬çš„task targetæ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥é•¿è¿œæ¥çœ‹ï¼Œè™½ç„¶option2çš„training/dev setå¹¶ä¸æ˜¯åŒä¸€distributionï¼Œä½†æ˜¯ä»é•¿è¿œçœ‹å®ƒçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚\n### Bias & variance with mismatched data distribution\nåœ¨training/dev/test setç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒtraining errorå’Œdev errorå°±å¯ä»¥å®šæ€§æ˜¯å¦å­˜åœ¨high varianceçš„é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“training setå’Œdev setä¸ç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ¤æ–­å°±æ˜¾å¾—æœ‰äº›å›°éš¾äº†ã€‚æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ\n\nè¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä»training setä¸­å–å‡ºä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œå‘½åä¸ºtraining-dev setï¼Œè¿™éƒ¨åˆ†æ•°æ®å°†ä¸å†è¿›è¡Œtrainingï¼Œè€Œæ˜¯ä½œä¸ºè¯„åˆ¤trainingæ•ˆæœçš„ä¸€ä¸ªsetï¼Œæ­¤æ—¶æˆ‘ä»¬å°±æœ‰äº†training errorï¼Œtraining-dev errorå’Œdev errorä¸‰ä¸ªerrorï¼Œå†ç»“åˆhuman errorï¼Œtraining errorå’Œtraining-dev errorä¹‹é—´çš„å·®å€¼å¯ä»¥åæ˜ å‡ºæ¨¡å‹æ˜¯å¦æœ‰high biasæˆ–è€…varianceï¼Œè¿™æ ·å¯ä»¥æ›´ç§‘å­¦çš„æ¥è¯„åˆ¤æ¨¡å‹æ•ˆæœã€‚ç›¸åº”çš„ï¼Œtraining-dev errorå’Œdev errorç›¸å·®è¶Šå¤šï¼Œdata mismatchçš„ç¨‹åº¦è¶Šå¤§ã€‚\n### Addressing data mismatch\næˆ‘ä»¬å¦‚ä½•addressing data mismatchå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹Ngçš„ä¸¤æ¡guidelineï¼š\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\nç†è§£ä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦é€šè¿‡äººå·¥çš„analysiså»åˆ†æå‡ºé€ æˆtraining setå’Œdev setä¹‹é—´distributionä¸åŒçš„åŸå› ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ— æ±½è½¦å™ªå£°ç­‰ç­‰ï¼›ç„¶åæˆ‘ä»¬éœ€è¦æ ¹æ®è¿™äº›å·®åˆ«ï¼Œè®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œç”šè‡³ç›¸é€šã€‚\n\nä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè¦é¿å…å‡ºç°overfittingçš„æƒ…å†µå‡ºç°ï¼Œä¾‹å¦‚Ngä¸¾å‡ºçš„ä¾‹å­ï¼Œåœ¨è¯†åˆ«è½¦å†…çš„äººå£°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥çš„åˆæˆæ±½è½¦å£°éŸ³ä¸äººçš„å£°éŸ³è®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„åªç”¨ä¸€æ®µæ±½è½¦å™ªéŸ³å¾ªç¯å¾€å¤çš„å»åšåˆæˆï¼Œä¾‹å¦‚å§1minçš„æ±½è½¦å™ªå£°å¾ªç¯çš„åˆæˆåˆ°1hçš„äººå£°ä¸­ï¼Œé‚£ç»“æœä¸€å®šæ˜¯ä¸å°½å¦‚äººæ„çš„ï¼Œå› ä¸ºå‡ºç°äº†overfitting.\n## Transfer learning\nä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¤§åé¼é¼çš„transfer learningï¼Œæ‰€è°“transferï¼Œå°±æ˜¯å­˜åœ¨ä¸€ç§ä»Aåˆ°Bçš„è½¬æ¢ï¼Œè€Œä¸”è¿™ç§æƒ…å†µå¾€å¾€æ˜¯Bçš„æ•°æ®é‡å¾ˆå°‘ï¼Œéœ€è¦é€šè¿‡Aæ¥åšä¸€ä¸ªpre-trainingè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png)\nå‡è®¾è¿™ä¸ªæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªneural networksè®­ç»ƒäº†ä¸€ä¸ªimage recognitionæ¨¡å‹ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†æœ€åçš„outputï¼Œä»¥åŠoutputå¯¹åº”çš„çš„\\\\(w\\\\)å’Œ\\\\(b\\\\)ä¹Ÿåˆ é™¤ï¼Œæ›´æ¢æˆä¾‹å¦‚æ”¾å°„æ•°æ®å†è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png)\næˆ‘ä»¬ä¸ä»…ä»…å¯ä»¥æŠŠoutputå±‚æ›´æ¢æˆä¸€ä¸ªæ–°çš„outputå±‚ï¼Œè¿˜å¯ä»¥å°†outputå±‚æ›´æ¢æˆå‡ ä¸ªæ–°å±‚ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥å°†transferä¹‹å‰çš„è®­ç»ƒè®¤ä¸ºæ˜¯ä¸€ç§pre-trainingï¼Œä½†æ˜¯transfer trainingéœ€è¦æœ‰å‡ ä¸ªæ¡ä»¶ï¼š\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\nç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶çš„åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è§†é¢‘ä¸­è¯†åˆ«è¡Œäººã€è½¦è¾†ã€åœè½¦æ ‡å¿—å’Œçº¢ç»¿ç¯ï¼ŒæŒ‰ç…§å¸¸ç†æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬çš„æ„å»º4ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™4ä¸ªæ¨¡å‹çš„ç‰¹å¾åœºæ™¯éƒ½æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ„å»º4ä¸ªæ¨¡å‹ç¨å¾®æœ‰ä¸€äº›æµªè´¹ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠè¿™å››ä¸ªä»»åŠ¡åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯Multi-task learning.\n\nåœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾\\\\(y\\\\)ï¼Œå°±ä¸å†æ˜¯ä¸€ä¸ªmÃ—1çš„çŸ©é˜µäº†ï¼Œè€Œæ˜¯ä¸€ä¸ªmÃ—4çš„çŸ©é˜µï¼Œå¯¹äºmulti-taskæ¥è¯´ï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ï¼š\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learningæ˜¯éšç€DLå…´èµ·åè€Œäº§ç”Ÿçš„ä¸€ç§learningæ–¹å¼ï¼Œåœ¨end2endä¸­ï¼Œæˆ‘ä»¬ä¸å†å…³æ³¨ä¸€äº›ä¸­é—´çš„æ­¥éª¤ï¼Œä¾‹å¦‚feature selectionæˆ–è€…image processingï¼Œæˆ‘ä»¬åªæ˜¯æŠŠåŸå§‹çš„æ•°æ®å’Œæœ€åçš„ç»“æœå‘Šè¯‰DLï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚\n\nå½“ç„¶end2end ä¹Ÿæ˜¯æœ‰ä¸€äº›ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼š\nProsï¼š\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\næ€»ä¹‹ï¼Œå¯¹äºend2endæ¥è¯´ï¼Œå¤§æ•°æ®é‡ï¼Œä¸€å®šæ˜¯æœ€é‡è¦çš„å› ç´ ï¼ŒåŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰å¯ä»¥æ‘†è„±ä¼ ç»Ÿçš„ä¸­é—´æ­¥éª¤ï¼Œå½»åº•å®ç°end to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","source":"_posts/course-deep-learning-course3-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course3, week2\ndate: 2017-10-18 21:28:12\ntags: \n\t- learning strategy\n\t- transfer learning\n\t- multi-task learning\t\ncategories: learning notes\n---\nHi allï¼Œcourse3æ¥åˆ°äº†week2ï¼Œæœ¬å‘¨çš„è¯¾ç¨‹ä¾ç„¶ä¸»è¦æ˜¯å…³äºä¸€äº›learning strategyï¼Œè¿™äº›æ–¹æ³•ç›¸å½“å®ç”¨ã€‚è™½ç„¶ä¸æ˜¯ä»€ä¹ˆå…·ä½“çš„ç®—æ³•ï¼Œä½†éƒ½éƒ½æ˜¯Ngåœ¨ç§‘ç ”å’Œå·¥ä½œä¸­ç§¯ç´¯ä¸‹æ¥çš„å®è´µç»éªŒï¼Œå¯¹äºå®é™…é—®é¢˜ååˆ†æœ‰æ•ˆã€‚\n\næˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ã€‚\n<!--more-->\n## Error analysis\n### Carry out error analysis\næŒ‰ç…§é€šå¸¸çš„æµç¨‹ï¼Œåœ¨è¿›è¡Œtrainingè¿‡ç¨‹åï¼Œæˆ‘ä»¬åœ¨dev setä¼šè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ï¼Œå¦‚æœdev erroræ¯”training errorå¤§å¾ˆå¤šçš„è¯ï¼Œæˆ‘ä»¬åº”è¯¥å»æ’æŸ¥é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨å‘¢ï¼ŸNgç»™å‡ºäº†solution\n\nä¾‹å¦‚åœ¨cat recognitionä¸­ï¼Œæˆ‘ä»¬å‘ç°é”™åˆ†çš„sampleæœ‰å¾ˆå¤šdogå›¾åƒï¼Œè¿˜æœ‰å¾ˆå¤šçŒ«ç§‘åŠ¨ç‰©çš„å›¾åƒï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ¨¡ç³Šçš„catå›¾åƒã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶è€Œç„¶çš„æƒ³åˆ°ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š\n* è§£å†³ç‹—é”™åˆ†ä¸ºçŒ«çš„é—®é¢˜\n* è§£å†³çŒ«ç§‘åŠ¨ç‰©è¢«é”™åˆ†æˆçŒ«çš„é—®é¢˜\n* æå‡æ¨¡ç³Šå›¾åƒè¢«è¯¯åˆ†çš„é—®é¢˜\n\nå¯æ˜¯ç”±äºæˆ‘ä»¬ç²¾åŠ›å’Œæ—¶é—´éƒ½æœ‰é™ï¼Œéœ€è¦æ‰¾å‡ºè¯¯åˆ†æœ€ä¸»è¦çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯æŠŠæ‰€æœ‰é”™åˆ†çš„å›¾åƒç½—åˆ—å‡ºæ¥ï¼Œæˆ–è€…éšæœºæŠ½æ ·ä¸€å®šçš„å›¾åƒï¼Œåˆ†ææ¯ç§é”™è¯¯å®ƒæœ‰å¤šå°‘ï¼Œå é”™åˆ†å›¾åƒå¤šå°‘æ¯”ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹æˆªå›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png)\næ¯ä¸€ä¸ªé”™åˆ†çš„å›¾åƒéƒ½ä¼šè¿›è¡Œæ ‡ç­¾åŒ–çš„ç»Ÿè®¡ï¼Œæœ€åé€šè¿‡ç»Ÿè®¡æ¯ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰¾å‡ºå½±å“é”™åˆ†æœ€ä¸¥é‡çš„å› ç´ ï¼Œä½œä¸ºæˆ‘ä»¬çš„æ”¹è¿›æ–¹å‘ã€‚\n### Clean up incorrectly labeled data\nåœ¨å¸¸è§çš„é”™è¯¯ä¸­ï¼Œé”™è¯¯çš„labelæ˜¯ä¸€ç§å¾ˆå¸¸è§çš„é—®é¢˜ï¼Œè¿™ç§é—®é¢˜å¾€å¾€æ¥è‡ªäºæ ‡æ³¨æ—¶å€™ï¼Œé”™è¯¯çš„labelä¼šå¯¹trainingé€ æˆè¯¯å¯¼ã€‚\n\né¦–å…ˆï¼Œå¯¹äºtraining setï¼Œæ¥è¯´ï¼Œincorrectly labeled dataåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿé¦–å…ˆï¼ŒNgå‘Šè¯‰äº†æˆ‘ä»¬ä¸€ä¸ªæ€§è´¨ï¼š\n\n> DL algorithms are quite robust to random errors in the training set\n\nDLå› ä¸ºå…¶è‡ªèº«çš„robustæ€§è´¨ï¼Œå½“training setä¸­æœ‰å°‘è®¸çš„ï¼Œéšæœºäº§ç”Ÿçš„incorrectly labeled dataæ—¶ï¼Œæ•ˆæœå¹¶ä¸ä¼šæœ‰å¤šå·®ï¼Œæˆ‘ä»¬å®Œå…¨ä¸éœ€è¦å»ç®¡ä»–ã€‚ä½†æ˜¯ï¼Œå½“è¿™incorrectly labeled dataå¾ˆå¤šæ—¶å°±ä¸è¡Œäº†ï¼Œå› ä¸ºå®ƒä»¬å¸¦æ¥çš„æ˜¯systematic errorsï¼Œæç«¯çš„æƒ³ï¼Œå¦‚æœæŠŠæ‰€æœ‰çš„ç™½ç‹—éƒ½é”™è¯¯çš„æ ‡æ³¨æˆäº†çŒ«ï¼Œé‚£ä¹ˆè¿™ä¸ªcat recognitionç³»ç»Ÿä¸€å®šä¸ä¼šå¥½ï¼Œå› ä¸ºå®ƒä¸€å®šä¼šæŠŠç™½è‰²çš„ç‹—åˆ¤æ–­æˆä¸ºçŒ«ã€‚\n\nå†æ¥çœ‹çœ‹dev/test setä¸­çš„incorrectly labeled dataï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼Œè¯„ä¼°incorrectly labeled dataå¯¹dev errorå¸¦æ¥äº†å¤šå°‘è´¡çŒ®ï¼Œè§£å†³çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¥çœ‹æˆªå›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png)\næˆ‘ä»¬æŠŠincorrectly labeledä¹Ÿä½œä¸ºä¸€ä¸ªè¦ç´ æˆ–æ ‡ç­¾ï¼Œæ”¾åœ¨é”™åˆ†å›¾åƒåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œ\nçœ‹çœ‹æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœï¼Œå†å†³å®šincorrectly labeled dataæ˜¯ä¸æ˜¯å½±å“dev errorçš„ä¸»è¦åŸå› ï¼Œæ˜¯å¦å€¼å¾—æˆ‘ä»¬å»fix it up.\n\næœ€åï¼Œå…³äºcorrecting incorrect dev/test set exampleï¼ŒNgç»™å‡ºäº†ä¸€äº›å»ºè®®ï¼š\n> Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.\n\nåœ¨ä¿®æ­£çš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦ä¿è¯devå’Œtest setåŒæ—¶è¢«ä¿®æ­£ï¼Œå¦‚æœä»–ä»¬ä¸å†ç¬¦åˆåŒä¸€distributionï¼Œé‚£ä¹ˆä¼šå¯¹äºåç»­çš„è¯„ä»·å¸¦æ¥ä¸€äº›é—®é¢˜ã€‚\n\n> Consider examining examples your algorithm got right as well as ones it got wrong.\n\næˆ‘ä»¬åœ¨æ›´æ­£çš„æ—¶å€™ï¼Œä¸èƒ½åªæ˜¯çœ‹è¢«é”™åˆ†çš„å›¾åƒï¼Œå¯¹äºè¢«æ­£ç¡®åˆ†ç±»çš„ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨incorrect labeled çš„æƒ…å†µã€‚\n\n> Tran and dev/test data may now come from slightly different distribution\n\næ­£å¦‚åˆšæ‰è®²çš„ï¼ŒDLå¯¹äºtrainingæœ‰ä¸€å®šç¨‹åº¦çš„robustæ€§ï¼Œincorrect labeled dataå¯èƒ½ä¸ä¼šå¯¹training setå¸¦æ¥è¿™äº›é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç”¨å»æ›´æ­£training setï¼Œè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥æ¥å—çš„ã€‚\n\n### Build up quickly and iterate\næœ€åNgç”¨ä¸€ä¸ªspeech recognitionä½œä¸ºä¾‹å­ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åˆ†æå‡ºå¯èƒ½å½±å“æ•ˆæœçš„ä¸€äº›å› ç´ ï¼š\n* Noisy background\n* Accented speech\n* Far from microphone\n* Young children's speech\n\n...\né’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•æ„é€ æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼ŒNgç»™å‡ºäº†å»ºè®®\n* Set up dev/test set and metric\n* Build initial system quickly\n* Use bias/variance analysis & error analysis to prioritize next steps.\n\næ€»è€Œè¨€ä¹‹ï¼Œguidelineæ˜¯\n> Build your first system quickly, then iterate.\n\n## Mismatched training and dev/test data\n### Training and testing on different distributions\nä¹‹å‰æˆ‘ä»¬å†ä¸‰å¼ºè°ƒè¿‡ä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯training/dev/test setä¸€å®šè¦åœ¨åŒä¸€ä¸ªdistributionä¸‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ„¿æœ›æ€»æ˜¯ç¾å¥½çš„è€Œç°å®å¾ˆæ®‹é…·ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šé¢å¯¹ä¸€äº›training and testing on different distributioné—®é¢˜ã€‚\n\nä¾‹å¦‚åœ¨çŒ«è¯†åˆ«çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹éƒ¨ç½²åˆ°æ‰‹æœºappä¸Šï¼Œæˆ‘ä»¬æ‰‹ä¸Šçš„æ•°æ®åªæœ‰10kæ˜¯ä»æ‰‹æœºæ‹æ‘„è·å¾—çš„ï¼Œè€Œæœ‰200kçš„æ•°æ®æ˜¯ä»ç½‘ç»œä¸Šè·å¾—çš„ï¼Œè¿™ä¸¤ç§å›¾åƒæ˜¾ç„¶ä¸å±äºåŒä¸€distributionï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ\n\né¦–å…ˆæ¥çœ‹option1ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„210kæ•°æ®å……åˆ†æ··åˆåœ¨ä¸€èµ·ï¼Œå…¶ä¸­205kä½œä¸ºtraining setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtest setã€‚è¿™æ ·çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼Œä½†æ˜¯ï¼Œç¡®å®å¾ˆä¸å¥½çš„ä¸€ä¸ªæ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ\n\nåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œdev/test setå…¶å®æ‰®æ¼”äº†ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬çš„targetï¼Œä¹Ÿå°±æ˜¯æ•´ä½“çš„ä¼˜åŒ–æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦ä¼˜åŒ–çš„æ–¹å‘æ˜¯appä¸Šçš„å›¾åƒï¼Œè€Œè¿™ç§data setåˆ†å‰²æ–¹æ³•å’Œæˆ‘ä»¬çš„task targetå¹¶ä¸ç¬¦åˆï¼Œå› æ­¤å¹¶ä¸ä¼˜ç§€ã€‚\n\næˆ‘ä»¬å†æ¥çœ‹option2ï¼Œæˆ‘ä»¬å°†200kçš„æ¥è‡ªç½‘ç»œçš„å›¾ç‰‡å…¨éƒ¨æ”¾å…¥training setï¼Œç„¶åå°†10kçš„appæ•°æ®ï¼Œ5kæ”¾å…¥training setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtestï¼Œè¿™æ ·åšçš„è¯ï¼Œdev/testå†³å®šçš„target å’Œæˆ‘ä»¬çš„task targetæ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥é•¿è¿œæ¥çœ‹ï¼Œè™½ç„¶option2çš„training/dev setå¹¶ä¸æ˜¯åŒä¸€distributionï¼Œä½†æ˜¯ä»é•¿è¿œçœ‹å®ƒçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚\n### Bias & variance with mismatched data distribution\nåœ¨training/dev/test setç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒtraining errorå’Œdev errorå°±å¯ä»¥å®šæ€§æ˜¯å¦å­˜åœ¨high varianceçš„é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“training setå’Œdev setä¸ç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ¤æ–­å°±æ˜¾å¾—æœ‰äº›å›°éš¾äº†ã€‚æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ\n\nè¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä»training setä¸­å–å‡ºä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œå‘½åä¸ºtraining-dev setï¼Œè¿™éƒ¨åˆ†æ•°æ®å°†ä¸å†è¿›è¡Œtrainingï¼Œè€Œæ˜¯ä½œä¸ºè¯„åˆ¤trainingæ•ˆæœçš„ä¸€ä¸ªsetï¼Œæ­¤æ—¶æˆ‘ä»¬å°±æœ‰äº†training errorï¼Œtraining-dev errorå’Œdev errorä¸‰ä¸ªerrorï¼Œå†ç»“åˆhuman errorï¼Œtraining errorå’Œtraining-dev errorä¹‹é—´çš„å·®å€¼å¯ä»¥åæ˜ å‡ºæ¨¡å‹æ˜¯å¦æœ‰high biasæˆ–è€…varianceï¼Œè¿™æ ·å¯ä»¥æ›´ç§‘å­¦çš„æ¥è¯„åˆ¤æ¨¡å‹æ•ˆæœã€‚ç›¸åº”çš„ï¼Œtraining-dev errorå’Œdev errorç›¸å·®è¶Šå¤šï¼Œdata mismatchçš„ç¨‹åº¦è¶Šå¤§ã€‚\n### Addressing data mismatch\næˆ‘ä»¬å¦‚ä½•addressing data mismatchå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹Ngçš„ä¸¤æ¡guidelineï¼š\n* Carry out manual error analysis to try to understand difference between training and dev/test sets\n* Make training data more similar;  or collect more data similar to dev/test sets\n\nç†è§£ä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦é€šè¿‡äººå·¥çš„analysiså»åˆ†æå‡ºé€ æˆtraining setå’Œdev setä¹‹é—´distributionä¸åŒçš„åŸå› ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ— æ±½è½¦å™ªå£°ç­‰ç­‰ï¼›ç„¶åæˆ‘ä»¬éœ€è¦æ ¹æ®è¿™äº›å·®åˆ«ï¼Œè®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œç”šè‡³ç›¸é€šã€‚\n\nä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè¦é¿å…å‡ºç°overfittingçš„æƒ…å†µå‡ºç°ï¼Œä¾‹å¦‚Ngä¸¾å‡ºçš„ä¾‹å­ï¼Œåœ¨è¯†åˆ«è½¦å†…çš„äººå£°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥çš„åˆæˆæ±½è½¦å£°éŸ³ä¸äººçš„å£°éŸ³è®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„åªç”¨ä¸€æ®µæ±½è½¦å™ªéŸ³å¾ªç¯å¾€å¤çš„å»åšåˆæˆï¼Œä¾‹å¦‚å§1minçš„æ±½è½¦å™ªå£°å¾ªç¯çš„åˆæˆåˆ°1hçš„äººå£°ä¸­ï¼Œé‚£ç»“æœä¸€å®šæ˜¯ä¸å°½å¦‚äººæ„çš„ï¼Œå› ä¸ºå‡ºç°äº†overfitting.\n## Transfer learning\nä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¤§åé¼é¼çš„transfer learningï¼Œæ‰€è°“transferï¼Œå°±æ˜¯å­˜åœ¨ä¸€ç§ä»Aåˆ°Bçš„è½¬æ¢ï¼Œè€Œä¸”è¿™ç§æƒ…å†µå¾€å¾€æ˜¯Bçš„æ•°æ®é‡å¾ˆå°‘ï¼Œéœ€è¦é€šè¿‡Aæ¥åšä¸€ä¸ªpre-trainingè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„neural networks\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png)\nå‡è®¾è¿™ä¸ªæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªneural networksè®­ç»ƒäº†ä¸€ä¸ªimage recognitionæ¨¡å‹ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†æœ€åçš„outputï¼Œä»¥åŠoutputå¯¹åº”çš„çš„\\\\(w\\\\)å’Œ\\\\(b\\\\)ä¹Ÿåˆ é™¤ï¼Œæ›´æ¢æˆä¾‹å¦‚æ”¾å°„æ•°æ®å†è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png)\næˆ‘ä»¬ä¸ä»…ä»…å¯ä»¥æŠŠoutputå±‚æ›´æ¢æˆä¸€ä¸ªæ–°çš„outputå±‚ï¼Œè¿˜å¯ä»¥å°†outputå±‚æ›´æ¢æˆå‡ ä¸ªæ–°å±‚ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥å°†transferä¹‹å‰çš„è®­ç»ƒè®¤ä¸ºæ˜¯ä¸€ç§pre-trainingï¼Œä½†æ˜¯transfer trainingéœ€è¦æœ‰å‡ ä¸ªæ¡ä»¶ï¼š\n* Task A and B have the same input x.\n* You have a lot more data for Task A than Task B.\n* Low level features from A could be helpful for learning B.\n\n## Multi-task learning\nç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶çš„åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è§†é¢‘ä¸­è¯†åˆ«è¡Œäººã€è½¦è¾†ã€åœè½¦æ ‡å¿—å’Œçº¢ç»¿ç¯ï¼ŒæŒ‰ç…§å¸¸ç†æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬çš„æ„å»º4ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™4ä¸ªæ¨¡å‹çš„ç‰¹å¾åœºæ™¯éƒ½æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ„å»º4ä¸ªæ¨¡å‹ç¨å¾®æœ‰ä¸€äº›æµªè´¹ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠè¿™å››ä¸ªä»»åŠ¡åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯Multi-task learning.\n\nåœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾\\\\(y\\\\)ï¼Œå°±ä¸å†æ˜¯ä¸€ä¸ªmÃ—1çš„çŸ©é˜µäº†ï¼Œè€Œæ˜¯ä¸€ä¸ªmÃ—4çš„çŸ©é˜µï¼Œå¯¹äºmulti-taskæ¥è¯´ï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ï¼š\n* Training on a set of tasks that could benefit from having shared lower-level features.\n* Usually: Amount of data you have for eachtask is quite similar.\n* Can train a big enough neural network to do well on all the tasks.\n\n## End to end learning\nEnd to end learningæ˜¯éšç€DLå…´èµ·åè€Œäº§ç”Ÿçš„ä¸€ç§learningæ–¹å¼ï¼Œåœ¨end2endä¸­ï¼Œæˆ‘ä»¬ä¸å†å…³æ³¨ä¸€äº›ä¸­é—´çš„æ­¥éª¤ï¼Œä¾‹å¦‚feature selectionæˆ–è€…image processingï¼Œæˆ‘ä»¬åªæ˜¯æŠŠåŸå§‹çš„æ•°æ®å’Œæœ€åçš„ç»“æœå‘Šè¯‰DLï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚\n\nå½“ç„¶end2end ä¹Ÿæ˜¯æœ‰ä¸€äº›ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼š\nProsï¼š\n* Let the data speak.\n* Less hand-desgining of components needed.\n\nCons:\n* May need large amount of data.\n* Excludes potentially userful hand-designed components.\n\næ€»ä¹‹ï¼Œå¯¹äºend2endæ¥è¯´ï¼Œå¤§æ•°æ®é‡ï¼Œä¸€å®šæ˜¯æœ€é‡è¦çš„å› ç´ ï¼ŒåŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰å¯ä»¥æ‘†è„±ä¼ ç»Ÿçš„ä¸­é—´æ­¥éª¤ï¼Œå½»åº•å®ç°end to end learning.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](https://www.coursera.org/specializations/deep-learning)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info)\n","slug":"course-deep-learning-course3-week2","published":1,"updated":"2018-11-19T06:43:39.611Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p10009tr8l8cxyfnhz","content":"<p>Hi allï¼Œcourse3æ¥åˆ°äº†week2ï¼Œæœ¬å‘¨çš„è¯¾ç¨‹ä¾ç„¶ä¸»è¦æ˜¯å…³äºä¸€äº›learning strategyï¼Œè¿™äº›æ–¹æ³•ç›¸å½“å®ç”¨ã€‚è™½ç„¶ä¸æ˜¯ä»€ä¹ˆå…·ä½“çš„ç®—æ³•ï¼Œä½†éƒ½éƒ½æ˜¯Ngåœ¨ç§‘ç ”å’Œå·¥ä½œä¸­ç§¯ç´¯ä¸‹æ¥çš„å®è´µç»éªŒï¼Œå¯¹äºå®é™…é—®é¢˜ååˆ†æœ‰æ•ˆã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>æŒ‰ç…§é€šå¸¸çš„æµç¨‹ï¼Œåœ¨è¿›è¡Œtrainingè¿‡ç¨‹åï¼Œæˆ‘ä»¬åœ¨dev setä¼šè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ï¼Œå¦‚æœdev erroræ¯”training errorå¤§å¾ˆå¤šçš„è¯ï¼Œæˆ‘ä»¬åº”è¯¥å»æ’æŸ¥é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨å‘¢ï¼ŸNgç»™å‡ºäº†solution</p>\n<p>ä¾‹å¦‚åœ¨cat recognitionä¸­ï¼Œæˆ‘ä»¬å‘ç°é”™åˆ†çš„sampleæœ‰å¾ˆå¤šdogå›¾åƒï¼Œè¿˜æœ‰å¾ˆå¤šçŒ«ç§‘åŠ¨ç‰©çš„å›¾åƒï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ¨¡ç³Šçš„catå›¾åƒã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶è€Œç„¶çš„æƒ³åˆ°ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š</p>\n<ul>\n<li>è§£å†³ç‹—é”™åˆ†ä¸ºçŒ«çš„é—®é¢˜</li>\n<li>è§£å†³çŒ«ç§‘åŠ¨ç‰©è¢«é”™åˆ†æˆçŒ«çš„é—®é¢˜</li>\n<li>æå‡æ¨¡ç³Šå›¾åƒè¢«è¯¯åˆ†çš„é—®é¢˜</li>\n</ul>\n<p>å¯æ˜¯ç”±äºæˆ‘ä»¬ç²¾åŠ›å’Œæ—¶é—´éƒ½æœ‰é™ï¼Œéœ€è¦æ‰¾å‡ºè¯¯åˆ†æœ€ä¸»è¦çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯æŠŠæ‰€æœ‰é”™åˆ†çš„å›¾åƒç½—åˆ—å‡ºæ¥ï¼Œæˆ–è€…éšæœºæŠ½æ ·ä¸€å®šçš„å›¾åƒï¼Œåˆ†ææ¯ç§é”™è¯¯å®ƒæœ‰å¤šå°‘ï¼Œå é”™åˆ†å›¾åƒå¤šå°‘æ¯”ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹æˆªå›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png\" alt=\"\"><br>æ¯ä¸€ä¸ªé”™åˆ†çš„å›¾åƒéƒ½ä¼šè¿›è¡Œæ ‡ç­¾åŒ–çš„ç»Ÿè®¡ï¼Œæœ€åé€šè¿‡ç»Ÿè®¡æ¯ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰¾å‡ºå½±å“é”™åˆ†æœ€ä¸¥é‡çš„å› ç´ ï¼Œä½œä¸ºæˆ‘ä»¬çš„æ”¹è¿›æ–¹å‘ã€‚</p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>åœ¨å¸¸è§çš„é”™è¯¯ä¸­ï¼Œé”™è¯¯çš„labelæ˜¯ä¸€ç§å¾ˆå¸¸è§çš„é—®é¢˜ï¼Œè¿™ç§é—®é¢˜å¾€å¾€æ¥è‡ªäºæ ‡æ³¨æ—¶å€™ï¼Œé”™è¯¯çš„labelä¼šå¯¹trainingé€ æˆè¯¯å¯¼ã€‚</p>\n<p>é¦–å…ˆï¼Œå¯¹äºtraining setï¼Œæ¥è¯´ï¼Œincorrectly labeled dataåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿé¦–å…ˆï¼ŒNgå‘Šè¯‰äº†æˆ‘ä»¬ä¸€ä¸ªæ€§è´¨ï¼š</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DLå› ä¸ºå…¶è‡ªèº«çš„robustæ€§è´¨ï¼Œå½“training setä¸­æœ‰å°‘è®¸çš„ï¼Œéšæœºäº§ç”Ÿçš„incorrectly labeled dataæ—¶ï¼Œæ•ˆæœå¹¶ä¸ä¼šæœ‰å¤šå·®ï¼Œæˆ‘ä»¬å®Œå…¨ä¸éœ€è¦å»ç®¡ä»–ã€‚ä½†æ˜¯ï¼Œå½“è¿™incorrectly labeled dataå¾ˆå¤šæ—¶å°±ä¸è¡Œäº†ï¼Œå› ä¸ºå®ƒä»¬å¸¦æ¥çš„æ˜¯systematic errorsï¼Œæç«¯çš„æƒ³ï¼Œå¦‚æœæŠŠæ‰€æœ‰çš„ç™½ç‹—éƒ½é”™è¯¯çš„æ ‡æ³¨æˆäº†çŒ«ï¼Œé‚£ä¹ˆè¿™ä¸ªcat recognitionç³»ç»Ÿä¸€å®šä¸ä¼šå¥½ï¼Œå› ä¸ºå®ƒä¸€å®šä¼šæŠŠç™½è‰²çš„ç‹—åˆ¤æ–­æˆä¸ºçŒ«ã€‚</p>\n<p>å†æ¥çœ‹çœ‹dev/test setä¸­çš„incorrectly labeled dataï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼Œè¯„ä¼°incorrectly labeled dataå¯¹dev errorå¸¦æ¥äº†å¤šå°‘è´¡çŒ®ï¼Œè§£å†³çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¥çœ‹æˆªå›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png\" alt=\"\"><br>æˆ‘ä»¬æŠŠincorrectly labeledä¹Ÿä½œä¸ºä¸€ä¸ªè¦ç´ æˆ–æ ‡ç­¾ï¼Œæ”¾åœ¨é”™åˆ†å›¾åƒåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œ<br>çœ‹çœ‹æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœï¼Œå†å†³å®šincorrectly labeled dataæ˜¯ä¸æ˜¯å½±å“dev errorçš„ä¸»è¦åŸå› ï¼Œæ˜¯å¦å€¼å¾—æˆ‘ä»¬å»fix it up.</p>\n<p>æœ€åï¼Œå…³äºcorrecting incorrect dev/test set exampleï¼ŒNgç»™å‡ºäº†ä¸€äº›å»ºè®®ï¼š</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>åœ¨ä¿®æ­£çš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦ä¿è¯devå’Œtest setåŒæ—¶è¢«ä¿®æ­£ï¼Œå¦‚æœä»–ä»¬ä¸å†ç¬¦åˆåŒä¸€distributionï¼Œé‚£ä¹ˆä¼šå¯¹äºåç»­çš„è¯„ä»·å¸¦æ¥ä¸€äº›é—®é¢˜ã€‚</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>æˆ‘ä»¬åœ¨æ›´æ­£çš„æ—¶å€™ï¼Œä¸èƒ½åªæ˜¯çœ‹è¢«é”™åˆ†çš„å›¾åƒï¼Œå¯¹äºè¢«æ­£ç¡®åˆ†ç±»çš„ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨incorrect labeled çš„æƒ…å†µã€‚</p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>æ­£å¦‚åˆšæ‰è®²çš„ï¼ŒDLå¯¹äºtrainingæœ‰ä¸€å®šç¨‹åº¦çš„robustæ€§ï¼Œincorrect labeled dataå¯èƒ½ä¸ä¼šå¯¹training setå¸¦æ¥è¿™äº›é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç”¨å»æ›´æ­£training setï¼Œè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥æ¥å—çš„ã€‚</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>æœ€åNgç”¨ä¸€ä¸ªspeech recognitionä½œä¸ºä¾‹å­ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åˆ†æå‡ºå¯èƒ½å½±å“æ•ˆæœçš„ä¸€äº›å› ç´ ï¼š</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young childrenâ€™s speech</li>\n</ul>\n<p>â€¦<br>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•æ„é€ æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼ŒNgç»™å‡ºäº†å»ºè®®</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>æ€»è€Œè¨€ä¹‹ï¼Œguidelineæ˜¯</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>ä¹‹å‰æˆ‘ä»¬å†ä¸‰å¼ºè°ƒè¿‡ä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯training/dev/test setä¸€å®šè¦åœ¨åŒä¸€ä¸ªdistributionä¸‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ„¿æœ›æ€»æ˜¯ç¾å¥½çš„è€Œç°å®å¾ˆæ®‹é…·ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šé¢å¯¹ä¸€äº›training and testing on different distributioné—®é¢˜ã€‚</p>\n<p>ä¾‹å¦‚åœ¨çŒ«è¯†åˆ«çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹éƒ¨ç½²åˆ°æ‰‹æœºappä¸Šï¼Œæˆ‘ä»¬æ‰‹ä¸Šçš„æ•°æ®åªæœ‰10kæ˜¯ä»æ‰‹æœºæ‹æ‘„è·å¾—çš„ï¼Œè€Œæœ‰200kçš„æ•°æ®æ˜¯ä»ç½‘ç»œä¸Šè·å¾—çš„ï¼Œè¿™ä¸¤ç§å›¾åƒæ˜¾ç„¶ä¸å±äºåŒä¸€distributionï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ</p>\n<p>é¦–å…ˆæ¥çœ‹option1ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„210kæ•°æ®å……åˆ†æ··åˆåœ¨ä¸€èµ·ï¼Œå…¶ä¸­205kä½œä¸ºtraining setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtest setã€‚è¿™æ ·çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼Œä½†æ˜¯ï¼Œç¡®å®å¾ˆä¸å¥½çš„ä¸€ä¸ªæ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ</p>\n<p>åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œdev/test setå…¶å®æ‰®æ¼”äº†ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬çš„targetï¼Œä¹Ÿå°±æ˜¯æ•´ä½“çš„ä¼˜åŒ–æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦ä¼˜åŒ–çš„æ–¹å‘æ˜¯appä¸Šçš„å›¾åƒï¼Œè€Œè¿™ç§data setåˆ†å‰²æ–¹æ³•å’Œæˆ‘ä»¬çš„task targetå¹¶ä¸ç¬¦åˆï¼Œå› æ­¤å¹¶ä¸ä¼˜ç§€ã€‚</p>\n<p>æˆ‘ä»¬å†æ¥çœ‹option2ï¼Œæˆ‘ä»¬å°†200kçš„æ¥è‡ªç½‘ç»œçš„å›¾ç‰‡å…¨éƒ¨æ”¾å…¥training setï¼Œç„¶åå°†10kçš„appæ•°æ®ï¼Œ5kæ”¾å…¥training setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtestï¼Œè¿™æ ·åšçš„è¯ï¼Œdev/testå†³å®šçš„target å’Œæˆ‘ä»¬çš„task targetæ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥é•¿è¿œæ¥çœ‹ï¼Œè™½ç„¶option2çš„training/dev setå¹¶ä¸æ˜¯åŒä¸€distributionï¼Œä½†æ˜¯ä»é•¿è¿œçœ‹å®ƒçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>åœ¨training/dev/test setç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒtraining errorå’Œdev errorå°±å¯ä»¥å®šæ€§æ˜¯å¦å­˜åœ¨high varianceçš„é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“training setå’Œdev setä¸ç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ¤æ–­å°±æ˜¾å¾—æœ‰äº›å›°éš¾äº†ã€‚æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ</p>\n<p>è¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä»training setä¸­å–å‡ºä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œå‘½åä¸ºtraining-dev setï¼Œè¿™éƒ¨åˆ†æ•°æ®å°†ä¸å†è¿›è¡Œtrainingï¼Œè€Œæ˜¯ä½œä¸ºè¯„åˆ¤trainingæ•ˆæœçš„ä¸€ä¸ªsetï¼Œæ­¤æ—¶æˆ‘ä»¬å°±æœ‰äº†training errorï¼Œtraining-dev errorå’Œdev errorä¸‰ä¸ªerrorï¼Œå†ç»“åˆhuman errorï¼Œtraining errorå’Œtraining-dev errorä¹‹é—´çš„å·®å€¼å¯ä»¥åæ˜ å‡ºæ¨¡å‹æ˜¯å¦æœ‰high biasæˆ–è€…varianceï¼Œè¿™æ ·å¯ä»¥æ›´ç§‘å­¦çš„æ¥è¯„åˆ¤æ¨¡å‹æ•ˆæœã€‚ç›¸åº”çš„ï¼Œtraining-dev errorå’Œdev errorç›¸å·®è¶Šå¤šï¼Œdata mismatchçš„ç¨‹åº¦è¶Šå¤§ã€‚</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>æˆ‘ä»¬å¦‚ä½•addressing data mismatchå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹Ngçš„ä¸¤æ¡guidelineï¼š</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>ç†è§£ä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦é€šè¿‡äººå·¥çš„analysiså»åˆ†æå‡ºé€ æˆtraining setå’Œdev setä¹‹é—´distributionä¸åŒçš„åŸå› ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ— æ±½è½¦å™ªå£°ç­‰ç­‰ï¼›ç„¶åæˆ‘ä»¬éœ€è¦æ ¹æ®è¿™äº›å·®åˆ«ï¼Œè®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œç”šè‡³ç›¸é€šã€‚</p>\n<p>ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè¦é¿å…å‡ºç°overfittingçš„æƒ…å†µå‡ºç°ï¼Œä¾‹å¦‚Ngä¸¾å‡ºçš„ä¾‹å­ï¼Œåœ¨è¯†åˆ«è½¦å†…çš„äººå£°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥çš„åˆæˆæ±½è½¦å£°éŸ³ä¸äººçš„å£°éŸ³è®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„åªç”¨ä¸€æ®µæ±½è½¦å™ªéŸ³å¾ªç¯å¾€å¤çš„å»åšåˆæˆï¼Œä¾‹å¦‚å§1minçš„æ±½è½¦å™ªå£°å¾ªç¯çš„åˆæˆåˆ°1hçš„äººå£°ä¸­ï¼Œé‚£ç»“æœä¸€å®šæ˜¯ä¸å°½å¦‚äººæ„çš„ï¼Œå› ä¸ºå‡ºç°äº†overfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>ä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¤§åé¼é¼çš„transfer learningï¼Œæ‰€è°“transferï¼Œå°±æ˜¯å­˜åœ¨ä¸€ç§ä»Aåˆ°Bçš„è½¬æ¢ï¼Œè€Œä¸”è¿™ç§æƒ…å†µå¾€å¾€æ˜¯Bçš„æ•°æ®é‡å¾ˆå°‘ï¼Œéœ€è¦é€šè¿‡Aæ¥åšä¸€ä¸ªpre-trainingè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png\" alt=\"\"><br>å‡è®¾è¿™ä¸ªæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªneural networksè®­ç»ƒäº†ä¸€ä¸ªimage recognitionæ¨¡å‹ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†æœ€åçš„outputï¼Œä»¥åŠoutputå¯¹åº”çš„çš„\\(w\\)å’Œ\\(b\\)ä¹Ÿåˆ é™¤ï¼Œæ›´æ¢æˆä¾‹å¦‚æ”¾å°„æ•°æ®å†è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png\" alt=\"\"><br>æˆ‘ä»¬ä¸ä»…ä»…å¯ä»¥æŠŠoutputå±‚æ›´æ¢æˆä¸€ä¸ªæ–°çš„outputå±‚ï¼Œè¿˜å¯ä»¥å°†outputå±‚æ›´æ¢æˆå‡ ä¸ªæ–°å±‚ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥å°†transferä¹‹å‰çš„è®­ç»ƒè®¤ä¸ºæ˜¯ä¸€ç§pre-trainingï¼Œä½†æ˜¯transfer trainingéœ€è¦æœ‰å‡ ä¸ªæ¡ä»¶ï¼š</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>ç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶çš„åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è§†é¢‘ä¸­è¯†åˆ«è¡Œäººã€è½¦è¾†ã€åœè½¦æ ‡å¿—å’Œçº¢ç»¿ç¯ï¼ŒæŒ‰ç…§å¸¸ç†æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬çš„æ„å»º4ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™4ä¸ªæ¨¡å‹çš„ç‰¹å¾åœºæ™¯éƒ½æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ„å»º4ä¸ªæ¨¡å‹ç¨å¾®æœ‰ä¸€äº›æµªè´¹ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠè¿™å››ä¸ªä»»åŠ¡åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯Multi-task learning.</p>\n<p>åœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾\\(y\\)ï¼Œå°±ä¸å†æ˜¯ä¸€ä¸ªmÃ—1çš„çŸ©é˜µäº†ï¼Œè€Œæ˜¯ä¸€ä¸ªmÃ—4çš„çŸ©é˜µï¼Œå¯¹äºmulti-taskæ¥è¯´ï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ï¼š</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learningæ˜¯éšç€DLå…´èµ·åè€Œäº§ç”Ÿçš„ä¸€ç§learningæ–¹å¼ï¼Œåœ¨end2endä¸­ï¼Œæˆ‘ä»¬ä¸å†å…³æ³¨ä¸€äº›ä¸­é—´çš„æ­¥éª¤ï¼Œä¾‹å¦‚feature selectionæˆ–è€…image processingï¼Œæˆ‘ä»¬åªæ˜¯æŠŠåŸå§‹çš„æ•°æ®å’Œæœ€åçš„ç»“æœå‘Šè¯‰DLï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚</p>\n<p>å½“ç„¶end2end ä¹Ÿæ˜¯æœ‰ä¸€äº›ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼š<br>Prosï¼š</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>æ€»ä¹‹ï¼Œå¯¹äºend2endæ¥è¯´ï¼Œå¤§æ•°æ®é‡ï¼Œä¸€å®šæ˜¯æœ€é‡è¦çš„å› ç´ ï¼ŒåŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰å¯ä»¥æ‘†è„±ä¼ ç»Ÿçš„ä¸­é—´æ­¥éª¤ï¼Œå½»åº•å®ç°end to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"learning strategy","path":"tags/learning-strategy/"},{"name":"transfer learning","path":"tags/transfer-learning/"},{"name":"multi-task learning","path":"tags/multi-task-learning/"}],"excerpt":"<p>Hi allï¼Œcourse3æ¥åˆ°äº†week2ï¼Œæœ¬å‘¨çš„è¯¾ç¨‹ä¾ç„¶ä¸»è¦æ˜¯å…³äºä¸€äº›learning strategyï¼Œè¿™äº›æ–¹æ³•ç›¸å½“å®ç”¨ã€‚è™½ç„¶ä¸æ˜¯ä»€ä¹ˆå…·ä½“çš„ç®—æ³•ï¼Œä½†éƒ½éƒ½æ˜¯Ngåœ¨ç§‘ç ”å’Œå·¥ä½œä¸­ç§¯ç´¯ä¸‹æ¥çš„å®è´µç»éªŒï¼Œå¯¹äºå®é™…é—®é¢˜ååˆ†æœ‰æ•ˆã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ã€‚<br></p>","more":"</p>\n<h2 id=\"Error-analysis\"><a href=\"#Error-analysis\" class=\"headerlink\" title=\"Error analysis\"></a>Error analysis</h2><h3 id=\"Carry-out-error-analysis\"><a href=\"#Carry-out-error-analysis\" class=\"headerlink\" title=\"Carry out error analysis\"></a>Carry out error analysis</h3><p>æŒ‰ç…§é€šå¸¸çš„æµç¨‹ï¼Œåœ¨è¿›è¡Œtrainingè¿‡ç¨‹åï¼Œæˆ‘ä»¬åœ¨dev setä¼šè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ï¼Œå¦‚æœdev erroræ¯”training errorå¤§å¾ˆå¤šçš„è¯ï¼Œæˆ‘ä»¬åº”è¯¥å»æ’æŸ¥é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨å‘¢ï¼ŸNgç»™å‡ºäº†solution</p>\n<p>ä¾‹å¦‚åœ¨cat recognitionä¸­ï¼Œæˆ‘ä»¬å‘ç°é”™åˆ†çš„sampleæœ‰å¾ˆå¤šdogå›¾åƒï¼Œè¿˜æœ‰å¾ˆå¤šçŒ«ç§‘åŠ¨ç‰©çš„å›¾åƒï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ¨¡ç³Šçš„catå›¾åƒã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶è€Œç„¶çš„æƒ³åˆ°ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š</p>\n<ul>\n<li>è§£å†³ç‹—é”™åˆ†ä¸ºçŒ«çš„é—®é¢˜</li>\n<li>è§£å†³çŒ«ç§‘åŠ¨ç‰©è¢«é”™åˆ†æˆçŒ«çš„é—®é¢˜</li>\n<li>æå‡æ¨¡ç³Šå›¾åƒè¢«è¯¯åˆ†çš„é—®é¢˜</li>\n</ul>\n<p>å¯æ˜¯ç”±äºæˆ‘ä»¬ç²¾åŠ›å’Œæ—¶é—´éƒ½æœ‰é™ï¼Œéœ€è¦æ‰¾å‡ºè¯¯åˆ†æœ€ä¸»è¦çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯æŠŠæ‰€æœ‰é”™åˆ†çš„å›¾åƒç½—åˆ—å‡ºæ¥ï¼Œæˆ–è€…éšæœºæŠ½æ ·ä¸€å®šçš„å›¾åƒï¼Œåˆ†ææ¯ç§é”™è¯¯å®ƒæœ‰å¤šå°‘ï¼Œå é”™åˆ†å›¾åƒå¤šå°‘æ¯”ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹æˆªå›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-1.png\" alt=\"\"><br>æ¯ä¸€ä¸ªé”™åˆ†çš„å›¾åƒéƒ½ä¼šè¿›è¡Œæ ‡ç­¾åŒ–çš„ç»Ÿè®¡ï¼Œæœ€åé€šè¿‡ç»Ÿè®¡æ¯ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰¾å‡ºå½±å“é”™åˆ†æœ€ä¸¥é‡çš„å› ç´ ï¼Œä½œä¸ºæˆ‘ä»¬çš„æ”¹è¿›æ–¹å‘ã€‚</p>\n<h3 id=\"Clean-up-incorrectly-labeled-data\"><a href=\"#Clean-up-incorrectly-labeled-data\" class=\"headerlink\" title=\"Clean up incorrectly labeled data\"></a>Clean up incorrectly labeled data</h3><p>åœ¨å¸¸è§çš„é”™è¯¯ä¸­ï¼Œé”™è¯¯çš„labelæ˜¯ä¸€ç§å¾ˆå¸¸è§çš„é—®é¢˜ï¼Œè¿™ç§é—®é¢˜å¾€å¾€æ¥è‡ªäºæ ‡æ³¨æ—¶å€™ï¼Œé”™è¯¯çš„labelä¼šå¯¹trainingé€ æˆè¯¯å¯¼ã€‚</p>\n<p>é¦–å…ˆï¼Œå¯¹äºtraining setï¼Œæ¥è¯´ï¼Œincorrectly labeled dataåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿé¦–å…ˆï¼ŒNgå‘Šè¯‰äº†æˆ‘ä»¬ä¸€ä¸ªæ€§è´¨ï¼š</p>\n<blockquote>\n<p>DL algorithms are quite robust to random errors in the training set</p>\n</blockquote>\n<p>DLå› ä¸ºå…¶è‡ªèº«çš„robustæ€§è´¨ï¼Œå½“training setä¸­æœ‰å°‘è®¸çš„ï¼Œéšæœºäº§ç”Ÿçš„incorrectly labeled dataæ—¶ï¼Œæ•ˆæœå¹¶ä¸ä¼šæœ‰å¤šå·®ï¼Œæˆ‘ä»¬å®Œå…¨ä¸éœ€è¦å»ç®¡ä»–ã€‚ä½†æ˜¯ï¼Œå½“è¿™incorrectly labeled dataå¾ˆå¤šæ—¶å°±ä¸è¡Œäº†ï¼Œå› ä¸ºå®ƒä»¬å¸¦æ¥çš„æ˜¯systematic errorsï¼Œæç«¯çš„æƒ³ï¼Œå¦‚æœæŠŠæ‰€æœ‰çš„ç™½ç‹—éƒ½é”™è¯¯çš„æ ‡æ³¨æˆäº†çŒ«ï¼Œé‚£ä¹ˆè¿™ä¸ªcat recognitionç³»ç»Ÿä¸€å®šä¸ä¼šå¥½ï¼Œå› ä¸ºå®ƒä¸€å®šä¼šæŠŠç™½è‰²çš„ç‹—åˆ¤æ–­æˆä¸ºçŒ«ã€‚</p>\n<p>å†æ¥çœ‹çœ‹dev/test setä¸­çš„incorrectly labeled dataï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼Œè¯„ä¼°incorrectly labeled dataå¯¹dev errorå¸¦æ¥äº†å¤šå°‘è´¡çŒ®ï¼Œè§£å†³çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¥çœ‹æˆªå›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-2.png\" alt=\"\"><br>æˆ‘ä»¬æŠŠincorrectly labeledä¹Ÿä½œä¸ºä¸€ä¸ªè¦ç´ æˆ–æ ‡ç­¾ï¼Œæ”¾åœ¨é”™åˆ†å›¾åƒåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œ<br>çœ‹çœ‹æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœï¼Œå†å†³å®šincorrectly labeled dataæ˜¯ä¸æ˜¯å½±å“dev errorçš„ä¸»è¦åŸå› ï¼Œæ˜¯å¦å€¼å¾—æˆ‘ä»¬å»fix it up.</p>\n<p>æœ€åï¼Œå…³äºcorrecting incorrect dev/test set exampleï¼ŒNgç»™å‡ºäº†ä¸€äº›å»ºè®®ï¼š</p>\n<blockquote>\n<p>Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution.</p>\n</blockquote>\n<p>åœ¨ä¿®æ­£çš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦ä¿è¯devå’Œtest setåŒæ—¶è¢«ä¿®æ­£ï¼Œå¦‚æœä»–ä»¬ä¸å†ç¬¦åˆåŒä¸€distributionï¼Œé‚£ä¹ˆä¼šå¯¹äºåç»­çš„è¯„ä»·å¸¦æ¥ä¸€äº›é—®é¢˜ã€‚</p>\n<blockquote>\n<p>Consider examining examples your algorithm got right as well as ones it got wrong.</p>\n</blockquote>\n<p>æˆ‘ä»¬åœ¨æ›´æ­£çš„æ—¶å€™ï¼Œä¸èƒ½åªæ˜¯çœ‹è¢«é”™åˆ†çš„å›¾åƒï¼Œå¯¹äºè¢«æ­£ç¡®åˆ†ç±»çš„ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨incorrect labeled çš„æƒ…å†µã€‚</p>\n<blockquote>\n<p>Tran and dev/test data may now come from slightly different distribution</p>\n</blockquote>\n<p>æ­£å¦‚åˆšæ‰è®²çš„ï¼ŒDLå¯¹äºtrainingæœ‰ä¸€å®šç¨‹åº¦çš„robustæ€§ï¼Œincorrect labeled dataå¯èƒ½ä¸ä¼šå¯¹training setå¸¦æ¥è¿™äº›é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç”¨å»æ›´æ­£training setï¼Œè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥æ¥å—çš„ã€‚</p>\n<h3 id=\"Build-up-quickly-and-iterate\"><a href=\"#Build-up-quickly-and-iterate\" class=\"headerlink\" title=\"Build up quickly and iterate\"></a>Build up quickly and iterate</h3><p>æœ€åNgç”¨ä¸€ä¸ªspeech recognitionä½œä¸ºä¾‹å­ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åˆ†æå‡ºå¯èƒ½å½±å“æ•ˆæœçš„ä¸€äº›å› ç´ ï¼š</p>\n<ul>\n<li>Noisy background</li>\n<li>Accented speech</li>\n<li>Far from microphone</li>\n<li>Young childrenâ€™s speech</li>\n</ul>\n<p>â€¦<br>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•æ„é€ æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼ŒNgç»™å‡ºäº†å»ºè®®</p>\n<ul>\n<li>Set up dev/test set and metric</li>\n<li>Build initial system quickly</li>\n<li>Use bias/variance analysis &amp; error analysis to prioritize next steps.</li>\n</ul>\n<p>æ€»è€Œè¨€ä¹‹ï¼Œguidelineæ˜¯</p>\n<blockquote>\n<p>Build your first system quickly, then iterate.</p>\n</blockquote>\n<h2 id=\"Mismatched-training-and-dev-test-data\"><a href=\"#Mismatched-training-and-dev-test-data\" class=\"headerlink\" title=\"Mismatched training and dev/test data\"></a>Mismatched training and dev/test data</h2><h3 id=\"Training-and-testing-on-different-distributions\"><a href=\"#Training-and-testing-on-different-distributions\" class=\"headerlink\" title=\"Training and testing on different distributions\"></a>Training and testing on different distributions</h3><p>ä¹‹å‰æˆ‘ä»¬å†ä¸‰å¼ºè°ƒè¿‡ä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯training/dev/test setä¸€å®šè¦åœ¨åŒä¸€ä¸ªdistributionä¸‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ„¿æœ›æ€»æ˜¯ç¾å¥½çš„è€Œç°å®å¾ˆæ®‹é…·ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šé¢å¯¹ä¸€äº›training and testing on different distributioné—®é¢˜ã€‚</p>\n<p>ä¾‹å¦‚åœ¨çŒ«è¯†åˆ«çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹éƒ¨ç½²åˆ°æ‰‹æœºappä¸Šï¼Œæˆ‘ä»¬æ‰‹ä¸Šçš„æ•°æ®åªæœ‰10kæ˜¯ä»æ‰‹æœºæ‹æ‘„è·å¾—çš„ï¼Œè€Œæœ‰200kçš„æ•°æ®æ˜¯ä»ç½‘ç»œä¸Šè·å¾—çš„ï¼Œè¿™ä¸¤ç§å›¾åƒæ˜¾ç„¶ä¸å±äºåŒä¸€distributionï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ</p>\n<p>é¦–å…ˆæ¥çœ‹option1ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„210kæ•°æ®å……åˆ†æ··åˆåœ¨ä¸€èµ·ï¼Œå…¶ä¸­205kä½œä¸ºtraining setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtest setã€‚è¿™æ ·çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼Œä½†æ˜¯ï¼Œç¡®å®å¾ˆä¸å¥½çš„ä¸€ä¸ªæ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ</p>\n<p>åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œdev/test setå…¶å®æ‰®æ¼”äº†ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬çš„targetï¼Œä¹Ÿå°±æ˜¯æ•´ä½“çš„ä¼˜åŒ–æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦ä¼˜åŒ–çš„æ–¹å‘æ˜¯appä¸Šçš„å›¾åƒï¼Œè€Œè¿™ç§data setåˆ†å‰²æ–¹æ³•å’Œæˆ‘ä»¬çš„task targetå¹¶ä¸ç¬¦åˆï¼Œå› æ­¤å¹¶ä¸ä¼˜ç§€ã€‚</p>\n<p>æˆ‘ä»¬å†æ¥çœ‹option2ï¼Œæˆ‘ä»¬å°†200kçš„æ¥è‡ªç½‘ç»œçš„å›¾ç‰‡å…¨éƒ¨æ”¾å…¥training setï¼Œç„¶åå°†10kçš„appæ•°æ®ï¼Œ5kæ”¾å…¥training setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtestï¼Œè¿™æ ·åšçš„è¯ï¼Œdev/testå†³å®šçš„target å’Œæˆ‘ä»¬çš„task targetæ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥é•¿è¿œæ¥çœ‹ï¼Œè™½ç„¶option2çš„training/dev setå¹¶ä¸æ˜¯åŒä¸€distributionï¼Œä½†æ˜¯ä»é•¿è¿œçœ‹å®ƒçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚</p>\n<h3 id=\"Bias-amp-variance-with-mismatched-data-distribution\"><a href=\"#Bias-amp-variance-with-mismatched-data-distribution\" class=\"headerlink\" title=\"Bias &amp; variance with mismatched data distribution\"></a>Bias &amp; variance with mismatched data distribution</h3><p>åœ¨training/dev/test setç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒtraining errorå’Œdev errorå°±å¯ä»¥å®šæ€§æ˜¯å¦å­˜åœ¨high varianceçš„é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“training setå’Œdev setä¸ç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ¤æ–­å°±æ˜¾å¾—æœ‰äº›å›°éš¾äº†ã€‚æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ</p>\n<p>è¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä»training setä¸­å–å‡ºä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œå‘½åä¸ºtraining-dev setï¼Œè¿™éƒ¨åˆ†æ•°æ®å°†ä¸å†è¿›è¡Œtrainingï¼Œè€Œæ˜¯ä½œä¸ºè¯„åˆ¤trainingæ•ˆæœçš„ä¸€ä¸ªsetï¼Œæ­¤æ—¶æˆ‘ä»¬å°±æœ‰äº†training errorï¼Œtraining-dev errorå’Œdev errorä¸‰ä¸ªerrorï¼Œå†ç»“åˆhuman errorï¼Œtraining errorå’Œtraining-dev errorä¹‹é—´çš„å·®å€¼å¯ä»¥åæ˜ å‡ºæ¨¡å‹æ˜¯å¦æœ‰high biasæˆ–è€…varianceï¼Œè¿™æ ·å¯ä»¥æ›´ç§‘å­¦çš„æ¥è¯„åˆ¤æ¨¡å‹æ•ˆæœã€‚ç›¸åº”çš„ï¼Œtraining-dev errorå’Œdev errorç›¸å·®è¶Šå¤šï¼Œdata mismatchçš„ç¨‹åº¦è¶Šå¤§ã€‚</p>\n<h3 id=\"Addressing-data-mismatch\"><a href=\"#Addressing-data-mismatch\" class=\"headerlink\" title=\"Addressing data mismatch\"></a>Addressing data mismatch</h3><p>æˆ‘ä»¬å¦‚ä½•addressing data mismatchå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹Ngçš„ä¸¤æ¡guidelineï¼š</p>\n<ul>\n<li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li>\n<li>Make training data more similar;  or collect more data similar to dev/test sets</li>\n</ul>\n<p>ç†è§£ä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦é€šè¿‡äººå·¥çš„analysiså»åˆ†æå‡ºé€ æˆtraining setå’Œdev setä¹‹é—´distributionä¸åŒçš„åŸå› ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ— æ±½è½¦å™ªå£°ç­‰ç­‰ï¼›ç„¶åæˆ‘ä»¬éœ€è¦æ ¹æ®è¿™äº›å·®åˆ«ï¼Œè®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œç”šè‡³ç›¸é€šã€‚</p>\n<p>ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè¦é¿å…å‡ºç°overfittingçš„æƒ…å†µå‡ºç°ï¼Œä¾‹å¦‚Ngä¸¾å‡ºçš„ä¾‹å­ï¼Œåœ¨è¯†åˆ«è½¦å†…çš„äººå£°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥çš„åˆæˆæ±½è½¦å£°éŸ³ä¸äººçš„å£°éŸ³è®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„åªç”¨ä¸€æ®µæ±½è½¦å™ªéŸ³å¾ªç¯å¾€å¤çš„å»åšåˆæˆï¼Œä¾‹å¦‚å§1minçš„æ±½è½¦å™ªå£°å¾ªç¯çš„åˆæˆåˆ°1hçš„äººå£°ä¸­ï¼Œé‚£ç»“æœä¸€å®šæ˜¯ä¸å°½å¦‚äººæ„çš„ï¼Œå› ä¸ºå‡ºç°äº†overfitting.</p>\n<h2 id=\"Transfer-learning\"><a href=\"#Transfer-learning\" class=\"headerlink\" title=\"Transfer learning\"></a>Transfer learning</h2><p>ä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¤§åé¼é¼çš„transfer learningï¼Œæ‰€è°“transferï¼Œå°±æ˜¯å­˜åœ¨ä¸€ç§ä»Aåˆ°Bçš„è½¬æ¢ï¼Œè€Œä¸”è¿™ç§æƒ…å†µå¾€å¾€æ˜¯Bçš„æ•°æ®é‡å¾ˆå°‘ï¼Œéœ€è¦é€šè¿‡Aæ¥åšä¸€ä¸ªpre-trainingè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„neural networks<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-3.png\" alt=\"\"><br>å‡è®¾è¿™ä¸ªæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªneural networksè®­ç»ƒäº†ä¸€ä¸ªimage recognitionæ¨¡å‹ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†æœ€åçš„outputï¼Œä»¥åŠoutputå¯¹åº”çš„çš„\\(w\\)å’Œ\\(b\\)ä¹Ÿåˆ é™¤ï¼Œæ›´æ¢æˆä¾‹å¦‚æ”¾å°„æ•°æ®å†è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/10/10-4.png\" alt=\"\"><br>æˆ‘ä»¬ä¸ä»…ä»…å¯ä»¥æŠŠoutputå±‚æ›´æ¢æˆä¸€ä¸ªæ–°çš„outputå±‚ï¼Œè¿˜å¯ä»¥å°†outputå±‚æ›´æ¢æˆå‡ ä¸ªæ–°å±‚ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥å°†transferä¹‹å‰çš„è®­ç»ƒè®¤ä¸ºæ˜¯ä¸€ç§pre-trainingï¼Œä½†æ˜¯transfer trainingéœ€è¦æœ‰å‡ ä¸ªæ¡ä»¶ï¼š</p>\n<ul>\n<li>Task A and B have the same input x.</li>\n<li>You have a lot more data for Task A than Task B.</li>\n<li>Low level features from A could be helpful for learning B.</li>\n</ul>\n<h2 id=\"Multi-task-learning\"><a href=\"#Multi-task-learning\" class=\"headerlink\" title=\"Multi-task learning\"></a>Multi-task learning</h2><p>ç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶çš„åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è§†é¢‘ä¸­è¯†åˆ«è¡Œäººã€è½¦è¾†ã€åœè½¦æ ‡å¿—å’Œçº¢ç»¿ç¯ï¼ŒæŒ‰ç…§å¸¸ç†æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬çš„æ„å»º4ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™4ä¸ªæ¨¡å‹çš„ç‰¹å¾åœºæ™¯éƒ½æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ„å»º4ä¸ªæ¨¡å‹ç¨å¾®æœ‰ä¸€äº›æµªè´¹ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠè¿™å››ä¸ªä»»åŠ¡åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯Multi-task learning.</p>\n<p>åœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾\\(y\\)ï¼Œå°±ä¸å†æ˜¯ä¸€ä¸ªmÃ—1çš„çŸ©é˜µäº†ï¼Œè€Œæ˜¯ä¸€ä¸ªmÃ—4çš„çŸ©é˜µï¼Œå¯¹äºmulti-taskæ¥è¯´ï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ï¼š</p>\n<ul>\n<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>\n<li>Usually: Amount of data you have for eachtask is quite similar.</li>\n<li>Can train a big enough neural network to do well on all the tasks.</li>\n</ul>\n<h2 id=\"End-to-end-learning\"><a href=\"#End-to-end-learning\" class=\"headerlink\" title=\"End to end learning\"></a>End to end learning</h2><p>End to end learningæ˜¯éšç€DLå…´èµ·åè€Œäº§ç”Ÿçš„ä¸€ç§learningæ–¹å¼ï¼Œåœ¨end2endä¸­ï¼Œæˆ‘ä»¬ä¸å†å…³æ³¨ä¸€äº›ä¸­é—´çš„æ­¥éª¤ï¼Œä¾‹å¦‚feature selectionæˆ–è€…image processingï¼Œæˆ‘ä»¬åªæ˜¯æŠŠåŸå§‹çš„æ•°æ®å’Œæœ€åçš„ç»“æœå‘Šè¯‰DLï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚</p>\n<p>å½“ç„¶end2end ä¹Ÿæ˜¯æœ‰ä¸€äº›ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼š<br>Prosï¼š</p>\n<ul>\n<li>Let the data speak.</li>\n<li>Less hand-desgining of components needed.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>May need large amount of data.</li>\n<li>Excludes potentially userful hand-designed components.</li>\n</ul>\n<p>æ€»ä¹‹ï¼Œå¯¹äºend2endæ¥è¯´ï¼Œå¤§æ•°æ®é‡ï¼Œä¸€å®šæ˜¯æœ€é‡è¦çš„å› ç´ ï¼ŒåŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰å¯ä»¥æ‘†è„±ä¼ ç»Ÿçš„ä¸­é—´æ­¥éª¤ï¼Œå½»åº•å®ç°end to end learning.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/deeplearning_ai-2001281003#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week2","date":"2017-11-29T08:26:12.000Z","_content":"æˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹course4çš„week2ï¼ŒCNNçš„çŸ¥è¯†è¿˜æ˜¯è›®ä¸°å¯Œçš„ï¼Œæœ¬å‘¨ä¸»è¦è®²äº†ä¸€äº›ç»å…¸çš„CNNç»“æ„ä»¥åŠä¸€äº›computer visionçš„æŠ€å·§å’ŒçŸ¥è¯†ï¼Œä¸€èµ·recapä¸€ä¸‹ã€‚\n<!--more-->\n## Classic Networks\nNgä¸€å…±ç»™æˆ‘ä»¬å¸¦æ¥äº†3ä¸ªæœ€ä¸ºç»å…¸çš„CNNç½‘ç»œï¼Œè¿™é‡Œæˆ‘ä¼šç»™å‡ºç½‘ç»œçš„æˆªå›¾å’ŒpaperåŸæ–‡ï¼ŒæŠ½ç©ºæˆ‘ä¹Ÿä¼šçœ‹çœ‹åŸæ–‡ï¼Œå¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·æ¥çœ‹çœ‹ã€‚\n### LeNet-5\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png)\n[LÃ©cun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png)\n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png)\n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\nä»¥ä¸Šå¯ä»¥è¯´æ˜¯æœ€ä¸ºç»å…¸çš„ä¸‰ä¸ªcnnç½‘ç»œäº†ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡é˜…è¯»paperè·å¾—ä¸€äº›è¯¦ç»†çš„çŸ¥è¯†ï¼Œéƒ½æ˜¯ç»å…¸ä¹‹ä½œï¼Œæ¨èé˜…è¯»ã€‚\n\n## Residual Networks(ResNets)\nå¯¹äºresidual networksï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå…·ä½“çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„å…·ä½“åŸç†å¯ä»¥é€šè¿‡ä¸‹å›¾çš„residual blockæ¥çœ‹çœ‹ï¼š\nå…¶å®ï¼Œresidual blockæ˜¯æŠŠ\\\\(a^{[l]}\\\\)ç›´æ¥ä½œä¸º\\\\(a^{[l+2]}\\\\)è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\nå…¶ä¸­\\\\(g\\\\)æ˜¯activation functionï¼Œå¦‚ReLUç­‰ã€‚è¿™ç§æ€æƒ³ä¹Ÿè¢«ç§°ä¸ºshort circuitæˆ–è€…skip connectionã€‚æŠŠä¸Šé¢çš„residual blockä¸²è”èµ·æ¥ï¼Œå°±å˜æˆäº†æˆ‘ä»¬çš„residual networksï¼Œå¦‚ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png)\nResidual networksæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯ï¼Œæ™®é€šnetworkséšç€layerå¢å¤§ï¼Œtraining errorç†è®ºä¸Šæ˜¯ä¼šå˜å°ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šåœ¨æŸä¸ªæœ€å°ç‚¹åå¢å¤§ï¼Œä½†æ˜¯residual networksåˆ™ä¼šä¸¥æ ¼çš„éšç€layerå¢å¤šè€Œå‡å°training errorï¼Œä¸‹é¢æ˜¯åŸæ–‡ï¼š\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 1Ã—1 Convolutions\né€šå¸¸æˆ‘ä»¬ä½¿ç”¨çš„filterï¼Œéƒ½æ˜¯å¥‡æ•°çš„kernel matrixï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ1Ã—1çš„filterä¹Ÿä¼šè¢«æˆ‘ä»¬ä½¿ç”¨ï¼Œå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png)\nä»è¿™å¼ å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œ1Ã—1çš„filterå¯ä»¥å‹ç¼©inputçš„channel(depth)ï¼Œå› æ­¤1Ã—1filterè¿˜æ˜¯æœ‰ä¸€äº›æ„æ€çš„ã€‚ä¸‹é¢æ˜¯åŸæ–‡ï¼š\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\nå…³äºinception networkï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png)\nå¯¹äºåŒä¸€ä¸ªinputï¼Œæˆ‘ä»¬åˆ†åˆ«é‡‡ç”¨ä¸åŒçš„filterï¼Œç”šè‡³max poolingï¼Œåœ¨ä¿è¯è¾“å‡ºçš„hightå’Œwidthä¸€æ ·çš„å‰æä¸‹ï¼Œå°†ç»“æœå †å èµ·æ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æŒ‘é€‰filterï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„å¯èƒ½éƒ½äº¤ç»™networkï¼Œè®©å®ƒæ¥å†³å®šå»é€‰æ‹©ä»€ä¹ˆæ ·å­çš„ç»“æ„ã€‚åŸæ–‡æ˜¯ï¼š\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\nåŒæ—¶ï¼ŒNgåœ¨è¯¾ç¨‹ä¸Šè¯´æ˜ï¼Œinception network ä¸­å¤§é‡ä½¿ç”¨äº†1Ã—1filteræ¥é™ä½è®¡ç®—é‡ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚\næˆ‘ä»¬æ¥çœ‹çœ‹Inception å•å…ƒçš„å›¾è§£ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png)\nè¿™ä¸€å‘¨çš„è¯¾ç¨‹æ„Ÿè§‰é‡å¾ˆå¤§ï¼Œä»‹ç»äº†å¾ˆå¤šçš„ç½‘ç»œï¼Œæˆ‘å‡†å¤‡ä¸‹é¢æ…¢æ…¢çš„çœ‹çœ‹è¿™äº›paperï¼Œç«™åœ¨å·¨äººçš„è‚©ä¸Šå»çœ‹ä¸–ç•Œï¼Œä¸€å®šä¼šæœ‰åˆ«æ ·çš„é£æ™¯ï¼\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","source":"_posts/course-deep-learning-course4-week2.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week2\ndate: 2017-11-29 16:26:12\ntags: CNN\ncategories: learning notes\n---\næˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹course4çš„week2ï¼ŒCNNçš„çŸ¥è¯†è¿˜æ˜¯è›®ä¸°å¯Œçš„ï¼Œæœ¬å‘¨ä¸»è¦è®²äº†ä¸€äº›ç»å…¸çš„CNNç»“æ„ä»¥åŠä¸€äº›computer visionçš„æŠ€å·§å’ŒçŸ¥è¯†ï¼Œä¸€èµ·recapä¸€ä¸‹ã€‚\n<!--more-->\n## Classic Networks\nNgä¸€å…±ç»™æˆ‘ä»¬å¸¦æ¥äº†3ä¸ªæœ€ä¸ºç»å…¸çš„CNNç½‘ç»œï¼Œè¿™é‡Œæˆ‘ä¼šç»™å‡ºç½‘ç»œçš„æˆªå›¾å’ŒpaperåŸæ–‡ï¼ŒæŠ½ç©ºæˆ‘ä¹Ÿä¼šçœ‹çœ‹åŸæ–‡ï¼Œå¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·æ¥çœ‹çœ‹ã€‚\n### LeNet-5\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png)\n[LÃ©cun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n### AlexNet\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png)\n[Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n### VGG-16\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png)\n[Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.](https://arxiv.org/pdf/1409.1556.pdf)\n\nä»¥ä¸Šå¯ä»¥è¯´æ˜¯æœ€ä¸ºç»å…¸çš„ä¸‰ä¸ªcnnç½‘ç»œäº†ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡é˜…è¯»paperè·å¾—ä¸€äº›è¯¦ç»†çš„çŸ¥è¯†ï¼Œéƒ½æ˜¯ç»å…¸ä¹‹ä½œï¼Œæ¨èé˜…è¯»ã€‚\n\n## Residual Networks(ResNets)\nå¯¹äºresidual networksï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå…·ä½“çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„å…·ä½“åŸç†å¯ä»¥é€šè¿‡ä¸‹å›¾çš„residual blockæ¥çœ‹çœ‹ï¼š\nå…¶å®ï¼Œresidual blockæ˜¯æŠŠ\\\\(a^{[l]}\\\\)ç›´æ¥ä½œä¸º\\\\(a^{[l+2]}\\\\)è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png)\n$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\nå…¶ä¸­\\\\(g\\\\)æ˜¯activation functionï¼Œå¦‚ReLUç­‰ã€‚è¿™ç§æ€æƒ³ä¹Ÿè¢«ç§°ä¸ºshort circuitæˆ–è€…skip connectionã€‚æŠŠä¸Šé¢çš„residual blockä¸²è”èµ·æ¥ï¼Œå°±å˜æˆäº†æˆ‘ä»¬çš„residual networksï¼Œå¦‚ä¸‹å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png)\nResidual networksæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯ï¼Œæ™®é€šnetworkséšç€layerå¢å¤§ï¼Œtraining errorç†è®ºä¸Šæ˜¯ä¼šå˜å°ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šåœ¨æŸä¸ªæœ€å°ç‚¹åå¢å¤§ï¼Œä½†æ˜¯residual networksåˆ™ä¼šä¸¥æ ¼çš„éšç€layerå¢å¤šè€Œå‡å°training errorï¼Œä¸‹é¢æ˜¯åŸæ–‡ï¼š\n[He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Network in Network and 1Ã—1 Convolutions\né€šå¸¸æˆ‘ä»¬ä½¿ç”¨çš„filterï¼Œéƒ½æ˜¯å¥‡æ•°çš„kernel matrixï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ1Ã—1çš„filterä¹Ÿä¼šè¢«æˆ‘ä»¬ä½¿ç”¨ï¼Œå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png)\nä»è¿™å¼ å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œ1Ã—1çš„filterå¯ä»¥å‹ç¼©inputçš„channel(depth)ï¼Œå› æ­¤1Ã—1filterè¿˜æ˜¯æœ‰ä¸€äº›æ„æ€çš„ã€‚ä¸‹é¢æ˜¯åŸæ–‡ï¼š\n[Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.](https://arxiv.org/pdf/1312.4400.pdf)\n\n## Inception Network\nå…³äºinception networkï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png)\nå¯¹äºåŒä¸€ä¸ªinputï¼Œæˆ‘ä»¬åˆ†åˆ«é‡‡ç”¨ä¸åŒçš„filterï¼Œç”šè‡³max poolingï¼Œåœ¨ä¿è¯è¾“å‡ºçš„hightå’Œwidthä¸€æ ·çš„å‰æä¸‹ï¼Œå°†ç»“æœå †å èµ·æ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æŒ‘é€‰filterï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„å¯èƒ½éƒ½äº¤ç»™networkï¼Œè®©å®ƒæ¥å†³å®šå»é€‰æ‹©ä»€ä¹ˆæ ·å­çš„ç»“æ„ã€‚åŸæ–‡æ˜¯ï¼š\n[Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.](https://arxiv.org/pdf/1409.4842.pdf)\nåŒæ—¶ï¼ŒNgåœ¨è¯¾ç¨‹ä¸Šè¯´æ˜ï¼Œinception network ä¸­å¤§é‡ä½¿ç”¨äº†1Ã—1filteræ¥é™ä½è®¡ç®—é‡ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚\næˆ‘ä»¬æ¥çœ‹çœ‹Inception å•å…ƒçš„å›¾è§£ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png)\nè¿™ä¸€å‘¨çš„è¯¾ç¨‹æ„Ÿè§‰é‡å¾ˆå¤§ï¼Œä»‹ç»äº†å¾ˆå¤šçš„ç½‘ç»œï¼Œæˆ‘å‡†å¤‡ä¸‹é¢æ…¢æ…¢çš„çœ‹çœ‹è¿™äº›paperï¼Œç«™åœ¨å·¨äººçš„è‚©ä¸Šå»çœ‹ä¸–ç•Œï¼Œä¸€å®šä¼šæœ‰åˆ«æ ·çš„é£æ™¯ï¼\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","slug":"course-deep-learning-course4-week2","published":1,"updated":"2018-11-19T06:47:23.155Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p3000ctr8lkate1mlw","content":"<p>æˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹course4çš„week2ï¼ŒCNNçš„çŸ¥è¯†è¿˜æ˜¯è›®ä¸°å¯Œçš„ï¼Œæœ¬å‘¨ä¸»è¦è®²äº†ä¸€äº›ç»å…¸çš„CNNç»“æ„ä»¥åŠä¸€äº›computer visionçš„æŠ€å·§å’ŒçŸ¥è¯†ï¼Œä¸€èµ·recapä¸€ä¸‹ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ngä¸€å…±ç»™æˆ‘ä»¬å¸¦æ¥äº†3ä¸ªæœ€ä¸ºç»å…¸çš„CNNç½‘ç»œï¼Œè¿™é‡Œæˆ‘ä¼šç»™å‡ºç½‘ç»œçš„æˆªå›¾å’ŒpaperåŸæ–‡ï¼ŒæŠ½ç©ºæˆ‘ä¹Ÿä¼šçœ‹çœ‹åŸæ–‡ï¼Œå¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·æ¥çœ‹çœ‹ã€‚</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"noopener\">LÃ©cun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>ä»¥ä¸Šå¯ä»¥è¯´æ˜¯æœ€ä¸ºç»å…¸çš„ä¸‰ä¸ªcnnç½‘ç»œäº†ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡é˜…è¯»paperè·å¾—ä¸€äº›è¯¦ç»†çš„çŸ¥è¯†ï¼Œéƒ½æ˜¯ç»å…¸ä¹‹ä½œï¼Œæ¨èé˜…è¯»ã€‚</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>å¯¹äºresidual networksï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå…·ä½“çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„å…·ä½“åŸç†å¯ä»¥é€šè¿‡ä¸‹å›¾çš„residual blockæ¥çœ‹çœ‹ï¼š<br>å…¶å®ï¼Œresidual blockæ˜¯æŠŠ\\(a^{[l]}\\)ç›´æ¥ä½œä¸º\\(a^{[l+2]}\\)è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png\" alt=\"\"><br>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$<br>å…¶ä¸­\\(g\\)æ˜¯activation functionï¼Œå¦‚ReLUç­‰ã€‚è¿™ç§æ€æƒ³ä¹Ÿè¢«ç§°ä¸ºshort circuitæˆ–è€…skip connectionã€‚æŠŠä¸Šé¢çš„residual blockä¸²è”èµ·æ¥ï¼Œå°±å˜æˆäº†æˆ‘ä»¬çš„residual networksï¼Œå¦‚ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png\" alt=\"\"><br>Residual networksæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯ï¼Œæ™®é€šnetworkséšç€layerå¢å¤§ï¼Œtraining errorç†è®ºä¸Šæ˜¯ä¼šå˜å°ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šåœ¨æŸä¸ªæœ€å°ç‚¹åå¢å¤§ï¼Œä½†æ˜¯residual networksåˆ™ä¼šä¸¥æ ¼çš„éšç€layerå¢å¤šè€Œå‡å°training errorï¼Œä¸‹é¢æ˜¯åŸæ–‡ï¼š<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-1Ã—1-Convolutions\"><a href=\"#Network-in-Network-and-1Ã—1-Convolutions\" class=\"headerlink\" title=\"Network in Network and 1Ã—1 Convolutions\"></a>Network in Network and 1Ã—1 Convolutions</h2><p>é€šå¸¸æˆ‘ä»¬ä½¿ç”¨çš„filterï¼Œéƒ½æ˜¯å¥‡æ•°çš„kernel matrixï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ1Ã—1çš„filterä¹Ÿä¼šè¢«æˆ‘ä»¬ä½¿ç”¨ï¼Œå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png\" alt=\"\"><br>ä»è¿™å¼ å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œ1Ã—1çš„filterå¯ä»¥å‹ç¼©inputçš„channel(depth)ï¼Œå› æ­¤1Ã—1filterè¿˜æ˜¯æœ‰ä¸€äº›æ„æ€çš„ã€‚ä¸‹é¢æ˜¯åŸæ–‡ï¼š<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>å…³äºinception networkï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png\" alt=\"\"><br>å¯¹äºåŒä¸€ä¸ªinputï¼Œæˆ‘ä»¬åˆ†åˆ«é‡‡ç”¨ä¸åŒçš„filterï¼Œç”šè‡³max poolingï¼Œåœ¨ä¿è¯è¾“å‡ºçš„hightå’Œwidthä¸€æ ·çš„å‰æä¸‹ï¼Œå°†ç»“æœå †å èµ·æ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æŒ‘é€‰filterï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„å¯èƒ½éƒ½äº¤ç»™networkï¼Œè®©å®ƒæ¥å†³å®šå»é€‰æ‹©ä»€ä¹ˆæ ·å­çš„ç»“æ„ã€‚åŸæ–‡æ˜¯ï¼š<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>åŒæ—¶ï¼ŒNgåœ¨è¯¾ç¨‹ä¸Šè¯´æ˜ï¼Œinception network ä¸­å¤§é‡ä½¿ç”¨äº†1Ã—1filteræ¥é™ä½è®¡ç®—é‡ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚<br>æˆ‘ä»¬æ¥çœ‹çœ‹Inception å•å…ƒçš„å›¾è§£ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png\" alt=\"\"><br>è¿™ä¸€å‘¨çš„è¯¾ç¨‹æ„Ÿè§‰é‡å¾ˆå¤§ï¼Œä»‹ç»äº†å¾ˆå¤šçš„ç½‘ç»œï¼Œæˆ‘å‡†å¤‡ä¸‹é¢æ…¢æ…¢çš„çœ‹çœ‹è¿™äº›paperï¼Œç«™åœ¨å·¨äººçš„è‚©ä¸Šå»çœ‹ä¸–ç•Œï¼Œä¸€å®šä¼šæœ‰åˆ«æ ·çš„é£æ™¯ï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"CNN","path":"tags/CNN/"}],"excerpt":"<p>æˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹course4çš„week2ï¼ŒCNNçš„çŸ¥è¯†è¿˜æ˜¯è›®ä¸°å¯Œçš„ï¼Œæœ¬å‘¨ä¸»è¦è®²äº†ä¸€äº›ç»å…¸çš„CNNç»“æ„ä»¥åŠä¸€äº›computer visionçš„æŠ€å·§å’ŒçŸ¥è¯†ï¼Œä¸€èµ·recapä¸€ä¸‹ã€‚<br></p>","more":"</p>\n<h2 id=\"Classic-Networks\"><a href=\"#Classic-Networks\" class=\"headerlink\" title=\"Classic Networks\"></a>Classic Networks</h2><p>Ngä¸€å…±ç»™æˆ‘ä»¬å¸¦æ¥äº†3ä¸ªæœ€ä¸ºç»å…¸çš„CNNç½‘ç»œï¼Œè¿™é‡Œæˆ‘ä¼šç»™å‡ºç½‘ç»œçš„æˆªå›¾å’ŒpaperåŸæ–‡ï¼ŒæŠ½ç©ºæˆ‘ä¹Ÿä¼šçœ‹çœ‹åŸæ–‡ï¼Œå¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·æ¥çœ‹çœ‹ã€‚</p>\n<h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-1.png\" alt=\"\"><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target=\"_blank\" rel=\"noopener\">LÃ©cun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</a></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-2.png\" alt=\"\"><br><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012.</a></p>\n<h3 id=\"VGG-16\"><a href=\"#VGG-16\" class=\"headerlink\" title=\"VGG-16\"></a>VGG-16</h3><p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-3.png\" alt=\"\"><br><a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.</a></p>\n<p>ä»¥ä¸Šå¯ä»¥è¯´æ˜¯æœ€ä¸ºç»å…¸çš„ä¸‰ä¸ªcnnç½‘ç»œäº†ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡é˜…è¯»paperè·å¾—ä¸€äº›è¯¦ç»†çš„çŸ¥è¯†ï¼Œéƒ½æ˜¯ç»å…¸ä¹‹ä½œï¼Œæ¨èé˜…è¯»ã€‚</p>\n<h2 id=\"Residual-Networks-ResNets\"><a href=\"#Residual-Networks-ResNets\" class=\"headerlink\" title=\"Residual Networks(ResNets)\"></a>Residual Networks(ResNets)</h2><p>å¯¹äºresidual networksï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå…·ä½“çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„å…·ä½“åŸç†å¯ä»¥é€šè¿‡ä¸‹å›¾çš„residual blockæ¥çœ‹çœ‹ï¼š<br>å…¶å®ï¼Œresidual blockæ˜¯æŠŠ\\(a^{[l]}\\)ç›´æ¥ä½œä¸º\\(a^{[l+2]}\\)è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-4.png\" alt=\"\"><br>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$<br>å…¶ä¸­\\(g\\)æ˜¯activation functionï¼Œå¦‚ReLUç­‰ã€‚è¿™ç§æ€æƒ³ä¹Ÿè¢«ç§°ä¸ºshort circuitæˆ–è€…skip connectionã€‚æŠŠä¸Šé¢çš„residual blockä¸²è”èµ·æ¥ï¼Œå°±å˜æˆäº†æˆ‘ä»¬çš„residual networksï¼Œå¦‚ä¸‹å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-5.png\" alt=\"\"><br>Residual networksæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯ï¼Œæ™®é€šnetworkséšç€layerå¢å¤§ï¼Œtraining errorç†è®ºä¸Šæ˜¯ä¼šå˜å°ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šåœ¨æŸä¸ªæœ€å°ç‚¹åå¢å¤§ï¼Œä½†æ˜¯residual networksåˆ™ä¼šä¸¥æ ¼çš„éšç€layerå¢å¤šè€Œå‡å°training errorï¼Œä¸‹é¢æ˜¯åŸæ–‡ï¼š<br><a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</a></p>\n<h2 id=\"Network-in-Network-and-1Ã—1-Convolutions\"><a href=\"#Network-in-Network-and-1Ã—1-Convolutions\" class=\"headerlink\" title=\"Network in Network and 1Ã—1 Convolutions\"></a>Network in Network and 1Ã—1 Convolutions</h2><p>é€šå¸¸æˆ‘ä»¬ä½¿ç”¨çš„filterï¼Œéƒ½æ˜¯å¥‡æ•°çš„kernel matrixï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ1Ã—1çš„filterä¹Ÿä¼šè¢«æˆ‘ä»¬ä½¿ç”¨ï¼Œå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-6.png\" alt=\"\"><br>ä»è¿™å¼ å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œ1Ã—1çš„filterå¯ä»¥å‹ç¼©inputçš„channel(depth)ï¼Œå› æ­¤1Ã—1filterè¿˜æ˜¯æœ‰ä¸€äº›æ„æ€çš„ã€‚ä¸‹é¢æ˜¯åŸæ–‡ï¼š<br><a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013.</a></p>\n<h2 id=\"Inception-Network\"><a href=\"#Inception-Network\" class=\"headerlink\" title=\"Inception Network\"></a>Inception Network</h2><p>å…³äºinception networkï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-7.png\" alt=\"\"><br>å¯¹äºåŒä¸€ä¸ªinputï¼Œæˆ‘ä»¬åˆ†åˆ«é‡‡ç”¨ä¸åŒçš„filterï¼Œç”šè‡³max poolingï¼Œåœ¨ä¿è¯è¾“å‡ºçš„hightå’Œwidthä¸€æ ·çš„å‰æä¸‹ï¼Œå°†ç»“æœå †å èµ·æ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æŒ‘é€‰filterï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„å¯èƒ½éƒ½äº¤ç»™networkï¼Œè®©å®ƒæ¥å†³å®šå»é€‰æ‹©ä»€ä¹ˆæ ·å­çš„ç»“æ„ã€‚åŸæ–‡æ˜¯ï¼š<br><a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C].  Computer Vision and Pattern Recognition. IEEE, 2015:1-9.</a><br>åŒæ—¶ï¼ŒNgåœ¨è¯¾ç¨‹ä¸Šè¯´æ˜ï¼Œinception network ä¸­å¤§é‡ä½¿ç”¨äº†1Ã—1filteræ¥é™ä½è®¡ç®—é‡ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚<br>æˆ‘ä»¬æ¥çœ‹çœ‹Inception å•å…ƒçš„å›¾è§£ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/14/14-8.png\" alt=\"\"><br>è¿™ä¸€å‘¨çš„è¯¾ç¨‹æ„Ÿè§‰é‡å¾ˆå¤§ï¼Œä»‹ç»äº†å¾ˆå¤šçš„ç½‘ç»œï¼Œæˆ‘å‡†å¤‡ä¸‹é¢æ…¢æ…¢çš„çœ‹çœ‹è¿™äº›paperï¼Œç«™åœ¨å·¨äººçš„è‚©ä¸Šå»çœ‹ä¸–ç•Œï¼Œä¸€å®šä¼šæœ‰åˆ«æ ·çš„é£æ™¯ï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>"},{"title":"Learning Notes-Deep Learning, course4, week1","date":"2017-11-26T12:30:47.000Z","_content":"Hi, all. æœ€è¿‘å¼€å§‹ä¼‘å‡äº†ï¼Œå¯ä»¥æœ‰ç©ºç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä¸€æ–¹é¢è¡¥ä¸€è¡¥å‰é¢çš„ä½œä¸šï¼Œä¸€æ–¹é¢ç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ°äº†course4ï¼Œä¹Ÿå°±æ˜¯convolutional neural networks çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ï¼\n<!--more-->\n## Convolution\nåœ¨è¯¾ç¨‹ä¸­ï¼ŒNgä»edge detectionçš„è§’åº¦æ¥ç»™å¤§å®¶è®²äº†è®²convolutionï¼Œå› ä¸ºæœ¬äººæ˜¯image processingå‡ºèº«ï¼Œæ‰€ä»¥è®¤ä¸ºNgåœ¨è¿™é‡Œè®²çš„è¿˜æ˜¯å¾ˆæµ…æ˜¾æ˜“æ‡‚çš„ï¼Œæˆ‘å°±ä¸å†ä¸“é—¨çš„markdownã€‚ä¸»è¦æ¥çœ‹çœ‹convolutionä¸­çš„ä¸€äº›æŠ€å·§ã€‚\n### Padding\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œåœ¨æœ€çº¯ç²¹çš„convolutionä¸­ï¼Œæˆ‘ä»¬å‡è®¾åŸimageå°ºå¯¸æ˜¯\\\\(n \\*n\\\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\\\(f \\*f\\\\)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\\\( (n-f+1) \\*(n-f+1)\\\\)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»“æœçš„å°ºå¯¸å˜å°äº†ã€‚å¦‚æœæƒ³è®©è¾“å‡ºimageçš„å°ºå¯¸ä¸å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦ä½¿ç”¨å¤§åé¼é¼çš„paddingäº†ã€‚\n\nPaddingå…¶å®å°±æ˜¯è¡¨ç¤ºï¼Œåœ¨åŸå§‹imageä¸­ï¼Œå‘å¤–æ‰©å¤§å¤šå°‘å°ºå¯¸ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šä½¿ç”¨ç®€å•å¤åˆ¶ç›¸é‚»å…ƒç´ å€¼çš„æ–¹æ³•è¿›è¡Œæ‰©å……ã€‚å‡è®¾å¯¹äºä¸€ä¸ª\\\\(6 \\*6\\\\)çš„åŸå§‹imageï¼Œé‡‡ç”¨\\\\(3 \\*3\\\\)çš„filterï¼ŒåŠ ä¸Š\\\\\\(p=1\\\\)çš„paddingï¼Œé‚£ä¹ˆåŸå§‹å›¾åƒå°ºå¯¸å˜æˆäº†\\\\(8 \\* 8\\\\)ï¼Œç»“æœå˜æˆäº†\\\\(6 \\*6\\\\)ï¼ŒåŸå§‹imageå’Œç»“æœimageä¸€æ¨¡ä¸€æ ·ï¼äºæ˜¯åŠ å…¥äº†paddingçš„convolutionå…¬å¼å°±æˆäº†\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\nåœ¨è¿™é‡ŒNgå¼•å…¥äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œvalidå’Œsame convolutionsï¼Œæ‰€è°“valid convolutionï¼Œå°±æ˜¯æ²¡æœ‰padding çš„convolutionï¼›æ‰€è°“same convolutionï¼Œå°±æ˜¯è¾“å…¥è¾“å‡ºçš„å°ºå¯¸å®Œå…¨ä¸€æ ·ã€‚\n\n### Stride\nç»§paddingä¹‹åï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å‚æ•°ï¼Œå°±æ˜¯æ­¥é•¿strideï¼Œæ­¥é•¿strideå†³å®šäº†filteråšconvolutionæ—¶å€™çš„æ­¥é•¿ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆfilterå°±ä¼šæŒ¨ç€è®¡ç®—ï¼Œå¦‚æœstride=2ï¼Œé‚£ä¹ˆå°±ä¼šè·³è·ƒè¿™è¿›è¡Œè®¡ç®—ã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼Œå‡è®¾åŸimageå°ºå¯¸æ˜¯\\\\(n \\*n\\\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\\\(f \\*f\\\\)ï¼Œpaddingå€¼æ˜¯\\\\(p\\\\)ï¼Œstrideå€¼æ˜¯\\\\(s\\\\)é‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)ï¼Œå¦‚æœé™¤ä¸å°½çš„è¯ï¼Œæˆ‘ä»¬é€‰æ‹©å‘ä¸‹å–æ•´ï¼Œä¹Ÿå°±æ˜¯ä¸è¶³ä»¥åšconvolutionçš„åŒºåŸŸï¼Œæˆ‘ä»¬é€‰æ‹©æ”¾å¼ƒã€‚\n\n##Convolution over Volume\nå¯¹äºä¸€èˆ¬çš„å›¾åƒå¤„ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯RGBå›¾åƒï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªchannelï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œconvolutionåº”è¯¥å¦‚ä½•åšï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png)\nå‡è®¾æˆ‘ä»¬çš„å›¾åƒæ˜¯6Ã—6Ã—3ï¼Œä¹Ÿå°±æ˜¯hightÃ—widthÃ—channel(depth)ï¼Œå› æ­¤å¯¹åº”çš„filterä¹Ÿè¦æœ‰3çš„channel(depth)ï¼Œæœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª4Ã—4çš„ç»“æœã€‚\n\nå½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸æ­¢ä¸€ä¸ªfilterï¼Œå¦‚å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png)\næˆ‘ä»¬åŠ å…¥äº†ä¸¤ä¸ªä¸åŒçš„filterï¼Œä»–ä»¬çš„å¤§å°éƒ½æ˜¯3Ã—3Ã—3ï¼Œäºæ˜¯æœ€ç»ˆçš„ç»“æœå°±æ˜¯4Ã—4Ã—2ï¼Œè¯·æ³¨æ„ï¼šç»“æœçš„channelæ•°ç›®å–å†³äºfilterçš„ä¸ªæ•°ï¼Œè€Œå’Œè¾“å…¥çš„channelæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚\n\n## CNN\n### Convolution Layer\nä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹CNNç½‘ç»œä¸­çš„ä¸€ä¸ªlayerçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Œé¦–å…ˆæ¥çœ‹æˆªå›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png)\nè¿™å¼ å›¾ååˆ†å¤æ‚ï¼Œæˆ‘ä»¬ä¸€èµ·ä»”ç»†çœ‹çœ‹è¿™å¼ å›¾ï¼Œå¯¹äºä¸€ä¸ª6Ã—6Ã—3çš„RGBå›¾åƒï¼Œæˆ‘ä»¬ç”¨äº†ä¸¤ä¸ª3Ã—3Ã—3çš„filterï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾“å…¥imageçœ‹åš\\\\(x\\\\)ï¼Œä¹Ÿå°±æ˜¯\\\\(a ^{[0]}\\\\)ï¼Œfilterçœ‹åš\\\\(w ^{[1]}\\\\)ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯\\\\(w ^{[1]}a ^{[0]}\\\\)ï¼Œæˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªbiasé¡¹\\\\(b^{[1]}\\\\)ï¼Œé‚£ä¹ˆå°±è·å¾—äº†ä¸€ä¸ªliner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)ï¼Œæˆ‘ä»¬å†ä½¿ç”¨ä¸€ä¸ªnon-liner functionä¾‹å¦‚ReLUï¼Œå¦‚æ­¤è·å¾—ä¸€ä¸ª4Ã—4Ã—2çš„outputã€‚å¦‚æ­¤å°±æ˜¯CNNçš„ä¸€ä¸ªlayer.\n\nå¦‚æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCNNå’Œä¹‹å‰çš„DNNå®è´¨ä¸Šéƒ½å­˜åœ¨ä¸€ç§liner functionåˆ°non-liner functionçš„è½¬åŒ–ï¼Œé€šè¿‡non-liner functionå»classifyçº¿æ€§ä¸å¯åˆ†çš„dataï¼Œå¦å¤–ï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸€ä¸ªfilterå°±å¯ä»¥è·å¾—ä¸€ä¸ªä¸åŒçš„featureï¼Œå¤šä¸ªfilterå¯ä»¥è®©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å»classify data.\n\nå¦å¤–ï¼Œç›¸æ¯”è¾ƒäºfully connected çš„DNNï¼ŒCNNæ‰€éœ€è¦çš„parametersä¹Ÿå°‘äº†å¾ˆå¤šï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚\n\n### Pooling\nPoolingåŸç†è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png)\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹max poolingï¼Œå¦‚å›¾ï¼Œæˆ‘ä»¬å–filterå°ºå¯¸\\\\(f=2\\\\)ï¼Œstrideå¤§å°\\\\(s=2\\\\)ï¼Œå¯¹äºä¸€ä¸ªfilterä¸­çš„å…ƒç´ ï¼Œæˆ‘ä»¬å–maxä½œä¸ºè¾“å‡ºï¼›ç›¸å¯¹åº”çš„ï¼Œå¦‚æœæˆ‘ä»¬å–averageï¼Œé‚£ä¹ˆå°±æˆäº†average poolingï¼Œpoolingä¸­çš„hyperparameteråªæœ‰filterå°ºå¯¸\\\\(f\\\\)å’Œstrideå¤§å°\\\\(s\\\\)ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œpoolingè¿‡ç¨‹ä¸­ä¸å­˜åœ¨å­¦ä¹ è¿‡ç¨‹ï¼Œno parameters to learn!\n\n### Fully Connected layer\nFully connected layeråœ¨CNNå…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å°†inputå±•å¼€ï¼ŒæŒ‰ç…§DNNçš„æ–¹æ³•è¿›è¡Œfully connectedå°±å¯ä»¥äº†ã€‚\n\nä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰prameterå˜åŒ–çš„æ‰ç®—ä¸€å±‚ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸ºpoolingæ˜¯ä¸€ä¸ªlayerï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¸€ä¸ªæœ€ç®€å•çš„CNNåšä¾‹å­ï¼šCONV-POOL-CONV-POOL-FC-FC-Softmaxï¼Œæˆ‘ä»¬è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„5å±‚çš„CNNï¼Œåœ¨ä¸‹å‘¨çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»å…¸çš„CNNæ¡†æ¶ï¼Œè¿™é‡Œå°±ä¸å†å¤è¿°ã€‚\n\n## Why Convolutions\nå…³äºè¿™ä¸ªé—®é¢˜ï¼ŒNgç»™å‡ºäº†ä¸¤ä¸ªæ„è§ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","source":"_posts/course-deep-learning-course4-week1.md","raw":"---\ntitle: Learning Notes-Deep Learning, course4, week1\ndate: 2017-11-26 20:30:47\ntags: CNN\ncategories: learning notes\n---\nHi, all. æœ€è¿‘å¼€å§‹ä¼‘å‡äº†ï¼Œå¯ä»¥æœ‰ç©ºç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä¸€æ–¹é¢è¡¥ä¸€è¡¥å‰é¢çš„ä½œä¸šï¼Œä¸€æ–¹é¢ç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ°äº†course4ï¼Œä¹Ÿå°±æ˜¯convolutional neural networks çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ï¼\n<!--more-->\n## Convolution\nåœ¨è¯¾ç¨‹ä¸­ï¼ŒNgä»edge detectionçš„è§’åº¦æ¥ç»™å¤§å®¶è®²äº†è®²convolutionï¼Œå› ä¸ºæœ¬äººæ˜¯image processingå‡ºèº«ï¼Œæ‰€ä»¥è®¤ä¸ºNgåœ¨è¿™é‡Œè®²çš„è¿˜æ˜¯å¾ˆæµ…æ˜¾æ˜“æ‡‚çš„ï¼Œæˆ‘å°±ä¸å†ä¸“é—¨çš„markdownã€‚ä¸»è¦æ¥çœ‹çœ‹convolutionä¸­çš„ä¸€äº›æŠ€å·§ã€‚\n### Padding\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œåœ¨æœ€çº¯ç²¹çš„convolutionä¸­ï¼Œæˆ‘ä»¬å‡è®¾åŸimageå°ºå¯¸æ˜¯\\\\(n \\*n\\\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\\\(f \\*f\\\\)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\\\( (n-f+1) \\*(n-f+1)\\\\)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»“æœçš„å°ºå¯¸å˜å°äº†ã€‚å¦‚æœæƒ³è®©è¾“å‡ºimageçš„å°ºå¯¸ä¸å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦ä½¿ç”¨å¤§åé¼é¼çš„paddingäº†ã€‚\n\nPaddingå…¶å®å°±æ˜¯è¡¨ç¤ºï¼Œåœ¨åŸå§‹imageä¸­ï¼Œå‘å¤–æ‰©å¤§å¤šå°‘å°ºå¯¸ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šä½¿ç”¨ç®€å•å¤åˆ¶ç›¸é‚»å…ƒç´ å€¼çš„æ–¹æ³•è¿›è¡Œæ‰©å……ã€‚å‡è®¾å¯¹äºä¸€ä¸ª\\\\(6 \\*6\\\\)çš„åŸå§‹imageï¼Œé‡‡ç”¨\\\\(3 \\*3\\\\)çš„filterï¼ŒåŠ ä¸Š\\\\\\(p=1\\\\)çš„paddingï¼Œé‚£ä¹ˆåŸå§‹å›¾åƒå°ºå¯¸å˜æˆäº†\\\\(8 \\* 8\\\\)ï¼Œç»“æœå˜æˆäº†\\\\(6 \\*6\\\\)ï¼ŒåŸå§‹imageå’Œç»“æœimageä¸€æ¨¡ä¸€æ ·ï¼äºæ˜¯åŠ å…¥äº†paddingçš„convolutionå…¬å¼å°±æˆäº†\\\\( (n+2p-f+1) \\*(n+2p-f+1)\\\\).\n\nåœ¨è¿™é‡ŒNgå¼•å…¥äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œvalidå’Œsame convolutionsï¼Œæ‰€è°“valid convolutionï¼Œå°±æ˜¯æ²¡æœ‰padding çš„convolutionï¼›æ‰€è°“same convolutionï¼Œå°±æ˜¯è¾“å…¥è¾“å‡ºçš„å°ºå¯¸å®Œå…¨ä¸€æ ·ã€‚\n\n### Stride\nç»§paddingä¹‹åï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å‚æ•°ï¼Œå°±æ˜¯æ­¥é•¿strideï¼Œæ­¥é•¿strideå†³å®šäº†filteråšconvolutionæ—¶å€™çš„æ­¥é•¿ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆfilterå°±ä¼šæŒ¨ç€è®¡ç®—ï¼Œå¦‚æœstride=2ï¼Œé‚£ä¹ˆå°±ä¼šè·³è·ƒè¿™è¿›è¡Œè®¡ç®—ã€‚\n\næ€»ç»“ä¸€ä¸‹ï¼Œå‡è®¾åŸimageå°ºå¯¸æ˜¯\\\\(n \\*n\\\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\\\(f \\*f\\\\)ï¼Œpaddingå€¼æ˜¯\\\\(p\\\\)ï¼Œstrideå€¼æ˜¯\\\\(s\\\\)é‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\\\( ( \\frac {n+2p-f}{s}+1) \\*( \\frac {n+2p-f}{s}+1)\\\\)ï¼Œå¦‚æœé™¤ä¸å°½çš„è¯ï¼Œæˆ‘ä»¬é€‰æ‹©å‘ä¸‹å–æ•´ï¼Œä¹Ÿå°±æ˜¯ä¸è¶³ä»¥åšconvolutionçš„åŒºåŸŸï¼Œæˆ‘ä»¬é€‰æ‹©æ”¾å¼ƒã€‚\n\n##Convolution over Volume\nå¯¹äºä¸€èˆ¬çš„å›¾åƒå¤„ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯RGBå›¾åƒï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªchannelï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œconvolutionåº”è¯¥å¦‚ä½•åšï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png)\nå‡è®¾æˆ‘ä»¬çš„å›¾åƒæ˜¯6Ã—6Ã—3ï¼Œä¹Ÿå°±æ˜¯hightÃ—widthÃ—channel(depth)ï¼Œå› æ­¤å¯¹åº”çš„filterä¹Ÿè¦æœ‰3çš„channel(depth)ï¼Œæœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª4Ã—4çš„ç»“æœã€‚\n\nå½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸æ­¢ä¸€ä¸ªfilterï¼Œå¦‚å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png)\næˆ‘ä»¬åŠ å…¥äº†ä¸¤ä¸ªä¸åŒçš„filterï¼Œä»–ä»¬çš„å¤§å°éƒ½æ˜¯3Ã—3Ã—3ï¼Œäºæ˜¯æœ€ç»ˆçš„ç»“æœå°±æ˜¯4Ã—4Ã—2ï¼Œè¯·æ³¨æ„ï¼šç»“æœçš„channelæ•°ç›®å–å†³äºfilterçš„ä¸ªæ•°ï¼Œè€Œå’Œè¾“å…¥çš„channelæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚\n\n## CNN\n### Convolution Layer\nä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹CNNç½‘ç»œä¸­çš„ä¸€ä¸ªlayerçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Œé¦–å…ˆæ¥çœ‹æˆªå›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png)\nè¿™å¼ å›¾ååˆ†å¤æ‚ï¼Œæˆ‘ä»¬ä¸€èµ·ä»”ç»†çœ‹çœ‹è¿™å¼ å›¾ï¼Œå¯¹äºä¸€ä¸ª6Ã—6Ã—3çš„RGBå›¾åƒï¼Œæˆ‘ä»¬ç”¨äº†ä¸¤ä¸ª3Ã—3Ã—3çš„filterï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾“å…¥imageçœ‹åš\\\\(x\\\\)ï¼Œä¹Ÿå°±æ˜¯\\\\(a ^{[0]}\\\\)ï¼Œfilterçœ‹åš\\\\(w ^{[1]}\\\\)ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯\\\\(w ^{[1]}a ^{[0]}\\\\)ï¼Œæˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªbiasé¡¹\\\\(b^{[1]}\\\\)ï¼Œé‚£ä¹ˆå°±è·å¾—äº†ä¸€ä¸ªliner output\\\\(w ^{[1]}a ^{[0]}+b^{[1]}\\\\)ï¼Œæˆ‘ä»¬å†ä½¿ç”¨ä¸€ä¸ªnon-liner functionä¾‹å¦‚ReLUï¼Œå¦‚æ­¤è·å¾—ä¸€ä¸ª4Ã—4Ã—2çš„outputã€‚å¦‚æ­¤å°±æ˜¯CNNçš„ä¸€ä¸ªlayer.\n\nå¦‚æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCNNå’Œä¹‹å‰çš„DNNå®è´¨ä¸Šéƒ½å­˜åœ¨ä¸€ç§liner functionåˆ°non-liner functionçš„è½¬åŒ–ï¼Œé€šè¿‡non-liner functionå»classifyçº¿æ€§ä¸å¯åˆ†çš„dataï¼Œå¦å¤–ï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸€ä¸ªfilterå°±å¯ä»¥è·å¾—ä¸€ä¸ªä¸åŒçš„featureï¼Œå¤šä¸ªfilterå¯ä»¥è®©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å»classify data.\n\nå¦å¤–ï¼Œç›¸æ¯”è¾ƒäºfully connected çš„DNNï¼ŒCNNæ‰€éœ€è¦çš„parametersä¹Ÿå°‘äº†å¾ˆå¤šï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚\n\n### Pooling\nPoolingåŸç†è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png)\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹max poolingï¼Œå¦‚å›¾ï¼Œæˆ‘ä»¬å–filterå°ºå¯¸\\\\(f=2\\\\)ï¼Œstrideå¤§å°\\\\(s=2\\\\)ï¼Œå¯¹äºä¸€ä¸ªfilterä¸­çš„å…ƒç´ ï¼Œæˆ‘ä»¬å–maxä½œä¸ºè¾“å‡ºï¼›ç›¸å¯¹åº”çš„ï¼Œå¦‚æœæˆ‘ä»¬å–averageï¼Œé‚£ä¹ˆå°±æˆäº†average poolingï¼Œpoolingä¸­çš„hyperparameteråªæœ‰filterå°ºå¯¸\\\\(f\\\\)å’Œstrideå¤§å°\\\\(s\\\\)ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œpoolingè¿‡ç¨‹ä¸­ä¸å­˜åœ¨å­¦ä¹ è¿‡ç¨‹ï¼Œno parameters to learn!\n\n### Fully Connected layer\nFully connected layeråœ¨CNNå…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å°†inputå±•å¼€ï¼ŒæŒ‰ç…§DNNçš„æ–¹æ³•è¿›è¡Œfully connectedå°±å¯ä»¥äº†ã€‚\n\nä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰prameterå˜åŒ–çš„æ‰ç®—ä¸€å±‚ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸ºpoolingæ˜¯ä¸€ä¸ªlayerï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¸€ä¸ªæœ€ç®€å•çš„CNNåšä¾‹å­ï¼šCONV-POOL-CONV-POOL-FC-FC-Softmaxï¼Œæˆ‘ä»¬è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„5å±‚çš„CNNï¼Œåœ¨ä¸‹å‘¨çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»å…¸çš„CNNæ¡†æ¶ï¼Œè¿™é‡Œå°±ä¸å†å¤è¿°ã€‚\n\n## Why Convolutions\nå…³äºè¿™ä¸ªé—®é¢˜ï¼ŒNgç»™å‡ºäº†ä¸¤ä¸ªæ„è§ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š\n> **Parameter sharing**: A feature detector (such as a vertical edge detetor) that's useful in one part of image is probably useful in another part of the image.\n**Sparsity of connections**: In  each layer, each output value depends only on a small number of inputs.\n\n## Reference\n* [Deep learning-Coursera Andrew Ng](hhttps://www.coursera.org/learn/convolutional-neural-networks)\n* [Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng](https://mooc.study.163.com/course/2001281004#/info)\n","slug":"course-deep-learning-course4-week1","published":1,"updated":"2018-11-19T06:45:05.266Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p4000dtr8lxso8020o","content":"<p>Hi, all. æœ€è¿‘å¼€å§‹ä¼‘å‡äº†ï¼Œå¯ä»¥æœ‰ç©ºç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä¸€æ–¹é¢è¡¥ä¸€è¡¥å‰é¢çš„ä½œä¸šï¼Œä¸€æ–¹é¢ç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ°äº†course4ï¼Œä¹Ÿå°±æ˜¯convolutional neural networks çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ï¼<br><a id=\"more\"></a></p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>åœ¨è¯¾ç¨‹ä¸­ï¼ŒNgä»edge detectionçš„è§’åº¦æ¥ç»™å¤§å®¶è®²äº†è®²convolutionï¼Œå› ä¸ºæœ¬äººæ˜¯image processingå‡ºèº«ï¼Œæ‰€ä»¥è®¤ä¸ºNgåœ¨è¿™é‡Œè®²çš„è¿˜æ˜¯å¾ˆæµ…æ˜¾æ˜“æ‡‚çš„ï¼Œæˆ‘å°±ä¸å†ä¸“é—¨çš„markdownã€‚ä¸»è¦æ¥çœ‹çœ‹convolutionä¸­çš„ä¸€äº›æŠ€å·§ã€‚</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œåœ¨æœ€çº¯ç²¹çš„convolutionä¸­ï¼Œæˆ‘ä»¬å‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( (n-f+1) *(n-f+1)\\)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»“æœçš„å°ºå¯¸å˜å°äº†ã€‚å¦‚æœæƒ³è®©è¾“å‡ºimageçš„å°ºå¯¸ä¸å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦ä½¿ç”¨å¤§åé¼é¼çš„paddingäº†ã€‚</p>\n<p>Paddingå…¶å®å°±æ˜¯è¡¨ç¤ºï¼Œåœ¨åŸå§‹imageä¸­ï¼Œå‘å¤–æ‰©å¤§å¤šå°‘å°ºå¯¸ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šä½¿ç”¨ç®€å•å¤åˆ¶ç›¸é‚»å…ƒç´ å€¼çš„æ–¹æ³•è¿›è¡Œæ‰©å……ã€‚å‡è®¾å¯¹äºä¸€ä¸ª\\(6 *6\\)çš„åŸå§‹imageï¼Œé‡‡ç”¨\\(3 *3\\)çš„filterï¼ŒåŠ ä¸Š\\(p=1\\)çš„paddingï¼Œé‚£ä¹ˆåŸå§‹å›¾åƒå°ºå¯¸å˜æˆäº†\\(8 * 8\\)ï¼Œç»“æœå˜æˆäº†\\(6 *6\\)ï¼ŒåŸå§‹imageå’Œç»“æœimageä¸€æ¨¡ä¸€æ ·ï¼äºæ˜¯åŠ å…¥äº†paddingçš„convolutionå…¬å¼å°±æˆäº†\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>åœ¨è¿™é‡ŒNgå¼•å…¥äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œvalidå’Œsame convolutionsï¼Œæ‰€è°“valid convolutionï¼Œå°±æ˜¯æ²¡æœ‰padding çš„convolutionï¼›æ‰€è°“same convolutionï¼Œå°±æ˜¯è¾“å…¥è¾“å‡ºçš„å°ºå¯¸å®Œå…¨ä¸€æ ·ã€‚</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>ç»§paddingä¹‹åï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å‚æ•°ï¼Œå°±æ˜¯æ­¥é•¿strideï¼Œæ­¥é•¿strideå†³å®šäº†filteråšconvolutionæ—¶å€™çš„æ­¥é•¿ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆfilterå°±ä¼šæŒ¨ç€è®¡ç®—ï¼Œå¦‚æœstride=2ï¼Œé‚£ä¹ˆå°±ä¼šè·³è·ƒè¿™è¿›è¡Œè®¡ç®—ã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼Œå‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œpaddingå€¼æ˜¯\\(p\\)ï¼Œstrideå€¼æ˜¯\\(s\\)é‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)ï¼Œå¦‚æœé™¤ä¸å°½çš„è¯ï¼Œæˆ‘ä»¬é€‰æ‹©å‘ä¸‹å–æ•´ï¼Œä¹Ÿå°±æ˜¯ä¸è¶³ä»¥åšconvolutionçš„åŒºåŸŸï¼Œæˆ‘ä»¬é€‰æ‹©æ”¾å¼ƒã€‚</p>\n<p>##Convolution over Volume<br>å¯¹äºä¸€èˆ¬çš„å›¾åƒå¤„ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯RGBå›¾åƒï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªchannelï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œconvolutionåº”è¯¥å¦‚ä½•åšï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png\" alt=\"\"><br>å‡è®¾æˆ‘ä»¬çš„å›¾åƒæ˜¯6Ã—6Ã—3ï¼Œä¹Ÿå°±æ˜¯hightÃ—widthÃ—channel(depth)ï¼Œå› æ­¤å¯¹åº”çš„filterä¹Ÿè¦æœ‰3çš„channel(depth)ï¼Œæœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª4Ã—4çš„ç»“æœã€‚</p>\n<p>å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸æ­¢ä¸€ä¸ªfilterï¼Œå¦‚å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png\" alt=\"\"><br>æˆ‘ä»¬åŠ å…¥äº†ä¸¤ä¸ªä¸åŒçš„filterï¼Œä»–ä»¬çš„å¤§å°éƒ½æ˜¯3Ã—3Ã—3ï¼Œäºæ˜¯æœ€ç»ˆçš„ç»“æœå°±æ˜¯4Ã—4Ã—2ï¼Œè¯·æ³¨æ„ï¼šç»“æœçš„channelæ•°ç›®å–å†³äºfilterçš„ä¸ªæ•°ï¼Œè€Œå’Œè¾“å…¥çš„channelæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹CNNç½‘ç»œä¸­çš„ä¸€ä¸ªlayerçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Œé¦–å…ˆæ¥çœ‹æˆªå›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png\" alt=\"\"><br>è¿™å¼ å›¾ååˆ†å¤æ‚ï¼Œæˆ‘ä»¬ä¸€èµ·ä»”ç»†çœ‹çœ‹è¿™å¼ å›¾ï¼Œå¯¹äºä¸€ä¸ª6Ã—6Ã—3çš„RGBå›¾åƒï¼Œæˆ‘ä»¬ç”¨äº†ä¸¤ä¸ª3Ã—3Ã—3çš„filterï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾“å…¥imageçœ‹åš\\(x\\)ï¼Œä¹Ÿå°±æ˜¯\\(a ^{[0]}\\)ï¼Œfilterçœ‹åš\\(w ^{[1]}\\)ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯\\(w ^{[1]}a ^{[0]}\\)ï¼Œæˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªbiasé¡¹\\(b^{[1]}\\)ï¼Œé‚£ä¹ˆå°±è·å¾—äº†ä¸€ä¸ªliner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)ï¼Œæˆ‘ä»¬å†ä½¿ç”¨ä¸€ä¸ªnon-liner functionä¾‹å¦‚ReLUï¼Œå¦‚æ­¤è·å¾—ä¸€ä¸ª4Ã—4Ã—2çš„outputã€‚å¦‚æ­¤å°±æ˜¯CNNçš„ä¸€ä¸ªlayer.</p>\n<p>å¦‚æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCNNå’Œä¹‹å‰çš„DNNå®è´¨ä¸Šéƒ½å­˜åœ¨ä¸€ç§liner functionåˆ°non-liner functionçš„è½¬åŒ–ï¼Œé€šè¿‡non-liner functionå»classifyçº¿æ€§ä¸å¯åˆ†çš„dataï¼Œå¦å¤–ï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸€ä¸ªfilterå°±å¯ä»¥è·å¾—ä¸€ä¸ªä¸åŒçš„featureï¼Œå¤šä¸ªfilterå¯ä»¥è®©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å»classify data.</p>\n<p>å¦å¤–ï¼Œç›¸æ¯”è¾ƒäºfully connected çš„DNNï¼ŒCNNæ‰€éœ€è¦çš„parametersä¹Ÿå°‘äº†å¾ˆå¤šï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>PoolingåŸç†è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png\" alt=\"\"><br>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹max poolingï¼Œå¦‚å›¾ï¼Œæˆ‘ä»¬å–filterå°ºå¯¸\\(f=2\\)ï¼Œstrideå¤§å°\\(s=2\\)ï¼Œå¯¹äºä¸€ä¸ªfilterä¸­çš„å…ƒç´ ï¼Œæˆ‘ä»¬å–maxä½œä¸ºè¾“å‡ºï¼›ç›¸å¯¹åº”çš„ï¼Œå¦‚æœæˆ‘ä»¬å–averageï¼Œé‚£ä¹ˆå°±æˆäº†average poolingï¼Œpoolingä¸­çš„hyperparameteråªæœ‰filterå°ºå¯¸\\(f\\)å’Œstrideå¤§å°\\(s\\)ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œpoolingè¿‡ç¨‹ä¸­ä¸å­˜åœ¨å­¦ä¹ è¿‡ç¨‹ï¼Œno parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layeråœ¨CNNå…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å°†inputå±•å¼€ï¼ŒæŒ‰ç…§DNNçš„æ–¹æ³•è¿›è¡Œfully connectedå°±å¯ä»¥äº†ã€‚</p>\n<p>ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰prameterå˜åŒ–çš„æ‰ç®—ä¸€å±‚ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸ºpoolingæ˜¯ä¸€ä¸ªlayerï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¸€ä¸ªæœ€ç®€å•çš„CNNåšä¾‹å­ï¼šCONV-POOL-CONV-POOL-FC-FC-Softmaxï¼Œæˆ‘ä»¬è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„5å±‚çš„CNNï¼Œåœ¨ä¸‹å‘¨çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»å…¸çš„CNNæ¡†æ¶ï¼Œè¿™é‡Œå°±ä¸å†å¤è¿°ã€‚</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>å…³äºè¿™ä¸ªé—®é¢˜ï¼ŒNgç»™å‡ºäº†ä¸¤ä¸ªæ„è§ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) thatâ€™s useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"learning notes","path":"categories/learning-notes/"}],"_tags":[{"name":"CNN","path":"tags/CNN/"}],"excerpt":"<p>Hi, all. æœ€è¿‘å¼€å§‹ä¼‘å‡äº†ï¼Œå¯ä»¥æœ‰ç©ºç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä¸€æ–¹é¢è¡¥ä¸€è¡¥å‰é¢çš„ä½œä¸šï¼Œä¸€æ–¹é¢ç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ°äº†course4ï¼Œä¹Ÿå°±æ˜¯convolutional neural networks çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ï¼<br></p>","more":"</p>\n<h2 id=\"Convolution\"><a href=\"#Convolution\" class=\"headerlink\" title=\"Convolution\"></a>Convolution</h2><p>åœ¨è¯¾ç¨‹ä¸­ï¼ŒNgä»edge detectionçš„è§’åº¦æ¥ç»™å¤§å®¶è®²äº†è®²convolutionï¼Œå› ä¸ºæœ¬äººæ˜¯image processingå‡ºèº«ï¼Œæ‰€ä»¥è®¤ä¸ºNgåœ¨è¿™é‡Œè®²çš„è¿˜æ˜¯å¾ˆæµ…æ˜¾æ˜“æ‡‚çš„ï¼Œæˆ‘å°±ä¸å†ä¸“é—¨çš„markdownã€‚ä¸»è¦æ¥çœ‹çœ‹convolutionä¸­çš„ä¸€äº›æŠ€å·§ã€‚</p>\n<h3 id=\"Padding\"><a href=\"#Padding\" class=\"headerlink\" title=\"Padding\"></a>Padding</h3><p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œåœ¨æœ€çº¯ç²¹çš„convolutionä¸­ï¼Œæˆ‘ä»¬å‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( (n-f+1) *(n-f+1)\\)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»“æœçš„å°ºå¯¸å˜å°äº†ã€‚å¦‚æœæƒ³è®©è¾“å‡ºimageçš„å°ºå¯¸ä¸å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦ä½¿ç”¨å¤§åé¼é¼çš„paddingäº†ã€‚</p>\n<p>Paddingå…¶å®å°±æ˜¯è¡¨ç¤ºï¼Œåœ¨åŸå§‹imageä¸­ï¼Œå‘å¤–æ‰©å¤§å¤šå°‘å°ºå¯¸ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šä½¿ç”¨ç®€å•å¤åˆ¶ç›¸é‚»å…ƒç´ å€¼çš„æ–¹æ³•è¿›è¡Œæ‰©å……ã€‚å‡è®¾å¯¹äºä¸€ä¸ª\\(6 *6\\)çš„åŸå§‹imageï¼Œé‡‡ç”¨\\(3 *3\\)çš„filterï¼ŒåŠ ä¸Š\\(p=1\\)çš„paddingï¼Œé‚£ä¹ˆåŸå§‹å›¾åƒå°ºå¯¸å˜æˆäº†\\(8 * 8\\)ï¼Œç»“æœå˜æˆäº†\\(6 *6\\)ï¼ŒåŸå§‹imageå’Œç»“æœimageä¸€æ¨¡ä¸€æ ·ï¼äºæ˜¯åŠ å…¥äº†paddingçš„convolutionå…¬å¼å°±æˆäº†\\( (n+2p-f+1) *(n+2p-f+1)\\).</p>\n<p>åœ¨è¿™é‡ŒNgå¼•å…¥äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œvalidå’Œsame convolutionsï¼Œæ‰€è°“valid convolutionï¼Œå°±æ˜¯æ²¡æœ‰padding çš„convolutionï¼›æ‰€è°“same convolutionï¼Œå°±æ˜¯è¾“å…¥è¾“å‡ºçš„å°ºå¯¸å®Œå…¨ä¸€æ ·ã€‚</p>\n<h3 id=\"Stride\"><a href=\"#Stride\" class=\"headerlink\" title=\"Stride\"></a>Stride</h3><p>ç»§paddingä¹‹åï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å‚æ•°ï¼Œå°±æ˜¯æ­¥é•¿strideï¼Œæ­¥é•¿strideå†³å®šäº†filteråšconvolutionæ—¶å€™çš„æ­¥é•¿ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆfilterå°±ä¼šæŒ¨ç€è®¡ç®—ï¼Œå¦‚æœstride=2ï¼Œé‚£ä¹ˆå°±ä¼šè·³è·ƒè¿™è¿›è¡Œè®¡ç®—ã€‚</p>\n<p>æ€»ç»“ä¸€ä¸‹ï¼Œå‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œpaddingå€¼æ˜¯\\(p\\)ï¼Œstrideå€¼æ˜¯\\(s\\)é‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)ï¼Œå¦‚æœé™¤ä¸å°½çš„è¯ï¼Œæˆ‘ä»¬é€‰æ‹©å‘ä¸‹å–æ•´ï¼Œä¹Ÿå°±æ˜¯ä¸è¶³ä»¥åšconvolutionçš„åŒºåŸŸï¼Œæˆ‘ä»¬é€‰æ‹©æ”¾å¼ƒã€‚</p>\n<p>##Convolution over Volume<br>å¯¹äºä¸€èˆ¬çš„å›¾åƒå¤„ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯RGBå›¾åƒï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªchannelï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œconvolutionåº”è¯¥å¦‚ä½•åšï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-1.png\" alt=\"\"><br>å‡è®¾æˆ‘ä»¬çš„å›¾åƒæ˜¯6Ã—6Ã—3ï¼Œä¹Ÿå°±æ˜¯hightÃ—widthÃ—channel(depth)ï¼Œå› æ­¤å¯¹åº”çš„filterä¹Ÿè¦æœ‰3çš„channel(depth)ï¼Œæœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª4Ã—4çš„ç»“æœã€‚</p>\n<p>å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸æ­¢ä¸€ä¸ªfilterï¼Œå¦‚å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-2.png\" alt=\"\"><br>æˆ‘ä»¬åŠ å…¥äº†ä¸¤ä¸ªä¸åŒçš„filterï¼Œä»–ä»¬çš„å¤§å°éƒ½æ˜¯3Ã—3Ã—3ï¼Œäºæ˜¯æœ€ç»ˆçš„ç»“æœå°±æ˜¯4Ã—4Ã—2ï¼Œè¯·æ³¨æ„ï¼šç»“æœçš„channelæ•°ç›®å–å†³äºfilterçš„ä¸ªæ•°ï¼Œè€Œå’Œè¾“å…¥çš„channelæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><h3 id=\"Convolution-Layer\"><a href=\"#Convolution-Layer\" class=\"headerlink\" title=\"Convolution Layer\"></a>Convolution Layer</h3><p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹CNNç½‘ç»œä¸­çš„ä¸€ä¸ªlayerçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Œé¦–å…ˆæ¥çœ‹æˆªå›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-3.png\" alt=\"\"><br>è¿™å¼ å›¾ååˆ†å¤æ‚ï¼Œæˆ‘ä»¬ä¸€èµ·ä»”ç»†çœ‹çœ‹è¿™å¼ å›¾ï¼Œå¯¹äºä¸€ä¸ª6Ã—6Ã—3çš„RGBå›¾åƒï¼Œæˆ‘ä»¬ç”¨äº†ä¸¤ä¸ª3Ã—3Ã—3çš„filterï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾“å…¥imageçœ‹åš\\(x\\)ï¼Œä¹Ÿå°±æ˜¯\\(a ^{[0]}\\)ï¼Œfilterçœ‹åš\\(w ^{[1]}\\)ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯\\(w ^{[1]}a ^{[0]}\\)ï¼Œæˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªbiasé¡¹\\(b^{[1]}\\)ï¼Œé‚£ä¹ˆå°±è·å¾—äº†ä¸€ä¸ªliner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)ï¼Œæˆ‘ä»¬å†ä½¿ç”¨ä¸€ä¸ªnon-liner functionä¾‹å¦‚ReLUï¼Œå¦‚æ­¤è·å¾—ä¸€ä¸ª4Ã—4Ã—2çš„outputã€‚å¦‚æ­¤å°±æ˜¯CNNçš„ä¸€ä¸ªlayer.</p>\n<p>å¦‚æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCNNå’Œä¹‹å‰çš„DNNå®è´¨ä¸Šéƒ½å­˜åœ¨ä¸€ç§liner functionåˆ°non-liner functionçš„è½¬åŒ–ï¼Œé€šè¿‡non-liner functionå»classifyçº¿æ€§ä¸å¯åˆ†çš„dataï¼Œå¦å¤–ï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸€ä¸ªfilterå°±å¯ä»¥è·å¾—ä¸€ä¸ªä¸åŒçš„featureï¼Œå¤šä¸ªfilterå¯ä»¥è®©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å»classify data.</p>\n<p>å¦å¤–ï¼Œç›¸æ¯”è¾ƒäºfully connected çš„DNNï¼ŒCNNæ‰€éœ€è¦çš„parametersä¹Ÿå°‘äº†å¾ˆå¤šï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚</p>\n<h3 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h3><p>PoolingåŸç†è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/13/13-4.png\" alt=\"\"><br>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹max poolingï¼Œå¦‚å›¾ï¼Œæˆ‘ä»¬å–filterå°ºå¯¸\\(f=2\\)ï¼Œstrideå¤§å°\\(s=2\\)ï¼Œå¯¹äºä¸€ä¸ªfilterä¸­çš„å…ƒç´ ï¼Œæˆ‘ä»¬å–maxä½œä¸ºè¾“å‡ºï¼›ç›¸å¯¹åº”çš„ï¼Œå¦‚æœæˆ‘ä»¬å–averageï¼Œé‚£ä¹ˆå°±æˆäº†average poolingï¼Œpoolingä¸­çš„hyperparameteråªæœ‰filterå°ºå¯¸\\(f\\)å’Œstrideå¤§å°\\(s\\)ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œpoolingè¿‡ç¨‹ä¸­ä¸å­˜åœ¨å­¦ä¹ è¿‡ç¨‹ï¼Œno parameters to learn!</p>\n<h3 id=\"Fully-Connected-layer\"><a href=\"#Fully-Connected-layer\" class=\"headerlink\" title=\"Fully Connected layer\"></a>Fully Connected layer</h3><p>Fully connected layeråœ¨CNNå…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å°†inputå±•å¼€ï¼ŒæŒ‰ç…§DNNçš„æ–¹æ³•è¿›è¡Œfully connectedå°±å¯ä»¥äº†ã€‚</p>\n<p>ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰prameterå˜åŒ–çš„æ‰ç®—ä¸€å±‚ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸ºpoolingæ˜¯ä¸€ä¸ªlayerï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¸€ä¸ªæœ€ç®€å•çš„CNNåšä¾‹å­ï¼šCONV-POOL-CONV-POOL-FC-FC-Softmaxï¼Œæˆ‘ä»¬è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„5å±‚çš„CNNï¼Œåœ¨ä¸‹å‘¨çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»å…¸çš„CNNæ¡†æ¶ï¼Œè¿™é‡Œå°±ä¸å†å¤è¿°ã€‚</p>\n<h2 id=\"Why-Convolutions\"><a href=\"#Why-Convolutions\" class=\"headerlink\" title=\"Why Convolutions\"></a>Why Convolutions</h2><p>å…³äºè¿™ä¸ªé—®é¢˜ï¼ŒNgç»™å‡ºäº†ä¸¤ä¸ªæ„è§ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š</p>\n<blockquote>\n<p><strong>Parameter sharing</strong>: A feature detector (such as a vertical edge detetor) thatâ€™s useful in one part of image is probably useful in another part of the image.<br><strong>Sparsity of connections</strong>: In  each layer, each output value depends only on a small number of inputs.</p>\n</blockquote>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"hhttps://www.coursera.org/learn/convolutional-neural-networks\" target=\"_blank\" rel=\"noopener\">Deep learning-Coursera Andrew Ng</a></li>\n<li><a href=\"https://mooc.study.163.com/course/2001281004#/info\" target=\"_blank\" rel=\"noopener\">Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng</a></li>\n</ul>"},{"title":"ä»å‡¸å‡½æ•°åˆ°æ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•","date":"2017-08-02T07:53:55.000Z","_content":"è®°å¾—æˆ‘åœ¨å’Œä¼˜ç”·ä¸€èµ·ç ”ç©¶logistic regressionçš„æ—¶å€™ï¼Œä»–é—®äº†æˆ‘å‡ ä¸ªéå¸¸å°–é”çš„é—®é¢˜ï¼Œè®©æˆ‘é¡¿æ—¶å“‘å£æ— è¨€\n* æ€ä¹ˆä¿è¯logistic regressioné€šè¿‡gradient descentæ‰¾åˆ°çš„æ˜¯æœ€ä¼˜è§£ï¼›\n* ä¸ºä»€ä¹ˆlogistic regressionå¯ä»¥ç”¨newton's methodå‘¢ï¼Ÿ\n* Newton's methodä¸­Hessian matrixå¿…é¡»positive definiteæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Œlog cost functionèƒ½ä¿è¯å—ï¼Ÿ\n\nè¿™äº›ç»†èŠ‚é—®é¢˜ï¼Œè¯´å®è¯æˆ‘ä¹Ÿæ²¡æœ‰è®¤çœŸçš„æƒ³è¿‡ã€‚åœ¨å¤¸å¥–ä»–ä¹‹ä½™ï¼Œæˆ‘ä»¬ä¹Ÿä¸€èµ·å¼€å§‹äº†ç ”ç©¶ï¼Œå¸Œæœ›ä»ä¸­å­¦ä¹ åˆ°ä¸€äº›æ›´æ·±å±‚çš„ä¸œè¥¿ï¼Œè¶ç€ç°åœ¨æœ‰ä¸ªblogåˆ†äº«ç»™å¤§å®¶\n<!--more-->\n\n## å‡¸å‡½æ•°(Convex function)\nåœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºæœ¯è¯­çš„å€¡è®®ã€‚ä¸­æ–‡é‡Œçš„â€œå‡¸å‡½æ•°â€ï¼Œçœ‹ä¸Šå»æ˜¯å‡¹ä¸‹å»çš„ï¼Œå¯¹åº”çš„ï¼Œä¸­æ–‡é‡Œçš„â€œå‡¹å‡½æ•°â€çœ‹ä¸Šå»å‡¸èµ·æ¥çš„ï¼Œamazingå§ï¼Ÿè¿™æ˜¯æœ‰ä¸€å®šå†å²åŸå› çš„ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å»æŸ¥é˜…ä¸‹èµ„æ–™ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å†å¤è¿°ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è®©å¤§å®¶äº§ç”Ÿè¯¯è§£ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä½¿ç”¨è‹±æ–‡ï¼Œ**convex function**å’Œ**concave function**.è¿™æ ·ä¼šé¿å…å¾ˆå¤šä¸å¿…è¦çš„éº»çƒ¦ã€‚\n\nOKï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ï¼Œconvex function\n\nå¯¹äºä¸€ç»´å‡½æ•° \\\\(f(x)\\\\)æ¥è¯´ï¼Œåœ¨å®šä¹‰åŸŸå†…çš„ä»»æ„å€¼ \\\\(a\\\\)å’Œ\\\\(b\\\\)ï¼Œå¯¹äºä»»æ„çš„ \\\\( 0 \\leq \\theta \\leq 1\\\\)ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°ä¸ºconvex function\n$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$\næˆ‘ä»¬å†ç”¨å›¾ç‰‡ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png)\næ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“å…¬å¼ä¸­ç­‰å·å»æ‰çš„æ—¶å€™ï¼Œå‡½æ•°å°±æ˜¯**strictly convex function**.\n\nConvex functionå…·æœ‰ä¸€å®šçš„æ€§è´¨ï¼Œæˆ‘ä»¬ç®€å•çš„æè¿°ä¸€ä¸‹ã€‚\n\n### First order condition\nå¯¹äº function \\\\(f\\\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…**ä¸€é˜¶å¯å¯¼**ï¼Œä¸”å¯¼æ•°ä¸º\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\né‚£ä¹ˆ \\\\(f\\\\)æ˜¯convex functionçš„**å……è¦æ¡ä»¶**æ˜¯ï¼šå¯¹äºå®šä¹‰åŸŸå†…ä»»æ„ \\\\(x\\\\) å’Œ \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOKï¼Œå†æ¥å¼ å›¾ç‰‡ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼š\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png)\n\nå…¶å®ç®€å•çš„æ¥è®²ï¼Œå°±æ˜¯å¯¹äºconvex function \\\\(f\\\\)ï¼Œå®ƒçš„å‡½æ•°å€¼æ°¸è¿œå¤§äºç­‰äºåˆ‡çº¿ä¸Šçš„å€¼ï¼\n\n### Second order condition\nå¯¹äº function \\\\(f\\\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…**äºŒé˜¶å¯å¯¼**ï¼Œä¸” \\\\(n\\\\) ç»´æ–¹é˜µHessian matrixçš„å…ƒç´ ä¸º\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\nå½“ä¸”ä»…å½“Hessian matrix positive semi-defniteçš„æ—¶å€™ï¼Œ\\\\(f\\\\) æ˜¯convex functionã€‚ä»¥ä¸Šäº’ä¸º**å……è¦æ¡ä»¶**ã€‚è¿™é‡Œçš„è¯æ˜æˆ‘ä¸æƒ³å±•å¼€è®²ï¼Œåœ¨åé¢æˆ‘ä¼šç»™å‡ºreferenceé“¾æ¥ã€‚\n\n---\nä¸‹é¢ç»™å‡ºä¸€äº› \\\\(\\Bbb R\\\\) ç©ºé—´ä¸‹å¸¸è§çš„convex functionï¼š\n* çº¿æ€§å‡½æ•°ï¼š\\\\(f(x) = ax+b\\\\)\n* æŒ‡æ•°å‡½æ•°ï¼š\\\\(f(x)=e^ {ax}\\\\)\n* è´Ÿç†µå‡½æ•°ï¼š \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\nå¯¹åº”çš„ï¼Œä¸€äº›å¸¸è§çš„concave functionï¼š\n* çº¿æ€§å‡½æ•°ï¼š\\\\(f(x) = ax+b\\\\)\n* å¯¹æ•°å‡½æ•°ï¼š \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\nå…¶ä¸­å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§å‡½æ•°æ—¢æ˜¯convexä¹Ÿæ˜¯concaveå‡½æ•°ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œè¿™å’Œå®ƒæœ¬èº«çš„first order conditionä¸ºå¸¸æ•°æœ‰å…³ã€‚\n\nä»¥ä¸Šå°±æ˜¯convex functionçš„ä¸€ä¸ªç®€å•ä»‹ç»ï¼Œä½ ä¹Ÿè®¸ä¼šé—®ï¼Œä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆå¤šåŠ›æ°”æ¥ä»‹ç»convex function. å…¶å®ï¼Œåœ¨machine learningä¸­ï¼Œconvex functionçš„ä¼˜åŒ–æ˜¯éå¸¸é‡è¦çš„ï¼Œå¾ˆå¤šç®—æ³•è¯´åˆ°åº•ï¼Œéƒ½æ˜¯è¦optimizeä¸€ä¸ªconvex functionï¼Œæˆ‘ä»¬ä¼šç”¨liner regressionå’Œlogistic regressionä¸ºä¾‹å­ï¼Œè¿›ä¸€æ­¥ä»convex functionçš„ç®€ä»‹è¿‡æ¸¡åˆ°gradient descentå’Œnewton's method.\n\n## æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)\nå…³äºgradient descentï¼Œæˆ‘ä»¬ä½¿ç”¨liner regressionä½œä¸ºä¾‹å­æ¥è®¨è®ºã€‚Liner regressionç®—æ³•çš„å®è´¨æ˜¯least square methodï¼Œä»–çš„cost functionæ˜¯\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\nå¯¹äºliner regressionæ¥è¯´ï¼Œç®—æ³•çš„å®è´¨å°±æ˜¯å»æ±‚å‡º\\\\( J( \\theta) \\\\) **ä»¥\\\\( \\theta\\\\)ä¸ºå‚æ•°**çš„minimumï¼Œgradient descentç®—æ³•çš„ä½œç”¨å°±æ˜¯å»å®ç°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œgradient descentçš„åŸºç¡€çŸ¥è¯†è¯¦è§reference. \n\né‚£ä¹ˆé’ˆå¯¹cost functionï¼Œgradient descentæ˜¯å¦‚ä½•ä¿è¯æ”¶æ•›çš„å‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹\n\nå¯¹äº\\\\(J( \\theta)\\\\)ï¼Œæˆ‘ä»¬å°†å…¶å¸¦å…¥convex functionçš„å®šä¹‰å…¬å¼ä¸­ï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬çš„è‡ªå˜é‡æ˜¯\\\\( \\theta\\\\)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨å¯¼è¯æ˜è¯¥å¼æˆç«‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œleast square cost functionæ˜¯convex function.\n\næ—¢ç„¶æœ‰è¿™ä¸ªç»“è®ºäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œleast square cost functionä½œä¸ºconvex functionï¼Œæ˜¯å­˜åœ¨å…¨å±€æœ€å°å€¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œgradient descentä¸ä¼šå‡ºç°é™·å…¥å±€éƒ¨æœ€ä¼˜æ— æ³•è‡ªæ‹”çš„ç°è±¡ï¼Œåªè¦gradient descentä¿è¯å‚æ•°è¶³å¤Ÿå¥½çš„æƒ…å†µä¸‹ï¼Œç†è®ºä¸Šï¼Œæ˜¯å®Œå…¨å¯ä»¥å¾ˆå¥½çš„é€¼è¿‘å…¨å±€æœ€ä¼˜çš„è§£çš„ã€‚\n\n> Gradien descentç®—æ³•æœ¬èº«å¹¶ä¸èƒ½ä¿è¯è·å¾—å…¨å±€æœ€å°å€¼ï¼Œåªæœ‰åœ¨objective functionæ˜¯convex functionçš„æ—¶å€™æ‰å¯ä»¥ä¿è¯\n\nä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œå³è¾¹çš„object functionæ˜¯non-convex functionï¼Œå› è€Œå¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼æ— æ³•è‡ªæ‹”ï¼Œè€Œå·¦è¾¹çš„objective functionæ˜¯ä¸€ä¸ªæ ‡å‡†çš„convex functionï¼Œåœ¨gradient descentå‚æ•°åˆç†çš„å‰æä¸‹ï¼Œå¯ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png)\n\nå½“ç„¶ï¼Œgradien descentçš„ä¸€äº›æ”¹è¿›æ–¹æ³•ï¼Œä¾‹å¦‚stochastic gradient descentåœ¨è§£å†³non-convex optimizationä¸Šæœ‰ä¸€äº›å¸®åŠ©ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä¸åšè®¨è®ºï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†å†™ã€‚\n\nç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼Œgradient descentä¸ä»…ä»…æ˜¯minimize liner regressionçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯convex optimizationçš„ä¸€ç§ç†æƒ³æ–¹æ³•\n\n## ç‰›é¡¿æ³•(Newton's method)\nNewton's method è¿™å—å†…å®¹ï¼Œæˆ‘ä»¬å°†ä¼šç”¨logistic regressionä½œä¸ºä¾‹å­ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å…ˆæ¥å…³æ³¨ä¸‹log cost functionï¼Œè¿™é‡Œï¼Œ**æˆ‘ä»¬å–labelä¸º-1å’Œ+1**ï¼Œå› ä¸ºè¿™æ ·å¾—åˆ°çš„cost functionæ¯”0,1ä¸‹çš„è®¡ç®—æ›´åŠ ç®€å•\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\nè¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†-1å’Œ+1ä½œä¸ºæ ‡ç­¾å€¼ï¼Œå’Œå¤§å¤šæ•°æ•™æä¸­ä¸ä¸€æ ·ï¼Œå¤§å®¶å¯ä»¥ä¸‹æ¥è‡ªå·±æ¨å¯¼ä¸€ä¸‹\\\\(J( \\omega)\\\\)ï¼Œè¿™ç§å†™æ³•å¹¿æ³›çš„åº”ç”¨åœ¨äº†æ¯”è¾ƒlogistic regressionå’ŒSVMä¸¤å¤§åˆ†ç±»å™¨çš„æ–‡çŒ®ä¸­ï¼Œå¸Œæœ›å¤§å®¶ç†ŸçŸ¥ã€‚\n\næ­¤å¤„æˆ‘ä»¬å¯¹åŸå§‹çš„likehood functionåŠ ä¸Šäº† \\\\(- \\frac{1}{m}\\\\)çš„ç³»æ•°ï¼ŒåŒæ ·ï¼Œå½“æˆ‘ä»¬æŠŠ \\\\(J( \\omega)\\\\)å¸¦å…¥åˆ°convex functionçš„å®šä¹‰ä¸­ï¼Œå¯ä»¥éªŒè¯ä¸Šå¼ä¸ºconvex functionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\\\(J( \\omega)\\\\)æ˜¯\\\\( \\omega\\\\)çš„å‡½æ•°ã€‚\n\nå…¶å®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†log cost functionå±•å¼€åï¼Œåˆ©ç”¨æœ€åŸºæœ¬çš„å‡½æ•°convexå’Œconcaveæ€§è´¨æ¥è·å¾—ä¸Šå¼æ˜¯convex functionçš„ç»“è®ºï¼Œç¢äºå…¬å¼å®åœ¨å¤ªéš¾æ‰“ï¼Œå°±ç•™ç»™å¤§å®¶å»è¯æ˜å§ã€‚\n\nOKï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œæˆ‘ä»¬ä¸€å®šæ˜¯å¯ä»¥ç”¨gradient descentå»æ±‚è§£çš„ã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨newton's methodå‘¢ï¼Ÿ\n\nNewton's methodçš„åŸºæœ¬åŸç†è¯¦è§referenceï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œé‚£ä¹ˆæ ¹æ®second order conditionå¯ä»¥çŸ¥é“ï¼Œå®ƒçš„Hessian matrixä¸€å®šæ˜¯positive semi-definiteçš„ã€‚å¦‚æœæˆ‘ä»¬åŠ ä¸Šäº†L2 regularizerï¼Œ**ç”±äºL2 regularizeræœ¬èº«å°±æ˜¯ä¸€ä¸ªstrict convex function**ï¼Œé‚£ä¹ˆlog cost functionå°±ä¸€å®šæ˜¯strict convex functionäº†ï¼Œä¹Ÿå°±æ˜¯ï¼š\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\nå› æ­¤ï¼Œåœ¨log cost functionä¸­ï¼Œ**Hessian matrixæ˜¯positive definiteçš„**ï¼Œå®Œå…¨æ»¡è¶³newton's method çš„è¦æ±‚ã€‚åŒæ ·ï¼Œç±»ä¼¼äºä¸Šä¸€éƒ¨åˆ†ï¼Œnewton's methodä¹Ÿå¯ä»¥æ‰¾åˆ°log cost functionçš„å…¨å±€æœ€ä¼˜ã€‚\n## Sum up\nOKï¼Œæˆ‘ä»¬è¯´åˆ°è¿™é‡Œä¹Ÿç¡®å®è®²äº†ä¸å°‘ï¼Œè¿™ç¯‡blogæœ‰äº›å†—é•¿ï¼Œå¸Œæœ›æœ‹å‹ä»¬ä¸è¦ç„¦è™‘ã€‚æ€»ä½“æ¥è¯´ï¼Œæˆ‘æƒ³è¡¨è¾¾çš„æ˜¯ä»¥ä¸‹å‡ ä¸ªè§‚ç‚¹ï¼š\n* Machine learningä¸­æˆ‘ä»¬å¯»æ±‚çš„å…¶å®å°±æ˜¯objective functionä¸€ä¸ªå…¨å±€æœ€ä¼˜å€¼ï¼Œè¿™äº›é—®é¢˜æ˜¯é€šè¿‡gradient descentç­‰æ–¹æ³•è§£å†³çš„ï¼›\n* Gradient descentå’Œnewton's methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œä»–ä»¬éƒ½å¯ä»¥å¯¹äºconvex functionè·å¾—å…¨å±€æœ€ä¼˜ï¼›\n* å¯¹äºnon-convex optimizationé—®é¢˜ï¼Œstochastic gradient descentä¹Ÿå¾ˆæœ‰æ•ˆæœï¼Œæˆ‘ä»¬åç»­å†æ…¢æ…¢å­¦ä¹ ã€‚\n\nå¥½äº†ï¼Œæ ¸å¿ƒæ€æƒ³å°±è¿™ä¸‰ç‚¹ï¼Œä»Šå¤©å…ˆè¯´è¿™ä¹ˆå¤šï¼\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLIå¤§ç¥çš„blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","source":"_posts/ml-convex-opt.md","raw":"---\ntitle: ä»å‡¸å‡½æ•°åˆ°æ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•\ndate: 2017-08-02 15:53:55\ntags: \n    - convex optimization\n    - gradient descent\n    - newton's method\ncategories: machine learning\n---\nè®°å¾—æˆ‘åœ¨å’Œä¼˜ç”·ä¸€èµ·ç ”ç©¶logistic regressionçš„æ—¶å€™ï¼Œä»–é—®äº†æˆ‘å‡ ä¸ªéå¸¸å°–é”çš„é—®é¢˜ï¼Œè®©æˆ‘é¡¿æ—¶å“‘å£æ— è¨€\n* æ€ä¹ˆä¿è¯logistic regressioné€šè¿‡gradient descentæ‰¾åˆ°çš„æ˜¯æœ€ä¼˜è§£ï¼›\n* ä¸ºä»€ä¹ˆlogistic regressionå¯ä»¥ç”¨newton's methodå‘¢ï¼Ÿ\n* Newton's methodä¸­Hessian matrixå¿…é¡»positive definiteæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Œlog cost functionèƒ½ä¿è¯å—ï¼Ÿ\n\nè¿™äº›ç»†èŠ‚é—®é¢˜ï¼Œè¯´å®è¯æˆ‘ä¹Ÿæ²¡æœ‰è®¤çœŸçš„æƒ³è¿‡ã€‚åœ¨å¤¸å¥–ä»–ä¹‹ä½™ï¼Œæˆ‘ä»¬ä¹Ÿä¸€èµ·å¼€å§‹äº†ç ”ç©¶ï¼Œå¸Œæœ›ä»ä¸­å­¦ä¹ åˆ°ä¸€äº›æ›´æ·±å±‚çš„ä¸œè¥¿ï¼Œè¶ç€ç°åœ¨æœ‰ä¸ªblogåˆ†äº«ç»™å¤§å®¶\n<!--more-->\n\n## å‡¸å‡½æ•°(Convex function)\nåœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºæœ¯è¯­çš„å€¡è®®ã€‚ä¸­æ–‡é‡Œçš„â€œå‡¸å‡½æ•°â€ï¼Œçœ‹ä¸Šå»æ˜¯å‡¹ä¸‹å»çš„ï¼Œå¯¹åº”çš„ï¼Œä¸­æ–‡é‡Œçš„â€œå‡¹å‡½æ•°â€çœ‹ä¸Šå»å‡¸èµ·æ¥çš„ï¼Œamazingå§ï¼Ÿè¿™æ˜¯æœ‰ä¸€å®šå†å²åŸå› çš„ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å»æŸ¥é˜…ä¸‹èµ„æ–™ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å†å¤è¿°ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è®©å¤§å®¶äº§ç”Ÿè¯¯è§£ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä½¿ç”¨è‹±æ–‡ï¼Œ**convex function**å’Œ**concave function**.è¿™æ ·ä¼šé¿å…å¾ˆå¤šä¸å¿…è¦çš„éº»çƒ¦ã€‚\n\nOKï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ï¼Œconvex function\n\nå¯¹äºä¸€ç»´å‡½æ•° \\\\(f(x)\\\\)æ¥è¯´ï¼Œåœ¨å®šä¹‰åŸŸå†…çš„ä»»æ„å€¼ \\\\(a\\\\)å’Œ\\\\(b\\\\)ï¼Œå¯¹äºä»»æ„çš„ \\\\( 0 \\leq \\theta \\leq 1\\\\)ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°ä¸ºconvex function\n$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$\næˆ‘ä»¬å†ç”¨å›¾ç‰‡ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png)\næ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“å…¬å¼ä¸­ç­‰å·å»æ‰çš„æ—¶å€™ï¼Œå‡½æ•°å°±æ˜¯**strictly convex function**.\n\nConvex functionå…·æœ‰ä¸€å®šçš„æ€§è´¨ï¼Œæˆ‘ä»¬ç®€å•çš„æè¿°ä¸€ä¸‹ã€‚\n\n### First order condition\nå¯¹äº function \\\\(f\\\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…**ä¸€é˜¶å¯å¯¼**ï¼Œä¸”å¯¼æ•°ä¸º\n$$ \t\\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},...,  \\frac{\\partial f(x)}{x_n})$$\né‚£ä¹ˆ \\\\(f\\\\)æ˜¯convex functionçš„**å……è¦æ¡ä»¶**æ˜¯ï¼šå¯¹äºå®šä¹‰åŸŸå†…ä»»æ„ \\\\(x\\\\) å’Œ \\\\(y\\\\)\n$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$\nOKï¼Œå†æ¥å¼ å›¾ç‰‡ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼š\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png)\n\nå…¶å®ç®€å•çš„æ¥è®²ï¼Œå°±æ˜¯å¯¹äºconvex function \\\\(f\\\\)ï¼Œå®ƒçš„å‡½æ•°å€¼æ°¸è¿œå¤§äºç­‰äºåˆ‡çº¿ä¸Šçš„å€¼ï¼\n\n### Second order condition\nå¯¹äº function \\\\(f\\\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…**äºŒé˜¶å¯å¯¼**ï¼Œä¸” \\\\(n\\\\) ç»´æ–¹é˜µHessian matrixçš„å…ƒç´ ä¸º\n$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,...,n$$\nå½“ä¸”ä»…å½“Hessian matrix positive semi-defniteçš„æ—¶å€™ï¼Œ\\\\(f\\\\) æ˜¯convex functionã€‚ä»¥ä¸Šäº’ä¸º**å……è¦æ¡ä»¶**ã€‚è¿™é‡Œçš„è¯æ˜æˆ‘ä¸æƒ³å±•å¼€è®²ï¼Œåœ¨åé¢æˆ‘ä¼šç»™å‡ºreferenceé“¾æ¥ã€‚\n\n---\nä¸‹é¢ç»™å‡ºä¸€äº› \\\\(\\Bbb R\\\\) ç©ºé—´ä¸‹å¸¸è§çš„convex functionï¼š\n* çº¿æ€§å‡½æ•°ï¼š\\\\(f(x) = ax+b\\\\)\n* æŒ‡æ•°å‡½æ•°ï¼š\\\\(f(x)=e^ {ax}\\\\)\n* è´Ÿç†µå‡½æ•°ï¼š \\\\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\\\)\n\nå¯¹åº”çš„ï¼Œä¸€äº›å¸¸è§çš„concave functionï¼š\n* çº¿æ€§å‡½æ•°ï¼š\\\\(f(x) = ax+b\\\\)\n* å¯¹æ•°å‡½æ•°ï¼š \\\\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\\\)\n\nå…¶ä¸­å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§å‡½æ•°æ—¢æ˜¯convexä¹Ÿæ˜¯concaveå‡½æ•°ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œè¿™å’Œå®ƒæœ¬èº«çš„first order conditionä¸ºå¸¸æ•°æœ‰å…³ã€‚\n\nä»¥ä¸Šå°±æ˜¯convex functionçš„ä¸€ä¸ªç®€å•ä»‹ç»ï¼Œä½ ä¹Ÿè®¸ä¼šé—®ï¼Œä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆå¤šåŠ›æ°”æ¥ä»‹ç»convex function. å…¶å®ï¼Œåœ¨machine learningä¸­ï¼Œconvex functionçš„ä¼˜åŒ–æ˜¯éå¸¸é‡è¦çš„ï¼Œå¾ˆå¤šç®—æ³•è¯´åˆ°åº•ï¼Œéƒ½æ˜¯è¦optimizeä¸€ä¸ªconvex functionï¼Œæˆ‘ä»¬ä¼šç”¨liner regressionå’Œlogistic regressionä¸ºä¾‹å­ï¼Œè¿›ä¸€æ­¥ä»convex functionçš„ç®€ä»‹è¿‡æ¸¡åˆ°gradient descentå’Œnewton's method.\n\n## æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)\nå…³äºgradient descentï¼Œæˆ‘ä»¬ä½¿ç”¨liner regressionä½œä¸ºä¾‹å­æ¥è®¨è®ºã€‚Liner regressionç®—æ³•çš„å®è´¨æ˜¯least square methodï¼Œä»–çš„cost functionæ˜¯\n$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$\nå¯¹äºliner regressionæ¥è¯´ï¼Œç®—æ³•çš„å®è´¨å°±æ˜¯å»æ±‚å‡º\\\\( J( \\theta) \\\\) **ä»¥\\\\( \\theta\\\\)ä¸ºå‚æ•°**çš„minimumï¼Œgradient descentç®—æ³•çš„ä½œç”¨å°±æ˜¯å»å®ç°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œgradient descentçš„åŸºç¡€çŸ¥è¯†è¯¦è§reference. \n\né‚£ä¹ˆé’ˆå¯¹cost functionï¼Œgradient descentæ˜¯å¦‚ä½•ä¿è¯æ”¶æ•›çš„å‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹\n\nå¯¹äº\\\\(J( \\theta)\\\\)ï¼Œæˆ‘ä»¬å°†å…¶å¸¦å…¥convex functionçš„å®šä¹‰å…¬å¼ä¸­ï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬çš„è‡ªå˜é‡æ˜¯\\\\( \\theta\\\\)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨å¯¼è¯æ˜è¯¥å¼æˆç«‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œleast square cost functionæ˜¯convex function.\n\næ—¢ç„¶æœ‰è¿™ä¸ªç»“è®ºäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œleast square cost functionä½œä¸ºconvex functionï¼Œæ˜¯å­˜åœ¨å…¨å±€æœ€å°å€¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œgradient descentä¸ä¼šå‡ºç°é™·å…¥å±€éƒ¨æœ€ä¼˜æ— æ³•è‡ªæ‹”çš„ç°è±¡ï¼Œåªè¦gradient descentä¿è¯å‚æ•°è¶³å¤Ÿå¥½çš„æƒ…å†µä¸‹ï¼Œç†è®ºä¸Šï¼Œæ˜¯å®Œå…¨å¯ä»¥å¾ˆå¥½çš„é€¼è¿‘å…¨å±€æœ€ä¼˜çš„è§£çš„ã€‚\n\n> Gradien descentç®—æ³•æœ¬èº«å¹¶ä¸èƒ½ä¿è¯è·å¾—å…¨å±€æœ€å°å€¼ï¼Œåªæœ‰åœ¨objective functionæ˜¯convex functionçš„æ—¶å€™æ‰å¯ä»¥ä¿è¯\n\nä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œå³è¾¹çš„object functionæ˜¯non-convex functionï¼Œå› è€Œå¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼æ— æ³•è‡ªæ‹”ï¼Œè€Œå·¦è¾¹çš„objective functionæ˜¯ä¸€ä¸ªæ ‡å‡†çš„convex functionï¼Œåœ¨gradient descentå‚æ•°åˆç†çš„å‰æä¸‹ï¼Œå¯ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png)\n\nå½“ç„¶ï¼Œgradien descentçš„ä¸€äº›æ”¹è¿›æ–¹æ³•ï¼Œä¾‹å¦‚stochastic gradient descentåœ¨è§£å†³non-convex optimizationä¸Šæœ‰ä¸€äº›å¸®åŠ©ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä¸åšè®¨è®ºï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†å†™ã€‚\n\nç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼Œgradient descentä¸ä»…ä»…æ˜¯minimize liner regressionçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯convex optimizationçš„ä¸€ç§ç†æƒ³æ–¹æ³•\n\n## ç‰›é¡¿æ³•(Newton's method)\nNewton's method è¿™å—å†…å®¹ï¼Œæˆ‘ä»¬å°†ä¼šç”¨logistic regressionä½œä¸ºä¾‹å­ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å…ˆæ¥å…³æ³¨ä¸‹log cost functionï¼Œè¿™é‡Œï¼Œ**æˆ‘ä»¬å–labelä¸º-1å’Œ+1**ï¼Œå› ä¸ºè¿™æ ·å¾—åˆ°çš„cost functionæ¯”0,1ä¸‹çš„è®¡ç®—æ›´åŠ ç®€å•\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$\nè¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†-1å’Œ+1ä½œä¸ºæ ‡ç­¾å€¼ï¼Œå’Œå¤§å¤šæ•°æ•™æä¸­ä¸ä¸€æ ·ï¼Œå¤§å®¶å¯ä»¥ä¸‹æ¥è‡ªå·±æ¨å¯¼ä¸€ä¸‹\\\\(J( \\omega)\\\\)ï¼Œè¿™ç§å†™æ³•å¹¿æ³›çš„åº”ç”¨åœ¨äº†æ¯”è¾ƒlogistic regressionå’ŒSVMä¸¤å¤§åˆ†ç±»å™¨çš„æ–‡çŒ®ä¸­ï¼Œå¸Œæœ›å¤§å®¶ç†ŸçŸ¥ã€‚\n\næ­¤å¤„æˆ‘ä»¬å¯¹åŸå§‹çš„likehood functionåŠ ä¸Šäº† \\\\(- \\frac{1}{m}\\\\)çš„ç³»æ•°ï¼ŒåŒæ ·ï¼Œå½“æˆ‘ä»¬æŠŠ \\\\(J( \\omega)\\\\)å¸¦å…¥åˆ°convex functionçš„å®šä¹‰ä¸­ï¼Œå¯ä»¥éªŒè¯ä¸Šå¼ä¸ºconvex functionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\\\(J( \\omega)\\\\)æ˜¯\\\\( \\omega\\\\)çš„å‡½æ•°ã€‚\n\nå…¶å®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†log cost functionå±•å¼€åï¼Œåˆ©ç”¨æœ€åŸºæœ¬çš„å‡½æ•°convexå’Œconcaveæ€§è´¨æ¥è·å¾—ä¸Šå¼æ˜¯convex functionçš„ç»“è®ºï¼Œç¢äºå…¬å¼å®åœ¨å¤ªéš¾æ‰“ï¼Œå°±ç•™ç»™å¤§å®¶å»è¯æ˜å§ã€‚\n\nOKï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œæˆ‘ä»¬ä¸€å®šæ˜¯å¯ä»¥ç”¨gradient descentå»æ±‚è§£çš„ã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨newton's methodå‘¢ï¼Ÿ\n\nNewton's methodçš„åŸºæœ¬åŸç†è¯¦è§referenceï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œé‚£ä¹ˆæ ¹æ®second order conditionå¯ä»¥çŸ¥é“ï¼Œå®ƒçš„Hessian matrixä¸€å®šæ˜¯positive semi-definiteçš„ã€‚å¦‚æœæˆ‘ä»¬åŠ ä¸Šäº†L2 regularizerï¼Œ**ç”±äºL2 regularizeræœ¬èº«å°±æ˜¯ä¸€ä¸ªstrict convex function**ï¼Œé‚£ä¹ˆlog cost functionå°±ä¸€å®šæ˜¯strict convex functionäº†ï¼Œä¹Ÿå°±æ˜¯ï¼š\n$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$\nå› æ­¤ï¼Œåœ¨log cost functionä¸­ï¼Œ**Hessian matrixæ˜¯positive definiteçš„**ï¼Œå®Œå…¨æ»¡è¶³newton's method çš„è¦æ±‚ã€‚åŒæ ·ï¼Œç±»ä¼¼äºä¸Šä¸€éƒ¨åˆ†ï¼Œnewton's methodä¹Ÿå¯ä»¥æ‰¾åˆ°log cost functionçš„å…¨å±€æœ€ä¼˜ã€‚\n## Sum up\nOKï¼Œæˆ‘ä»¬è¯´åˆ°è¿™é‡Œä¹Ÿç¡®å®è®²äº†ä¸å°‘ï¼Œè¿™ç¯‡blogæœ‰äº›å†—é•¿ï¼Œå¸Œæœ›æœ‹å‹ä»¬ä¸è¦ç„¦è™‘ã€‚æ€»ä½“æ¥è¯´ï¼Œæˆ‘æƒ³è¡¨è¾¾çš„æ˜¯ä»¥ä¸‹å‡ ä¸ªè§‚ç‚¹ï¼š\n* Machine learningä¸­æˆ‘ä»¬å¯»æ±‚çš„å…¶å®å°±æ˜¯objective functionä¸€ä¸ªå…¨å±€æœ€ä¼˜å€¼ï¼Œè¿™äº›é—®é¢˜æ˜¯é€šè¿‡gradient descentç­‰æ–¹æ³•è§£å†³çš„ï¼›\n* Gradient descentå’Œnewton's methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œä»–ä»¬éƒ½å¯ä»¥å¯¹äºconvex functionè·å¾—å…¨å±€æœ€ä¼˜ï¼›\n* å¯¹äºnon-convex optimizationé—®é¢˜ï¼Œstochastic gradient descentä¹Ÿå¾ˆæœ‰æ•ˆæœï¼Œæˆ‘ä»¬åç»­å†æ…¢æ…¢å­¦ä¹ ã€‚\n\nå¥½äº†ï¼Œæ ¸å¿ƒæ€æƒ³å°±è¿™ä¸‰ç‚¹ï¼Œä»Šå¤©å…ˆè¯´è¿™ä¹ˆå¤šï¼\n\n## Reference\n* [EE364, Convex Optimization Stanford University](https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf)\n* [Regularized Logistic Regression is Strictly Convex](http://qwone.com/~jason/writing/convexLR.pdf)\n* [XinyiLIå¤§ç¥çš„blog](https://www.yangzhou301.com/2016/03/14/826442654/)\n* [Liner regression](https://en.wikipedia.org/wiki/Linear_regression)\n* [Logsitc regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n* [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n\n","slug":"ml-convex-opt","published":1,"updated":"2018-11-19T06:34:15.106Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p6000htr8l2853x7ox","content":"<p>è®°å¾—æˆ‘åœ¨å’Œä¼˜ç”·ä¸€èµ·ç ”ç©¶logistic regressionçš„æ—¶å€™ï¼Œä»–é—®äº†æˆ‘å‡ ä¸ªéå¸¸å°–é”çš„é—®é¢˜ï¼Œè®©æˆ‘é¡¿æ—¶å“‘å£æ— è¨€</p>\n<ul>\n<li>æ€ä¹ˆä¿è¯logistic regressioné€šè¿‡gradient descentæ‰¾åˆ°çš„æ˜¯æœ€ä¼˜è§£ï¼›</li>\n<li>ä¸ºä»€ä¹ˆlogistic regressionå¯ä»¥ç”¨newtonâ€™s methodå‘¢ï¼Ÿ</li>\n<li>Newtonâ€™s methodä¸­Hessian matrixå¿…é¡»positive definiteæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Œlog cost functionèƒ½ä¿è¯å—ï¼Ÿ</li>\n</ul>\n<p>è¿™äº›ç»†èŠ‚é—®é¢˜ï¼Œè¯´å®è¯æˆ‘ä¹Ÿæ²¡æœ‰è®¤çœŸçš„æƒ³è¿‡ã€‚åœ¨å¤¸å¥–ä»–ä¹‹ä½™ï¼Œæˆ‘ä»¬ä¹Ÿä¸€èµ·å¼€å§‹äº†ç ”ç©¶ï¼Œå¸Œæœ›ä»ä¸­å­¦ä¹ åˆ°ä¸€äº›æ›´æ·±å±‚çš„ä¸œè¥¿ï¼Œè¶ç€ç°åœ¨æœ‰ä¸ªblogåˆ†äº«ç»™å¤§å®¶<br><a id=\"more\"></a></p>\n<h2 id=\"å‡¸å‡½æ•°-Convex-function\"><a href=\"#å‡¸å‡½æ•°-Convex-function\" class=\"headerlink\" title=\"å‡¸å‡½æ•°(Convex function)\"></a>å‡¸å‡½æ•°(Convex function)</h2><p>åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºæœ¯è¯­çš„å€¡è®®ã€‚ä¸­æ–‡é‡Œçš„â€œå‡¸å‡½æ•°â€ï¼Œçœ‹ä¸Šå»æ˜¯å‡¹ä¸‹å»çš„ï¼Œå¯¹åº”çš„ï¼Œä¸­æ–‡é‡Œçš„â€œå‡¹å‡½æ•°â€çœ‹ä¸Šå»å‡¸èµ·æ¥çš„ï¼Œamazingå§ï¼Ÿè¿™æ˜¯æœ‰ä¸€å®šå†å²åŸå› çš„ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å»æŸ¥é˜…ä¸‹èµ„æ–™ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å†å¤è¿°ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è®©å¤§å®¶äº§ç”Ÿè¯¯è§£ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä½¿ç”¨è‹±æ–‡ï¼Œ<strong>convex function</strong>å’Œ<strong>concave function</strong>.è¿™æ ·ä¼šé¿å…å¾ˆå¤šä¸å¿…è¦çš„éº»çƒ¦ã€‚</p>\n<p>OKï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ï¼Œconvex function</p>\n<p>å¯¹äºä¸€ç»´å‡½æ•° \\(f(x)\\)æ¥è¯´ï¼Œåœ¨å®šä¹‰åŸŸå†…çš„ä»»æ„å€¼ \\(a\\)å’Œ\\(b\\)ï¼Œå¯¹äºä»»æ„çš„ \\( 0 \\leq \\theta \\leq 1\\)ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°ä¸ºconvex function<br>$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$<br>æˆ‘ä»¬å†ç”¨å›¾ç‰‡ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png\" alt=\"\"><br>æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“å…¬å¼ä¸­ç­‰å·å»æ‰çš„æ—¶å€™ï¼Œå‡½æ•°å°±æ˜¯<strong>strictly convex function</strong>.</p>\n<p>Convex functionå…·æœ‰ä¸€å®šçš„æ€§è´¨ï¼Œæˆ‘ä»¬ç®€å•çš„æè¿°ä¸€ä¸‹ã€‚</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>å¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…<strong>ä¸€é˜¶å¯å¯¼</strong>ï¼Œä¸”å¯¼æ•°ä¸º<br>$$     \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},â€¦,  \\frac{\\partial f(x)}{x_n})$$<br>é‚£ä¹ˆ \\(f\\)æ˜¯convex functionçš„<strong>å……è¦æ¡ä»¶</strong>æ˜¯ï¼šå¯¹äºå®šä¹‰åŸŸå†…ä»»æ„ \\(x\\) å’Œ \\(y\\)<br>$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$<br>OKï¼Œå†æ¥å¼ å›¾ç‰‡ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼š</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png\" alt=\"\"></p>\n<p>å…¶å®ç®€å•çš„æ¥è®²ï¼Œå°±æ˜¯å¯¹äºconvex function \\(f\\)ï¼Œå®ƒçš„å‡½æ•°å€¼æ°¸è¿œå¤§äºç­‰äºåˆ‡çº¿ä¸Šçš„å€¼ï¼</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>å¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…<strong>äºŒé˜¶å¯å¯¼</strong>ï¼Œä¸” \\(n\\) ç»´æ–¹é˜µHessian matrixçš„å…ƒç´ ä¸º<br>$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,â€¦,n$$<br>å½“ä¸”ä»…å½“Hessian matrix positive semi-defniteçš„æ—¶å€™ï¼Œ\\(f\\) æ˜¯convex functionã€‚ä»¥ä¸Šäº’ä¸º<strong>å……è¦æ¡ä»¶</strong>ã€‚è¿™é‡Œçš„è¯æ˜æˆ‘ä¸æƒ³å±•å¼€è®²ï¼Œåœ¨åé¢æˆ‘ä¼šç»™å‡ºreferenceé“¾æ¥ã€‚</p>\n<hr>\n<p>ä¸‹é¢ç»™å‡ºä¸€äº› \\(\\Bbb R\\) ç©ºé—´ä¸‹å¸¸è§çš„convex functionï¼š</p>\n<ul>\n<li>çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\)</li>\n<li>æŒ‡æ•°å‡½æ•°ï¼š\\(f(x)=e^ {ax}\\)</li>\n<li>è´Ÿç†µå‡½æ•°ï¼š \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>å¯¹åº”çš„ï¼Œä¸€äº›å¸¸è§çš„concave functionï¼š</p>\n<ul>\n<li>çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\)</li>\n<li>å¯¹æ•°å‡½æ•°ï¼š \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>å…¶ä¸­å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§å‡½æ•°æ—¢æ˜¯convexä¹Ÿæ˜¯concaveå‡½æ•°ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œè¿™å’Œå®ƒæœ¬èº«çš„first order conditionä¸ºå¸¸æ•°æœ‰å…³ã€‚</p>\n<p>ä»¥ä¸Šå°±æ˜¯convex functionçš„ä¸€ä¸ªç®€å•ä»‹ç»ï¼Œä½ ä¹Ÿè®¸ä¼šé—®ï¼Œä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆå¤šåŠ›æ°”æ¥ä»‹ç»convex function. å…¶å®ï¼Œåœ¨machine learningä¸­ï¼Œconvex functionçš„ä¼˜åŒ–æ˜¯éå¸¸é‡è¦çš„ï¼Œå¾ˆå¤šç®—æ³•è¯´åˆ°åº•ï¼Œéƒ½æ˜¯è¦optimizeä¸€ä¸ªconvex functionï¼Œæˆ‘ä»¬ä¼šç”¨liner regressionå’Œlogistic regressionä¸ºä¾‹å­ï¼Œè¿›ä¸€æ­¥ä»convex functionçš„ç®€ä»‹è¿‡æ¸¡åˆ°gradient descentå’Œnewtonâ€™s method.</p>\n<h2 id=\"æ¢¯åº¦ä¸‹é™æ³•-Gradient-descent\"><a href=\"#æ¢¯åº¦ä¸‹é™æ³•-Gradient-descent\" class=\"headerlink\" title=\"æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)\"></a>æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)</h2><p>å…³äºgradient descentï¼Œæˆ‘ä»¬ä½¿ç”¨liner regressionä½œä¸ºä¾‹å­æ¥è®¨è®ºã€‚Liner regressionç®—æ³•çš„å®è´¨æ˜¯least square methodï¼Œä»–çš„cost functionæ˜¯<br>$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$<br>å¯¹äºliner regressionæ¥è¯´ï¼Œç®—æ³•çš„å®è´¨å°±æ˜¯å»æ±‚å‡º\\( J( \\theta) \\) <strong>ä»¥\\( \\theta\\)ä¸ºå‚æ•°</strong>çš„minimumï¼Œgradient descentç®—æ³•çš„ä½œç”¨å°±æ˜¯å»å®ç°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œgradient descentçš„åŸºç¡€çŸ¥è¯†è¯¦è§reference. </p>\n<p>é‚£ä¹ˆé’ˆå¯¹cost functionï¼Œgradient descentæ˜¯å¦‚ä½•ä¿è¯æ”¶æ•›çš„å‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹</p>\n<p>å¯¹äº\\(J( \\theta)\\)ï¼Œæˆ‘ä»¬å°†å…¶å¸¦å…¥convex functionçš„å®šä¹‰å…¬å¼ä¸­ï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬çš„è‡ªå˜é‡æ˜¯\\( \\theta\\)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨å¯¼è¯æ˜è¯¥å¼æˆç«‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œleast square cost functionæ˜¯convex function.</p>\n<p>æ—¢ç„¶æœ‰è¿™ä¸ªç»“è®ºäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œleast square cost functionä½œä¸ºconvex functionï¼Œæ˜¯å­˜åœ¨å…¨å±€æœ€å°å€¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œgradient descentä¸ä¼šå‡ºç°é™·å…¥å±€éƒ¨æœ€ä¼˜æ— æ³•è‡ªæ‹”çš„ç°è±¡ï¼Œåªè¦gradient descentä¿è¯å‚æ•°è¶³å¤Ÿå¥½çš„æƒ…å†µä¸‹ï¼Œç†è®ºä¸Šï¼Œæ˜¯å®Œå…¨å¯ä»¥å¾ˆå¥½çš„é€¼è¿‘å…¨å±€æœ€ä¼˜çš„è§£çš„ã€‚</p>\n<blockquote>\n<p>Gradien descentç®—æ³•æœ¬èº«å¹¶ä¸èƒ½ä¿è¯è·å¾—å…¨å±€æœ€å°å€¼ï¼Œåªæœ‰åœ¨objective functionæ˜¯convex functionçš„æ—¶å€™æ‰å¯ä»¥ä¿è¯</p>\n</blockquote>\n<p>ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œå³è¾¹çš„object functionæ˜¯non-convex functionï¼Œå› è€Œå¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼æ— æ³•è‡ªæ‹”ï¼Œè€Œå·¦è¾¹çš„objective functionæ˜¯ä¸€ä¸ªæ ‡å‡†çš„convex functionï¼Œåœ¨gradient descentå‚æ•°åˆç†çš„å‰æä¸‹ï¼Œå¯ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png\" alt=\"\"></p>\n<p>å½“ç„¶ï¼Œgradien descentçš„ä¸€äº›æ”¹è¿›æ–¹æ³•ï¼Œä¾‹å¦‚stochastic gradient descentåœ¨è§£å†³non-convex optimizationä¸Šæœ‰ä¸€äº›å¸®åŠ©ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä¸åšè®¨è®ºï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†å†™ã€‚</p>\n<p>ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼Œgradient descentä¸ä»…ä»…æ˜¯minimize liner regressionçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯convex optimizationçš„ä¸€ç§ç†æƒ³æ–¹æ³•</p>\n<h2 id=\"ç‰›é¡¿æ³•-Newtonâ€™s-method\"><a href=\"#ç‰›é¡¿æ³•-Newtonâ€™s-method\" class=\"headerlink\" title=\"ç‰›é¡¿æ³•(Newtonâ€™s method)\"></a>ç‰›é¡¿æ³•(Newtonâ€™s method)</h2><p>Newtonâ€™s method è¿™å—å†…å®¹ï¼Œæˆ‘ä»¬å°†ä¼šç”¨logistic regressionä½œä¸ºä¾‹å­ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å…ˆæ¥å…³æ³¨ä¸‹log cost functionï¼Œè¿™é‡Œï¼Œ<strong>æˆ‘ä»¬å–labelä¸º-1å’Œ+1</strong>ï¼Œå› ä¸ºè¿™æ ·å¾—åˆ°çš„cost functionæ¯”0,1ä¸‹çš„è®¡ç®—æ›´åŠ ç®€å•<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$<br>è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†-1å’Œ+1ä½œä¸ºæ ‡ç­¾å€¼ï¼Œå’Œå¤§å¤šæ•°æ•™æä¸­ä¸ä¸€æ ·ï¼Œå¤§å®¶å¯ä»¥ä¸‹æ¥è‡ªå·±æ¨å¯¼ä¸€ä¸‹\\(J( \\omega)\\)ï¼Œè¿™ç§å†™æ³•å¹¿æ³›çš„åº”ç”¨åœ¨äº†æ¯”è¾ƒlogistic regressionå’ŒSVMä¸¤å¤§åˆ†ç±»å™¨çš„æ–‡çŒ®ä¸­ï¼Œå¸Œæœ›å¤§å®¶ç†ŸçŸ¥ã€‚</p>\n<p>æ­¤å¤„æˆ‘ä»¬å¯¹åŸå§‹çš„likehood functionåŠ ä¸Šäº† \\(- \\frac{1}{m}\\)çš„ç³»æ•°ï¼ŒåŒæ ·ï¼Œå½“æˆ‘ä»¬æŠŠ \\(J( \\omega)\\)å¸¦å…¥åˆ°convex functionçš„å®šä¹‰ä¸­ï¼Œå¯ä»¥éªŒè¯ä¸Šå¼ä¸ºconvex functionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(J( \\omega)\\)æ˜¯\\( \\omega\\)çš„å‡½æ•°ã€‚</p>\n<p>å…¶å®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†log cost functionå±•å¼€åï¼Œåˆ©ç”¨æœ€åŸºæœ¬çš„å‡½æ•°convexå’Œconcaveæ€§è´¨æ¥è·å¾—ä¸Šå¼æ˜¯convex functionçš„ç»“è®ºï¼Œç¢äºå…¬å¼å®åœ¨å¤ªéš¾æ‰“ï¼Œå°±ç•™ç»™å¤§å®¶å»è¯æ˜å§ã€‚</p>\n<p>OKï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œæˆ‘ä»¬ä¸€å®šæ˜¯å¯ä»¥ç”¨gradient descentå»æ±‚è§£çš„ã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨newtonâ€™s methodå‘¢ï¼Ÿ</p>\n<p>Newtonâ€™s methodçš„åŸºæœ¬åŸç†è¯¦è§referenceï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œé‚£ä¹ˆæ ¹æ®second order conditionå¯ä»¥çŸ¥é“ï¼Œå®ƒçš„Hessian matrixä¸€å®šæ˜¯positive semi-definiteçš„ã€‚å¦‚æœæˆ‘ä»¬åŠ ä¸Šäº†L2 regularizerï¼Œ<strong>ç”±äºL2 regularizeræœ¬èº«å°±æ˜¯ä¸€ä¸ªstrict convex function</strong>ï¼Œé‚£ä¹ˆlog cost functionå°±ä¸€å®šæ˜¯strict convex functionäº†ï¼Œä¹Ÿå°±æ˜¯ï¼š<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$<br>å› æ­¤ï¼Œåœ¨log cost functionä¸­ï¼Œ<strong>Hessian matrixæ˜¯positive definiteçš„</strong>ï¼Œå®Œå…¨æ»¡è¶³newtonâ€™s method çš„è¦æ±‚ã€‚åŒæ ·ï¼Œç±»ä¼¼äºä¸Šä¸€éƒ¨åˆ†ï¼Œnewtonâ€™s methodä¹Ÿå¯ä»¥æ‰¾åˆ°log cost functionçš„å…¨å±€æœ€ä¼˜ã€‚</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OKï¼Œæˆ‘ä»¬è¯´åˆ°è¿™é‡Œä¹Ÿç¡®å®è®²äº†ä¸å°‘ï¼Œè¿™ç¯‡blogæœ‰äº›å†—é•¿ï¼Œå¸Œæœ›æœ‹å‹ä»¬ä¸è¦ç„¦è™‘ã€‚æ€»ä½“æ¥è¯´ï¼Œæˆ‘æƒ³è¡¨è¾¾çš„æ˜¯ä»¥ä¸‹å‡ ä¸ªè§‚ç‚¹ï¼š</p>\n<ul>\n<li>Machine learningä¸­æˆ‘ä»¬å¯»æ±‚çš„å…¶å®å°±æ˜¯objective functionä¸€ä¸ªå…¨å±€æœ€ä¼˜å€¼ï¼Œè¿™äº›é—®é¢˜æ˜¯é€šè¿‡gradient descentç­‰æ–¹æ³•è§£å†³çš„ï¼›</li>\n<li>Gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œä»–ä»¬éƒ½å¯ä»¥å¯¹äºconvex functionè·å¾—å…¨å±€æœ€ä¼˜ï¼›</li>\n<li>å¯¹äºnon-convex optimizationé—®é¢˜ï¼Œstochastic gradient descentä¹Ÿå¾ˆæœ‰æ•ˆæœï¼Œæˆ‘ä»¬åç»­å†æ…¢æ…¢å­¦ä¹ ã€‚</li>\n</ul>\n<p>å¥½äº†ï¼Œæ ¸å¿ƒæ€æƒ³å°±è¿™ä¸‰ç‚¹ï¼Œä»Šå¤©å…ˆè¯´è¿™ä¹ˆå¤šï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"noopener\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"noopener\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"noopener\">XinyiLIå¤§ç¥çš„blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"noopener\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"noopener\">Newtonâ€™s method</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"convex optimization","path":"tags/convex-optimization/"},{"name":"newton's method","path":"tags/newton-s-method/"}],"excerpt":"<p>è®°å¾—æˆ‘åœ¨å’Œä¼˜ç”·ä¸€èµ·ç ”ç©¶logistic regressionçš„æ—¶å€™ï¼Œä»–é—®äº†æˆ‘å‡ ä¸ªéå¸¸å°–é”çš„é—®é¢˜ï¼Œè®©æˆ‘é¡¿æ—¶å“‘å£æ— è¨€</p>\n<ul>\n<li>æ€ä¹ˆä¿è¯logistic regressioné€šè¿‡gradient descentæ‰¾åˆ°çš„æ˜¯æœ€ä¼˜è§£ï¼›</li>\n<li>ä¸ºä»€ä¹ˆlogistic regressionå¯ä»¥ç”¨newtonâ€™s methodå‘¢ï¼Ÿ</li>\n<li>Newtonâ€™s methodä¸­Hessian matrixå¿…é¡»positive definiteæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Œlog cost functionèƒ½ä¿è¯å—ï¼Ÿ</li>\n</ul>\n<p>è¿™äº›ç»†èŠ‚é—®é¢˜ï¼Œè¯´å®è¯æˆ‘ä¹Ÿæ²¡æœ‰è®¤çœŸçš„æƒ³è¿‡ã€‚åœ¨å¤¸å¥–ä»–ä¹‹ä½™ï¼Œæˆ‘ä»¬ä¹Ÿä¸€èµ·å¼€å§‹äº†ç ”ç©¶ï¼Œå¸Œæœ›ä»ä¸­å­¦ä¹ åˆ°ä¸€äº›æ›´æ·±å±‚çš„ä¸œè¥¿ï¼Œè¶ç€ç°åœ¨æœ‰ä¸ªblogåˆ†äº«ç»™å¤§å®¶<br></p>","more":"</p>\n<h2 id=\"å‡¸å‡½æ•°-Convex-function\"><a href=\"#å‡¸å‡½æ•°-Convex-function\" class=\"headerlink\" title=\"å‡¸å‡½æ•°(Convex function)\"></a>å‡¸å‡½æ•°(Convex function)</h2><p>åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºæœ¯è¯­çš„å€¡è®®ã€‚ä¸­æ–‡é‡Œçš„â€œå‡¸å‡½æ•°â€ï¼Œçœ‹ä¸Šå»æ˜¯å‡¹ä¸‹å»çš„ï¼Œå¯¹åº”çš„ï¼Œä¸­æ–‡é‡Œçš„â€œå‡¹å‡½æ•°â€çœ‹ä¸Šå»å‡¸èµ·æ¥çš„ï¼Œamazingå§ï¼Ÿè¿™æ˜¯æœ‰ä¸€å®šå†å²åŸå› çš„ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å»æŸ¥é˜…ä¸‹èµ„æ–™ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å†å¤è¿°ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è®©å¤§å®¶äº§ç”Ÿè¯¯è§£ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä½¿ç”¨è‹±æ–‡ï¼Œ<strong>convex function</strong>å’Œ<strong>concave function</strong>.è¿™æ ·ä¼šé¿å…å¾ˆå¤šä¸å¿…è¦çš„éº»çƒ¦ã€‚</p>\n<p>OKï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ï¼Œconvex function</p>\n<p>å¯¹äºä¸€ç»´å‡½æ•° \\(f(x)\\)æ¥è¯´ï¼Œåœ¨å®šä¹‰åŸŸå†…çš„ä»»æ„å€¼ \\(a\\)å’Œ\\(b\\)ï¼Œå¯¹äºä»»æ„çš„ \\( 0 \\leq \\theta \\leq 1\\)ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°ä¸ºconvex function<br>$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$<br>æˆ‘ä»¬å†ç”¨å›¾ç‰‡ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-1.png\" alt=\"\"><br>æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“å…¬å¼ä¸­ç­‰å·å»æ‰çš„æ—¶å€™ï¼Œå‡½æ•°å°±æ˜¯<strong>strictly convex function</strong>.</p>\n<p>Convex functionå…·æœ‰ä¸€å®šçš„æ€§è´¨ï¼Œæˆ‘ä»¬ç®€å•çš„æè¿°ä¸€ä¸‹ã€‚</p>\n<h3 id=\"First-order-condition\"><a href=\"#First-order-condition\" class=\"headerlink\" title=\"First order condition\"></a>First order condition</h3><p>å¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…<strong>ä¸€é˜¶å¯å¯¼</strong>ï¼Œä¸”å¯¼æ•°ä¸º<br>$$     \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},â€¦,  \\frac{\\partial f(x)}{x_n})$$<br>é‚£ä¹ˆ \\(f\\)æ˜¯convex functionçš„<strong>å……è¦æ¡ä»¶</strong>æ˜¯ï¼šå¯¹äºå®šä¹‰åŸŸå†…ä»»æ„ \\(x\\) å’Œ \\(y\\)<br>$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$<br>OKï¼Œå†æ¥å¼ å›¾ç‰‡ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼š</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-2.png\" alt=\"\"></p>\n<p>å…¶å®ç®€å•çš„æ¥è®²ï¼Œå°±æ˜¯å¯¹äºconvex function \\(f\\)ï¼Œå®ƒçš„å‡½æ•°å€¼æ°¸è¿œå¤§äºç­‰äºåˆ‡çº¿ä¸Šçš„å€¼ï¼</p>\n<h3 id=\"Second-order-condition\"><a href=\"#Second-order-condition\" class=\"headerlink\" title=\"Second order condition\"></a>Second order condition</h3><p>å¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…<strong>äºŒé˜¶å¯å¯¼</strong>ï¼Œä¸” \\(n\\) ç»´æ–¹é˜µHessian matrixçš„å…ƒç´ ä¸º<br>$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,â€¦,n$$<br>å½“ä¸”ä»…å½“Hessian matrix positive semi-defniteçš„æ—¶å€™ï¼Œ\\(f\\) æ˜¯convex functionã€‚ä»¥ä¸Šäº’ä¸º<strong>å……è¦æ¡ä»¶</strong>ã€‚è¿™é‡Œçš„è¯æ˜æˆ‘ä¸æƒ³å±•å¼€è®²ï¼Œåœ¨åé¢æˆ‘ä¼šç»™å‡ºreferenceé“¾æ¥ã€‚</p>\n<hr>\n<p>ä¸‹é¢ç»™å‡ºä¸€äº› \\(\\Bbb R\\) ç©ºé—´ä¸‹å¸¸è§çš„convex functionï¼š</p>\n<ul>\n<li>çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\)</li>\n<li>æŒ‡æ•°å‡½æ•°ï¼š\\(f(x)=e^ {ax}\\)</li>\n<li>è´Ÿç†µå‡½æ•°ï¼š \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>å¯¹åº”çš„ï¼Œä¸€äº›å¸¸è§çš„concave functionï¼š</p>\n<ul>\n<li>çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\)</li>\n<li>å¯¹æ•°å‡½æ•°ï¼š \\(f(x)=logx  \\quad on \\quad \\Bbb R_{++}\\)</li>\n</ul>\n<p>å…¶ä¸­å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§å‡½æ•°æ—¢æ˜¯convexä¹Ÿæ˜¯concaveå‡½æ•°ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œè¿™å’Œå®ƒæœ¬èº«çš„first order conditionä¸ºå¸¸æ•°æœ‰å…³ã€‚</p>\n<p>ä»¥ä¸Šå°±æ˜¯convex functionçš„ä¸€ä¸ªç®€å•ä»‹ç»ï¼Œä½ ä¹Ÿè®¸ä¼šé—®ï¼Œä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆå¤šåŠ›æ°”æ¥ä»‹ç»convex function. å…¶å®ï¼Œåœ¨machine learningä¸­ï¼Œconvex functionçš„ä¼˜åŒ–æ˜¯éå¸¸é‡è¦çš„ï¼Œå¾ˆå¤šç®—æ³•è¯´åˆ°åº•ï¼Œéƒ½æ˜¯è¦optimizeä¸€ä¸ªconvex functionï¼Œæˆ‘ä»¬ä¼šç”¨liner regressionå’Œlogistic regressionä¸ºä¾‹å­ï¼Œè¿›ä¸€æ­¥ä»convex functionçš„ç®€ä»‹è¿‡æ¸¡åˆ°gradient descentå’Œnewtonâ€™s method.</p>\n<h2 id=\"æ¢¯åº¦ä¸‹é™æ³•-Gradient-descent\"><a href=\"#æ¢¯åº¦ä¸‹é™æ³•-Gradient-descent\" class=\"headerlink\" title=\"æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)\"></a>æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)</h2><p>å…³äºgradient descentï¼Œæˆ‘ä»¬ä½¿ç”¨liner regressionä½œä¸ºä¾‹å­æ¥è®¨è®ºã€‚Liner regressionç®—æ³•çš„å®è´¨æ˜¯least square methodï¼Œä»–çš„cost functionæ˜¯<br>$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$<br>å¯¹äºliner regressionæ¥è¯´ï¼Œç®—æ³•çš„å®è´¨å°±æ˜¯å»æ±‚å‡º\\( J( \\theta) \\) <strong>ä»¥\\( \\theta\\)ä¸ºå‚æ•°</strong>çš„minimumï¼Œgradient descentç®—æ³•çš„ä½œç”¨å°±æ˜¯å»å®ç°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œgradient descentçš„åŸºç¡€çŸ¥è¯†è¯¦è§reference. </p>\n<p>é‚£ä¹ˆé’ˆå¯¹cost functionï¼Œgradient descentæ˜¯å¦‚ä½•ä¿è¯æ”¶æ•›çš„å‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹</p>\n<p>å¯¹äº\\(J( \\theta)\\)ï¼Œæˆ‘ä»¬å°†å…¶å¸¦å…¥convex functionçš„å®šä¹‰å…¬å¼ä¸­ï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬çš„è‡ªå˜é‡æ˜¯\\( \\theta\\)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨å¯¼è¯æ˜è¯¥å¼æˆç«‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œleast square cost functionæ˜¯convex function.</p>\n<p>æ—¢ç„¶æœ‰è¿™ä¸ªç»“è®ºäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œleast square cost functionä½œä¸ºconvex functionï¼Œæ˜¯å­˜åœ¨å…¨å±€æœ€å°å€¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œgradient descentä¸ä¼šå‡ºç°é™·å…¥å±€éƒ¨æœ€ä¼˜æ— æ³•è‡ªæ‹”çš„ç°è±¡ï¼Œåªè¦gradient descentä¿è¯å‚æ•°è¶³å¤Ÿå¥½çš„æƒ…å†µä¸‹ï¼Œç†è®ºä¸Šï¼Œæ˜¯å®Œå…¨å¯ä»¥å¾ˆå¥½çš„é€¼è¿‘å…¨å±€æœ€ä¼˜çš„è§£çš„ã€‚</p>\n<blockquote>\n<p>Gradien descentç®—æ³•æœ¬èº«å¹¶ä¸èƒ½ä¿è¯è·å¾—å…¨å±€æœ€å°å€¼ï¼Œåªæœ‰åœ¨objective functionæ˜¯convex functionçš„æ—¶å€™æ‰å¯ä»¥ä¿è¯</p>\n</blockquote>\n<p>ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œå³è¾¹çš„object functionæ˜¯non-convex functionï¼Œå› è€Œå¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼æ— æ³•è‡ªæ‹”ï¼Œè€Œå·¦è¾¹çš„objective functionæ˜¯ä¸€ä¸ªæ ‡å‡†çš„convex functionï¼Œåœ¨gradient descentå‚æ•°åˆç†çš„å‰æä¸‹ï¼Œå¯ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/1/1-3.png\" alt=\"\"></p>\n<p>å½“ç„¶ï¼Œgradien descentçš„ä¸€äº›æ”¹è¿›æ–¹æ³•ï¼Œä¾‹å¦‚stochastic gradient descentåœ¨è§£å†³non-convex optimizationä¸Šæœ‰ä¸€äº›å¸®åŠ©ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä¸åšè®¨è®ºï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†å†™ã€‚</p>\n<p>ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼Œgradient descentä¸ä»…ä»…æ˜¯minimize liner regressionçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯convex optimizationçš„ä¸€ç§ç†æƒ³æ–¹æ³•</p>\n<h2 id=\"ç‰›é¡¿æ³•-Newtonâ€™s-method\"><a href=\"#ç‰›é¡¿æ³•-Newtonâ€™s-method\" class=\"headerlink\" title=\"ç‰›é¡¿æ³•(Newtonâ€™s method)\"></a>ç‰›é¡¿æ³•(Newtonâ€™s method)</h2><p>Newtonâ€™s method è¿™å—å†…å®¹ï¼Œæˆ‘ä»¬å°†ä¼šç”¨logistic regressionä½œä¸ºä¾‹å­ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å…ˆæ¥å…³æ³¨ä¸‹log cost functionï¼Œè¿™é‡Œï¼Œ<strong>æˆ‘ä»¬å–labelä¸º-1å’Œ+1</strong>ï¼Œå› ä¸ºè¿™æ ·å¾—åˆ°çš„cost functionæ¯”0,1ä¸‹çš„è®¡ç®—æ›´åŠ ç®€å•<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$<br>è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†-1å’Œ+1ä½œä¸ºæ ‡ç­¾å€¼ï¼Œå’Œå¤§å¤šæ•°æ•™æä¸­ä¸ä¸€æ ·ï¼Œå¤§å®¶å¯ä»¥ä¸‹æ¥è‡ªå·±æ¨å¯¼ä¸€ä¸‹\\(J( \\omega)\\)ï¼Œè¿™ç§å†™æ³•å¹¿æ³›çš„åº”ç”¨åœ¨äº†æ¯”è¾ƒlogistic regressionå’ŒSVMä¸¤å¤§åˆ†ç±»å™¨çš„æ–‡çŒ®ä¸­ï¼Œå¸Œæœ›å¤§å®¶ç†ŸçŸ¥ã€‚</p>\n<p>æ­¤å¤„æˆ‘ä»¬å¯¹åŸå§‹çš„likehood functionåŠ ä¸Šäº† \\(- \\frac{1}{m}\\)çš„ç³»æ•°ï¼ŒåŒæ ·ï¼Œå½“æˆ‘ä»¬æŠŠ \\(J( \\omega)\\)å¸¦å…¥åˆ°convex functionçš„å®šä¹‰ä¸­ï¼Œå¯ä»¥éªŒè¯ä¸Šå¼ä¸ºconvex functionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(J( \\omega)\\)æ˜¯\\( \\omega\\)çš„å‡½æ•°ã€‚</p>\n<p>å…¶å®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†log cost functionå±•å¼€åï¼Œåˆ©ç”¨æœ€åŸºæœ¬çš„å‡½æ•°convexå’Œconcaveæ€§è´¨æ¥è·å¾—ä¸Šå¼æ˜¯convex functionçš„ç»“è®ºï¼Œç¢äºå…¬å¼å®åœ¨å¤ªéš¾æ‰“ï¼Œå°±ç•™ç»™å¤§å®¶å»è¯æ˜å§ã€‚</p>\n<p>OKï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œæˆ‘ä»¬ä¸€å®šæ˜¯å¯ä»¥ç”¨gradient descentå»æ±‚è§£çš„ã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨newtonâ€™s methodå‘¢ï¼Ÿ</p>\n<p>Newtonâ€™s methodçš„åŸºæœ¬åŸç†è¯¦è§referenceï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œé‚£ä¹ˆæ ¹æ®second order conditionå¯ä»¥çŸ¥é“ï¼Œå®ƒçš„Hessian matrixä¸€å®šæ˜¯positive semi-definiteçš„ã€‚å¦‚æœæˆ‘ä»¬åŠ ä¸Šäº†L2 regularizerï¼Œ<strong>ç”±äºL2 regularizeræœ¬èº«å°±æ˜¯ä¸€ä¸ªstrict convex function</strong>ï¼Œé‚£ä¹ˆlog cost functionå°±ä¸€å®šæ˜¯strict convex functionäº†ï¼Œä¹Ÿå°±æ˜¯ï¼š<br>$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$<br>å› æ­¤ï¼Œåœ¨log cost functionä¸­ï¼Œ<strong>Hessian matrixæ˜¯positive definiteçš„</strong>ï¼Œå®Œå…¨æ»¡è¶³newtonâ€™s method çš„è¦æ±‚ã€‚åŒæ ·ï¼Œç±»ä¼¼äºä¸Šä¸€éƒ¨åˆ†ï¼Œnewtonâ€™s methodä¹Ÿå¯ä»¥æ‰¾åˆ°log cost functionçš„å…¨å±€æœ€ä¼˜ã€‚</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>OKï¼Œæˆ‘ä»¬è¯´åˆ°è¿™é‡Œä¹Ÿç¡®å®è®²äº†ä¸å°‘ï¼Œè¿™ç¯‡blogæœ‰äº›å†—é•¿ï¼Œå¸Œæœ›æœ‹å‹ä»¬ä¸è¦ç„¦è™‘ã€‚æ€»ä½“æ¥è¯´ï¼Œæˆ‘æƒ³è¡¨è¾¾çš„æ˜¯ä»¥ä¸‹å‡ ä¸ªè§‚ç‚¹ï¼š</p>\n<ul>\n<li>Machine learningä¸­æˆ‘ä»¬å¯»æ±‚çš„å…¶å®å°±æ˜¯objective functionä¸€ä¸ªå…¨å±€æœ€ä¼˜å€¼ï¼Œè¿™äº›é—®é¢˜æ˜¯é€šè¿‡gradient descentç­‰æ–¹æ³•è§£å†³çš„ï¼›</li>\n<li>Gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œä»–ä»¬éƒ½å¯ä»¥å¯¹äºconvex functionè·å¾—å…¨å±€æœ€ä¼˜ï¼›</li>\n<li>å¯¹äºnon-convex optimizationé—®é¢˜ï¼Œstochastic gradient descentä¹Ÿå¾ˆæœ‰æ•ˆæœï¼Œæˆ‘ä»¬åç»­å†æ…¢æ…¢å­¦ä¹ ã€‚</li>\n</ul>\n<p>å¥½äº†ï¼Œæ ¸å¿ƒæ€æƒ³å°±è¿™ä¸‰ç‚¹ï¼Œä»Šå¤©å…ˆè¯´è¿™ä¹ˆå¤šï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://see.stanford.edu/materials/lsocoee364a/03ConvexFunctions.pdf\" target=\"_blank\" rel=\"noopener\">EE364, Convex Optimization Stanford University</a></li>\n<li><a href=\"http://qwone.com/~jason/writing/convexLR.pdf\" target=\"_blank\" rel=\"noopener\">Regularized Logistic Regression is Strictly Convex</a></li>\n<li><a href=\"https://www.yangzhou301.com/2016/03/14/826442654/\" target=\"_blank\" rel=\"noopener\">XinyiLIå¤§ç¥çš„blog</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener\">Liner regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener\">Logsitc regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"noopener\">Gradient descent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Newton%27s_method\" target=\"_blank\" rel=\"noopener\">Newtonâ€™s method</a></li>\n</ul>"},{"title":"å†æ·±å…¥èŠèŠæ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•","date":"2017-08-11T09:26:56.000Z","_content":"ä¸Šæ¬¡æˆ‘ä»¬ä¸€èµ·èŠåˆ°äº†gradient descentå’Œnewton's methodï¼Œè€Œä¸”æˆ‘ä»¬å·²ç»çŸ¥é“äº†gradient descentå’Œnewton's methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œè¿™æ¬¡æˆ‘ä»¬å°±è·³å‡ºconvex optimizationï¼Œä»æ›´å¤§çš„unconstrained optimizationè§’åº¦æ¥æ¢è®¨ä¸‹è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’ŒåŒºåˆ«ã€‚\n<!--more-->\n\nå‡è®¾æˆ‘ä»¬ç°æœ‰ä¸€ä¸ªçš„optimization taskï¼Œè¦æ±‚objective function \\\\(f(x)\\\\)çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š\n* è€ƒè™‘åˆ°\\\\(f(x)\\\\)çš„æœ€å°å€¼å¾ˆæœ‰å¯èƒ½æ˜¯å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾\\\\( \\nabla f(x)=0\\\\)çš„ç‚¹æ¥ç¡®å®šæœ€å°å€¼ï¼Œè¿™å°±æ˜¯**newton's method**çš„æ€æƒ³\n* æ—¢ç„¶æˆ‘ä»¬è¦å¯»æ‰¾æœ€å°å€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é¡ºç€ä¸€æ¡\\\\(f(x)\\\\)é€æ¸å‡å°çš„è·¯å¾„ï¼Œé¡ºç€è¿™æ¡è·¯å¾„ä¸€ç›´èµ°ä¸‹å»ï¼Œç›´åˆ°ä¸å†å˜å°ï¼Œè¿™å°±æ˜¯**gradient descent**çš„æ€æƒ³\n\nOKï¼Œç®€å•çš„å™è¿°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ­£é¢˜ï¼\n\n## æ³°å‹’çº§æ•°(Taylor series)\né¦–å…ˆæˆ‘ä»¬éœ€è¦å›å¿†ä¸€ä¸‹é«˜ç­‰æ•°å­¦ä¸­é‡è¦çš„Taylor seriesï¼Œå¦‚æœ\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)çš„é¢†åŸŸå†…å…·æœ‰\\\\(n+1\\\\)é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆï¼Œåœ¨è¯¥é¢†åŸŸå†…ï¼Œ\\\\( f(x)\\\\)å¯å±•å¼€æˆ\\\\(n\\\\)é˜¶Taylor seriesï¼Œå¿½ç•¥æ— é™å¤§æ¬¡é¡¹çš„å½¢å¼å°±æ˜¯\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\nå…¶å®åœ¨é«˜ç­‰æ•°å­¦ä¸­å­¦åˆ°Taylor seriesçš„æ—¶å€™ï¼Œæˆ‘æœ¬äººæ˜¯ååˆ†æ— æ„Ÿçš„ï¼Œæˆ‘å¹¶ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿åˆ°åº•æœ‰ä»€ä¹ˆç”¨å¤„ï¼Œç›¸ä¿¡å¾ˆå¤šäººå’Œæˆ‘æœ‰ç›¸ä¼¼çš„ç»å†ã€‚\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\näº‹å®ä¸Šï¼ŒTaylor seriesæ‰€è¡¨ç°çš„æ˜¯ï¼Œå¯¹äº\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)é™„è¿‘çš„ä¸€ä¸ªä¼°è®¡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼Œæ ¹æ®\\\\( x_0\\\\)ç‚¹å¤„çš„å„é˜¶derivativesä¹‹å’Œæ„æˆä¸€ä¸ªæ–°çš„functionï¼Œè¿™ä¸ªfunctionå°±æ˜¯å¯¹\\\\(f(x)\\\\)çš„é€¼è¿‘å’Œæ‹Ÿåˆï¼Œè€Œä¸”è¿™ç§é€¼è¿‘å’Œæ‹Ÿåˆï¼Œéšç€Taylor seriesé˜¶æ•°å¢åŠ è€Œæ›´æ¥è¿‘äºçœŸå®çš„\\\\(f(x)\\\\)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨0é˜¶Taylor seriesæ¥é€¼è¿‘çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±ç²—æš´çš„è®¤ä¸ºï¼Œ\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)é™„è¿‘çš„å€¼å°±éƒ½æ˜¯\\\\(x_0\\\\)ï¼Œè¿™å½“ç„¶å¤ªç²—æš´ç›´æ¥äº†ï¼Œå“ˆå“ˆã€‚\n\næ—¢ç„¶è¿™å¤ªç²—æš´äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç”¨1st order Taylor seriesæ¥åšä¸€ä¸ªé€¼è¿‘å’Œä¼°è®¡ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ï¼›å¦‚æœæˆ‘ä»¬ç”¨2nd order Taylor seriesæ¥ä¼°è®¡å‘¢ï¼Œé‚£å°±æˆäº†newton's methodäº†\n\nOKï¼Œæˆ‘ä»¬ç»§ç»­å¨“å¨“é“æ¥ã€‚\n\n## 1st order Taylor series & gradient descent\nå‡è®¾\\\\(x_k\\\\)æ˜¯ç¬¬kæ¬¡gradient descentè¿­ä»£åçš„\\\\(x\\\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„1st order Taylor series å°±æ˜¯\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\nå…¶ä¸­\\\\(x\\\\)æ˜¯è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ–¹å‘ï¼Œgradient descentçš„ç›®æ ‡å°±æ˜¯è®©\\\\(f(x)\\\\)è¾¾åˆ°å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œä¹Ÿéœ€è¦å°½å¯èƒ½çš„å‡å°æ›´å¤šä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé‚£ä¹ˆ\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\næ˜¾ç„¶ï¼Œä¸Šå¼åº”è¯¥å°½å¯èƒ½çš„å¤§ï¼Œå³**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)è¶Šå¤§è¶Šå¥½**ï¼Œæˆ‘ä»¬ç°åœ¨æŠŠ\\\\((x-x_k)\\\\)åšä¸€ä¸ªæ›¿æ¢ï¼Œç”¨å•ä½å‘é‡\\\\(\\vec g\\\\)å’Œæ ‡é‡\\\\( \\alpha\\\\)åˆ†åˆ«ä»£è¡¨æ–¹å‘å’Œå¤§å°ï¼Œç°åœ¨çš„ä»»åŠ¡å°±å˜æˆäº†\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}â‹… \\vec g)$$\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œ**å¯¹äºä¸¤ä¸ªå‘é‡æ¥è¯´ï¼Œå½“ä»–ä»¬æ–¹å‘ç›¸åæ—¶ï¼Œä»–ä»¬çš„å†…ç§¯æ˜¯æœ€å°çš„**ã€‚\n\n>æ¢¯åº¦æ–¹å‘çš„å®šä¹‰æ˜¯è¯¥ç‚¹æ¢¯åº¦åœ¨æ ‡é‡åœºå¢é•¿æœ€å¿«çš„æ–¹å‘\n\nå› æ­¤å½“\\\\(\\vec g\\\\)çš„æ–¹å‘æ˜¯\\\\( \\vec{\\nabla f(x_k)}\\\\)çš„åæ–¹å‘æ—¶ï¼Œä¸Šå¼å¯ä»¥å–åˆ°æœ€å°å€¼ï¼Œäºæ˜¯å°±æœ‰\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\nåˆ°è¿™ä¸€æ­¥ï¼Œæ˜¯ä¸æ˜¯çœ‹åˆ°äº†ç†Ÿæ‚‰çš„gradient descentå‘¢ï¼Œyeah mateï¼We make it!\n## 2nd order Taylor series & newton's method\nå’Œä¸Šé¢çš„gradient descentç›¸ä¼¼ï¼Œå‡è®¾\\\\(x_k\\\\)æ˜¯ç¬¬\\\\(k\\\\)æ¬¡newton's methodè¿­ä»£åçš„\\\\(x\\\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„2nd order Taylor series æ˜¯\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\næˆ‘ä»¬å¯¹ç­‰å·ä¸¤è¾¹åŒæ—¶å¯¹\\\\(x\\\\)æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸ºé›¶\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\nç”±äºnewton's methodçš„åŸç†å°±æ˜¯é€šè¿‡\\\\(\\nabla f(x)=0\\\\)æ¥å¯»æ‰¾æœ€å°å€¼ï¼Œ**æ•…ä¸Šå¼ä¸ºé›¶çš„è§£\\\\(x\\\\)å…¶å®å°±æ˜¯newton's methodåœ¨\\\\(k+1\\\\)æ¬¡è¿­ä»£åçš„æ–°çš„\\\\(x\\\\)å€¼**ã€‚å…¶ä¸­\\\\(\\nabla f(x_k)\\\\)æ˜¯\\\\(x_k\\\\)å¤„çš„ä¸€é˜¶å¯¼æ•°ï¼Œ\\\\( \\nabla^2 f(x_k)\\\\)æ˜¯\\\\(x_k\\\\)å¤„çš„äºŒé˜¶å¯¼æ•°HessiançŸ©é˜µå…ƒç´ \n\næˆ‘ä»¬ä»¤\\\\(\\nabla f(x_k)=g\\\\)ï¼Œ\\\\(\\nabla^2 f(x_k)=H\\\\)ï¼Œåˆ™ä¸Šå¼å˜æˆ\n$$g+H(x-x_k)=0$$\nè¿›ä¸€æ­¥çš„\n$$x=x_k-H^{-1}g$$\nç”±äº\\\\(-g H^{-1} \\\\) æ˜¯ä¼˜åŒ–çš„å‰è¿›æ–¹å‘ï¼Œåœ¨å¯»æ‰¾æœ€å°å€¼çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯å’Œæ¢¯åº¦æ–¹å‘\\\\(g\\\\)ç›¸åæ‰å¯ä»¥æ›´å¿«çš„ä¸‹é™ï¼Œé‚£ä¹ˆå°±æœ‰\\\\( g^T H^{-1} g > 0\\\\)ï¼Œè¿™ä¸å°±æ˜¯positive definiteçš„å®šä¹‰å—ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œ**HessiançŸ©é˜µæ˜¯positive definiteçš„**ã€‚\n\næƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœHessianæ˜¯negative definiteçš„è¯ï¼Œå‚æ•°æ›´æ–°çš„æ–¹å‘å°±æˆäº†å’Œ\\\\(g\\\\)ç›¸åŒçš„æ–¹å‘ï¼Œnewton's methodå°†ä¼šå‘æ•£ï¼Œè¿™ä¸€ç‚¹ï¼Œä¹Ÿæ˜¯newton's methodçš„ç¼ºç‚¹ã€‚åœ¨objective functionæ˜¯non-convex functionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬\\\\(k\\\\)æ¬¡è¿­ä»£è·å¾—çš„\\\\(x_k\\\\)å¤„çš„Hessian matrix negative definiteï¼Œé‚£ä¹ˆnewton's methodå°†ä¼šå‘æ•£ï¼Œä»è€Œå¯¼è‡´ä¸æ”¶æ•›ã€‚å½“ç„¶ï¼Œä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œåç»­æœ‰æ”¹è¿›çš„BFGSç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæš‚æ—¶ä¸è¯¦ç»†è®¨è®ºã€‚\n## Sum up\nä¸‹é¢æˆ‘ä»¬å†æ¥æ€»ç»“æ€§è´¨çš„å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§æ–¹æ³•ï¼Œæ¥çœ‹ä¸€å¼ å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png)\näº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ä¸€ç§é€¼è¿‘å’Œæ‹Ÿåˆçš„æ€æƒ³ã€‚å‡è®¾ç°åœ¨å¤„äºè¿­ä»£\\\\(k\\\\)æ¬¡ä¹‹åçš„\\\\(x_k\\\\)ç‚¹ï¼Œå¯¹äºobjective functionï¼Œæˆ‘ä»¬ç”¨\\\\(x_k\\\\)ç‚¹çš„Taylor series \\\\(f(x)\\\\)æ¥é€¼è¿‘å’Œæ‹Ÿåˆï¼Œå½“ç„¶äº†ï¼Œä¸Šå›¾æˆ‘ä»¬çœ‹åˆ°ï¼Œgradient descentæ˜¯ç”¨ä¸€æ¬¡functionè€Œnewton's methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡functionï¼Œè¿™æ˜¯äºŒè€…ä¹‹é—´æœ€æ˜¾è‘—çš„åŒºåˆ«ã€‚\n\nå¯¹äºnew's methodï¼Œåœ¨æ‹Ÿåˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡\\\\( \\nabla f(x)=0\\\\)æ±‚å¾—çš„\\\\(x \\_{k+1}\\\\)ç‚¹ä½œä¸ºæ­¤æ¬¡è¿­ä»£çš„ç»“æœï¼Œä¸‹æ¬¡è¿­ä»£æ—¶å€™ï¼Œåˆåœ¨\\\\(x \\_{k+1}\\\\)å¤„æ¬¡è¿›è¡ŒäºŒæ¬¡functionçš„æ‹Ÿåˆï¼Œå¹¶å¦‚æ­¤è¿­ä»£ä¸‹å»ã€‚\n\nNewton's methodé‡‡ç”¨äºŒæ¬¡functionæ¥æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æ„Ÿæ€§çš„ç†è§£ä¸ºï¼Œnewton's methodåœ¨å¯»æ‰¾ä¸‹é™çš„æ–¹å‘æ—¶å€™ï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯æ­¤å¤„objective function valueæ˜¯ä¸æ˜¯å‡å°(ä¸€é˜¶value)ï¼Œè¿˜å…³æ³¨æ­¤å¤„valueä¸‹é™çš„è¶‹åŠ¿å¦‚ä½•(äºŒé˜¶value)ï¼Œè€Œgradient descentåªå…³å¿ƒæ­¤å¤„function valueæ˜¯ä¸æ˜¯å‡å°ï¼Œå› æ­¤newton's methodå¯ä»¥è¿­ä»£æ›´å°‘æ¬¡æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¯¹äºæ ‡å‡†äºŒæ¬¡å‹çš„objective functionï¼Œnewton's methodç”šè‡³å¯ä»¥ä¸€æ¬¡è¿­ä»£å°±æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚\n\nä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢æ‰€è¯´çš„æ ‡å‡†äºŒæ¬¡å‹functionï¼Œå®è´¨ä¸Šæ˜¯convex functionï¼Œåœ¨ä¸€èˆ¬çš„unconstrained optimizationä¸­ï¼Œæ›´å¤šçš„æƒ…å†µåˆ™æ˜¯non-convex optimizationï¼Œå¯¹äºä¸€èˆ¬çš„non-convex optimizationï¼Œnewton's methodæ˜¯ç›¸å¯¹ä¸ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾ä¿è¯Hessian matrixçš„positive definiteã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬ä¼šåŠ å…¥æ­¥é•¿\\\\(\\lambda\\\\)é™åˆ¶ï¼Œé˜²æ­¢å…¶ä¸€æ¬¡è¿­ä»£è¿‡å¤§è€Œå¸¦æ¥è¿­ä»£åHessian matrix negative definiteçš„æƒ…å†µï¼Œå³\n$$x:=x- \\lambda H^{-1} g$$\nå¯¹äºè¿™ç§æ€æƒ³ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œæ˜¯åœ¨æ•´ä½“non-convex functionä¸­å¯»æ‰¾ä¸€ä¸ªå±€éƒ¨çš„convex functionï¼Œé€šè¿‡æ­¥é•¿å°†newton's methodé™åˆ¶åœ¨è¿™ä¸ªå±€éƒ¨ä¸­ï¼Œæœ€åæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ä¸­ã€‚ç”±æ­¤å¯è§ï¼Œnewton's mtehodåœ¨non-convexä¸­å—é™åˆ¶æ¯”è¾ƒå¤§ã€‚\n\nç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºgradient descenté‡‡ç”¨çš„ä¸€æ¬¡functionåšæ‹Ÿåˆï¼Œåªéœ€è¦è€ƒè™‘æ²¿ç€æ¢¯åº¦åæ–¹å‘å¯»æ‰¾æœ€å°å€¼ï¼Œå› æ­¤gradient descenté€‚ç”¨äºå„ç§åœºæ™¯ï¼Œç”šè‡³æ˜¯non-convex optimizationï¼Œè™½ç„¶ä¸èƒ½ä¿è¯æ˜¯å…¨å±€æœ€ä¼˜ï¼Œä½†è‡³å°‘gradient descentæ˜¯å¯ä»¥å€¼å¾—ä¸€è¯•çš„æ–¹æ³•ã€‚\n\nä¸‹é¢æ¥æ€»ç»“ä¸€ä¸‹ï¼š\n* Gradient descent å’Œ newton's methodéƒ½æ˜¯åˆ©ç”¨Taylor serieså¯¹objective functionè¿›è¡Œæ‹Ÿåˆæ¥å®ç°è¿­ä»£çš„ï¼›\n* Gradient descent é‡‡ç”¨ä¸€æ¬¡å‹functionæ‹Ÿåˆè€Œ newton's methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡å‹functionï¼Œå› æ­¤newton's methodè¿­ä»£æ›´è¿…é€Ÿï¼›\n* Newton's methodæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—Hessian matrixçš„é€†ï¼Œåœ¨é«˜ç»´featureæƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æ¯æ¬¡è¿­ä»£ä¼šæ¯”è¾ƒæ…¢ï¼›\n* Newton's methodåœ¨non-convex optimizationä¸­å¾ˆå—é™åˆ¶ï¼Œè€Œgradient descentåˆ™ä¸å—å½±å“ã€‚\n\nå¥½äº†ï¼Œå…ˆå†™è¿™ä¹ˆå¤šï¼Œè¿™å…¶ä¸­çš„çŸ¥è¯†é‡è¿˜æ˜¯å¾ˆæ·±å¥¥çš„ï¼Œä¹Ÿä¸çŸ¥é“è‡ªå·±æœ‰æ²¡æœ‰å™è¿°æ˜ç™½ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥è®¨è®ºï¼\n\n**æœ€åæ„Ÿè°¢ä¼˜ç”·çš„å®è´µæ„è§ï¼**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","source":"_posts/ml-gd-and-nm.md","raw":"---\ntitle: å†æ·±å…¥èŠèŠæ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•\ndate: 2017-08-11 17:26:56\ntags: \n\t- unconstrained optimization\n\t- gradient descent\n\t- newton's method\ncategories: machine learning\n---\nä¸Šæ¬¡æˆ‘ä»¬ä¸€èµ·èŠåˆ°äº†gradient descentå’Œnewton's methodï¼Œè€Œä¸”æˆ‘ä»¬å·²ç»çŸ¥é“äº†gradient descentå’Œnewton's methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œè¿™æ¬¡æˆ‘ä»¬å°±è·³å‡ºconvex optimizationï¼Œä»æ›´å¤§çš„unconstrained optimizationè§’åº¦æ¥æ¢è®¨ä¸‹è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’ŒåŒºåˆ«ã€‚\n<!--more-->\n\nå‡è®¾æˆ‘ä»¬ç°æœ‰ä¸€ä¸ªçš„optimization taskï¼Œè¦æ±‚objective function \\\\(f(x)\\\\)çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š\n* è€ƒè™‘åˆ°\\\\(f(x)\\\\)çš„æœ€å°å€¼å¾ˆæœ‰å¯èƒ½æ˜¯å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾\\\\( \\nabla f(x)=0\\\\)çš„ç‚¹æ¥ç¡®å®šæœ€å°å€¼ï¼Œè¿™å°±æ˜¯**newton's method**çš„æ€æƒ³\n* æ—¢ç„¶æˆ‘ä»¬è¦å¯»æ‰¾æœ€å°å€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é¡ºç€ä¸€æ¡\\\\(f(x)\\\\)é€æ¸å‡å°çš„è·¯å¾„ï¼Œé¡ºç€è¿™æ¡è·¯å¾„ä¸€ç›´èµ°ä¸‹å»ï¼Œç›´åˆ°ä¸å†å˜å°ï¼Œè¿™å°±æ˜¯**gradient descent**çš„æ€æƒ³\n\nOKï¼Œç®€å•çš„å™è¿°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ­£é¢˜ï¼\n\n## æ³°å‹’çº§æ•°(Taylor series)\né¦–å…ˆæˆ‘ä»¬éœ€è¦å›å¿†ä¸€ä¸‹é«˜ç­‰æ•°å­¦ä¸­é‡è¦çš„Taylor seriesï¼Œå¦‚æœ\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)çš„é¢†åŸŸå†…å…·æœ‰\\\\(n+1\\\\)é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆï¼Œåœ¨è¯¥é¢†åŸŸå†…ï¼Œ\\\\( f(x)\\\\)å¯å±•å¼€æˆ\\\\(n\\\\)é˜¶Taylor seriesï¼Œå¿½ç•¥æ— é™å¤§æ¬¡é¡¹çš„å½¢å¼å°±æ˜¯\n$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +...+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$\nå…¶å®åœ¨é«˜ç­‰æ•°å­¦ä¸­å­¦åˆ°Taylor seriesçš„æ—¶å€™ï¼Œæˆ‘æœ¬äººæ˜¯ååˆ†æ— æ„Ÿçš„ï¼Œæˆ‘å¹¶ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿åˆ°åº•æœ‰ä»€ä¹ˆç”¨å¤„ï¼Œç›¸ä¿¡å¾ˆå¤šäººå’Œæˆ‘æœ‰ç›¸ä¼¼çš„ç»å†ã€‚\n\n> In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\näº‹å®ä¸Šï¼ŒTaylor seriesæ‰€è¡¨ç°çš„æ˜¯ï¼Œå¯¹äº\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)é™„è¿‘çš„ä¸€ä¸ªä¼°è®¡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼Œæ ¹æ®\\\\( x_0\\\\)ç‚¹å¤„çš„å„é˜¶derivativesä¹‹å’Œæ„æˆä¸€ä¸ªæ–°çš„functionï¼Œè¿™ä¸ªfunctionå°±æ˜¯å¯¹\\\\(f(x)\\\\)çš„é€¼è¿‘å’Œæ‹Ÿåˆï¼Œè€Œä¸”è¿™ç§é€¼è¿‘å’Œæ‹Ÿåˆï¼Œéšç€Taylor seriesé˜¶æ•°å¢åŠ è€Œæ›´æ¥è¿‘äºçœŸå®çš„\\\\(f(x)\\\\)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨0é˜¶Taylor seriesæ¥é€¼è¿‘çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±ç²—æš´çš„è®¤ä¸ºï¼Œ\\\\( f(x)\\\\)åœ¨ç‚¹\\\\( x_0\\\\)é™„è¿‘çš„å€¼å°±éƒ½æ˜¯\\\\(x_0\\\\)ï¼Œè¿™å½“ç„¶å¤ªç²—æš´ç›´æ¥äº†ï¼Œå“ˆå“ˆã€‚\n\næ—¢ç„¶è¿™å¤ªç²—æš´äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç”¨1st order Taylor seriesæ¥åšä¸€ä¸ªé€¼è¿‘å’Œä¼°è®¡ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ï¼›å¦‚æœæˆ‘ä»¬ç”¨2nd order Taylor seriesæ¥ä¼°è®¡å‘¢ï¼Œé‚£å°±æˆäº†newton's methodäº†\n\nOKï¼Œæˆ‘ä»¬ç»§ç»­å¨“å¨“é“æ¥ã€‚\n\n## 1st order Taylor series & gradient descent\nå‡è®¾\\\\(x_k\\\\)æ˜¯ç¬¬kæ¬¡gradient descentè¿­ä»£åçš„\\\\(x\\\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„1st order Taylor series å°±æ˜¯\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$\nå…¶ä¸­\\\\(x\\\\)æ˜¯è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ–¹å‘ï¼Œgradient descentçš„ç›®æ ‡å°±æ˜¯è®©\\\\(f(x)\\\\)è¾¾åˆ°å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œä¹Ÿéœ€è¦å°½å¯èƒ½çš„å‡å°æ›´å¤šä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé‚£ä¹ˆ\n$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$\næ˜¾ç„¶ï¼Œä¸Šå¼åº”è¯¥å°½å¯èƒ½çš„å¤§ï¼Œå³**\\\\(- \\nabla f(x_k)(x-x_k)\\\\)è¶Šå¤§è¶Šå¥½**ï¼Œæˆ‘ä»¬ç°åœ¨æŠŠ\\\\((x-x_k)\\\\)åšä¸€ä¸ªæ›¿æ¢ï¼Œç”¨å•ä½å‘é‡\\\\(\\vec g\\\\)å’Œæ ‡é‡\\\\( \\alpha\\\\)åˆ†åˆ«ä»£è¡¨æ–¹å‘å’Œå¤§å°ï¼Œç°åœ¨çš„ä»»åŠ¡å°±å˜æˆäº†\n$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}â‹… \\vec g)$$\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œ**å¯¹äºä¸¤ä¸ªå‘é‡æ¥è¯´ï¼Œå½“ä»–ä»¬æ–¹å‘ç›¸åæ—¶ï¼Œä»–ä»¬çš„å†…ç§¯æ˜¯æœ€å°çš„**ã€‚\n\n>æ¢¯åº¦æ–¹å‘çš„å®šä¹‰æ˜¯è¯¥ç‚¹æ¢¯åº¦åœ¨æ ‡é‡åœºå¢é•¿æœ€å¿«çš„æ–¹å‘\n\nå› æ­¤å½“\\\\(\\vec g\\\\)çš„æ–¹å‘æ˜¯\\\\( \\vec{\\nabla f(x_k)}\\\\)çš„åæ–¹å‘æ—¶ï¼Œä¸Šå¼å¯ä»¥å–åˆ°æœ€å°å€¼ï¼Œäºæ˜¯å°±æœ‰\n$$x-x_k=- \\alpha \\nabla f(x_k)$$\n$$x:=x_k- \\alpha \\nabla f(x_k)$$\nåˆ°è¿™ä¸€æ­¥ï¼Œæ˜¯ä¸æ˜¯çœ‹åˆ°äº†ç†Ÿæ‚‰çš„gradient descentå‘¢ï¼Œyeah mateï¼We make it!\n## 2nd order Taylor series & newton's method\nå’Œä¸Šé¢çš„gradient descentç›¸ä¼¼ï¼Œå‡è®¾\\\\(x_k\\\\)æ˜¯ç¬¬\\\\(k\\\\)æ¬¡newton's methodè¿­ä»£åçš„\\\\(x\\\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„2nd order Taylor series æ˜¯\n$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$\næˆ‘ä»¬å¯¹ç­‰å·ä¸¤è¾¹åŒæ—¶å¯¹\\\\(x\\\\)æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸ºé›¶\n$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$\nç”±äºnewton's methodçš„åŸç†å°±æ˜¯é€šè¿‡\\\\(\\nabla f(x)=0\\\\)æ¥å¯»æ‰¾æœ€å°å€¼ï¼Œ**æ•…ä¸Šå¼ä¸ºé›¶çš„è§£\\\\(x\\\\)å…¶å®å°±æ˜¯newton's methodåœ¨\\\\(k+1\\\\)æ¬¡è¿­ä»£åçš„æ–°çš„\\\\(x\\\\)å€¼**ã€‚å…¶ä¸­\\\\(\\nabla f(x_k)\\\\)æ˜¯\\\\(x_k\\\\)å¤„çš„ä¸€é˜¶å¯¼æ•°ï¼Œ\\\\( \\nabla^2 f(x_k)\\\\)æ˜¯\\\\(x_k\\\\)å¤„çš„äºŒé˜¶å¯¼æ•°HessiançŸ©é˜µå…ƒç´ \n\næˆ‘ä»¬ä»¤\\\\(\\nabla f(x_k)=g\\\\)ï¼Œ\\\\(\\nabla^2 f(x_k)=H\\\\)ï¼Œåˆ™ä¸Šå¼å˜æˆ\n$$g+H(x-x_k)=0$$\nè¿›ä¸€æ­¥çš„\n$$x=x_k-H^{-1}g$$\nç”±äº\\\\(-g H^{-1} \\\\) æ˜¯ä¼˜åŒ–çš„å‰è¿›æ–¹å‘ï¼Œåœ¨å¯»æ‰¾æœ€å°å€¼çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯å’Œæ¢¯åº¦æ–¹å‘\\\\(g\\\\)ç›¸åæ‰å¯ä»¥æ›´å¿«çš„ä¸‹é™ï¼Œé‚£ä¹ˆå°±æœ‰\\\\( g^T H^{-1} g > 0\\\\)ï¼Œè¿™ä¸å°±æ˜¯positive definiteçš„å®šä¹‰å—ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œ**HessiançŸ©é˜µæ˜¯positive definiteçš„**ã€‚\n\næƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœHessianæ˜¯negative definiteçš„è¯ï¼Œå‚æ•°æ›´æ–°çš„æ–¹å‘å°±æˆäº†å’Œ\\\\(g\\\\)ç›¸åŒçš„æ–¹å‘ï¼Œnewton's methodå°†ä¼šå‘æ•£ï¼Œè¿™ä¸€ç‚¹ï¼Œä¹Ÿæ˜¯newton's methodçš„ç¼ºç‚¹ã€‚åœ¨objective functionæ˜¯non-convex functionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬\\\\(k\\\\)æ¬¡è¿­ä»£è·å¾—çš„\\\\(x_k\\\\)å¤„çš„Hessian matrix negative definiteï¼Œé‚£ä¹ˆnewton's methodå°†ä¼šå‘æ•£ï¼Œä»è€Œå¯¼è‡´ä¸æ”¶æ•›ã€‚å½“ç„¶ï¼Œä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œåç»­æœ‰æ”¹è¿›çš„BFGSç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæš‚æ—¶ä¸è¯¦ç»†è®¨è®ºã€‚\n## Sum up\nä¸‹é¢æˆ‘ä»¬å†æ¥æ€»ç»“æ€§è´¨çš„å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§æ–¹æ³•ï¼Œæ¥çœ‹ä¸€å¼ å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png)\näº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ä¸€ç§é€¼è¿‘å’Œæ‹Ÿåˆçš„æ€æƒ³ã€‚å‡è®¾ç°åœ¨å¤„äºè¿­ä»£\\\\(k\\\\)æ¬¡ä¹‹åçš„\\\\(x_k\\\\)ç‚¹ï¼Œå¯¹äºobjective functionï¼Œæˆ‘ä»¬ç”¨\\\\(x_k\\\\)ç‚¹çš„Taylor series \\\\(f(x)\\\\)æ¥é€¼è¿‘å’Œæ‹Ÿåˆï¼Œå½“ç„¶äº†ï¼Œä¸Šå›¾æˆ‘ä»¬çœ‹åˆ°ï¼Œgradient descentæ˜¯ç”¨ä¸€æ¬¡functionè€Œnewton's methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡functionï¼Œè¿™æ˜¯äºŒè€…ä¹‹é—´æœ€æ˜¾è‘—çš„åŒºåˆ«ã€‚\n\nå¯¹äºnew's methodï¼Œåœ¨æ‹Ÿåˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡\\\\( \\nabla f(x)=0\\\\)æ±‚å¾—çš„\\\\(x \\_{k+1}\\\\)ç‚¹ä½œä¸ºæ­¤æ¬¡è¿­ä»£çš„ç»“æœï¼Œä¸‹æ¬¡è¿­ä»£æ—¶å€™ï¼Œåˆåœ¨\\\\(x \\_{k+1}\\\\)å¤„æ¬¡è¿›è¡ŒäºŒæ¬¡functionçš„æ‹Ÿåˆï¼Œå¹¶å¦‚æ­¤è¿­ä»£ä¸‹å»ã€‚\n\nNewton's methodé‡‡ç”¨äºŒæ¬¡functionæ¥æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æ„Ÿæ€§çš„ç†è§£ä¸ºï¼Œnewton's methodåœ¨å¯»æ‰¾ä¸‹é™çš„æ–¹å‘æ—¶å€™ï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯æ­¤å¤„objective function valueæ˜¯ä¸æ˜¯å‡å°(ä¸€é˜¶value)ï¼Œè¿˜å…³æ³¨æ­¤å¤„valueä¸‹é™çš„è¶‹åŠ¿å¦‚ä½•(äºŒé˜¶value)ï¼Œè€Œgradient descentåªå…³å¿ƒæ­¤å¤„function valueæ˜¯ä¸æ˜¯å‡å°ï¼Œå› æ­¤newton's methodå¯ä»¥è¿­ä»£æ›´å°‘æ¬¡æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¯¹äºæ ‡å‡†äºŒæ¬¡å‹çš„objective functionï¼Œnewton's methodç”šè‡³å¯ä»¥ä¸€æ¬¡è¿­ä»£å°±æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚\n\nä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢æ‰€è¯´çš„æ ‡å‡†äºŒæ¬¡å‹functionï¼Œå®è´¨ä¸Šæ˜¯convex functionï¼Œåœ¨ä¸€èˆ¬çš„unconstrained optimizationä¸­ï¼Œæ›´å¤šçš„æƒ…å†µåˆ™æ˜¯non-convex optimizationï¼Œå¯¹äºä¸€èˆ¬çš„non-convex optimizationï¼Œnewton's methodæ˜¯ç›¸å¯¹ä¸ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾ä¿è¯Hessian matrixçš„positive definiteã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬ä¼šåŠ å…¥æ­¥é•¿\\\\(\\lambda\\\\)é™åˆ¶ï¼Œé˜²æ­¢å…¶ä¸€æ¬¡è¿­ä»£è¿‡å¤§è€Œå¸¦æ¥è¿­ä»£åHessian matrix negative definiteçš„æƒ…å†µï¼Œå³\n$$x:=x- \\lambda H^{-1} g$$\nå¯¹äºè¿™ç§æ€æƒ³ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œæ˜¯åœ¨æ•´ä½“non-convex functionä¸­å¯»æ‰¾ä¸€ä¸ªå±€éƒ¨çš„convex functionï¼Œé€šè¿‡æ­¥é•¿å°†newton's methodé™åˆ¶åœ¨è¿™ä¸ªå±€éƒ¨ä¸­ï¼Œæœ€åæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ä¸­ã€‚ç”±æ­¤å¯è§ï¼Œnewton's mtehodåœ¨non-convexä¸­å—é™åˆ¶æ¯”è¾ƒå¤§ã€‚\n\nç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºgradient descenté‡‡ç”¨çš„ä¸€æ¬¡functionåšæ‹Ÿåˆï¼Œåªéœ€è¦è€ƒè™‘æ²¿ç€æ¢¯åº¦åæ–¹å‘å¯»æ‰¾æœ€å°å€¼ï¼Œå› æ­¤gradient descenté€‚ç”¨äºå„ç§åœºæ™¯ï¼Œç”šè‡³æ˜¯non-convex optimizationï¼Œè™½ç„¶ä¸èƒ½ä¿è¯æ˜¯å…¨å±€æœ€ä¼˜ï¼Œä½†è‡³å°‘gradient descentæ˜¯å¯ä»¥å€¼å¾—ä¸€è¯•çš„æ–¹æ³•ã€‚\n\nä¸‹é¢æ¥æ€»ç»“ä¸€ä¸‹ï¼š\n* Gradient descent å’Œ newton's methodéƒ½æ˜¯åˆ©ç”¨Taylor serieså¯¹objective functionè¿›è¡Œæ‹Ÿåˆæ¥å®ç°è¿­ä»£çš„ï¼›\n* Gradient descent é‡‡ç”¨ä¸€æ¬¡å‹functionæ‹Ÿåˆè€Œ newton's methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡å‹functionï¼Œå› æ­¤newton's methodè¿­ä»£æ›´è¿…é€Ÿï¼›\n* Newton's methodæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—Hessian matrixçš„é€†ï¼Œåœ¨é«˜ç»´featureæƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æ¯æ¬¡è¿­ä»£ä¼šæ¯”è¾ƒæ…¢ï¼›\n* Newton's methodåœ¨non-convex optimizationä¸­å¾ˆå—é™åˆ¶ï¼Œè€Œgradient descentåˆ™ä¸å—å½±å“ã€‚\n\nå¥½äº†ï¼Œå…ˆå†™è¿™ä¹ˆå¤šï¼Œè¿™å…¶ä¸­çš„çŸ¥è¯†é‡è¿˜æ˜¯å¾ˆæ·±å¥¥çš„ï¼Œä¹Ÿä¸çŸ¥é“è‡ªå·±æœ‰æ²¡æœ‰å™è¿°æ˜ç™½ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥è®¨è®ºï¼\n\n**æœ€åæ„Ÿè°¢ä¼˜ç”·çš„å®è´µæ„è§ï¼**\n## Reference\n* [UCLA courseware](http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf)\n* [CCU courseware](https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n* [Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n","slug":"ml-gd-and-nm","published":1,"updated":"2018-11-19T06:27:04.265Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4p9000ktr8l7t81sfvg","content":"<p>ä¸Šæ¬¡æˆ‘ä»¬ä¸€èµ·èŠåˆ°äº†gradient descentå’Œnewtonâ€™s methodï¼Œè€Œä¸”æˆ‘ä»¬å·²ç»çŸ¥é“äº†gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œè¿™æ¬¡æˆ‘ä»¬å°±è·³å‡ºconvex optimizationï¼Œä»æ›´å¤§çš„unconstrained optimizationè§’åº¦æ¥æ¢è®¨ä¸‹è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’ŒåŒºåˆ«ã€‚<br><a id=\"more\"></a></p>\n<p>å‡è®¾æˆ‘ä»¬ç°æœ‰ä¸€ä¸ªçš„optimization taskï¼Œè¦æ±‚objective function \\(f(x)\\)çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š</p>\n<ul>\n<li>è€ƒè™‘åˆ°\\(f(x)\\)çš„æœ€å°å€¼å¾ˆæœ‰å¯èƒ½æ˜¯å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾\\( \\nabla f(x)=0\\)çš„ç‚¹æ¥ç¡®å®šæœ€å°å€¼ï¼Œè¿™å°±æ˜¯<strong>newtonâ€™s method</strong>çš„æ€æƒ³</li>\n<li>æ—¢ç„¶æˆ‘ä»¬è¦å¯»æ‰¾æœ€å°å€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é¡ºç€ä¸€æ¡\\(f(x)\\)é€æ¸å‡å°çš„è·¯å¾„ï¼Œé¡ºç€è¿™æ¡è·¯å¾„ä¸€ç›´èµ°ä¸‹å»ï¼Œç›´åˆ°ä¸å†å˜å°ï¼Œè¿™å°±æ˜¯<strong>gradient descent</strong>çš„æ€æƒ³</li>\n</ul>\n<p>OKï¼Œç®€å•çš„å™è¿°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ­£é¢˜ï¼</p>\n<h2 id=\"æ³°å‹’çº§æ•°-Taylor-series\"><a href=\"#æ³°å‹’çº§æ•°-Taylor-series\" class=\"headerlink\" title=\"æ³°å‹’çº§æ•°(Taylor series)\"></a>æ³°å‹’çº§æ•°(Taylor series)</h2><p>é¦–å…ˆæˆ‘ä»¬éœ€è¦å›å¿†ä¸€ä¸‹é«˜ç­‰æ•°å­¦ä¸­é‡è¦çš„Taylor seriesï¼Œå¦‚æœ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)çš„é¢†åŸŸå†…å…·æœ‰\\(n+1\\)é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆï¼Œåœ¨è¯¥é¢†åŸŸå†…ï¼Œ\\( f(x)\\)å¯å±•å¼€æˆ\\(n\\)é˜¶Taylor seriesï¼Œå¿½ç•¥æ— é™å¤§æ¬¡é¡¹çš„å½¢å¼å°±æ˜¯<br>$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +â€¦+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$<br>å…¶å®åœ¨é«˜ç­‰æ•°å­¦ä¸­å­¦åˆ°Taylor seriesçš„æ—¶å€™ï¼Œæˆ‘æœ¬äººæ˜¯ååˆ†æ— æ„Ÿçš„ï¼Œæˆ‘å¹¶ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿åˆ°åº•æœ‰ä»€ä¹ˆç”¨å¤„ï¼Œç›¸ä¿¡å¾ˆå¤šäººå’Œæˆ‘æœ‰ç›¸ä¼¼çš„ç»å†ã€‚</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functionâ€™s derivatives at a single point.</p>\n</blockquote>\n<p>äº‹å®ä¸Šï¼ŒTaylor seriesæ‰€è¡¨ç°çš„æ˜¯ï¼Œå¯¹äº\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„ä¸€ä¸ªä¼°è®¡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼Œæ ¹æ®\\( x_0\\)ç‚¹å¤„çš„å„é˜¶derivativesä¹‹å’Œæ„æˆä¸€ä¸ªæ–°çš„functionï¼Œè¿™ä¸ªfunctionå°±æ˜¯å¯¹\\(f(x)\\)çš„é€¼è¿‘å’Œæ‹Ÿåˆï¼Œè€Œä¸”è¿™ç§é€¼è¿‘å’Œæ‹Ÿåˆï¼Œéšç€Taylor seriesé˜¶æ•°å¢åŠ è€Œæ›´æ¥è¿‘äºçœŸå®çš„\\(f(x)\\)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨0é˜¶Taylor seriesæ¥é€¼è¿‘çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±ç²—æš´çš„è®¤ä¸ºï¼Œ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„å€¼å°±éƒ½æ˜¯\\(x_0\\)ï¼Œè¿™å½“ç„¶å¤ªç²—æš´ç›´æ¥äº†ï¼Œå“ˆå“ˆã€‚</p>\n<p>æ—¢ç„¶è¿™å¤ªç²—æš´äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç”¨1st order Taylor seriesæ¥åšä¸€ä¸ªé€¼è¿‘å’Œä¼°è®¡ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ï¼›å¦‚æœæˆ‘ä»¬ç”¨2nd order Taylor seriesæ¥ä¼°è®¡å‘¢ï¼Œé‚£å°±æˆäº†newtonâ€™s methodäº†</p>\n<p>OKï¼Œæˆ‘ä»¬ç»§ç»­å¨“å¨“é“æ¥ã€‚</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>å‡è®¾\\(x_k\\)æ˜¯ç¬¬kæ¬¡gradient descentè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„1st order Taylor series å°±æ˜¯<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$<br>å…¶ä¸­\\(x\\)æ˜¯è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ–¹å‘ï¼Œgradient descentçš„ç›®æ ‡å°±æ˜¯è®©\\(f(x)\\)è¾¾åˆ°å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œä¹Ÿéœ€è¦å°½å¯èƒ½çš„å‡å°æ›´å¤šä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé‚£ä¹ˆ<br>$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$<br>æ˜¾ç„¶ï¼Œä¸Šå¼åº”è¯¥å°½å¯èƒ½çš„å¤§ï¼Œå³<strong>\\(- \\nabla f(x_k)(x-x_k)\\)è¶Šå¤§è¶Šå¥½</strong>ï¼Œæˆ‘ä»¬ç°åœ¨æŠŠ\\((x-x_k)\\)åšä¸€ä¸ªæ›¿æ¢ï¼Œç”¨å•ä½å‘é‡\\(\\vec g\\)å’Œæ ‡é‡\\( \\alpha\\)åˆ†åˆ«ä»£è¡¨æ–¹å‘å’Œå¤§å°ï¼Œç°åœ¨çš„ä»»åŠ¡å°±å˜æˆäº†<br>$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}â‹… \\vec g)$$<br>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œ<strong>å¯¹äºä¸¤ä¸ªå‘é‡æ¥è¯´ï¼Œå½“ä»–ä»¬æ–¹å‘ç›¸åæ—¶ï¼Œä»–ä»¬çš„å†…ç§¯æ˜¯æœ€å°çš„</strong>ã€‚</p>\n<blockquote>\n<p>æ¢¯åº¦æ–¹å‘çš„å®šä¹‰æ˜¯è¯¥ç‚¹æ¢¯åº¦åœ¨æ ‡é‡åœºå¢é•¿æœ€å¿«çš„æ–¹å‘</p>\n</blockquote>\n<p>å› æ­¤å½“\\(\\vec g\\)çš„æ–¹å‘æ˜¯\\( \\vec{\\nabla f(x_k)}\\)çš„åæ–¹å‘æ—¶ï¼Œä¸Šå¼å¯ä»¥å–åˆ°æœ€å°å€¼ï¼Œäºæ˜¯å°±æœ‰<br>$$x-x_k=- \\alpha \\nabla f(x_k)$$<br>$$x:=x_k- \\alpha \\nabla f(x_k)$$<br>åˆ°è¿™ä¸€æ­¥ï¼Œæ˜¯ä¸æ˜¯çœ‹åˆ°äº†ç†Ÿæ‚‰çš„gradient descentå‘¢ï¼Œyeah mateï¼We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newtonâ€™s-method\"><a href=\"#2nd-order-Taylor-series-amp-newtonâ€™s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newtonâ€™s method\"></a>2nd order Taylor series &amp; newtonâ€™s method</h2><p>å’Œä¸Šé¢çš„gradient descentç›¸ä¼¼ï¼Œå‡è®¾\\(x_k\\)æ˜¯ç¬¬\\(k\\)æ¬¡newtonâ€™s methodè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„2nd order Taylor series æ˜¯<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$<br>æˆ‘ä»¬å¯¹ç­‰å·ä¸¤è¾¹åŒæ—¶å¯¹\\(x\\)æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸ºé›¶<br>$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$<br>ç”±äºnewtonâ€™s methodçš„åŸç†å°±æ˜¯é€šè¿‡\\(\\nabla f(x)=0\\)æ¥å¯»æ‰¾æœ€å°å€¼ï¼Œ<strong>æ•…ä¸Šå¼ä¸ºé›¶çš„è§£\\(x\\)å…¶å®å°±æ˜¯newtonâ€™s methodåœ¨\\(k+1\\)æ¬¡è¿­ä»£åçš„æ–°çš„\\(x\\)å€¼</strong>ã€‚å…¶ä¸­\\(\\nabla f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„ä¸€é˜¶å¯¼æ•°ï¼Œ\\( \\nabla^2 f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„äºŒé˜¶å¯¼æ•°HessiançŸ©é˜µå…ƒç´ </p>\n<p>æˆ‘ä»¬ä»¤\\(\\nabla f(x_k)=g\\)ï¼Œ\\(\\nabla^2 f(x_k)=H\\)ï¼Œåˆ™ä¸Šå¼å˜æˆ<br>$$g+H(x-x_k)=0$$<br>è¿›ä¸€æ­¥çš„<br>$$x=x_k-H^{-1}g$$<br>ç”±äº\\(-g H^{-1} \\) æ˜¯ä¼˜åŒ–çš„å‰è¿›æ–¹å‘ï¼Œåœ¨å¯»æ‰¾æœ€å°å€¼çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯å’Œæ¢¯åº¦æ–¹å‘\\(g\\)ç›¸åæ‰å¯ä»¥æ›´å¿«çš„ä¸‹é™ï¼Œé‚£ä¹ˆå°±æœ‰\\( g^T H^{-1} g &gt; 0\\)ï¼Œè¿™ä¸å°±æ˜¯positive definiteçš„å®šä¹‰å—ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>HessiançŸ©é˜µæ˜¯positive definiteçš„</strong>ã€‚</p>\n<p>æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœHessianæ˜¯negative definiteçš„è¯ï¼Œå‚æ•°æ›´æ–°çš„æ–¹å‘å°±æˆäº†å’Œ\\(g\\)ç›¸åŒçš„æ–¹å‘ï¼Œnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œè¿™ä¸€ç‚¹ï¼Œä¹Ÿæ˜¯newtonâ€™s methodçš„ç¼ºç‚¹ã€‚åœ¨objective functionæ˜¯non-convex functionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬\\(k\\)æ¬¡è¿­ä»£è·å¾—çš„\\(x_k\\)å¤„çš„Hessian matrix negative definiteï¼Œé‚£ä¹ˆnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œä»è€Œå¯¼è‡´ä¸æ”¶æ•›ã€‚å½“ç„¶ï¼Œä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œåç»­æœ‰æ”¹è¿›çš„BFGSç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæš‚æ—¶ä¸è¯¦ç»†è®¨è®ºã€‚</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>ä¸‹é¢æˆ‘ä»¬å†æ¥æ€»ç»“æ€§è´¨çš„å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§æ–¹æ³•ï¼Œæ¥çœ‹ä¸€å¼ å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png\" alt=\"\"><br>äº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ä¸€ç§é€¼è¿‘å’Œæ‹Ÿåˆçš„æ€æƒ³ã€‚å‡è®¾ç°åœ¨å¤„äºè¿­ä»£\\(k\\)æ¬¡ä¹‹åçš„\\(x_k\\)ç‚¹ï¼Œå¯¹äºobjective functionï¼Œæˆ‘ä»¬ç”¨\\(x_k\\)ç‚¹çš„Taylor series \\(f(x)\\)æ¥é€¼è¿‘å’Œæ‹Ÿåˆï¼Œå½“ç„¶äº†ï¼Œä¸Šå›¾æˆ‘ä»¬çœ‹åˆ°ï¼Œgradient descentæ˜¯ç”¨ä¸€æ¬¡functionè€Œnewtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡functionï¼Œè¿™æ˜¯äºŒè€…ä¹‹é—´æœ€æ˜¾è‘—çš„åŒºåˆ«ã€‚</p>\n<p>å¯¹äºnewâ€™s methodï¼Œåœ¨æ‹Ÿåˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡\\( \\nabla f(x)=0\\)æ±‚å¾—çš„\\(x _{k+1}\\)ç‚¹ä½œä¸ºæ­¤æ¬¡è¿­ä»£çš„ç»“æœï¼Œä¸‹æ¬¡è¿­ä»£æ—¶å€™ï¼Œåˆåœ¨\\(x _{k+1}\\)å¤„æ¬¡è¿›è¡ŒäºŒæ¬¡functionçš„æ‹Ÿåˆï¼Œå¹¶å¦‚æ­¤è¿­ä»£ä¸‹å»ã€‚</p>\n<p>Newtonâ€™s methodé‡‡ç”¨äºŒæ¬¡functionæ¥æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æ„Ÿæ€§çš„ç†è§£ä¸ºï¼Œnewtonâ€™s methodåœ¨å¯»æ‰¾ä¸‹é™çš„æ–¹å‘æ—¶å€™ï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯æ­¤å¤„objective function valueæ˜¯ä¸æ˜¯å‡å°(ä¸€é˜¶value)ï¼Œè¿˜å…³æ³¨æ­¤å¤„valueä¸‹é™çš„è¶‹åŠ¿å¦‚ä½•(äºŒé˜¶value)ï¼Œè€Œgradient descentåªå…³å¿ƒæ­¤å¤„function valueæ˜¯ä¸æ˜¯å‡å°ï¼Œå› æ­¤newtonâ€™s methodå¯ä»¥è¿­ä»£æ›´å°‘æ¬¡æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¯¹äºæ ‡å‡†äºŒæ¬¡å‹çš„objective functionï¼Œnewtonâ€™s methodç”šè‡³å¯ä»¥ä¸€æ¬¡è¿­ä»£å°±æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚</p>\n<p>ä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢æ‰€è¯´çš„æ ‡å‡†äºŒæ¬¡å‹functionï¼Œå®è´¨ä¸Šæ˜¯convex functionï¼Œåœ¨ä¸€èˆ¬çš„unconstrained optimizationä¸­ï¼Œæ›´å¤šçš„æƒ…å†µåˆ™æ˜¯non-convex optimizationï¼Œå¯¹äºä¸€èˆ¬çš„non-convex optimizationï¼Œnewtonâ€™s methodæ˜¯ç›¸å¯¹ä¸ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾ä¿è¯Hessian matrixçš„positive definiteã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬ä¼šåŠ å…¥æ­¥é•¿\\(\\lambda\\)é™åˆ¶ï¼Œé˜²æ­¢å…¶ä¸€æ¬¡è¿­ä»£è¿‡å¤§è€Œå¸¦æ¥è¿­ä»£åHessian matrix negative definiteçš„æƒ…å†µï¼Œå³<br>$$x:=x- \\lambda H^{-1} g$$<br>å¯¹äºè¿™ç§æ€æƒ³ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œæ˜¯åœ¨æ•´ä½“non-convex functionä¸­å¯»æ‰¾ä¸€ä¸ªå±€éƒ¨çš„convex functionï¼Œé€šè¿‡æ­¥é•¿å°†newtonâ€™s methodé™åˆ¶åœ¨è¿™ä¸ªå±€éƒ¨ä¸­ï¼Œæœ€åæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ä¸­ã€‚ç”±æ­¤å¯è§ï¼Œnewtonâ€™s mtehodåœ¨non-convexä¸­å—é™åˆ¶æ¯”è¾ƒå¤§ã€‚</p>\n<p>ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºgradient descenté‡‡ç”¨çš„ä¸€æ¬¡functionåšæ‹Ÿåˆï¼Œåªéœ€è¦è€ƒè™‘æ²¿ç€æ¢¯åº¦åæ–¹å‘å¯»æ‰¾æœ€å°å€¼ï¼Œå› æ­¤gradient descenté€‚ç”¨äºå„ç§åœºæ™¯ï¼Œç”šè‡³æ˜¯non-convex optimizationï¼Œè™½ç„¶ä¸èƒ½ä¿è¯æ˜¯å…¨å±€æœ€ä¼˜ï¼Œä½†è‡³å°‘gradient descentæ˜¯å¯ä»¥å€¼å¾—ä¸€è¯•çš„æ–¹æ³•ã€‚</p>\n<p>ä¸‹é¢æ¥æ€»ç»“ä¸€ä¸‹ï¼š</p>\n<ul>\n<li>Gradient descent å’Œ newtonâ€™s methodéƒ½æ˜¯åˆ©ç”¨Taylor serieså¯¹objective functionè¿›è¡Œæ‹Ÿåˆæ¥å®ç°è¿­ä»£çš„ï¼›</li>\n<li>Gradient descent é‡‡ç”¨ä¸€æ¬¡å‹functionæ‹Ÿåˆè€Œ newtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡å‹functionï¼Œå› æ­¤newtonâ€™s methodè¿­ä»£æ›´è¿…é€Ÿï¼›</li>\n<li>Newtonâ€™s methodæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—Hessian matrixçš„é€†ï¼Œåœ¨é«˜ç»´featureæƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æ¯æ¬¡è¿­ä»£ä¼šæ¯”è¾ƒæ…¢ï¼›</li>\n<li>Newtonâ€™s methodåœ¨non-convex optimizationä¸­å¾ˆå—é™åˆ¶ï¼Œè€Œgradient descentåˆ™ä¸å—å½±å“ã€‚</li>\n</ul>\n<p>å¥½äº†ï¼Œå…ˆå†™è¿™ä¹ˆå¤šï¼Œè¿™å…¶ä¸­çš„çŸ¥è¯†é‡è¿˜æ˜¯å¾ˆæ·±å¥¥çš„ï¼Œä¹Ÿä¸çŸ¥é“è‡ªå·±æœ‰æ²¡æœ‰å™è¿°æ˜ç™½ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥è®¨è®ºï¼</p>\n<p><strong>æœ€åæ„Ÿè°¢ä¼˜ç”·çš„å®è´µæ„è§ï¼</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"noopener\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"noopener\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"noopener\">Taylor series</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"newton's method","path":"tags/newton-s-method/"},{"name":"unconstrained optimization","path":"tags/unconstrained-optimization/"}],"excerpt":"<p>ä¸Šæ¬¡æˆ‘ä»¬ä¸€èµ·èŠåˆ°äº†gradient descentå’Œnewtonâ€™s methodï¼Œè€Œä¸”æˆ‘ä»¬å·²ç»çŸ¥é“äº†gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œè¿™æ¬¡æˆ‘ä»¬å°±è·³å‡ºconvex optimizationï¼Œä»æ›´å¤§çš„unconstrained optimizationè§’åº¦æ¥æ¢è®¨ä¸‹è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’ŒåŒºåˆ«ã€‚<br></p>","more":"</p>\n<p>å‡è®¾æˆ‘ä»¬ç°æœ‰ä¸€ä¸ªçš„optimization taskï¼Œè¦æ±‚objective function \\(f(x)\\)çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š</p>\n<ul>\n<li>è€ƒè™‘åˆ°\\(f(x)\\)çš„æœ€å°å€¼å¾ˆæœ‰å¯èƒ½æ˜¯å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾\\( \\nabla f(x)=0\\)çš„ç‚¹æ¥ç¡®å®šæœ€å°å€¼ï¼Œè¿™å°±æ˜¯<strong>newtonâ€™s method</strong>çš„æ€æƒ³</li>\n<li>æ—¢ç„¶æˆ‘ä»¬è¦å¯»æ‰¾æœ€å°å€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é¡ºç€ä¸€æ¡\\(f(x)\\)é€æ¸å‡å°çš„è·¯å¾„ï¼Œé¡ºç€è¿™æ¡è·¯å¾„ä¸€ç›´èµ°ä¸‹å»ï¼Œç›´åˆ°ä¸å†å˜å°ï¼Œè¿™å°±æ˜¯<strong>gradient descent</strong>çš„æ€æƒ³</li>\n</ul>\n<p>OKï¼Œç®€å•çš„å™è¿°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ­£é¢˜ï¼</p>\n<h2 id=\"æ³°å‹’çº§æ•°-Taylor-series\"><a href=\"#æ³°å‹’çº§æ•°-Taylor-series\" class=\"headerlink\" title=\"æ³°å‹’çº§æ•°(Taylor series)\"></a>æ³°å‹’çº§æ•°(Taylor series)</h2><p>é¦–å…ˆæˆ‘ä»¬éœ€è¦å›å¿†ä¸€ä¸‹é«˜ç­‰æ•°å­¦ä¸­é‡è¦çš„Taylor seriesï¼Œå¦‚æœ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)çš„é¢†åŸŸå†…å…·æœ‰\\(n+1\\)é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆï¼Œåœ¨è¯¥é¢†åŸŸå†…ï¼Œ\\( f(x)\\)å¯å±•å¼€æˆ\\(n\\)é˜¶Taylor seriesï¼Œå¿½ç•¥æ— é™å¤§æ¬¡é¡¹çš„å½¢å¼å°±æ˜¯<br>$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +â€¦+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$<br>å…¶å®åœ¨é«˜ç­‰æ•°å­¦ä¸­å­¦åˆ°Taylor seriesçš„æ—¶å€™ï¼Œæˆ‘æœ¬äººæ˜¯ååˆ†æ— æ„Ÿçš„ï¼Œæˆ‘å¹¶ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿åˆ°åº•æœ‰ä»€ä¹ˆç”¨å¤„ï¼Œç›¸ä¿¡å¾ˆå¤šäººå’Œæˆ‘æœ‰ç›¸ä¼¼çš„ç»å†ã€‚</p>\n<blockquote>\n<p>In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functionâ€™s derivatives at a single point.</p>\n</blockquote>\n<p>äº‹å®ä¸Šï¼ŒTaylor seriesæ‰€è¡¨ç°çš„æ˜¯ï¼Œå¯¹äº\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„ä¸€ä¸ªä¼°è®¡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼Œæ ¹æ®\\( x_0\\)ç‚¹å¤„çš„å„é˜¶derivativesä¹‹å’Œæ„æˆä¸€ä¸ªæ–°çš„functionï¼Œè¿™ä¸ªfunctionå°±æ˜¯å¯¹\\(f(x)\\)çš„é€¼è¿‘å’Œæ‹Ÿåˆï¼Œè€Œä¸”è¿™ç§é€¼è¿‘å’Œæ‹Ÿåˆï¼Œéšç€Taylor seriesé˜¶æ•°å¢åŠ è€Œæ›´æ¥è¿‘äºçœŸå®çš„\\(f(x)\\)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨0é˜¶Taylor seriesæ¥é€¼è¿‘çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±ç²—æš´çš„è®¤ä¸ºï¼Œ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„å€¼å°±éƒ½æ˜¯\\(x_0\\)ï¼Œè¿™å½“ç„¶å¤ªç²—æš´ç›´æ¥äº†ï¼Œå“ˆå“ˆã€‚</p>\n<p>æ—¢ç„¶è¿™å¤ªç²—æš´äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç”¨1st order Taylor seriesæ¥åšä¸€ä¸ªé€¼è¿‘å’Œä¼°è®¡ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ï¼›å¦‚æœæˆ‘ä»¬ç”¨2nd order Taylor seriesæ¥ä¼°è®¡å‘¢ï¼Œé‚£å°±æˆäº†newtonâ€™s methodäº†</p>\n<p>OKï¼Œæˆ‘ä»¬ç»§ç»­å¨“å¨“é“æ¥ã€‚</p>\n<h2 id=\"1st-order-Taylor-series-amp-gradient-descent\"><a href=\"#1st-order-Taylor-series-amp-gradient-descent\" class=\"headerlink\" title=\"1st order Taylor series &amp; gradient descent\"></a>1st order Taylor series &amp; gradient descent</h2><p>å‡è®¾\\(x_k\\)æ˜¯ç¬¬kæ¬¡gradient descentè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„1st order Taylor series å°±æ˜¯<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$<br>å…¶ä¸­\\(x\\)æ˜¯è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ–¹å‘ï¼Œgradient descentçš„ç›®æ ‡å°±æ˜¯è®©\\(f(x)\\)è¾¾åˆ°å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œä¹Ÿéœ€è¦å°½å¯èƒ½çš„å‡å°æ›´å¤šä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé‚£ä¹ˆ<br>$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$<br>æ˜¾ç„¶ï¼Œä¸Šå¼åº”è¯¥å°½å¯èƒ½çš„å¤§ï¼Œå³<strong>\\(- \\nabla f(x_k)(x-x_k)\\)è¶Šå¤§è¶Šå¥½</strong>ï¼Œæˆ‘ä»¬ç°åœ¨æŠŠ\\((x-x_k)\\)åšä¸€ä¸ªæ›¿æ¢ï¼Œç”¨å•ä½å‘é‡\\(\\vec g\\)å’Œæ ‡é‡\\( \\alpha\\)åˆ†åˆ«ä»£è¡¨æ–¹å‘å’Œå¤§å°ï¼Œç°åœ¨çš„ä»»åŠ¡å°±å˜æˆäº†<br>$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}â‹… \\vec g)$$<br>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œ<strong>å¯¹äºä¸¤ä¸ªå‘é‡æ¥è¯´ï¼Œå½“ä»–ä»¬æ–¹å‘ç›¸åæ—¶ï¼Œä»–ä»¬çš„å†…ç§¯æ˜¯æœ€å°çš„</strong>ã€‚</p>\n<blockquote>\n<p>æ¢¯åº¦æ–¹å‘çš„å®šä¹‰æ˜¯è¯¥ç‚¹æ¢¯åº¦åœ¨æ ‡é‡åœºå¢é•¿æœ€å¿«çš„æ–¹å‘</p>\n</blockquote>\n<p>å› æ­¤å½“\\(\\vec g\\)çš„æ–¹å‘æ˜¯\\( \\vec{\\nabla f(x_k)}\\)çš„åæ–¹å‘æ—¶ï¼Œä¸Šå¼å¯ä»¥å–åˆ°æœ€å°å€¼ï¼Œäºæ˜¯å°±æœ‰<br>$$x-x_k=- \\alpha \\nabla f(x_k)$$<br>$$x:=x_k- \\alpha \\nabla f(x_k)$$<br>åˆ°è¿™ä¸€æ­¥ï¼Œæ˜¯ä¸æ˜¯çœ‹åˆ°äº†ç†Ÿæ‚‰çš„gradient descentå‘¢ï¼Œyeah mateï¼We make it!</p>\n<h2 id=\"2nd-order-Taylor-series-amp-newtonâ€™s-method\"><a href=\"#2nd-order-Taylor-series-amp-newtonâ€™s-method\" class=\"headerlink\" title=\"2nd order Taylor series &amp; newtonâ€™s method\"></a>2nd order Taylor series &amp; newtonâ€™s method</h2><p>å’Œä¸Šé¢çš„gradient descentç›¸ä¼¼ï¼Œå‡è®¾\\(x_k\\)æ˜¯ç¬¬\\(k\\)æ¬¡newtonâ€™s methodè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„2nd order Taylor series æ˜¯<br>$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$<br>æˆ‘ä»¬å¯¹ç­‰å·ä¸¤è¾¹åŒæ—¶å¯¹\\(x\\)æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸ºé›¶<br>$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$<br>ç”±äºnewtonâ€™s methodçš„åŸç†å°±æ˜¯é€šè¿‡\\(\\nabla f(x)=0\\)æ¥å¯»æ‰¾æœ€å°å€¼ï¼Œ<strong>æ•…ä¸Šå¼ä¸ºé›¶çš„è§£\\(x\\)å…¶å®å°±æ˜¯newtonâ€™s methodåœ¨\\(k+1\\)æ¬¡è¿­ä»£åçš„æ–°çš„\\(x\\)å€¼</strong>ã€‚å…¶ä¸­\\(\\nabla f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„ä¸€é˜¶å¯¼æ•°ï¼Œ\\( \\nabla^2 f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„äºŒé˜¶å¯¼æ•°HessiançŸ©é˜µå…ƒç´ </p>\n<p>æˆ‘ä»¬ä»¤\\(\\nabla f(x_k)=g\\)ï¼Œ\\(\\nabla^2 f(x_k)=H\\)ï¼Œåˆ™ä¸Šå¼å˜æˆ<br>$$g+H(x-x_k)=0$$<br>è¿›ä¸€æ­¥çš„<br>$$x=x_k-H^{-1}g$$<br>ç”±äº\\(-g H^{-1} \\) æ˜¯ä¼˜åŒ–çš„å‰è¿›æ–¹å‘ï¼Œåœ¨å¯»æ‰¾æœ€å°å€¼çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯å’Œæ¢¯åº¦æ–¹å‘\\(g\\)ç›¸åæ‰å¯ä»¥æ›´å¿«çš„ä¸‹é™ï¼Œé‚£ä¹ˆå°±æœ‰\\( g^T H^{-1} g &gt; 0\\)ï¼Œè¿™ä¸å°±æ˜¯positive definiteçš„å®šä¹‰å—ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>HessiançŸ©é˜µæ˜¯positive definiteçš„</strong>ã€‚</p>\n<p>æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœHessianæ˜¯negative definiteçš„è¯ï¼Œå‚æ•°æ›´æ–°çš„æ–¹å‘å°±æˆäº†å’Œ\\(g\\)ç›¸åŒçš„æ–¹å‘ï¼Œnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œè¿™ä¸€ç‚¹ï¼Œä¹Ÿæ˜¯newtonâ€™s methodçš„ç¼ºç‚¹ã€‚åœ¨objective functionæ˜¯non-convex functionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬\\(k\\)æ¬¡è¿­ä»£è·å¾—çš„\\(x_k\\)å¤„çš„Hessian matrix negative definiteï¼Œé‚£ä¹ˆnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œä»è€Œå¯¼è‡´ä¸æ”¶æ•›ã€‚å½“ç„¶ï¼Œä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œåç»­æœ‰æ”¹è¿›çš„BFGSç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæš‚æ—¶ä¸è¯¦ç»†è®¨è®ºã€‚</p>\n<h2 id=\"Sum-up\"><a href=\"#Sum-up\" class=\"headerlink\" title=\"Sum up\"></a>Sum up</h2><p>ä¸‹é¢æˆ‘ä»¬å†æ¥æ€»ç»“æ€§è´¨çš„å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§æ–¹æ³•ï¼Œæ¥çœ‹ä¸€å¼ å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/2/2-1.png\" alt=\"\"><br>äº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ä¸€ç§é€¼è¿‘å’Œæ‹Ÿåˆçš„æ€æƒ³ã€‚å‡è®¾ç°åœ¨å¤„äºè¿­ä»£\\(k\\)æ¬¡ä¹‹åçš„\\(x_k\\)ç‚¹ï¼Œå¯¹äºobjective functionï¼Œæˆ‘ä»¬ç”¨\\(x_k\\)ç‚¹çš„Taylor series \\(f(x)\\)æ¥é€¼è¿‘å’Œæ‹Ÿåˆï¼Œå½“ç„¶äº†ï¼Œä¸Šå›¾æˆ‘ä»¬çœ‹åˆ°ï¼Œgradient descentæ˜¯ç”¨ä¸€æ¬¡functionè€Œnewtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡functionï¼Œè¿™æ˜¯äºŒè€…ä¹‹é—´æœ€æ˜¾è‘—çš„åŒºåˆ«ã€‚</p>\n<p>å¯¹äºnewâ€™s methodï¼Œåœ¨æ‹Ÿåˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡\\( \\nabla f(x)=0\\)æ±‚å¾—çš„\\(x _{k+1}\\)ç‚¹ä½œä¸ºæ­¤æ¬¡è¿­ä»£çš„ç»“æœï¼Œä¸‹æ¬¡è¿­ä»£æ—¶å€™ï¼Œåˆåœ¨\\(x _{k+1}\\)å¤„æ¬¡è¿›è¡ŒäºŒæ¬¡functionçš„æ‹Ÿåˆï¼Œå¹¶å¦‚æ­¤è¿­ä»£ä¸‹å»ã€‚</p>\n<p>Newtonâ€™s methodé‡‡ç”¨äºŒæ¬¡functionæ¥æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æ„Ÿæ€§çš„ç†è§£ä¸ºï¼Œnewtonâ€™s methodåœ¨å¯»æ‰¾ä¸‹é™çš„æ–¹å‘æ—¶å€™ï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯æ­¤å¤„objective function valueæ˜¯ä¸æ˜¯å‡å°(ä¸€é˜¶value)ï¼Œè¿˜å…³æ³¨æ­¤å¤„valueä¸‹é™çš„è¶‹åŠ¿å¦‚ä½•(äºŒé˜¶value)ï¼Œè€Œgradient descentåªå…³å¿ƒæ­¤å¤„function valueæ˜¯ä¸æ˜¯å‡å°ï¼Œå› æ­¤newtonâ€™s methodå¯ä»¥è¿­ä»£æ›´å°‘æ¬¡æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¯¹äºæ ‡å‡†äºŒæ¬¡å‹çš„objective functionï¼Œnewtonâ€™s methodç”šè‡³å¯ä»¥ä¸€æ¬¡è¿­ä»£å°±æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚</p>\n<p>ä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢æ‰€è¯´çš„æ ‡å‡†äºŒæ¬¡å‹functionï¼Œå®è´¨ä¸Šæ˜¯convex functionï¼Œåœ¨ä¸€èˆ¬çš„unconstrained optimizationä¸­ï¼Œæ›´å¤šçš„æƒ…å†µåˆ™æ˜¯non-convex optimizationï¼Œå¯¹äºä¸€èˆ¬çš„non-convex optimizationï¼Œnewtonâ€™s methodæ˜¯ç›¸å¯¹ä¸ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾ä¿è¯Hessian matrixçš„positive definiteã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬ä¼šåŠ å…¥æ­¥é•¿\\(\\lambda\\)é™åˆ¶ï¼Œé˜²æ­¢å…¶ä¸€æ¬¡è¿­ä»£è¿‡å¤§è€Œå¸¦æ¥è¿­ä»£åHessian matrix negative definiteçš„æƒ…å†µï¼Œå³<br>$$x:=x- \\lambda H^{-1} g$$<br>å¯¹äºè¿™ç§æ€æƒ³ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œæ˜¯åœ¨æ•´ä½“non-convex functionä¸­å¯»æ‰¾ä¸€ä¸ªå±€éƒ¨çš„convex functionï¼Œé€šè¿‡æ­¥é•¿å°†newtonâ€™s methodé™åˆ¶åœ¨è¿™ä¸ªå±€éƒ¨ä¸­ï¼Œæœ€åæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ä¸­ã€‚ç”±æ­¤å¯è§ï¼Œnewtonâ€™s mtehodåœ¨non-convexä¸­å—é™åˆ¶æ¯”è¾ƒå¤§ã€‚</p>\n<p>ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºgradient descenté‡‡ç”¨çš„ä¸€æ¬¡functionåšæ‹Ÿåˆï¼Œåªéœ€è¦è€ƒè™‘æ²¿ç€æ¢¯åº¦åæ–¹å‘å¯»æ‰¾æœ€å°å€¼ï¼Œå› æ­¤gradient descenté€‚ç”¨äºå„ç§åœºæ™¯ï¼Œç”šè‡³æ˜¯non-convex optimizationï¼Œè™½ç„¶ä¸èƒ½ä¿è¯æ˜¯å…¨å±€æœ€ä¼˜ï¼Œä½†è‡³å°‘gradient descentæ˜¯å¯ä»¥å€¼å¾—ä¸€è¯•çš„æ–¹æ³•ã€‚</p>\n<p>ä¸‹é¢æ¥æ€»ç»“ä¸€ä¸‹ï¼š</p>\n<ul>\n<li>Gradient descent å’Œ newtonâ€™s methodéƒ½æ˜¯åˆ©ç”¨Taylor serieså¯¹objective functionè¿›è¡Œæ‹Ÿåˆæ¥å®ç°è¿­ä»£çš„ï¼›</li>\n<li>Gradient descent é‡‡ç”¨ä¸€æ¬¡å‹functionæ‹Ÿåˆè€Œ newtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡å‹functionï¼Œå› æ­¤newtonâ€™s methodè¿­ä»£æ›´è¿…é€Ÿï¼›</li>\n<li>Newtonâ€™s methodæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—Hessian matrixçš„é€†ï¼Œåœ¨é«˜ç»´featureæƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æ¯æ¬¡è¿­ä»£ä¼šæ¯”è¾ƒæ…¢ï¼›</li>\n<li>Newtonâ€™s methodåœ¨non-convex optimizationä¸­å¾ˆå—é™åˆ¶ï¼Œè€Œgradient descentåˆ™ä¸å—å½±å“ã€‚</li>\n</ul>\n<p>å¥½äº†ï¼Œå…ˆå†™è¿™ä¹ˆå¤šï¼Œè¿™å…¶ä¸­çš„çŸ¥è¯†é‡è¿˜æ˜¯å¾ˆæ·±å¥¥çš„ï¼Œä¹Ÿä¸çŸ¥é“è‡ªå·±æœ‰æ²¡æœ‰å™è¿°æ˜ç™½ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥è®¨è®ºï¼</p>\n<p><strong>æœ€åæ„Ÿè°¢ä¼˜ç”·çš„å®è´µæ„è§ï¼</strong></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf\" target=\"_blank\" rel=\"noopener\">UCLA courseware</a></li>\n<li><a href=\"https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf\" target=\"_blank\" rel=\"noopener\">CCU courseware</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"noopener\">Taylor series</a></li>\n</ul>"},{"title":"æ·±å…¥èŠèŠæ­£åˆ™åŒ–","date":"2017-08-26T17:55:07.000Z","_content":"æœ€è¿‘å’Œä¼˜ç”·ä¸€èµ·èŠåˆ°äº†L1å’ŒL2 regularizationï¼ŒæœŸé—´é‡åˆ°äº†å¾ˆå¤šæ²¡æœ‰æƒ³æ˜ç™½çš„é—®é¢˜ï¼ŒåŠ ä¸Šæœ€è¿‘å·¥ä½œæœ‰äº›å¿™ï¼Œç©ºä½™æ—¶é—´ç”¨æ¥å€’è…¾æ–°åˆ°è´§çš„å°ç±³è·¯ç”±å™¨ï¼Œåªèƒ½è¶å‘¨æœ«è‡ªå·±ç ”ç©¶ç ”ç©¶ï¼Œä¸‹é¢å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹regularizationä¸­ä¸€äº›æ·±å…¥çš„é—®é¢˜ã€‚ä¸è®¨è®ºåŸºç¡€çŸ¥è¯†ï¼Œç›´æ¥ä¸Šå¹²è´§ã€‚\n<!--more-->\n## MAP and regularization\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå½“cost functionåœ¨æ²¡æœ‰åŠ regularizationçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹å‚æ•°ä½¿ç”¨çš„æ˜¯**MLE**(Maximum likelihood estimation)ï¼Œå¯¹åº”é¢‘ç‡å­¦æ´¾æ‰€è®¤ä¸ºçš„å‚æ•°æœ¬æ— åˆ†å¸ƒè§„å¾‹çš„è§‚ç‚¹ï¼›åœ¨Andrew Ngç»å…¸çš„CS229ä¸­ï¼Œè¿™ä½AIå¤§å¸ˆæ›¾ç»æåˆ°ï¼Œregularizationå…¶å®æ˜¯å¯¹å‚æ•°çš„**MAP**(Maximum a posteriori estimation)ï¼Œæ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾è®¤ä¸ºçš„å‚æ•°æœ¬æœ‰**priori distribution**ï¼ŒåŒæ—¶å¸çº³äº†MLEçš„ä¸€ç§ä¸­é—´è§‚ç‚¹ã€‚\n\nè¿™é‡Œçš„priori distributionï¼Œå°±æ˜¯æ ¹æ®ç»éªŒï¼Œè®¤ä¸ºå‚æ•°åº”è¯¥å¤§è‡´ç¬¦åˆæŸä¸ªdistributionï¼Œè¿™æ ·ï¼Œæœ€ç»ˆè·å¾—çš„å‚æ•°ä¼°è®¡ç»“æœä¹Ÿä¼šå’Œè¿™ä¸ªè¢«è®¤ä¸ºçš„distributionæœ‰ä¸€äº›ç›¸è¿‘\n\nè€Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„L1 regularizationï¼Œå…¶å®å°±æ˜¯è®¤ä¸ºå‚æ•°çš„priori distributionæ˜¯**Laplacian distribution**ï¼Œè€ŒL2 regularizationï¼Œåˆ™è®¤ä¸ºå‚æ•°çš„priorit distributionæ˜¯**Gaussian distribution**ï¼Œç›¸ä¿¡å¤§å®¶å¯¹Gaussian distributionæ˜¯å¾ˆç†Ÿæ‚‰çš„ï¼Œè€Œå¯¹äºLaplacian distributionï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\nä¸‹å›¾å°±æ˜¯ä¸¤è€…çš„ä¸€ä¸ªæ¯”è¾ƒï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png)\nåœ¨åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¸¤ç§distributionçš„ç‰¹ç‚¹å†³å®šäº†ä¸¤ç§regularizationçš„æ€§è´¨ã€‚\n### Lasso regression\nåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\\\( \\theta\\\\)æœä»Laplacian distributionï¼Œcost functionå°±æˆäº†\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\nä¸Šå¼å°±æ˜¯Lasso regression\n### Ridge regression\nåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\\\( \\theta\\\\)æœä»Gaussian distributionï¼Œcost functionå°±æˆäº†\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\nä¸Šå¼å°±æ˜¯Ridge regressionæˆ–shrinkage\n## geometry of error surfaces\nåœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œcost functionçš„å½¢å¼æ˜¯\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\nç”¨äºŒç»´æˆªé¢å›¾å±•ç¤ºå°±æ˜¯\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png)\nå›¾ä¸­åªæœ‰objective functionï¼Œæ¨ªçºµè½´æ˜¯å‚æ•°\\\\( \\theta\\\\)ï¼Œæˆªå–è¿‡æ¥çš„å›¾ï¼Œæ‰€ä»¥ä¸Šé¢çš„å‚æ•°æ˜¯\\\\(w\\\\)ï¼Œ\\\\(l\\\\)æ˜¯losså€¼ï¼Œç®­å¤´æŒ‡å‘çš„ç‚¹å°±æ˜¯cost functionçš„æå°ç‚¹ã€‚åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬çš„optimization target.\n\nä¸‹é¢å¤§å®¶æ¥æˆ‘ä¸€èµ·åšä¸€ä¸ªå¤´è„‘é£æš´ï¼Œæ‰€è°“å‚æ•°çš„priori distributionï¼Œå…¶å®å°±æ˜¯ç”¨æ¥é™åˆ¶æœ€åoptimizationç»“æœçš„ä¸€ä¸ªé™å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨åšä¸€ä¸ªå—é™åˆ¶çš„çš„convex optimizationï¼Œå³ï¼š\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\nå…¶ä¸­ï¼Œ\\\\( \\beta\\\\)æ˜¯ridgeæˆ–è€…lassoçš„æœ€å°å€¼ã€‚\né‚£ä¹ˆæ­¤æ—¶çš„å›¾å°±å˜æˆäº†ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png)\næˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŠ å…¥äº†é™åˆ¶åï¼Œæœ€ç»ˆçš„optimizationä¸æ˜¯è½åœ¨æå°å€¼ç‚¹ï¼Œè€Œæ˜¯è½åœ¨å›¾ä¸­æ‰€ç¤ºçš„ä½ç½®ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥æƒ³ï¼Œregularization itemçš„åŠ å…¥ï¼Œä½¿å¾—æ•´ä¸ªcost functionåœ¨å¯»æ‰¾æœ€å°å€¼çš„æ—¶å€™ï¼Œè¦å‡è¡¡çš„è€ƒè™‘objective functionå’Œregularization itemçš„å¤§å°.\n\nåœ¨è¿™ä¸ªåœ°æ–¹ï¼Œæˆ‘å’Œä¼˜ç”·è®¨è®ºçš„æ—¶å€™æœ‰ä¸€ä¸ªåœ°æ–¹æ²¡æœ‰æƒ³é€šï¼Œä¾‹å¦‚ä½¿ç”¨gradient descentè¿›è¡Œoptimzationçš„æ—¶å€™ï¼Œæ€ä¹ˆä¿è¯ä¼˜åŒ–å¯ä»¥è½åˆ°å›¾ä¸­çš„ç‚¹å‘¢ï¼Œæˆ‘æ˜¯è¿™ä¹ˆè€ƒè™‘çš„ï¼šå½“åŠ å…¥regularizationåï¼Œcost functionæœ¬èº«å°±æœ‰äº†å˜åŒ–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯gradientä¹Ÿå‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨gradient descentè¿­ä»£è¿‡ç¨‹ä¸­å°±å·²ç»æŠŠregularizationçš„å½±å“å¸¦äº†è¿›å»ï¼Œå› æ­¤åœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œå®é™…ä¸Šåº”è¯¥éƒ½æ˜¯æŒ‰ç…§ä¸Šå¼çš„é™åˆ¶è¿›è¡Œä¼˜åŒ–çš„ã€‚\n\nå½“ç„¶ï¼Œä¸Šå›¾ä¹Ÿå¯ä»¥ç”¨æ¥å°±æ˜¯ä¸ºä»€ä¹ˆlassoå¯ä»¥è·å¾—ç¨€ç–ç‰¹å¾ï¼Œé‚£å°±æ˜¯å› ä¸ºlassoæ›´å¯èƒ½åœ¨åæ ‡è½´ä¸Šå’Œobjective functionäº§ç”Ÿäº¤ç‚¹ï¼Œè¿›è€Œä½¿å¾—ä¸€äº›ç‰¹å¾å˜æˆ0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)\n","source":"_posts/ml-ridge-lasso.md","raw":"---\ntitle: æ·±å…¥èŠèŠæ­£åˆ™åŒ–\ndate: 2017-08-27 01:55:07\ntags: \n\t- regularization\n\t- MAP\n\t- ridge regression\n\t- lasso regression\ncategories: machine learning\n---\næœ€è¿‘å’Œä¼˜ç”·ä¸€èµ·èŠåˆ°äº†L1å’ŒL2 regularizationï¼ŒæœŸé—´é‡åˆ°äº†å¾ˆå¤šæ²¡æœ‰æƒ³æ˜ç™½çš„é—®é¢˜ï¼ŒåŠ ä¸Šæœ€è¿‘å·¥ä½œæœ‰äº›å¿™ï¼Œç©ºä½™æ—¶é—´ç”¨æ¥å€’è…¾æ–°åˆ°è´§çš„å°ç±³è·¯ç”±å™¨ï¼Œåªèƒ½è¶å‘¨æœ«è‡ªå·±ç ”ç©¶ç ”ç©¶ï¼Œä¸‹é¢å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹regularizationä¸­ä¸€äº›æ·±å…¥çš„é—®é¢˜ã€‚ä¸è®¨è®ºåŸºç¡€çŸ¥è¯†ï¼Œç›´æ¥ä¸Šå¹²è´§ã€‚\n<!--more-->\n## MAP and regularization\næˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå½“cost functionåœ¨æ²¡æœ‰åŠ regularizationçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹å‚æ•°ä½¿ç”¨çš„æ˜¯**MLE**(Maximum likelihood estimation)ï¼Œå¯¹åº”é¢‘ç‡å­¦æ´¾æ‰€è®¤ä¸ºçš„å‚æ•°æœ¬æ— åˆ†å¸ƒè§„å¾‹çš„è§‚ç‚¹ï¼›åœ¨Andrew Ngç»å…¸çš„CS229ä¸­ï¼Œè¿™ä½AIå¤§å¸ˆæ›¾ç»æåˆ°ï¼Œregularizationå…¶å®æ˜¯å¯¹å‚æ•°çš„**MAP**(Maximum a posteriori estimation)ï¼Œæ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾è®¤ä¸ºçš„å‚æ•°æœ¬æœ‰**priori distribution**ï¼ŒåŒæ—¶å¸çº³äº†MLEçš„ä¸€ç§ä¸­é—´è§‚ç‚¹ã€‚\n\nè¿™é‡Œçš„priori distributionï¼Œå°±æ˜¯æ ¹æ®ç»éªŒï¼Œè®¤ä¸ºå‚æ•°åº”è¯¥å¤§è‡´ç¬¦åˆæŸä¸ªdistributionï¼Œè¿™æ ·ï¼Œæœ€ç»ˆè·å¾—çš„å‚æ•°ä¼°è®¡ç»“æœä¹Ÿä¼šå’Œè¿™ä¸ªè¢«è®¤ä¸ºçš„distributionæœ‰ä¸€äº›ç›¸è¿‘\n\nè€Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„L1 regularizationï¼Œå…¶å®å°±æ˜¯è®¤ä¸ºå‚æ•°çš„priori distributionæ˜¯**Laplacian distribution**ï¼Œè€ŒL2 regularizationï¼Œåˆ™è®¤ä¸ºå‚æ•°çš„priorit distributionæ˜¯**Gaussian distribution**ï¼Œç›¸ä¿¡å¤§å®¶å¯¹Gaussian distributionæ˜¯å¾ˆç†Ÿæ‚‰çš„ï¼Œè€Œå¯¹äºLaplacian distributionï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯\n$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$\nä¸‹å›¾å°±æ˜¯ä¸¤è€…çš„ä¸€ä¸ªæ¯”è¾ƒï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png)\nåœ¨åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¸¤ç§distributionçš„ç‰¹ç‚¹å†³å®šäº†ä¸¤ç§regularizationçš„æ€§è´¨ã€‚\n### Lasso regression\nåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\\\( \\theta\\\\)æœä»Laplacian distributionï¼Œcost functionå°±æˆäº†\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1}| \\theta^{(i)}|$$\nä¸Šå¼å°±æ˜¯Lasso regression\n### Ridge regression\nåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\\\( \\theta\\\\)æœä»Gaussian distributionï¼Œcost functionå°±æˆäº†\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}_{j=1} ( \\theta^{(i)})^2$$\nä¸Šå¼å°±æ˜¯Ridge regressionæˆ–shrinkage\n## geometry of error surfaces\nåœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œcost functionçš„å½¢å¼æ˜¯\n$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\nç”¨äºŒç»´æˆªé¢å›¾å±•ç¤ºå°±æ˜¯\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png)\nå›¾ä¸­åªæœ‰objective functionï¼Œæ¨ªçºµè½´æ˜¯å‚æ•°\\\\( \\theta\\\\)ï¼Œæˆªå–è¿‡æ¥çš„å›¾ï¼Œæ‰€ä»¥ä¸Šé¢çš„å‚æ•°æ˜¯\\\\(w\\\\)ï¼Œ\\\\(l\\\\)æ˜¯losså€¼ï¼Œç®­å¤´æŒ‡å‘çš„ç‚¹å°±æ˜¯cost functionçš„æå°ç‚¹ã€‚åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬çš„optimization target.\n\nä¸‹é¢å¤§å®¶æ¥æˆ‘ä¸€èµ·åšä¸€ä¸ªå¤´è„‘é£æš´ï¼Œæ‰€è°“å‚æ•°çš„priori distributionï¼Œå…¶å®å°±æ˜¯ç”¨æ¥é™åˆ¶æœ€åoptimizationç»“æœçš„ä¸€ä¸ªé™å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨åšä¸€ä¸ªå—é™åˆ¶çš„çš„convex optimizationï¼Œå³ï¼š\n$$ \\theta=argmin \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$\n$$ s.t. \\sum^{d}_{j=1}| \\theta^{(i)}|^p \\geq \\beta$$\nå…¶ä¸­ï¼Œ\\\\( \\beta\\\\)æ˜¯ridgeæˆ–è€…lassoçš„æœ€å°å€¼ã€‚\né‚£ä¹ˆæ­¤æ—¶çš„å›¾å°±å˜æˆäº†ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png)\næˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŠ å…¥äº†é™åˆ¶åï¼Œæœ€ç»ˆçš„optimizationä¸æ˜¯è½åœ¨æå°å€¼ç‚¹ï¼Œè€Œæ˜¯è½åœ¨å›¾ä¸­æ‰€ç¤ºçš„ä½ç½®ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥æƒ³ï¼Œregularization itemçš„åŠ å…¥ï¼Œä½¿å¾—æ•´ä¸ªcost functionåœ¨å¯»æ‰¾æœ€å°å€¼çš„æ—¶å€™ï¼Œè¦å‡è¡¡çš„è€ƒè™‘objective functionå’Œregularization itemçš„å¤§å°.\n\nåœ¨è¿™ä¸ªåœ°æ–¹ï¼Œæˆ‘å’Œä¼˜ç”·è®¨è®ºçš„æ—¶å€™æœ‰ä¸€ä¸ªåœ°æ–¹æ²¡æœ‰æƒ³é€šï¼Œä¾‹å¦‚ä½¿ç”¨gradient descentè¿›è¡Œoptimzationçš„æ—¶å€™ï¼Œæ€ä¹ˆä¿è¯ä¼˜åŒ–å¯ä»¥è½åˆ°å›¾ä¸­çš„ç‚¹å‘¢ï¼Œæˆ‘æ˜¯è¿™ä¹ˆè€ƒè™‘çš„ï¼šå½“åŠ å…¥regularizationåï¼Œcost functionæœ¬èº«å°±æœ‰äº†å˜åŒ–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯gradientä¹Ÿå‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨gradient descentè¿­ä»£è¿‡ç¨‹ä¸­å°±å·²ç»æŠŠregularizationçš„å½±å“å¸¦äº†è¿›å»ï¼Œå› æ­¤åœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œå®é™…ä¸Šåº”è¯¥éƒ½æ˜¯æŒ‰ç…§ä¸Šå¼çš„é™åˆ¶è¿›è¡Œä¼˜åŒ–çš„ã€‚\n\nå½“ç„¶ï¼Œä¸Šå›¾ä¹Ÿå¯ä»¥ç”¨æ¥å°±æ˜¯ä¸ºä»€ä¹ˆlassoå¯ä»¥è·å¾—ç¨€ç–ç‰¹å¾ï¼Œé‚£å°±æ˜¯å› ä¸ºlassoæ›´å¯èƒ½åœ¨åæ ‡è½´ä¸Šå’Œobjective functionäº§ç”Ÿäº¤ç‚¹ï¼Œè¿›è€Œä½¿å¾—ä¸€äº›ç‰¹å¾å˜æˆ0.\n\n## Reference\n* [CS 195-5: Machine Learning](https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf)\n* [STAT 897D](https://onlinecourses.science.psu.edu/stat857/node/155)\n","slug":"ml-ridge-lasso","published":1,"updated":"2018-11-19T06:31:11.218Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pb000ptr8lfpv0o4d4","content":"<p>æœ€è¿‘å’Œä¼˜ç”·ä¸€èµ·èŠåˆ°äº†L1å’ŒL2 regularizationï¼ŒæœŸé—´é‡åˆ°äº†å¾ˆå¤šæ²¡æœ‰æƒ³æ˜ç™½çš„é—®é¢˜ï¼ŒåŠ ä¸Šæœ€è¿‘å·¥ä½œæœ‰äº›å¿™ï¼Œç©ºä½™æ—¶é—´ç”¨æ¥å€’è…¾æ–°åˆ°è´§çš„å°ç±³è·¯ç”±å™¨ï¼Œåªèƒ½è¶å‘¨æœ«è‡ªå·±ç ”ç©¶ç ”ç©¶ï¼Œä¸‹é¢å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹regularizationä¸­ä¸€äº›æ·±å…¥çš„é—®é¢˜ã€‚ä¸è®¨è®ºåŸºç¡€çŸ¥è¯†ï¼Œç›´æ¥ä¸Šå¹²è´§ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå½“cost functionåœ¨æ²¡æœ‰åŠ regularizationçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹å‚æ•°ä½¿ç”¨çš„æ˜¯<strong>MLE</strong>(Maximum likelihood estimation)ï¼Œå¯¹åº”é¢‘ç‡å­¦æ´¾æ‰€è®¤ä¸ºçš„å‚æ•°æœ¬æ— åˆ†å¸ƒè§„å¾‹çš„è§‚ç‚¹ï¼›åœ¨Andrew Ngç»å…¸çš„CS229ä¸­ï¼Œè¿™ä½AIå¤§å¸ˆæ›¾ç»æåˆ°ï¼Œregularizationå…¶å®æ˜¯å¯¹å‚æ•°çš„<strong>MAP</strong>(Maximum a posteriori estimation)ï¼Œæ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾è®¤ä¸ºçš„å‚æ•°æœ¬æœ‰<strong>priori distribution</strong>ï¼ŒåŒæ—¶å¸çº³äº†MLEçš„ä¸€ç§ä¸­é—´è§‚ç‚¹ã€‚</p>\n<p>è¿™é‡Œçš„priori distributionï¼Œå°±æ˜¯æ ¹æ®ç»éªŒï¼Œè®¤ä¸ºå‚æ•°åº”è¯¥å¤§è‡´ç¬¦åˆæŸä¸ªdistributionï¼Œè¿™æ ·ï¼Œæœ€ç»ˆè·å¾—çš„å‚æ•°ä¼°è®¡ç»“æœä¹Ÿä¼šå’Œè¿™ä¸ªè¢«è®¤ä¸ºçš„distributionæœ‰ä¸€äº›ç›¸è¿‘</p>\n<p>è€Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„L1 regularizationï¼Œå…¶å®å°±æ˜¯è®¤ä¸ºå‚æ•°çš„priori distributionæ˜¯<strong>Laplacian distribution</strong>ï¼Œè€ŒL2 regularizationï¼Œåˆ™è®¤ä¸ºå‚æ•°çš„priorit distributionæ˜¯<strong>Gaussian distribution</strong>ï¼Œç›¸ä¿¡å¤§å®¶å¯¹Gaussian distributionæ˜¯å¾ˆç†Ÿæ‚‰çš„ï¼Œè€Œå¯¹äºLaplacian distributionï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯<br>$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$<br>ä¸‹å›¾å°±æ˜¯ä¸¤è€…çš„ä¸€ä¸ªæ¯”è¾ƒï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png\" alt=\"\"><br>åœ¨åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¸¤ç§distributionçš„ç‰¹ç‚¹å†³å®šäº†ä¸¤ç§regularizationçš„æ€§è´¨ã€‚</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>åœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Laplacian distributionï¼Œcost functionå°±æˆäº†<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1}| \\theta^{(i)}|$$<br>ä¸Šå¼å°±æ˜¯Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>åœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Gaussian distributionï¼Œcost functionå°±æˆäº†<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1} ( \\theta^{(i)})^2$$<br>ä¸Šå¼å°±æ˜¯Ridge regressionæˆ–shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œcost functionçš„å½¢å¼æ˜¯<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>ç”¨äºŒç»´æˆªé¢å›¾å±•ç¤ºå°±æ˜¯<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png\" alt=\"\"><br>å›¾ä¸­åªæœ‰objective functionï¼Œæ¨ªçºµè½´æ˜¯å‚æ•°\\( \\theta\\)ï¼Œæˆªå–è¿‡æ¥çš„å›¾ï¼Œæ‰€ä»¥ä¸Šé¢çš„å‚æ•°æ˜¯\\(w\\)ï¼Œ\\(l\\)æ˜¯losså€¼ï¼Œç®­å¤´æŒ‡å‘çš„ç‚¹å°±æ˜¯cost functionçš„æå°ç‚¹ã€‚åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬çš„optimization target.</p>\n<p>ä¸‹é¢å¤§å®¶æ¥æˆ‘ä¸€èµ·åšä¸€ä¸ªå¤´è„‘é£æš´ï¼Œæ‰€è°“å‚æ•°çš„priori distributionï¼Œå…¶å®å°±æ˜¯ç”¨æ¥é™åˆ¶æœ€åoptimizationç»“æœçš„ä¸€ä¸ªé™å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨åšä¸€ä¸ªå—é™åˆ¶çš„çš„convex optimizationï¼Œå³ï¼š<br>$$ \\theta=argmin \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>$$ s.t. \\sum^{d}</em>{j=1}| \\theta^{(i)}|^p \\geq \\beta$$<br>å…¶ä¸­ï¼Œ\\( \\beta\\)æ˜¯ridgeæˆ–è€…lassoçš„æœ€å°å€¼ã€‚<br>é‚£ä¹ˆæ­¤æ—¶çš„å›¾å°±å˜æˆäº†ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png\" alt=\"\"><br>æˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŠ å…¥äº†é™åˆ¶åï¼Œæœ€ç»ˆçš„optimizationä¸æ˜¯è½åœ¨æå°å€¼ç‚¹ï¼Œè€Œæ˜¯è½åœ¨å›¾ä¸­æ‰€ç¤ºçš„ä½ç½®ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥æƒ³ï¼Œregularization itemçš„åŠ å…¥ï¼Œä½¿å¾—æ•´ä¸ªcost functionåœ¨å¯»æ‰¾æœ€å°å€¼çš„æ—¶å€™ï¼Œè¦å‡è¡¡çš„è€ƒè™‘objective functionå’Œregularization itemçš„å¤§å°.</p>\n<p>åœ¨è¿™ä¸ªåœ°æ–¹ï¼Œæˆ‘å’Œä¼˜ç”·è®¨è®ºçš„æ—¶å€™æœ‰ä¸€ä¸ªåœ°æ–¹æ²¡æœ‰æƒ³é€šï¼Œä¾‹å¦‚ä½¿ç”¨gradient descentè¿›è¡Œoptimzationçš„æ—¶å€™ï¼Œæ€ä¹ˆä¿è¯ä¼˜åŒ–å¯ä»¥è½åˆ°å›¾ä¸­çš„ç‚¹å‘¢ï¼Œæˆ‘æ˜¯è¿™ä¹ˆè€ƒè™‘çš„ï¼šå½“åŠ å…¥regularizationåï¼Œcost functionæœ¬èº«å°±æœ‰äº†å˜åŒ–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯gradientä¹Ÿå‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨gradient descentè¿­ä»£è¿‡ç¨‹ä¸­å°±å·²ç»æŠŠregularizationçš„å½±å“å¸¦äº†è¿›å»ï¼Œå› æ­¤åœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œå®é™…ä¸Šåº”è¯¥éƒ½æ˜¯æŒ‰ç…§ä¸Šå¼çš„é™åˆ¶è¿›è¡Œä¼˜åŒ–çš„ã€‚</p>\n<p>å½“ç„¶ï¼Œä¸Šå›¾ä¹Ÿå¯ä»¥ç”¨æ¥å°±æ˜¯ä¸ºä»€ä¹ˆlassoå¯ä»¥è·å¾—ç¨€ç–ç‰¹å¾ï¼Œé‚£å°±æ˜¯å› ä¸ºlassoæ›´å¯èƒ½åœ¨åæ ‡è½´ä¸Šå’Œobjective functionäº§ç”Ÿäº¤ç‚¹ï¼Œè¿›è€Œä½¿å¾—ä¸€äº›ç‰¹å¾å˜æˆ0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"noopener\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"noopener\">STAT 897D</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"regularization","path":"tags/regularization/"},{"name":"MAP","path":"tags/MAP/"},{"name":"ridge regression","path":"tags/ridge-regression/"},{"name":"lasso regression","path":"tags/lasso-regression/"}],"excerpt":"<p>æœ€è¿‘å’Œä¼˜ç”·ä¸€èµ·èŠåˆ°äº†L1å’ŒL2 regularizationï¼ŒæœŸé—´é‡åˆ°äº†å¾ˆå¤šæ²¡æœ‰æƒ³æ˜ç™½çš„é—®é¢˜ï¼ŒåŠ ä¸Šæœ€è¿‘å·¥ä½œæœ‰äº›å¿™ï¼Œç©ºä½™æ—¶é—´ç”¨æ¥å€’è…¾æ–°åˆ°è´§çš„å°ç±³è·¯ç”±å™¨ï¼Œåªèƒ½è¶å‘¨æœ«è‡ªå·±ç ”ç©¶ç ”ç©¶ï¼Œä¸‹é¢å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹regularizationä¸­ä¸€äº›æ·±å…¥çš„é—®é¢˜ã€‚ä¸è®¨è®ºåŸºç¡€çŸ¥è¯†ï¼Œç›´æ¥ä¸Šå¹²è´§ã€‚<br></p>","more":"</p>\n<h2 id=\"MAP-and-regularization\"><a href=\"#MAP-and-regularization\" class=\"headerlink\" title=\"MAP and regularization\"></a>MAP and regularization</h2><p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå½“cost functionåœ¨æ²¡æœ‰åŠ regularizationçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹å‚æ•°ä½¿ç”¨çš„æ˜¯<strong>MLE</strong>(Maximum likelihood estimation)ï¼Œå¯¹åº”é¢‘ç‡å­¦æ´¾æ‰€è®¤ä¸ºçš„å‚æ•°æœ¬æ— åˆ†å¸ƒè§„å¾‹çš„è§‚ç‚¹ï¼›åœ¨Andrew Ngç»å…¸çš„CS229ä¸­ï¼Œè¿™ä½AIå¤§å¸ˆæ›¾ç»æåˆ°ï¼Œregularizationå…¶å®æ˜¯å¯¹å‚æ•°çš„<strong>MAP</strong>(Maximum a posteriori estimation)ï¼Œæ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾è®¤ä¸ºçš„å‚æ•°æœ¬æœ‰<strong>priori distribution</strong>ï¼ŒåŒæ—¶å¸çº³äº†MLEçš„ä¸€ç§ä¸­é—´è§‚ç‚¹ã€‚</p>\n<p>è¿™é‡Œçš„priori distributionï¼Œå°±æ˜¯æ ¹æ®ç»éªŒï¼Œè®¤ä¸ºå‚æ•°åº”è¯¥å¤§è‡´ç¬¦åˆæŸä¸ªdistributionï¼Œè¿™æ ·ï¼Œæœ€ç»ˆè·å¾—çš„å‚æ•°ä¼°è®¡ç»“æœä¹Ÿä¼šå’Œè¿™ä¸ªè¢«è®¤ä¸ºçš„distributionæœ‰ä¸€äº›ç›¸è¿‘</p>\n<p>è€Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„L1 regularizationï¼Œå…¶å®å°±æ˜¯è®¤ä¸ºå‚æ•°çš„priori distributionæ˜¯<strong>Laplacian distribution</strong>ï¼Œè€ŒL2 regularizationï¼Œåˆ™è®¤ä¸ºå‚æ•°çš„priorit distributionæ˜¯<strong>Gaussian distribution</strong>ï¼Œç›¸ä¿¡å¤§å®¶å¯¹Gaussian distributionæ˜¯å¾ˆç†Ÿæ‚‰çš„ï¼Œè€Œå¯¹äºLaplacian distributionï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯<br>$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$<br>ä¸‹å›¾å°±æ˜¯ä¸¤è€…çš„ä¸€ä¸ªæ¯”è¾ƒï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-1.png\" alt=\"\"><br>åœ¨åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¸¤ç§distributionçš„ç‰¹ç‚¹å†³å®šäº†ä¸¤ç§regularizationçš„æ€§è´¨ã€‚</p>\n<h3 id=\"Lasso-regression\"><a href=\"#Lasso-regression\" class=\"headerlink\" title=\"Lasso regression\"></a>Lasso regression</h3><p>åœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Laplacian distributionï¼Œcost functionå°±æˆäº†<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1}| \\theta^{(i)}|$$<br>ä¸Šå¼å°±æ˜¯Lasso regression</p>\n<h3 id=\"Ridge-regression\"><a href=\"#Ridge-regression\" class=\"headerlink\" title=\"Ridge regression\"></a>Ridge regression</h3><p>åœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Gaussian distributionï¼Œcost functionå°±æˆäº†<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}</em>{j=1} ( \\theta^{(i)})^2$$<br>ä¸Šå¼å°±æ˜¯Ridge regressionæˆ–shrinkage</p>\n<h2 id=\"geometry-of-error-surfaces\"><a href=\"#geometry-of-error-surfaces\" class=\"headerlink\" title=\"geometry of error surfaces\"></a>geometry of error surfaces</h2><p>åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œcost functionçš„å½¢å¼æ˜¯<br>$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>ç”¨äºŒç»´æˆªé¢å›¾å±•ç¤ºå°±æ˜¯<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-2.png\" alt=\"\"><br>å›¾ä¸­åªæœ‰objective functionï¼Œæ¨ªçºµè½´æ˜¯å‚æ•°\\( \\theta\\)ï¼Œæˆªå–è¿‡æ¥çš„å›¾ï¼Œæ‰€ä»¥ä¸Šé¢çš„å‚æ•°æ˜¯\\(w\\)ï¼Œ\\(l\\)æ˜¯losså€¼ï¼Œç®­å¤´æŒ‡å‘çš„ç‚¹å°±æ˜¯cost functionçš„æå°ç‚¹ã€‚åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬çš„optimization target.</p>\n<p>ä¸‹é¢å¤§å®¶æ¥æˆ‘ä¸€èµ·åšä¸€ä¸ªå¤´è„‘é£æš´ï¼Œæ‰€è°“å‚æ•°çš„priori distributionï¼Œå…¶å®å°±æ˜¯ç”¨æ¥é™åˆ¶æœ€åoptimizationç»“æœçš„ä¸€ä¸ªé™å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨åšä¸€ä¸ªå—é™åˆ¶çš„çš„convex optimizationï¼Œå³ï¼š<br>$$ \\theta=argmin \\frac{1}{2} \\sum^{m}<em>{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$<br>$$ s.t. \\sum^{d}</em>{j=1}| \\theta^{(i)}|^p \\geq \\beta$$<br>å…¶ä¸­ï¼Œ\\( \\beta\\)æ˜¯ridgeæˆ–è€…lassoçš„æœ€å°å€¼ã€‚<br>é‚£ä¹ˆæ­¤æ—¶çš„å›¾å°±å˜æˆäº†ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/4/4-3.png\" alt=\"\"><br>æˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŠ å…¥äº†é™åˆ¶åï¼Œæœ€ç»ˆçš„optimizationä¸æ˜¯è½åœ¨æå°å€¼ç‚¹ï¼Œè€Œæ˜¯è½åœ¨å›¾ä¸­æ‰€ç¤ºçš„ä½ç½®ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥æƒ³ï¼Œregularization itemçš„åŠ å…¥ï¼Œä½¿å¾—æ•´ä¸ªcost functionåœ¨å¯»æ‰¾æœ€å°å€¼çš„æ—¶å€™ï¼Œè¦å‡è¡¡çš„è€ƒè™‘objective functionå’Œregularization itemçš„å¤§å°.</p>\n<p>åœ¨è¿™ä¸ªåœ°æ–¹ï¼Œæˆ‘å’Œä¼˜ç”·è®¨è®ºçš„æ—¶å€™æœ‰ä¸€ä¸ªåœ°æ–¹æ²¡æœ‰æƒ³é€šï¼Œä¾‹å¦‚ä½¿ç”¨gradient descentè¿›è¡Œoptimzationçš„æ—¶å€™ï¼Œæ€ä¹ˆä¿è¯ä¼˜åŒ–å¯ä»¥è½åˆ°å›¾ä¸­çš„ç‚¹å‘¢ï¼Œæˆ‘æ˜¯è¿™ä¹ˆè€ƒè™‘çš„ï¼šå½“åŠ å…¥regularizationåï¼Œcost functionæœ¬èº«å°±æœ‰äº†å˜åŒ–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯gradientä¹Ÿå‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨gradient descentè¿­ä»£è¿‡ç¨‹ä¸­å°±å·²ç»æŠŠregularizationçš„å½±å“å¸¦äº†è¿›å»ï¼Œå› æ­¤åœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œå®é™…ä¸Šåº”è¯¥éƒ½æ˜¯æŒ‰ç…§ä¸Šå¼çš„é™åˆ¶è¿›è¡Œä¼˜åŒ–çš„ã€‚</p>\n<p>å½“ç„¶ï¼Œä¸Šå›¾ä¹Ÿå¯ä»¥ç”¨æ¥å°±æ˜¯ä¸ºä»€ä¹ˆlassoå¯ä»¥è·å¾—ç¨€ç–ç‰¹å¾ï¼Œé‚£å°±æ˜¯å› ä¸ºlassoæ›´å¯èƒ½åœ¨åæ ‡è½´ä¸Šå’Œobjective functionäº§ç”Ÿäº¤ç‚¹ï¼Œè¿›è€Œä½¿å¾—ä¸€äº›ç‰¹å¾å˜æˆ0.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/91a9/5626d24c8393e3b784e44f62de201d20dede.pdf\" target=\"_blank\" rel=\"noopener\">CS 195-5: Machine Learning</a></li>\n<li><a href=\"https://onlinecourses.science.psu.edu/stat857/node/155\" target=\"_blank\" rel=\"noopener\">STAT 897D</a></li>\n</ul>"},{"title":"Hello World","date":"2017-07-26T09:40:40.000Z","_content":"**æˆ‘ç»ˆäºæŠŠblogæ­å»ºèµ·æ¥äº†!**\n\nè¿™æ˜¯ä¸€ä¸ªå±äº**Asir** è‡ªå·±çš„åšå®¢,åœ¨è¿™é‡Œæˆ‘ä¼šå†™ä¸€äº›æŠ€æœ¯åˆ†äº«,è®°å½•è‡ªå·±å¹³æ—¶å­¦åˆ°çš„ä¸œè¥¿,ä¹Ÿä¼šæ•´ç‚¹åæ§½æˆ–è€…é¸¡æ±¤.æ€»ä¹‹,æœ‰äº†ä¸€ä¸ªçœŸæ­£çš„å±äºè‡ªå·±çš„å¤©åœ°,å¯ä»¥éšä¾¿æ•´,è¿™ç§æ„Ÿè§‰éå¸¸æ£’.\n\nå…¶å®è‡ªå·±åœ¨åšå®¢å›­ä¹Ÿå°è¯•è¿‡ä¸€æ¬¡,å¯æ˜¯ä½“éªŒå¹¶ä¸æ˜¯å¾ˆç†æƒ³,åœ¨è¿™é‡Œæˆ‘å¹¶æ²¡æœ‰æŠ¨å‡»çš„æ„æ€,å› ä¸ºè‡ªå·±æ­å»ºèµ·æ¥çš„æˆå°±æ„Ÿé‚£ä¸æ˜¯ä¸€ç‚¹ä¸¤ç‚¹.å¸Œæœ›åé¢å¯ä»¥è¶çƒ­æ‰“é“,å¼€å¯blogä¹‹æ—….\n<!--more-->\n***\nåœ¨è¿™é‡Œæ„Ÿè°¢ä¸‹äº²é“[**åœˆç¾Š**](https://www.unbelievable9.info/)ä¸ºæˆ‘æä¾›çš„å®Œç¾è®¾å¤‡å’Œæ·±å¤œæŠ€æœ¯æ”¯æŒ,éå¸¸æ£’!\n\næœ€å,ä½œä¸ºä¸€ä¸ªcoding man, åœ¨æ‰€æœ‰äº‹æƒ…çš„æœ€å¼€å§‹,éƒ½ä¸åº”è¯¥ç¼ºå°‘è¿™å¥è¯\n\n**Hello world!!!**\n","source":"_posts/other-hello.md","raw":"---\ntitle: Hello World\ndate: 2017-07-26 17:40:40\ntags: life\ncategories: others\n---\n**æˆ‘ç»ˆäºæŠŠblogæ­å»ºèµ·æ¥äº†!**\n\nè¿™æ˜¯ä¸€ä¸ªå±äº**Asir** è‡ªå·±çš„åšå®¢,åœ¨è¿™é‡Œæˆ‘ä¼šå†™ä¸€äº›æŠ€æœ¯åˆ†äº«,è®°å½•è‡ªå·±å¹³æ—¶å­¦åˆ°çš„ä¸œè¥¿,ä¹Ÿä¼šæ•´ç‚¹åæ§½æˆ–è€…é¸¡æ±¤.æ€»ä¹‹,æœ‰äº†ä¸€ä¸ªçœŸæ­£çš„å±äºè‡ªå·±çš„å¤©åœ°,å¯ä»¥éšä¾¿æ•´,è¿™ç§æ„Ÿè§‰éå¸¸æ£’.\n\nå…¶å®è‡ªå·±åœ¨åšå®¢å›­ä¹Ÿå°è¯•è¿‡ä¸€æ¬¡,å¯æ˜¯ä½“éªŒå¹¶ä¸æ˜¯å¾ˆç†æƒ³,åœ¨è¿™é‡Œæˆ‘å¹¶æ²¡æœ‰æŠ¨å‡»çš„æ„æ€,å› ä¸ºè‡ªå·±æ­å»ºèµ·æ¥çš„æˆå°±æ„Ÿé‚£ä¸æ˜¯ä¸€ç‚¹ä¸¤ç‚¹.å¸Œæœ›åé¢å¯ä»¥è¶çƒ­æ‰“é“,å¼€å¯blogä¹‹æ—….\n<!--more-->\n***\nåœ¨è¿™é‡Œæ„Ÿè°¢ä¸‹äº²é“[**åœˆç¾Š**](https://www.unbelievable9.info/)ä¸ºæˆ‘æä¾›çš„å®Œç¾è®¾å¤‡å’Œæ·±å¤œæŠ€æœ¯æ”¯æŒ,éå¸¸æ£’!\n\næœ€å,ä½œä¸ºä¸€ä¸ªcoding man, åœ¨æ‰€æœ‰äº‹æƒ…çš„æœ€å¼€å§‹,éƒ½ä¸åº”è¯¥ç¼ºå°‘è¿™å¥è¯\n\n**Hello world!!!**\n","slug":"other-hello","published":1,"updated":"2018-10-06T08:01:10.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pc000rtr8lpp3mfyfw","content":"<p><strong>æˆ‘ç»ˆäºæŠŠblogæ­å»ºèµ·æ¥äº†!</strong></p>\n<p>è¿™æ˜¯ä¸€ä¸ªå±äº<strong>Asir</strong> è‡ªå·±çš„åšå®¢,åœ¨è¿™é‡Œæˆ‘ä¼šå†™ä¸€äº›æŠ€æœ¯åˆ†äº«,è®°å½•è‡ªå·±å¹³æ—¶å­¦åˆ°çš„ä¸œè¥¿,ä¹Ÿä¼šæ•´ç‚¹åæ§½æˆ–è€…é¸¡æ±¤.æ€»ä¹‹,æœ‰äº†ä¸€ä¸ªçœŸæ­£çš„å±äºè‡ªå·±çš„å¤©åœ°,å¯ä»¥éšä¾¿æ•´,è¿™ç§æ„Ÿè§‰éå¸¸æ£’.</p>\n<p>å…¶å®è‡ªå·±åœ¨åšå®¢å›­ä¹Ÿå°è¯•è¿‡ä¸€æ¬¡,å¯æ˜¯ä½“éªŒå¹¶ä¸æ˜¯å¾ˆç†æƒ³,åœ¨è¿™é‡Œæˆ‘å¹¶æ²¡æœ‰æŠ¨å‡»çš„æ„æ€,å› ä¸ºè‡ªå·±æ­å»ºèµ·æ¥çš„æˆå°±æ„Ÿé‚£ä¸æ˜¯ä¸€ç‚¹ä¸¤ç‚¹.å¸Œæœ›åé¢å¯ä»¥è¶çƒ­æ‰“é“,å¼€å¯blogä¹‹æ—….<br><a id=\"more\"></a></p>\n<hr>\n<p>åœ¨è¿™é‡Œæ„Ÿè°¢ä¸‹äº²é“<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"noopener\"><strong>åœˆç¾Š</strong></a>ä¸ºæˆ‘æä¾›çš„å®Œç¾è®¾å¤‡å’Œæ·±å¤œæŠ€æœ¯æ”¯æŒ,éå¸¸æ£’!</p>\n<p>æœ€å,ä½œä¸ºä¸€ä¸ªcoding man, åœ¨æ‰€æœ‰äº‹æƒ…çš„æœ€å¼€å§‹,éƒ½ä¸åº”è¯¥ç¼ºå°‘è¿™å¥è¯</p>\n<p><strong>Hello world!!!</strong></p>\n","site":{"data":{}},"_categories":[{"name":"others","path":"categories/others/"}],"_tags":[{"name":"life","path":"tags/life/"}],"excerpt":"<p><strong>æˆ‘ç»ˆäºæŠŠblogæ­å»ºèµ·æ¥äº†!</strong></p>\n<p>è¿™æ˜¯ä¸€ä¸ªå±äº<strong>Asir</strong> è‡ªå·±çš„åšå®¢,åœ¨è¿™é‡Œæˆ‘ä¼šå†™ä¸€äº›æŠ€æœ¯åˆ†äº«,è®°å½•è‡ªå·±å¹³æ—¶å­¦åˆ°çš„ä¸œè¥¿,ä¹Ÿä¼šæ•´ç‚¹åæ§½æˆ–è€…é¸¡æ±¤.æ€»ä¹‹,æœ‰äº†ä¸€ä¸ªçœŸæ­£çš„å±äºè‡ªå·±çš„å¤©åœ°,å¯ä»¥éšä¾¿æ•´,è¿™ç§æ„Ÿè§‰éå¸¸æ£’.</p>\n<p>å…¶å®è‡ªå·±åœ¨åšå®¢å›­ä¹Ÿå°è¯•è¿‡ä¸€æ¬¡,å¯æ˜¯ä½“éªŒå¹¶ä¸æ˜¯å¾ˆç†æƒ³,åœ¨è¿™é‡Œæˆ‘å¹¶æ²¡æœ‰æŠ¨å‡»çš„æ„æ€,å› ä¸ºè‡ªå·±æ­å»ºèµ·æ¥çš„æˆå°±æ„Ÿé‚£ä¸æ˜¯ä¸€ç‚¹ä¸¤ç‚¹.å¸Œæœ›åé¢å¯ä»¥è¶çƒ­æ‰“é“,å¼€å¯blogä¹‹æ—….<br></p>","more":"</p>\n<hr>\n<p>åœ¨è¿™é‡Œæ„Ÿè°¢ä¸‹äº²é“<a href=\"https://www.unbelievable9.info/\" target=\"_blank\" rel=\"noopener\"><strong>åœˆç¾Š</strong></a>ä¸ºæˆ‘æä¾›çš„å®Œç¾è®¾å¤‡å’Œæ·±å¤œæŠ€æœ¯æ”¯æŒ,éå¸¸æ£’!</p>\n<p>æœ€å,ä½œä¸ºä¸€ä¸ªcoding man, åœ¨æ‰€æœ‰äº‹æƒ…çš„æœ€å¼€å§‹,éƒ½ä¸åº”è¯¥ç¼ºå°‘è¿™å¥è¯</p>\n<p><strong>Hello world!!!</strong></p>"},{"title":"Imbalanced data é—®é¢˜æ€»ç»“æ–¹æ³•æ±‡æ€»","date":"2017-11-11T15:01:09.000Z","_content":"Helloï¼Œå¤§å®¶å¥½ï¼ŒåŒåä¸€çœŸçš„å¾ˆç´¯ï¼Œä¸€ç›´åœ¨åŠ ç­ï¼Œå¿™é‡Œå·é—²çœ‹äº†[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)ï¼Œæ„Ÿè§‰paperå‘ˆç°çš„ç ”ç©¶å†…å®¹æ„Ÿè§‰å¾ˆä¸€èˆ¬ï¼Œä½†æ˜¯ï¼Œpaperä¸­å…³äºimbalanced dataçš„solutionæ–¹æ³•å€’æ˜¯å†™çš„å¾ˆä¸é”™ï¼Œä¹Ÿå‹¾èµ·äº†æˆ‘å¯¹äºè¿™ä¸€å—æ€»ç»“çš„æ¬²æœ›ã€‚ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡å…³äºimbalanced dataçš„paper notesï¼Œä½†æ˜¯å¯¹äºè¿™ä¸€å—çš„å…·ä½“æ–¹æ³•æ€»ç»“è¿˜ä¸æ˜¯å¾ˆè¶³å¤Ÿï¼Œäºæ˜¯ç”¨è¿™ç¯‡paperä¸ºä¸»çº¿å¥½å¥½sum upä¸€è®¡ã€‚\n\næˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚\n<!--more-->\n## Data level methods\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹data level methodsï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªå…±æ€§ï¼Œé‚£å°±æ˜¯é€šè¿‡æ”¹å˜dataçš„æ•°é‡æ¥å®Œæˆå¯¹imbalanced data problemçš„è§£å†³ã€‚\n### Oversampling\nOversamplingå¯ä»¥è¯´æ˜¯æœ€ç›´è§‚çš„solutionä¹‹ä¸€ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¯¹äºè¾ƒå°‘ä¸€ç±»åˆ«çš„samplesï¼Œè¿‡æ­¤é‡å¤é‡‡æ ·ï¼Œä»¥æ­¤è®©ä¸¤ç§ç±»åˆ«çš„æ ·æœ¬æ¥è¿‘å¹³è¡¡ã€‚**ä½†æ˜¯ï¼Œå¯¹äºä¸€ä¸ªsampleå¤šæ¬¡é‡å¤è®­ç»ƒï¼Œå¾ˆæœ‰å¯èƒ½å¸¦æ¥overfitting**ï¼Œå› æ­¤ï¼Œç®€å•ç²—æš´çš„é‡å¤é‡‡æ ·å¹¶ä¸å¯å–ã€‚å› æ­¤ï¼Œå¾ˆå¤šæ”¹è¿›çš„ç‰ˆæœ¬åº”è¿è€Œç”Ÿï¼š\n#### SMOTE\nSMOTEç®—æ³•æ˜¯ä¸€ç§ç»å…¸çš„oversamplingæ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯å¯¹è¾ƒå°‘æ•°ç›®ç±»åˆ«çš„æ ·æœ¬ï¼ŒéšæœºæŠ½å–\\\\(m\\\\)ä¸ªæ ·æœ¬ï¼Œå¯¹äºéšæœºæŠ½å–å‡ºçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰å–è·ç¦»æœ€è¿‘çš„\\\\(n\\\\)ä¸ªæ ·æœ¬ï¼Œåœ¨ä»–ä»¬çš„è¿çº¿ä¸Šéšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼Œä½œä¸ºè¾ƒå°‘ç±»åˆ«çš„è¡¥å……æ ·æœ¬ã€‚å‡è®¾åŸæ ·æœ¬ç‚¹ä¸º\\\\(x\\\\)ï¼Œè¢«é€‰ä¸­çš„é™„è¿‘çš„ç‚¹ä¸º\\\\(x'\\\\)ï¼Œåˆ™æ–°çš„æ ·æœ¬ç‚¹ä¸ºï¼š\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSMOTEå¯ä»¥å¯¹è¾ƒå°‘ç±»åˆ«æ ·æœ¬è¿›è¡Œæ‰©å……ï¼Œè¿›è€Œå®ç°oversamplingï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚\n#### Cluster-base oversampling\nCluster-basedæ–¹æ³•çš„æœ€å¤§ç‰¹ç‚¹è«è¿‡äºæœ€å¼€å§‹å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªèšç±»åˆ†æï¼Œæ•°æ®ä¼šå˜æˆæ•°ä¸ªclusterï¼Œç„¶åå¯¹äºæ¯ä¸€ä¸ªclusteråœ¨è¿›è¡Œæ•°æ®çš„oversamplingï¼Œ**åŒæ—¶å…¼é¡¾ç±»åˆ«ä¹‹é—´çš„between-class imbalanceï¼Œè¿˜è¦è€ƒè™‘åˆ°ç±»å†…éƒ¨å„ä¸ªclusterçš„within-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\nåŸpaperå¤§è‡´å™è¿°äº†æ•´ä¸ªæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬å¯¹imbalanced dataè¿›è¡Œk-means(æˆ–è€…å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥)èšç±»ï¼Œèšæˆå¤šä¸ªclusterä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œoversamplingï¼Œå‡è®¾majority classæœ‰\\\\(m\\\\)ä¸ªclusterï¼Œminorityæœ‰\\\\(n\\\\)ä¸ªclusterï¼Œæˆ‘ä»¬ä»¥clusteræœ€å¤§çš„dataæ•°ç›®\\\\(k\\\\)ä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬å…ˆå¯¹majority classä¸­æ‰€æœ‰clusterï¼Œéƒ½è¿›è¡Œoversamplingï¼Œä½¿å¾—ä»–ä»¬çš„æ•°ç›®éƒ½è¾¾åˆ°\\\\(k\\\\)ï¼Œéšåï¼Œå¯¹äºminorityä¸­æ¯ä¸ªclusterè¿›è¡Œoversamplingï¼Œä½¿å¾—æ¯ä¸€ä¸ªclusteræ•°ç›®å˜æˆ\\\\(m * k /n\\\\)ï¼Œæœ€ç»ˆå®ç°between-class balanceå’Œwithin-class balance.\n\n### Undersampling\nä¸oversamplingç›¸å¯¹åº”çš„åˆ™æ˜¯undersamplingï¼Œundersamplingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹äºè¾ƒå¤šç±»åˆ«çš„samplesæŠ½æ ·ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«æ•°æ®è¶‹äºç›¸è¿‘ã€‚ä½†æ˜¯ï¼ŒéšæœºæŠ½æ ·è·å¾—ä¼šä½¿å¾—ç±»åˆ«ä¸§å¤±å¾ˆå¤šçš„ä¿¡æ¯ï¼Œç”šè‡³å¯¼è‡´æ•°æ®åˆ†å¸ƒå‘ç”Ÿæ”¹å˜ã€‚\n#### One-sided selection\none-sided selectionçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œä¸ºäº†ä¿è¯æ•°æ®æ•´ä½“çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¼˜å…ˆå»é™¤é è¿‘è¾¹ç•Œçš„æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¾ƒå¤šåˆ†ç±»çš„æ•°æ®åˆ†å¸ƒã€‚\n\n## Classifier level methods\nä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹é€šè¿‡æ”¹å˜classifier levelæ¥è§£å†³imbalanced dataçš„æ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡äºåˆ†ç±»å™¨æœ¬èº«çš„ä¸€äº›æ€§è´¨è€Œå¹¶éä¸¤ç±»æ•°æ®çš„ä¸ªæ•°ã€‚\n### Thresholding\næˆ‘åœ¨ä¹‹å‰çš„åšå®¢ä¸­èŠåˆ°è¿‡ï¼Œimbalanced dataçš„åˆ†ç±»å¹³é¢ä¼šå€¾å‘äºè¾ƒå°‘æ•°æ®çš„åˆ†ç±»ä¸€ä¾§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ç±»åˆ«é¢„æµ‹çš„probabiltyçš„thresholdæ¥ä¿®æ­£åˆ†ç±»å¹³é¢ã€‚å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯åŠ å…¥å…³äºç±»åˆ«æ•°ç›®çš„prior probabilityï¼š\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholdingæ–¹æ³•å…¶å®å¯¹å·²ç»trainå¥½çš„æ¨¡å‹çš„é‡‡å–çš„ä¸€ç§æ–¹å¼ã€‚ç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒçš„æ—¶å€™å°±æ¥æ¶ˆé™¤imbalanced dataçš„ä¸€äº›å½±å“ï¼Œå¦‚ä½•åšåˆ°å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯cost function.\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´learning rateï¼ŒåŠ å¼ºå¯¹costæ¯”è¾ƒå¤§çš„samplesï¼Œå¹¶ä¸”æœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ä»æ ‡å‡†çš„cost functionå˜æˆmisclassification costï¼Œå¦‚æ­¤å°±å¯ä»¥è§£å†³imbalanced dataçš„é—®é¢˜äº†\n### One-class classification\nè¯¥æ–¹æ³•å¯ä»¥è¯´æ˜¯æ¢äº†ä¸€ç§æ€ç»´çœ‹é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å†å°†classificationä½œä¸ºæˆ‘ä»¬çš„taskï¼Œè€Œæ˜¯å˜æˆäº†å¯¹äºä¸€ç§å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚æˆ‘ä»¬åªæ˜¯ç€çœ¼äºè¾ƒå¤šsamplesçš„ç±»åˆ«ï¼Œè®¤ä¸ºå¦ä¸€ç±»åˆ«çš„samplesæ˜¯ä¸€ç§å¼‚å¸¸å€¼ã€‚\n\nå½“ç„¶ï¼Œè¿™ç§æ–¹æ³•é€‚åˆé‚£ç§æç«¯çš„imbalanced dataï¼Œå¯¹äºä¸€èˆ¬çš„æƒ…å†µå¹¶ä¸ä¸€å®šå¾ˆé€‚ç”¨ã€‚\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","source":"_posts/ml-imbalanced-data-solution.md","raw":"---\ntitle: Imbalanced data é—®é¢˜æ€»ç»“æ–¹æ³•æ±‡æ€»\ndate: 2017-11-11 23:01:09\ntags:\n\t- imbalanced data\ncategories: machine learning\n---\nHelloï¼Œå¤§å®¶å¥½ï¼ŒåŒåä¸€çœŸçš„å¾ˆç´¯ï¼Œä¸€ç›´åœ¨åŠ ç­ï¼Œå¿™é‡Œå·é—²çœ‹äº†[A systematic study of the class imbalance  problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)ï¼Œæ„Ÿè§‰paperå‘ˆç°çš„ç ”ç©¶å†…å®¹æ„Ÿè§‰å¾ˆä¸€èˆ¬ï¼Œä½†æ˜¯ï¼Œpaperä¸­å…³äºimbalanced dataçš„solutionæ–¹æ³•å€’æ˜¯å†™çš„å¾ˆä¸é”™ï¼Œä¹Ÿå‹¾èµ·äº†æˆ‘å¯¹äºè¿™ä¸€å—æ€»ç»“çš„æ¬²æœ›ã€‚ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡å…³äºimbalanced dataçš„paper notesï¼Œä½†æ˜¯å¯¹äºè¿™ä¸€å—çš„å…·ä½“æ–¹æ³•æ€»ç»“è¿˜ä¸æ˜¯å¾ˆè¶³å¤Ÿï¼Œäºæ˜¯ç”¨è¿™ç¯‡paperä¸ºä¸»çº¿å¥½å¥½sum upä¸€è®¡ã€‚\n\næˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚\n<!--more-->\n## Data level methods\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹data level methodsï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªå…±æ€§ï¼Œé‚£å°±æ˜¯é€šè¿‡æ”¹å˜dataçš„æ•°é‡æ¥å®Œæˆå¯¹imbalanced data problemçš„è§£å†³ã€‚\n### Oversampling\nOversamplingå¯ä»¥è¯´æ˜¯æœ€ç›´è§‚çš„solutionä¹‹ä¸€ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¯¹äºè¾ƒå°‘ä¸€ç±»åˆ«çš„samplesï¼Œè¿‡æ­¤é‡å¤é‡‡æ ·ï¼Œä»¥æ­¤è®©ä¸¤ç§ç±»åˆ«çš„æ ·æœ¬æ¥è¿‘å¹³è¡¡ã€‚**ä½†æ˜¯ï¼Œå¯¹äºä¸€ä¸ªsampleå¤šæ¬¡é‡å¤è®­ç»ƒï¼Œå¾ˆæœ‰å¯èƒ½å¸¦æ¥overfitting**ï¼Œå› æ­¤ï¼Œç®€å•ç²—æš´çš„é‡å¤é‡‡æ ·å¹¶ä¸å¯å–ã€‚å› æ­¤ï¼Œå¾ˆå¤šæ”¹è¿›çš„ç‰ˆæœ¬åº”è¿è€Œç”Ÿï¼š\n#### SMOTE\nSMOTEç®—æ³•æ˜¯ä¸€ç§ç»å…¸çš„oversamplingæ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯å¯¹è¾ƒå°‘æ•°ç›®ç±»åˆ«çš„æ ·æœ¬ï¼ŒéšæœºæŠ½å–\\\\(m\\\\)ä¸ªæ ·æœ¬ï¼Œå¯¹äºéšæœºæŠ½å–å‡ºçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰å–è·ç¦»æœ€è¿‘çš„\\\\(n\\\\)ä¸ªæ ·æœ¬ï¼Œåœ¨ä»–ä»¬çš„è¿çº¿ä¸Šéšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼Œä½œä¸ºè¾ƒå°‘ç±»åˆ«çš„è¡¥å……æ ·æœ¬ã€‚å‡è®¾åŸæ ·æœ¬ç‚¹ä¸º\\\\(x\\\\)ï¼Œè¢«é€‰ä¸­çš„é™„è¿‘çš„ç‚¹ä¸º\\\\(x'\\\\)ï¼Œåˆ™æ–°çš„æ ·æœ¬ç‚¹ä¸ºï¼š\n$$x_{new}= x + rand(0,1) \\cdot |x-x'|$$\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSMOTEå¯ä»¥å¯¹è¾ƒå°‘ç±»åˆ«æ ·æœ¬è¿›è¡Œæ‰©å……ï¼Œè¿›è€Œå®ç°oversamplingï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚\n#### Cluster-base oversampling\nCluster-basedæ–¹æ³•çš„æœ€å¤§ç‰¹ç‚¹è«è¿‡äºæœ€å¼€å§‹å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªèšç±»åˆ†æï¼Œæ•°æ®ä¼šå˜æˆæ•°ä¸ªclusterï¼Œç„¶åå¯¹äºæ¯ä¸€ä¸ªclusteråœ¨è¿›è¡Œæ•°æ®çš„oversamplingï¼Œ**åŒæ—¶å…¼é¡¾ç±»åˆ«ä¹‹é—´çš„between-class imbalanceï¼Œè¿˜è¦è€ƒè™‘åˆ°ç±»å†…éƒ¨å„ä¸ªclusterçš„within-class imbalance**.\n> Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.\n\nåŸpaperå¤§è‡´å™è¿°äº†æ•´ä¸ªæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬å¯¹imbalanced dataè¿›è¡Œk-means(æˆ–è€…å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥)èšç±»ï¼Œèšæˆå¤šä¸ªclusterä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œoversamplingï¼Œå‡è®¾majority classæœ‰\\\\(m\\\\)ä¸ªclusterï¼Œminorityæœ‰\\\\(n\\\\)ä¸ªclusterï¼Œæˆ‘ä»¬ä»¥clusteræœ€å¤§çš„dataæ•°ç›®\\\\(k\\\\)ä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬å…ˆå¯¹majority classä¸­æ‰€æœ‰clusterï¼Œéƒ½è¿›è¡Œoversamplingï¼Œä½¿å¾—ä»–ä»¬çš„æ•°ç›®éƒ½è¾¾åˆ°\\\\(k\\\\)ï¼Œéšåï¼Œå¯¹äºminorityä¸­æ¯ä¸ªclusterè¿›è¡Œoversamplingï¼Œä½¿å¾—æ¯ä¸€ä¸ªclusteræ•°ç›®å˜æˆ\\\\(m * k /n\\\\)ï¼Œæœ€ç»ˆå®ç°between-class balanceå’Œwithin-class balance.\n\n### Undersampling\nä¸oversamplingç›¸å¯¹åº”çš„åˆ™æ˜¯undersamplingï¼Œundersamplingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹äºè¾ƒå¤šç±»åˆ«çš„samplesæŠ½æ ·ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«æ•°æ®è¶‹äºç›¸è¿‘ã€‚ä½†æ˜¯ï¼ŒéšæœºæŠ½æ ·è·å¾—ä¼šä½¿å¾—ç±»åˆ«ä¸§å¤±å¾ˆå¤šçš„ä¿¡æ¯ï¼Œç”šè‡³å¯¼è‡´æ•°æ®åˆ†å¸ƒå‘ç”Ÿæ”¹å˜ã€‚\n#### One-sided selection\none-sided selectionçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œä¸ºäº†ä¿è¯æ•°æ®æ•´ä½“çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¼˜å…ˆå»é™¤é è¿‘è¾¹ç•Œçš„æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¾ƒå¤šåˆ†ç±»çš„æ•°æ®åˆ†å¸ƒã€‚\n\n## Classifier level methods\nä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹é€šè¿‡æ”¹å˜classifier levelæ¥è§£å†³imbalanced dataçš„æ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡äºåˆ†ç±»å™¨æœ¬èº«çš„ä¸€äº›æ€§è´¨è€Œå¹¶éä¸¤ç±»æ•°æ®çš„ä¸ªæ•°ã€‚\n### Thresholding\næˆ‘åœ¨ä¹‹å‰çš„åšå®¢ä¸­èŠåˆ°è¿‡ï¼Œimbalanced dataçš„åˆ†ç±»å¹³é¢ä¼šå€¾å‘äºè¾ƒå°‘æ•°æ®çš„åˆ†ç±»ä¸€ä¾§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ç±»åˆ«é¢„æµ‹çš„probabiltyçš„thresholdæ¥ä¿®æ­£åˆ†ç±»å¹³é¢ã€‚å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯åŠ å…¥å…³äºç±»åˆ«æ•°ç›®çš„prior probabilityï¼š\n$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$\n### Cost sensitive learning\nThresholdingæ–¹æ³•å…¶å®å¯¹å·²ç»trainå¥½çš„æ¨¡å‹çš„é‡‡å–çš„ä¸€ç§æ–¹å¼ã€‚ç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒçš„æ—¶å€™å°±æ¥æ¶ˆé™¤imbalanced dataçš„ä¸€äº›å½±å“ï¼Œå¦‚ä½•åšåˆ°å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯cost function.\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´learning rateï¼ŒåŠ å¼ºå¯¹costæ¯”è¾ƒå¤§çš„samplesï¼Œå¹¶ä¸”æœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ä»æ ‡å‡†çš„cost functionå˜æˆmisclassification costï¼Œå¦‚æ­¤å°±å¯ä»¥è§£å†³imbalanced dataçš„é—®é¢˜äº†\n### One-class classification\nè¯¥æ–¹æ³•å¯ä»¥è¯´æ˜¯æ¢äº†ä¸€ç§æ€ç»´çœ‹é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å†å°†classificationä½œä¸ºæˆ‘ä»¬çš„taskï¼Œè€Œæ˜¯å˜æˆäº†å¯¹äºä¸€ç§å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚æˆ‘ä»¬åªæ˜¯ç€çœ¼äºè¾ƒå¤šsamplesçš„ç±»åˆ«ï¼Œè®¤ä¸ºå¦ä¸€ç±»åˆ«çš„samplesæ˜¯ä¸€ç§å¼‚å¸¸å€¼ã€‚\n\nå½“ç„¶ï¼Œè¿™ç§æ–¹æ³•é€‚åˆé‚£ç§æç«¯çš„imbalanced dataï¼Œå¯¹äºä¸€èˆ¬çš„æƒ…å†µå¹¶ä¸ä¸€å®šå¾ˆé€‚ç”¨ã€‚\n\n## Recommendation\nProjection:[Imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n\n## Reference\n* [Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. \"A systematic study of the class imbalance problem in convolutional neural networks.\" arXiv preprint arXiv:1710.05381 (2017).](https://arxiv.org/pdf/1710.05381.pdf)\n* [Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002): 321-357.](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n* [Jo, Taeho, and Nathalie Japkowicz. \"Class imbalances versus small disjuncts.\" ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.](http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf)\n* [Richard, Michael D., and Richard P. Lippmann. \"Neural network classifiers estimate Bayesian a posteriori probabilities.\" Neural computation 3.4 (1991): 461-483.](http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf)\n","slug":"ml-imbalanced-data-solution","published":1,"updated":"2018-11-19T06:26:13.784Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pe000utr8l8jqdu3gd","content":"<p>Helloï¼Œå¤§å®¶å¥½ï¼ŒåŒåä¸€çœŸçš„å¾ˆç´¯ï¼Œä¸€ç›´åœ¨åŠ ç­ï¼Œå¿™é‡Œå·é—²çœ‹äº†<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">A systematic study of the class imbalance  problem in convolutional neural networks</a>ï¼Œæ„Ÿè§‰paperå‘ˆç°çš„ç ”ç©¶å†…å®¹æ„Ÿè§‰å¾ˆä¸€èˆ¬ï¼Œä½†æ˜¯ï¼Œpaperä¸­å…³äºimbalanced dataçš„solutionæ–¹æ³•å€’æ˜¯å†™çš„å¾ˆä¸é”™ï¼Œä¹Ÿå‹¾èµ·äº†æˆ‘å¯¹äºè¿™ä¸€å—æ€»ç»“çš„æ¬²æœ›ã€‚ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡å…³äºimbalanced dataçš„paper notesï¼Œä½†æ˜¯å¯¹äºè¿™ä¸€å—çš„å…·ä½“æ–¹æ³•æ€»ç»“è¿˜ä¸æ˜¯å¾ˆè¶³å¤Ÿï¼Œäºæ˜¯ç”¨è¿™ç¯‡paperä¸ºä¸»çº¿å¥½å¥½sum upä¸€è®¡ã€‚</p>\n<p>æˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹data level methodsï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªå…±æ€§ï¼Œé‚£å°±æ˜¯é€šè¿‡æ”¹å˜dataçš„æ•°é‡æ¥å®Œæˆå¯¹imbalanced data problemçš„è§£å†³ã€‚</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversamplingå¯ä»¥è¯´æ˜¯æœ€ç›´è§‚çš„solutionä¹‹ä¸€ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¯¹äºè¾ƒå°‘ä¸€ç±»åˆ«çš„samplesï¼Œè¿‡æ­¤é‡å¤é‡‡æ ·ï¼Œä»¥æ­¤è®©ä¸¤ç§ç±»åˆ«çš„æ ·æœ¬æ¥è¿‘å¹³è¡¡ã€‚<strong>ä½†æ˜¯ï¼Œå¯¹äºä¸€ä¸ªsampleå¤šæ¬¡é‡å¤è®­ç»ƒï¼Œå¾ˆæœ‰å¯èƒ½å¸¦æ¥overfitting</strong>ï¼Œå› æ­¤ï¼Œç®€å•ç²—æš´çš„é‡å¤é‡‡æ ·å¹¶ä¸å¯å–ã€‚å› æ­¤ï¼Œå¾ˆå¤šæ”¹è¿›çš„ç‰ˆæœ¬åº”è¿è€Œç”Ÿï¼š</p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTEç®—æ³•æ˜¯ä¸€ç§ç»å…¸çš„oversamplingæ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯å¯¹è¾ƒå°‘æ•°ç›®ç±»åˆ«çš„æ ·æœ¬ï¼ŒéšæœºæŠ½å–\\(m\\)ä¸ªæ ·æœ¬ï¼Œå¯¹äºéšæœºæŠ½å–å‡ºçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰å–è·ç¦»æœ€è¿‘çš„\\(n\\)ä¸ªæ ·æœ¬ï¼Œåœ¨ä»–ä»¬çš„è¿çº¿ä¸Šéšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼Œä½œä¸ºè¾ƒå°‘ç±»åˆ«çš„è¡¥å……æ ·æœ¬ã€‚å‡è®¾åŸæ ·æœ¬ç‚¹ä¸º\\(x\\)ï¼Œè¢«é€‰ä¸­çš„é™„è¿‘çš„ç‚¹ä¸º\\(xâ€™\\)ï¼Œåˆ™æ–°çš„æ ·æœ¬ç‚¹ä¸ºï¼š<br>$$x_{new}= x + rand(0,1) \\cdot |x-xâ€™|$$</p>\n<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSMOTEå¯ä»¥å¯¹è¾ƒå°‘ç±»åˆ«æ ·æœ¬è¿›è¡Œæ‰©å……ï¼Œè¿›è€Œå®ç°oversamplingï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-basedæ–¹æ³•çš„æœ€å¤§ç‰¹ç‚¹è«è¿‡äºæœ€å¼€å§‹å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªèšç±»åˆ†æï¼Œæ•°æ®ä¼šå˜æˆæ•°ä¸ªclusterï¼Œç„¶åå¯¹äºæ¯ä¸€ä¸ªclusteråœ¨è¿›è¡Œæ•°æ®çš„oversamplingï¼Œ<strong>åŒæ—¶å…¼é¡¾ç±»åˆ«ä¹‹é—´çš„between-class imbalanceï¼Œè¿˜è¦è€ƒè™‘åˆ°ç±»å†…éƒ¨å„ä¸ªclusterçš„within-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>åŸpaperå¤§è‡´å™è¿°äº†æ•´ä¸ªæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬å¯¹imbalanced dataè¿›è¡Œk-means(æˆ–è€…å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥)èšç±»ï¼Œèšæˆå¤šä¸ªclusterä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œoversamplingï¼Œå‡è®¾majority classæœ‰\\(m\\)ä¸ªclusterï¼Œminorityæœ‰\\(n\\)ä¸ªclusterï¼Œæˆ‘ä»¬ä»¥clusteræœ€å¤§çš„dataæ•°ç›®\\(k\\)ä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬å…ˆå¯¹majority classä¸­æ‰€æœ‰clusterï¼Œéƒ½è¿›è¡Œoversamplingï¼Œä½¿å¾—ä»–ä»¬çš„æ•°ç›®éƒ½è¾¾åˆ°\\(k\\)ï¼Œéšåï¼Œå¯¹äºminorityä¸­æ¯ä¸ªclusterè¿›è¡Œoversamplingï¼Œä½¿å¾—æ¯ä¸€ä¸ªclusteræ•°ç›®å˜æˆ\\(m * k /n\\)ï¼Œæœ€ç»ˆå®ç°between-class balanceå’Œwithin-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>ä¸oversamplingç›¸å¯¹åº”çš„åˆ™æ˜¯undersamplingï¼Œundersamplingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹äºè¾ƒå¤šç±»åˆ«çš„samplesæŠ½æ ·ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«æ•°æ®è¶‹äºç›¸è¿‘ã€‚ä½†æ˜¯ï¼ŒéšæœºæŠ½æ ·è·å¾—ä¼šä½¿å¾—ç±»åˆ«ä¸§å¤±å¾ˆå¤šçš„ä¿¡æ¯ï¼Œç”šè‡³å¯¼è‡´æ•°æ®åˆ†å¸ƒå‘ç”Ÿæ”¹å˜ã€‚</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selectionçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œä¸ºäº†ä¿è¯æ•°æ®æ•´ä½“çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¼˜å…ˆå»é™¤é è¿‘è¾¹ç•Œçš„æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¾ƒå¤šåˆ†ç±»çš„æ•°æ®åˆ†å¸ƒã€‚</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹é€šè¿‡æ”¹å˜classifier levelæ¥è§£å†³imbalanced dataçš„æ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡äºåˆ†ç±»å™¨æœ¬èº«çš„ä¸€äº›æ€§è´¨è€Œå¹¶éä¸¤ç±»æ•°æ®çš„ä¸ªæ•°ã€‚</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>æˆ‘åœ¨ä¹‹å‰çš„åšå®¢ä¸­èŠåˆ°è¿‡ï¼Œimbalanced dataçš„åˆ†ç±»å¹³é¢ä¼šå€¾å‘äºè¾ƒå°‘æ•°æ®çš„åˆ†ç±»ä¸€ä¾§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ç±»åˆ«é¢„æµ‹çš„probabiltyçš„thresholdæ¥ä¿®æ­£åˆ†ç±»å¹³é¢ã€‚å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯åŠ å…¥å…³äºç±»åˆ«æ•°ç›®çš„prior probabilityï¼š<br>$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$</p>\n<h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholdingæ–¹æ³•å…¶å®å¯¹å·²ç»trainå¥½çš„æ¨¡å‹çš„é‡‡å–çš„ä¸€ç§æ–¹å¼ã€‚ç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒçš„æ—¶å€™å°±æ¥æ¶ˆé™¤imbalanced dataçš„ä¸€äº›å½±å“ï¼Œå¦‚ä½•åšåˆ°å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯cost function.</p>\n<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´learning rateï¼ŒåŠ å¼ºå¯¹costæ¯”è¾ƒå¤§çš„samplesï¼Œå¹¶ä¸”æœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ä»æ ‡å‡†çš„cost functionå˜æˆmisclassification costï¼Œå¦‚æ­¤å°±å¯ä»¥è§£å†³imbalanced dataçš„é—®é¢˜äº†</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>è¯¥æ–¹æ³•å¯ä»¥è¯´æ˜¯æ¢äº†ä¸€ç§æ€ç»´çœ‹é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å†å°†classificationä½œä¸ºæˆ‘ä»¬çš„taskï¼Œè€Œæ˜¯å˜æˆäº†å¯¹äºä¸€ç§å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚æˆ‘ä»¬åªæ˜¯ç€çœ¼äºè¾ƒå¤šsamplesçš„ç±»åˆ«ï¼Œè®¤ä¸ºå¦ä¸€ç±»åˆ«çš„samplesæ˜¯ä¸€ç§å¼‚å¸¸å€¼ã€‚</p>\n<p>å½“ç„¶ï¼Œè¿™ç§æ–¹æ³•é€‚åˆé‚£ç§æç«¯çš„imbalanced dataï¼Œå¯¹äºä¸€èˆ¬çš„æƒ…å†µå¹¶ä¸ä¸€å®šå¾ˆé€‚ç”¨ã€‚</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"noopener\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. â€œA systematic study of the class imbalance problem in convolutional neural networks.â€ arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"noopener\">Chawla, Nitesh V., et al. â€œSMOTE: synthetic minority over-sampling technique.â€ Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"noopener\">Jo, Taeho, and Nathalie Japkowicz. â€œClass imbalances versus small disjuncts.â€ ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"noopener\">Richard, Michael D., and Richard P. Lippmann. â€œNeural network classifiers estimate Bayesian a posteriori probabilities.â€ Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"machine learning","path":"categories/machine-learning/"}],"_tags":[{"name":"imbalanced data","path":"tags/imbalanced-data/"}],"excerpt":"<p>Helloï¼Œå¤§å®¶å¥½ï¼ŒåŒåä¸€çœŸçš„å¾ˆç´¯ï¼Œä¸€ç›´åœ¨åŠ ç­ï¼Œå¿™é‡Œå·é—²çœ‹äº†<a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">A systematic study of the class imbalance  problem in convolutional neural networks</a>ï¼Œæ„Ÿè§‰paperå‘ˆç°çš„ç ”ç©¶å†…å®¹æ„Ÿè§‰å¾ˆä¸€èˆ¬ï¼Œä½†æ˜¯ï¼Œpaperä¸­å…³äºimbalanced dataçš„solutionæ–¹æ³•å€’æ˜¯å†™çš„å¾ˆä¸é”™ï¼Œä¹Ÿå‹¾èµ·äº†æˆ‘å¯¹äºè¿™ä¸€å—æ€»ç»“çš„æ¬²æœ›ã€‚ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡å…³äºimbalanced dataçš„paper notesï¼Œä½†æ˜¯å¯¹äºè¿™ä¸€å—çš„å…·ä½“æ–¹æ³•æ€»ç»“è¿˜ä¸æ˜¯å¾ˆè¶³å¤Ÿï¼Œäºæ˜¯ç”¨è¿™ç¯‡paperä¸ºä¸»çº¿å¥½å¥½sum upä¸€è®¡ã€‚</p>\n<p>æˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚<br></p>","more":"</p>\n<h2 id=\"Data-level-methods\"><a href=\"#Data-level-methods\" class=\"headerlink\" title=\"Data level methods\"></a>Data level methods</h2><p>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹data level methodsï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªå…±æ€§ï¼Œé‚£å°±æ˜¯é€šè¿‡æ”¹å˜dataçš„æ•°é‡æ¥å®Œæˆå¯¹imbalanced data problemçš„è§£å†³ã€‚</p>\n<h3 id=\"Oversampling\"><a href=\"#Oversampling\" class=\"headerlink\" title=\"Oversampling\"></a>Oversampling</h3><p>Oversamplingå¯ä»¥è¯´æ˜¯æœ€ç›´è§‚çš„solutionä¹‹ä¸€ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¯¹äºè¾ƒå°‘ä¸€ç±»åˆ«çš„samplesï¼Œè¿‡æ­¤é‡å¤é‡‡æ ·ï¼Œä»¥æ­¤è®©ä¸¤ç§ç±»åˆ«çš„æ ·æœ¬æ¥è¿‘å¹³è¡¡ã€‚<strong>ä½†æ˜¯ï¼Œå¯¹äºä¸€ä¸ªsampleå¤šæ¬¡é‡å¤è®­ç»ƒï¼Œå¾ˆæœ‰å¯èƒ½å¸¦æ¥overfitting</strong>ï¼Œå› æ­¤ï¼Œç®€å•ç²—æš´çš„é‡å¤é‡‡æ ·å¹¶ä¸å¯å–ã€‚å› æ­¤ï¼Œå¾ˆå¤šæ”¹è¿›çš„ç‰ˆæœ¬åº”è¿è€Œç”Ÿï¼š</p>\n<h4 id=\"SMOTE\"><a href=\"#SMOTE\" class=\"headerlink\" title=\"SMOTE\"></a>SMOTE</h4><p>SMOTEç®—æ³•æ˜¯ä¸€ç§ç»å…¸çš„oversamplingæ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯å¯¹è¾ƒå°‘æ•°ç›®ç±»åˆ«çš„æ ·æœ¬ï¼ŒéšæœºæŠ½å–\\(m\\)ä¸ªæ ·æœ¬ï¼Œå¯¹äºéšæœºæŠ½å–å‡ºçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰å–è·ç¦»æœ€è¿‘çš„\\(n\\)ä¸ªæ ·æœ¬ï¼Œåœ¨ä»–ä»¬çš„è¿çº¿ä¸Šéšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼Œä½œä¸ºè¾ƒå°‘ç±»åˆ«çš„è¡¥å……æ ·æœ¬ã€‚å‡è®¾åŸæ ·æœ¬ç‚¹ä¸º\\(x\\)ï¼Œè¢«é€‰ä¸­çš„é™„è¿‘çš„ç‚¹ä¸º\\(xâ€™\\)ï¼Œåˆ™æ–°çš„æ ·æœ¬ç‚¹ä¸ºï¼š<br>$$x_{new}= x + rand(0,1) \\cdot |x-xâ€™|$$</p>\n<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSMOTEå¯ä»¥å¯¹è¾ƒå°‘ç±»åˆ«æ ·æœ¬è¿›è¡Œæ‰©å……ï¼Œè¿›è€Œå®ç°oversamplingï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚</p>\n<h4 id=\"Cluster-base-oversampling\"><a href=\"#Cluster-base-oversampling\" class=\"headerlink\" title=\"Cluster-base oversampling\"></a>Cluster-base oversampling</h4><p>Cluster-basedæ–¹æ³•çš„æœ€å¤§ç‰¹ç‚¹è«è¿‡äºæœ€å¼€å§‹å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªèšç±»åˆ†æï¼Œæ•°æ®ä¼šå˜æˆæ•°ä¸ªclusterï¼Œç„¶åå¯¹äºæ¯ä¸€ä¸ªclusteråœ¨è¿›è¡Œæ•°æ®çš„oversamplingï¼Œ<strong>åŒæ—¶å…¼é¡¾ç±»åˆ«ä¹‹é—´çš„between-class imbalanceï¼Œè¿˜è¦è€ƒè™‘åˆ°ç±»å†…éƒ¨å„ä¸ªclusterçš„within-class imbalance</strong>.</p>\n<blockquote>\n<p>Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously.</p>\n</blockquote>\n<p>åŸpaperå¤§è‡´å™è¿°äº†æ•´ä¸ªæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬å¯¹imbalanced dataè¿›è¡Œk-means(æˆ–è€…å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥)èšç±»ï¼Œèšæˆå¤šä¸ªclusterä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œoversamplingï¼Œå‡è®¾majority classæœ‰\\(m\\)ä¸ªclusterï¼Œminorityæœ‰\\(n\\)ä¸ªclusterï¼Œæˆ‘ä»¬ä»¥clusteræœ€å¤§çš„dataæ•°ç›®\\(k\\)ä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬å…ˆå¯¹majority classä¸­æ‰€æœ‰clusterï¼Œéƒ½è¿›è¡Œoversamplingï¼Œä½¿å¾—ä»–ä»¬çš„æ•°ç›®éƒ½è¾¾åˆ°\\(k\\)ï¼Œéšåï¼Œå¯¹äºminorityä¸­æ¯ä¸ªclusterè¿›è¡Œoversamplingï¼Œä½¿å¾—æ¯ä¸€ä¸ªclusteræ•°ç›®å˜æˆ\\(m * k /n\\)ï¼Œæœ€ç»ˆå®ç°between-class balanceå’Œwithin-class balance.</p>\n<h3 id=\"Undersampling\"><a href=\"#Undersampling\" class=\"headerlink\" title=\"Undersampling\"></a>Undersampling</h3><p>ä¸oversamplingç›¸å¯¹åº”çš„åˆ™æ˜¯undersamplingï¼Œundersamplingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹äºè¾ƒå¤šç±»åˆ«çš„samplesæŠ½æ ·ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«æ•°æ®è¶‹äºç›¸è¿‘ã€‚ä½†æ˜¯ï¼ŒéšæœºæŠ½æ ·è·å¾—ä¼šä½¿å¾—ç±»åˆ«ä¸§å¤±å¾ˆå¤šçš„ä¿¡æ¯ï¼Œç”šè‡³å¯¼è‡´æ•°æ®åˆ†å¸ƒå‘ç”Ÿæ”¹å˜ã€‚</p>\n<h4 id=\"One-sided-selection\"><a href=\"#One-sided-selection\" class=\"headerlink\" title=\"One-sided selection\"></a>One-sided selection</h4><p>one-sided selectionçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œä¸ºäº†ä¿è¯æ•°æ®æ•´ä½“çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¼˜å…ˆå»é™¤é è¿‘è¾¹ç•Œçš„æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¾ƒå¤šåˆ†ç±»çš„æ•°æ®åˆ†å¸ƒã€‚</p>\n<h2 id=\"Classifier-level-methods\"><a href=\"#Classifier-level-methods\" class=\"headerlink\" title=\"Classifier level methods\"></a>Classifier level methods</h2><p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹é€šè¿‡æ”¹å˜classifier levelæ¥è§£å†³imbalanced dataçš„æ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡äºåˆ†ç±»å™¨æœ¬èº«çš„ä¸€äº›æ€§è´¨è€Œå¹¶éä¸¤ç±»æ•°æ®çš„ä¸ªæ•°ã€‚</p>\n<h3 id=\"Thresholding\"><a href=\"#Thresholding\" class=\"headerlink\" title=\"Thresholding\"></a>Thresholding</h3><p>æˆ‘åœ¨ä¹‹å‰çš„åšå®¢ä¸­èŠåˆ°è¿‡ï¼Œimbalanced dataçš„åˆ†ç±»å¹³é¢ä¼šå€¾å‘äºè¾ƒå°‘æ•°æ®çš„åˆ†ç±»ä¸€ä¾§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ç±»åˆ«é¢„æµ‹çš„probabiltyçš„thresholdæ¥ä¿®æ­£åˆ†ç±»å¹³é¢ã€‚å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯åŠ å…¥å…³äºç±»åˆ«æ•°ç›®çš„prior probabilityï¼š<br>$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$</p>\n<h3 id=\"Cost-sensitive-learning\"><a href=\"#Cost-sensitive-learning\" class=\"headerlink\" title=\"Cost sensitive learning\"></a>Cost sensitive learning</h3><p>Thresholdingæ–¹æ³•å…¶å®å¯¹å·²ç»trainå¥½çš„æ¨¡å‹çš„é‡‡å–çš„ä¸€ç§æ–¹å¼ã€‚ç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒçš„æ—¶å€™å°±æ¥æ¶ˆé™¤imbalanced dataçš„ä¸€äº›å½±å“ï¼Œå¦‚ä½•åšåˆ°å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯cost function.</p>\n<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´learning rateï¼ŒåŠ å¼ºå¯¹costæ¯”è¾ƒå¤§çš„samplesï¼Œå¹¶ä¸”æœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ä»æ ‡å‡†çš„cost functionå˜æˆmisclassification costï¼Œå¦‚æ­¤å°±å¯ä»¥è§£å†³imbalanced dataçš„é—®é¢˜äº†</p>\n<h3 id=\"One-class-classification\"><a href=\"#One-class-classification\" class=\"headerlink\" title=\"One-class classification\"></a>One-class classification</h3><p>è¯¥æ–¹æ³•å¯ä»¥è¯´æ˜¯æ¢äº†ä¸€ç§æ€ç»´çœ‹é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å†å°†classificationä½œä¸ºæˆ‘ä»¬çš„taskï¼Œè€Œæ˜¯å˜æˆäº†å¯¹äºä¸€ç§å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚æˆ‘ä»¬åªæ˜¯ç€çœ¼äºè¾ƒå¤šsamplesçš„ç±»åˆ«ï¼Œè®¤ä¸ºå¦ä¸€ç±»åˆ«çš„samplesæ˜¯ä¸€ç§å¼‚å¸¸å€¼ã€‚</p>\n<p>å½“ç„¶ï¼Œè¿™ç§æ–¹æ³•é€‚åˆé‚£ç§æç«¯çš„imbalanced dataï¼Œå¯¹äºä¸€èˆ¬çš„æƒ…å†µå¹¶ä¸ä¸€å®šå¾ˆé€‚ç”¨ã€‚</p>\n<h2 id=\"Recommendation\"><a href=\"#Recommendation\" class=\"headerlink\" title=\"Recommendation\"></a>Recommendation</h2><p>Projection:<a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\" target=\"_blank\" rel=\"noopener\">Imbalanced-learn</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05381.pdf\" target=\"_blank\" rel=\"noopener\">Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. â€œA systematic study of the class imbalance problem in convolutional neural networks.â€ arXiv preprint arXiv:1710.05381 (2017).</a></li>\n<li><a href=\"https://www.jair.org/media/953/live-953-2037-jair.pdf\" target=\"_blank\" rel=\"noopener\">Chawla, Nitesh V., et al. â€œSMOTE: synthetic minority over-sampling technique.â€ Journal of artificial intelligence research 16 (2002): 321-357.</a></li>\n<li><a href=\"http://sci2s.ugr.es/keel/pdf/specific/articulo/jo.pdf\" target=\"_blank\" rel=\"noopener\">Jo, Taeho, and Nathalie Japkowicz. â€œClass imbalances versus small disjuncts.â€ ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49.</a></li>\n<li><a href=\"http://www.ee.iisc.ac.in/new/people/faculty/prasantg/downloads/NeuralNetworksPosteriors_Lippmann1991.pdf\" target=\"_blank\" rel=\"noopener\">Richard, Michael D., and Richard P. Lippmann. â€œNeural network classifiers estimate Bayesian a posteriori probabilities.â€ Neural computation 3.4 (1991): 461-483.</a></li>\n</ul>"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","date":"2017-08-23T03:30:43.000Z","_content":"OKï¼Œä»Šå¤©æˆ‘ä»¬æ¥reviewä¸€ç¯‡ç»å…¸çš„paperï¼Œè¿™ç¯‡paperæ˜¯3å¹´å‰facebookçš„ç ”ç©¶æˆæœï¼Œå…³äºgbtå’Œlrçš„ç»“åˆï¼Œè¿™ä¸ªæ­é…å¯¹äºè¿‘å‡ å¹´çš„CTRé¢„æµ‹ä»¥åŠæ¨èç³»ç»Ÿçš„å‘å±•éƒ½äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚è™½ç„¶å·²ç»å¾ˆéš¾è¢«ç§°ä¸ºä¸€ç¯‡æ–°paperäº†ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—æˆ‘ä»¬å»çœ‹çœ‹ã€‚\n\næˆ‘ä»¬ä¸€èµ·ç®€å•çœ‹çœ‹è¿™ç¯‡paperçš„æ ¸å¿ƒpoint.\n<!--more-->\n## Notes\nä¼ ç»ŸCTRé¢„æµ‹ä¸­ï¼Œlogistic regressionä¸€ç›´æœ‰ç€å¾ˆå¥½çš„æ•ˆæœï¼Œlrä¸ä»…å¯ä»¥çº¿æ€§åˆ†ç±»ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç»™å‡ºæ ·æœ¬å±äºè¯¥ç±»åˆ«çš„posterior probability\n\nä½†æ˜¯ä¼ ç»Ÿçš„lrä¹Ÿæœ‰ç€æœ¬èº«çš„ç¼ºæ†¾ï¼Œlræœ¬èº«å°±æ˜¯lineråˆ†ç±»å™¨ï¼Œå¯¹äºçº¿æ€§ä¸å¯åˆ†çš„featuresæ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ã€‚åŒæ—¶åœ¨å¯¹äºè¿ç»­featureç¦»æ•£åŒ–çš„æ—¶å€™ï¼Œæ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¾èµ–äºç¦»æ•£åˆ†æ¡¶çš„äººä¸ºç»éªŒã€‚\n\nè¯¥paperæå‡ºäº†ä¸€ç§ä¾é gbtè¿›è¡Œfeature transformçš„æ–¹æ³•ï¼Œä¸å¤šè¯´åºŸè¯ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šå›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png)\nè¿™å°±æ˜¯è¿™ç¯‡paperæœ€æœ€æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ã€‚\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\nä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹featureè¢«gbtè¿›è¡Œäº†transformï¼Œæ ·æœ¬è½å…¥åˆ°å“ªä¸ªtree nodeï¼Œåˆ™è¯¥ä½ç½®1ï¼Œå…¶ä»–ä½ç½®0ï¼Œéšåå†è¿›å…¥çº¿æ€§åˆ†ç±»å™¨lrä¸­è¿›è¡Œæœ€åçš„åˆ†ç±»ã€‚\n\nå‡è®¾æœ‰ä¸€ä¸ªsampleï¼Œåœ¨å›¾ä¸­æ‰€ç¤ºçš„æ¨¡å‹ä¸­ï¼Œgbtæœ‰ä¸¤æ£µæ ‘ï¼Œä»å·¦åˆ°å³æ˜¯tree1å’Œtree2ï¼Œtree1ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬ä¸€ä¸ªtree nodeï¼Œtree2ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬äºŒä¸ªtree nodeï¼Œé‚£ä¹ˆæœ€ç»ˆtransformå¾—åˆ°çš„new sampleå°±å˜æˆäº†(1,0,0,0,1)\n\né€šè¿‡gbtçš„transformåï¼Œfeatureä¸ä»…ä»éçº¿æ€§è½¬æ¢æˆäº†çº¿æ€§ï¼ˆç±»ä¼¼äºSVMçš„kernelæ•ˆæœï¼‰ï¼Œè€Œä¸”featureè¢«å®Œå…¨çš„ç¦»æ•£æˆäº†0-1ç¨€ç–featureï¼Œæ— è®ºä»çº¿æ€§å¯åˆ†è¿˜æ˜¯ç‰¹å¾ç¨€ç–çš„è§’åº¦ä¸Šï¼Œéƒ½å˜å¾—æ¯”åŸå§‹featureæ›´åŠ ç†æƒ³ï¼\n\nå› ä¸ºæ˜¯ä¸€ç¯‡ç›¸å¯¹è€ä¸€äº›çš„paperï¼Œæ‰€ä»¥æˆ‘å™è¿°çš„æ¯”è¾ƒç®€å•ï¼Œå¤§å®¶å¯ä»¥getåˆ°gbt+lrè¿™ä¸ªæ¨¡å‹çš„åŸºæœ¬åŸç†å°±å¯ä»¥äº†ã€‚æˆ‘è‡ªå·±åœ¨ç§ä¸‹ä¹Ÿç”¨pythonå†™äº†ä¸€ä¸ªç®€å•çš„demoï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹[å¯ä»¥çœ‹çœ‹](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)ï¼Œæ¬¢è¿æå‡ºæ„è§ï¼Œæ¬¢è¿folkï¼\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","source":"_posts/paper-facebook.md","raw":"---\ntitle: Reading Notes-Practical lessons from predicting clicks on ads at facebook\ndate: 2017-08-23 11:30:43\ntags: \n\t- gbt\n\t- logistic regression\n\t- gradient descent\ncategories: reading notes\n---\nOKï¼Œä»Šå¤©æˆ‘ä»¬æ¥reviewä¸€ç¯‡ç»å…¸çš„paperï¼Œè¿™ç¯‡paperæ˜¯3å¹´å‰facebookçš„ç ”ç©¶æˆæœï¼Œå…³äºgbtå’Œlrçš„ç»“åˆï¼Œè¿™ä¸ªæ­é…å¯¹äºè¿‘å‡ å¹´çš„CTRé¢„æµ‹ä»¥åŠæ¨èç³»ç»Ÿçš„å‘å±•éƒ½äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚è™½ç„¶å·²ç»å¾ˆéš¾è¢«ç§°ä¸ºä¸€ç¯‡æ–°paperäº†ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—æˆ‘ä»¬å»çœ‹çœ‹ã€‚\n\næˆ‘ä»¬ä¸€èµ·ç®€å•çœ‹çœ‹è¿™ç¯‡paperçš„æ ¸å¿ƒpoint.\n<!--more-->\n## Notes\nä¼ ç»ŸCTRé¢„æµ‹ä¸­ï¼Œlogistic regressionä¸€ç›´æœ‰ç€å¾ˆå¥½çš„æ•ˆæœï¼Œlrä¸ä»…å¯ä»¥çº¿æ€§åˆ†ç±»ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç»™å‡ºæ ·æœ¬å±äºè¯¥ç±»åˆ«çš„posterior probability\n\nä½†æ˜¯ä¼ ç»Ÿçš„lrä¹Ÿæœ‰ç€æœ¬èº«çš„ç¼ºæ†¾ï¼Œlræœ¬èº«å°±æ˜¯lineråˆ†ç±»å™¨ï¼Œå¯¹äºçº¿æ€§ä¸å¯åˆ†çš„featuresæ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ã€‚åŒæ—¶åœ¨å¯¹äºè¿ç»­featureç¦»æ•£åŒ–çš„æ—¶å€™ï¼Œæ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¾èµ–äºç¦»æ•£åˆ†æ¡¶çš„äººä¸ºç»éªŒã€‚\n\nè¯¥paperæå‡ºäº†ä¸€ç§ä¾é gbtè¿›è¡Œfeature transformçš„æ–¹æ³•ï¼Œä¸å¤šè¯´åºŸè¯ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šå›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png)\nè¿™å°±æ˜¯è¿™ç¯‡paperæœ€æœ€æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ã€‚\n\n> Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.\n\nä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹featureè¢«gbtè¿›è¡Œäº†transformï¼Œæ ·æœ¬è½å…¥åˆ°å“ªä¸ªtree nodeï¼Œåˆ™è¯¥ä½ç½®1ï¼Œå…¶ä»–ä½ç½®0ï¼Œéšåå†è¿›å…¥çº¿æ€§åˆ†ç±»å™¨lrä¸­è¿›è¡Œæœ€åçš„åˆ†ç±»ã€‚\n\nå‡è®¾æœ‰ä¸€ä¸ªsampleï¼Œåœ¨å›¾ä¸­æ‰€ç¤ºçš„æ¨¡å‹ä¸­ï¼Œgbtæœ‰ä¸¤æ£µæ ‘ï¼Œä»å·¦åˆ°å³æ˜¯tree1å’Œtree2ï¼Œtree1ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬ä¸€ä¸ªtree nodeï¼Œtree2ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬äºŒä¸ªtree nodeï¼Œé‚£ä¹ˆæœ€ç»ˆtransformå¾—åˆ°çš„new sampleå°±å˜æˆäº†(1,0,0,0,1)\n\né€šè¿‡gbtçš„transformåï¼Œfeatureä¸ä»…ä»éçº¿æ€§è½¬æ¢æˆäº†çº¿æ€§ï¼ˆç±»ä¼¼äºSVMçš„kernelæ•ˆæœï¼‰ï¼Œè€Œä¸”featureè¢«å®Œå…¨çš„ç¦»æ•£æˆäº†0-1ç¨€ç–featureï¼Œæ— è®ºä»çº¿æ€§å¯åˆ†è¿˜æ˜¯ç‰¹å¾ç¨€ç–çš„è§’åº¦ä¸Šï¼Œéƒ½å˜å¾—æ¯”åŸå§‹featureæ›´åŠ ç†æƒ³ï¼\n\nå› ä¸ºæ˜¯ä¸€ç¯‡ç›¸å¯¹è€ä¸€äº›çš„paperï¼Œæ‰€ä»¥æˆ‘å™è¿°çš„æ¯”è¾ƒç®€å•ï¼Œå¤§å®¶å¯ä»¥getåˆ°gbt+lrè¿™ä¸ªæ¨¡å‹çš„åŸºæœ¬åŸç†å°±å¯ä»¥äº†ã€‚æˆ‘è‡ªå·±åœ¨ç§ä¸‹ä¹Ÿç”¨pythonå†™äº†ä¸€ä¸ªç®€å•çš„demoï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹[å¯ä»¥çœ‹çœ‹](https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py)ï¼Œæ¬¢è¿æå‡ºæ„è§ï¼Œæ¬¢è¿folkï¼\n\n## Reference\n* [He, Xinran, et al. \"Practical lessons from predicting clicks on ads at facebook.\" Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)\n","slug":"paper-facebook","published":1,"updated":"2018-11-19T06:20:19.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pf000xtr8l0ewy67ma","content":"<p>OKï¼Œä»Šå¤©æˆ‘ä»¬æ¥reviewä¸€ç¯‡ç»å…¸çš„paperï¼Œè¿™ç¯‡paperæ˜¯3å¹´å‰facebookçš„ç ”ç©¶æˆæœï¼Œå…³äºgbtå’Œlrçš„ç»“åˆï¼Œè¿™ä¸ªæ­é…å¯¹äºè¿‘å‡ å¹´çš„CTRé¢„æµ‹ä»¥åŠæ¨èç³»ç»Ÿçš„å‘å±•éƒ½äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚è™½ç„¶å·²ç»å¾ˆéš¾è¢«ç§°ä¸ºä¸€ç¯‡æ–°paperäº†ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—æˆ‘ä»¬å»çœ‹çœ‹ã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·ç®€å•çœ‹çœ‹è¿™ç¯‡paperçš„æ ¸å¿ƒpoint.<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>ä¼ ç»ŸCTRé¢„æµ‹ä¸­ï¼Œlogistic regressionä¸€ç›´æœ‰ç€å¾ˆå¥½çš„æ•ˆæœï¼Œlrä¸ä»…å¯ä»¥çº¿æ€§åˆ†ç±»ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç»™å‡ºæ ·æœ¬å±äºè¯¥ç±»åˆ«çš„posterior probability</p>\n<p>ä½†æ˜¯ä¼ ç»Ÿçš„lrä¹Ÿæœ‰ç€æœ¬èº«çš„ç¼ºæ†¾ï¼Œlræœ¬èº«å°±æ˜¯lineråˆ†ç±»å™¨ï¼Œå¯¹äºçº¿æ€§ä¸å¯åˆ†çš„featuresæ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ã€‚åŒæ—¶åœ¨å¯¹äºè¿ç»­featureç¦»æ•£åŒ–çš„æ—¶å€™ï¼Œæ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¾èµ–äºç¦»æ•£åˆ†æ¡¶çš„äººä¸ºç»éªŒã€‚</p>\n<p>è¯¥paperæå‡ºäº†ä¸€ç§ä¾é gbtè¿›è¡Œfeature transformçš„æ–¹æ³•ï¼Œä¸å¤šè¯´åºŸè¯ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šå›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png\" alt=\"\"><br>è¿™å°±æ˜¯è¿™ç¯‡paperæœ€æœ€æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ã€‚</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹featureè¢«gbtè¿›è¡Œäº†transformï¼Œæ ·æœ¬è½å…¥åˆ°å“ªä¸ªtree nodeï¼Œåˆ™è¯¥ä½ç½®1ï¼Œå…¶ä»–ä½ç½®0ï¼Œéšåå†è¿›å…¥çº¿æ€§åˆ†ç±»å™¨lrä¸­è¿›è¡Œæœ€åçš„åˆ†ç±»ã€‚</p>\n<p>å‡è®¾æœ‰ä¸€ä¸ªsampleï¼Œåœ¨å›¾ä¸­æ‰€ç¤ºçš„æ¨¡å‹ä¸­ï¼Œgbtæœ‰ä¸¤æ£µæ ‘ï¼Œä»å·¦åˆ°å³æ˜¯tree1å’Œtree2ï¼Œtree1ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬ä¸€ä¸ªtree nodeï¼Œtree2ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬äºŒä¸ªtree nodeï¼Œé‚£ä¹ˆæœ€ç»ˆtransformå¾—åˆ°çš„new sampleå°±å˜æˆäº†(1,0,0,0,1)</p>\n<p>é€šè¿‡gbtçš„transformåï¼Œfeatureä¸ä»…ä»éçº¿æ€§è½¬æ¢æˆäº†çº¿æ€§ï¼ˆç±»ä¼¼äºSVMçš„kernelæ•ˆæœï¼‰ï¼Œè€Œä¸”featureè¢«å®Œå…¨çš„ç¦»æ•£æˆäº†0-1ç¨€ç–featureï¼Œæ— è®ºä»çº¿æ€§å¯åˆ†è¿˜æ˜¯ç‰¹å¾ç¨€ç–çš„è§’åº¦ä¸Šï¼Œéƒ½å˜å¾—æ¯”åŸå§‹featureæ›´åŠ ç†æƒ³ï¼</p>\n<p>å› ä¸ºæ˜¯ä¸€ç¯‡ç›¸å¯¹è€ä¸€äº›çš„paperï¼Œæ‰€ä»¥æˆ‘å™è¿°çš„æ¯”è¾ƒç®€å•ï¼Œå¤§å®¶å¯ä»¥getåˆ°gbt+lrè¿™ä¸ªæ¨¡å‹çš„åŸºæœ¬åŸç†å°±å¯ä»¥äº†ã€‚æˆ‘è‡ªå·±åœ¨ç§ä¸‹ä¹Ÿç”¨pythonå†™äº†ä¸€ä¸ªç®€å•çš„demoï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"noopener\">å¯ä»¥çœ‹çœ‹</a>ï¼Œæ¬¢è¿æå‡ºæ„è§ï¼Œæ¬¢è¿folkï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"noopener\">He, Xinran, et al. â€œPractical lessons from predicting clicks on ads at facebook.â€ Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"gradient descent","path":"tags/gradient-descent/"},{"name":"gbt","path":"tags/gbt/"},{"name":"logistic regression","path":"tags/logistic-regression/"}],"excerpt":"<p>OKï¼Œä»Šå¤©æˆ‘ä»¬æ¥reviewä¸€ç¯‡ç»å…¸çš„paperï¼Œè¿™ç¯‡paperæ˜¯3å¹´å‰facebookçš„ç ”ç©¶æˆæœï¼Œå…³äºgbtå’Œlrçš„ç»“åˆï¼Œè¿™ä¸ªæ­é…å¯¹äºè¿‘å‡ å¹´çš„CTRé¢„æµ‹ä»¥åŠæ¨èç³»ç»Ÿçš„å‘å±•éƒ½äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚è™½ç„¶å·²ç»å¾ˆéš¾è¢«ç§°ä¸ºä¸€ç¯‡æ–°paperäº†ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—æˆ‘ä»¬å»çœ‹çœ‹ã€‚</p>\n<p>æˆ‘ä»¬ä¸€èµ·ç®€å•çœ‹çœ‹è¿™ç¯‡paperçš„æ ¸å¿ƒpoint.<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>ä¼ ç»ŸCTRé¢„æµ‹ä¸­ï¼Œlogistic regressionä¸€ç›´æœ‰ç€å¾ˆå¥½çš„æ•ˆæœï¼Œlrä¸ä»…å¯ä»¥çº¿æ€§åˆ†ç±»ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç»™å‡ºæ ·æœ¬å±äºè¯¥ç±»åˆ«çš„posterior probability</p>\n<p>ä½†æ˜¯ä¼ ç»Ÿçš„lrä¹Ÿæœ‰ç€æœ¬èº«çš„ç¼ºæ†¾ï¼Œlræœ¬èº«å°±æ˜¯lineråˆ†ç±»å™¨ï¼Œå¯¹äºçº¿æ€§ä¸å¯åˆ†çš„featuresæ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ã€‚åŒæ—¶åœ¨å¯¹äºè¿ç»­featureç¦»æ•£åŒ–çš„æ—¶å€™ï¼Œæ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¾èµ–äºç¦»æ•£åˆ†æ¡¶çš„äººä¸ºç»éªŒã€‚</p>\n<p>è¯¥paperæå‡ºäº†ä¸€ç§ä¾é gbtè¿›è¡Œfeature transformçš„æ–¹æ³•ï¼Œä¸å¤šè¯´åºŸè¯ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šå›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/3/3-1.png\" alt=\"\"><br>è¿™å°±æ˜¯è¿™ç¯‡paperæœ€æœ€æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ã€‚</p>\n<blockquote>\n<p>Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms.</p>\n</blockquote>\n<p>ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹featureè¢«gbtè¿›è¡Œäº†transformï¼Œæ ·æœ¬è½å…¥åˆ°å“ªä¸ªtree nodeï¼Œåˆ™è¯¥ä½ç½®1ï¼Œå…¶ä»–ä½ç½®0ï¼Œéšåå†è¿›å…¥çº¿æ€§åˆ†ç±»å™¨lrä¸­è¿›è¡Œæœ€åçš„åˆ†ç±»ã€‚</p>\n<p>å‡è®¾æœ‰ä¸€ä¸ªsampleï¼Œåœ¨å›¾ä¸­æ‰€ç¤ºçš„æ¨¡å‹ä¸­ï¼Œgbtæœ‰ä¸¤æ£µæ ‘ï¼Œä»å·¦åˆ°å³æ˜¯tree1å’Œtree2ï¼Œtree1ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬ä¸€ä¸ªtree nodeï¼Œtree2ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬äºŒä¸ªtree nodeï¼Œé‚£ä¹ˆæœ€ç»ˆtransformå¾—åˆ°çš„new sampleå°±å˜æˆäº†(1,0,0,0,1)</p>\n<p>é€šè¿‡gbtçš„transformåï¼Œfeatureä¸ä»…ä»éçº¿æ€§è½¬æ¢æˆäº†çº¿æ€§ï¼ˆç±»ä¼¼äºSVMçš„kernelæ•ˆæœï¼‰ï¼Œè€Œä¸”featureè¢«å®Œå…¨çš„ç¦»æ•£æˆäº†0-1ç¨€ç–featureï¼Œæ— è®ºä»çº¿æ€§å¯åˆ†è¿˜æ˜¯ç‰¹å¾ç¨€ç–çš„è§’åº¦ä¸Šï¼Œéƒ½å˜å¾—æ¯”åŸå§‹featureæ›´åŠ ç†æƒ³ï¼</p>\n<p>å› ä¸ºæ˜¯ä¸€ç¯‡ç›¸å¯¹è€ä¸€äº›çš„paperï¼Œæ‰€ä»¥æˆ‘å™è¿°çš„æ¯”è¾ƒç®€å•ï¼Œå¤§å®¶å¯ä»¥getåˆ°gbt+lrè¿™ä¸ªæ¨¡å‹çš„åŸºæœ¬åŸç†å°±å¯ä»¥äº†ã€‚æˆ‘è‡ªå·±åœ¨ç§ä¸‹ä¹Ÿç”¨pythonå†™äº†ä¸€ä¸ªç®€å•çš„demoï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹<a href=\"https://github.com/JoeAsir/Machine-learning-demo/blob/master/algorithm/gbtWithLogisticRegression/gradient_logistic.py\" target=\"_blank\" rel=\"noopener\">å¯ä»¥çœ‹çœ‹</a>ï¼Œæ¬¢è¿æå‡ºæ„è§ï¼Œæ¬¢è¿folkï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://quinonero.net/Publications/predicting-clicks-facebook.pdf\" target=\"_blank\" rel=\"noopener\">He, Xinran, et al. â€œPractical lessons from predicting clicks on ads at facebook.â€ Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.</a></li>\n</ul>"},{"title":"Reading Notes-Class Imbalance, Redux","date":"2017-09-10T05:21:56.000Z","_content":"å†æ¬¡æ„Ÿè°¢ä¼˜ç”·ï¼Œå‘æˆ‘æå‡ºäº†åˆä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œä½¿å¾—æˆ‘æœ‰æœºä¼šæ€è€ƒå’Œç ”ç©¶ï¼Œå¹¶ä¸”æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ç¯‡paperï¼Œå¹¶ä¸”æœ€åå¯ä»¥åˆ†äº«ç»™å¤§å®¶ã€‚\n\næˆ‘ä¸ªäººåœ¨å·¥ä½œä¹‹ä¸­é‡åˆ°è¿‡imbalanced dataçš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯ç›´è§‚çš„æ„Ÿå—åˆ°ï¼Œimbalanced dataçš„æœ€åæ•ˆæœå¾€å¾€ä¸æ˜¯å¾ˆæ£’ï¼Œç½‘ä¸Šä¹Ÿåªæ˜¯ç»™å‡ºäº†oversamplingå’Œundersamplingçš„å»ºè®®ï¼Œå¹¶æ²¡æœ‰æåŠè¿™å…¶ä¸­çš„ä¸€äº›ç¼˜æ•…ï¼Œä»Šå¤©æˆ‘ä»¬ä¸€èµ·é€šè¿‡è¿™ç¯‡paperæ¥å­¦ä¹ å­¦ä¹ ã€‚\n<!--more-->\n## Notes\næˆ‘ä»¬å‡è®¾æœ‰positiveå’Œnegativeä¸¤ç±»sampleï¼Œå…¶ä¸­positive samplesç¬¦åˆ\\\\(P(x)\\\\)çš„Guassianåˆ†å¸ƒï¼Œnegative samplesç¬¦åˆ\\\\(G(x)\\\\)çš„Guassianåˆ†å¸ƒï¼Œåˆ†ç±»å¹³é¢å°†ç©ºé—´åˆ’åˆ†æˆpositive region\\\\(\\cal R^{+} \\_{w}\\\\)å’Œnegative region\\\\(\\cal R^{-} \\_{w}\\\\)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png)\nå›¾ä¸­\\\\(w^{ \\*}\\\\)æ˜¯ç†æƒ³çš„åˆ†å‰²å¹³é¢ï¼Œ\\\\(w^{ \\*}\\\\) åº”è¯¥æ˜¯ä½¿lossæœ€å°çš„å–å€¼ï¼Œå³\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\nå¯¹äºlosså€¼ï¼Œå…¶å®å°±æ˜¯åˆ†ç±»ä¸­è¢«é”™åˆ†çš„fn(false negative)å’Œfp(false positive)çš„æœŸæœ›å€¼ï¼Œæ˜¾ç„¶ï¼Œé€šè¿‡minimunè¯¥losså¾—åˆ°çš„ä¼šæ˜¯å›¾ä¸­çš„\\\\(w^{*}\\\\)ï¼Œå› ä¸ºè¿™ä¸ªåˆ†ç±»å¹³é¢æ‰€å¸¦æ¥çš„erroræ˜æ˜¾æ˜¯æœ€å°‘çš„ã€‚\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\nå¯¹äºæ•´ä¸ªæ•°æ®é›†\\\\(\\cal D \\\\)æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é‡è¾ƒå°‘çš„ä¸€ç±»(paperä¸­è®¾å®špositiveç±»è¾ƒå°‘)æ‰€å æ¯”ä¾‹ä¸º\\\\(\\pi\\\\)(å°äº0.5)ï¼Œé‚£ä¹ˆå¯¹äºå¸¦æœ‰æ¯”ä¾‹\\\\(\\pi\\\\)çš„æ•°æ®é›†\\\\(\\cal D_{\\pi}\\\\)ï¼Œå…¨å±€æœŸæœ›æ˜¯\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\næ­¤å¤„ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œåœ¨ä¸¤ç±»æ•°æ®å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¨å±€æƒ…å†µä¸‹çš„æœŸæœ›å…¶å®æ˜¯å’Œä¸Šé¢çš„lossç­‰ä»·çš„ï¼Œä½†æ˜¯imbalanced dataå¸¦æ¥äº†ä¸å‡è¡¡çš„å› å­\\\\(\\pi\\\\)ï¼Œå› æ­¤ï¼Œä¸¤ä¸ªå…¬å¼ä¸å†ç­‰ä»·ã€‚\n\nOKï¼Œæ—¢ç„¶ä¸ç­‰ä»·ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼Œpaperä¸Šè¯´ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æœŸæœ›è·å¾—çš„\\\\(\\hat w\\\\)ï¼Œæ˜¯å‘ç€è¾ƒå°‘æ•°é‡ç±»åˆ«çš„æ ·æœ¬å€¾æ–œï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€å¹…å›¾ä¸­ï¼Œå‘è¾ƒå°‘çš„postiveé‚£è¾¹skewedï¼ŒåŸå› æ˜¯å› ä¸º\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), ä¹Ÿå°±æ˜¯è¯´ï¼Œ\\\\(\\hat w\\\\)åˆ†å‰²çš„positive regioné¢ç§¯å°äº\\\\(w^{\\*}\\\\)åˆ†å‰²å‡ºçš„é¢ç§¯ï¼Œé¢ç§¯çš„å‡å°åŠ¿å¿…å¯¼è‡´åˆ†å‰²å¹³é¢å‘positiveç±»åˆ«æ–¹å‘åç§»ã€‚\n\né—æ†¾çš„æ˜¯ï¼Œå…³äºé¢ç§¯çš„è¯æ˜æˆ‘å®åœ¨çœ‹ä¸æ˜ç™½ï¼Œä¹Ÿemailäº†ä¸€äº›äººï¼Œä¹Ÿæ²¡æœ‰å¾—åˆ°ä¸€ä¸ªæ»¡æ„çš„ç­”æ¡ˆï¼Œå¦‚æœæœ‰æœ‹å‹çœ‹æ˜ç™½äº†çš„è¯ï¼Œ**è®°å¾—ç•™è¨€æˆ–è€…emailæˆ‘ï¼**\n\nåˆ°äº†è¿™é‡Œï¼Œpaperå¤§æ¦‚ä»‹ç»äº†undersamplingçš„è£¨ç›Šï¼Œundersamplingçš„æ ¸å¿ƒå…¶å®å°±æ˜¯æ¶ˆé™¤å‰é¢æåˆ°çš„æ¯”ä¾‹\\\\(\\pi\\\\)ï¼Œè®©å®ƒè¶‹è¿‘äº0.5åï¼Œåˆ†ç±»å¹³é¢\\\\(\\hat w\\\\)å°±ä¼šè¶‹è¿‘äºç†æƒ³åˆ†ç±»å¹³é¢\\\\(w^{*}\\\\)ã€‚\n\nè¿™é‡Œï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªbaggingæ–¹æ³•ï¼Œå°±æ˜¯å¤šæ¬¡åšundersamplingï¼Œæœ€åæœ€ç»“æœåšbaggingå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¦‚ä¸‹å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png)\npaperè¿˜å¯¹æ¯”äº†å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Weighted Empirical Cost Minimization(å¦‚weighted SVM)å’ŒSMOTEæ–¹æ³•æ•ˆæœä¸å¦‚bagging undersamplingï¼Œæˆ‘ä¸Šä¸€å¹…å›¾è¯´æ˜ä¸‹SMOTEçš„ç¼ºç‚¹ï¼Œæ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥è¯¦ç»†çœ‹çœ‹paperï¼Œå¦‚å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png)\nSMOTEæ–¹æ³•æ˜¯éšæœºé€‰æ‹©æ–¹å‘ç”Ÿæˆæ–°çš„sampleï¼Œä½†æ˜¯å¦‚æœæ–°çš„sampleäº§ç”Ÿäº†å›¾ä¸­ä½ç½®ï¼Œåˆ™æ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚\n\nOKï¼Œä»Šå¤©å°±è¿™ä¹ˆå¤šï¼Œè®°å¾—çœ‹æ˜ç™½äº†ä¸­é—´çš„æ¨å¯¼ä¸€èµ·åˆ†äº«å•Šï¼\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)\n","source":"_posts/paper-imbalance.md","raw":"---\ntitle: Reading Notes-Class Imbalance, Redux\ndate: 2017-09-10 13:21:56\ntags: \n\t- imbalanced data\n\t- undersampling\n\t- bagging\ncategories: reading notes\n---\nå†æ¬¡æ„Ÿè°¢ä¼˜ç”·ï¼Œå‘æˆ‘æå‡ºäº†åˆä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œä½¿å¾—æˆ‘æœ‰æœºä¼šæ€è€ƒå’Œç ”ç©¶ï¼Œå¹¶ä¸”æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ç¯‡paperï¼Œå¹¶ä¸”æœ€åå¯ä»¥åˆ†äº«ç»™å¤§å®¶ã€‚\n\næˆ‘ä¸ªäººåœ¨å·¥ä½œä¹‹ä¸­é‡åˆ°è¿‡imbalanced dataçš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯ç›´è§‚çš„æ„Ÿå—åˆ°ï¼Œimbalanced dataçš„æœ€åæ•ˆæœå¾€å¾€ä¸æ˜¯å¾ˆæ£’ï¼Œç½‘ä¸Šä¹Ÿåªæ˜¯ç»™å‡ºäº†oversamplingå’Œundersamplingçš„å»ºè®®ï¼Œå¹¶æ²¡æœ‰æåŠè¿™å…¶ä¸­çš„ä¸€äº›ç¼˜æ•…ï¼Œä»Šå¤©æˆ‘ä»¬ä¸€èµ·é€šè¿‡è¿™ç¯‡paperæ¥å­¦ä¹ å­¦ä¹ ã€‚\n<!--more-->\n## Notes\næˆ‘ä»¬å‡è®¾æœ‰positiveå’Œnegativeä¸¤ç±»sampleï¼Œå…¶ä¸­positive samplesç¬¦åˆ\\\\(P(x)\\\\)çš„Guassianåˆ†å¸ƒï¼Œnegative samplesç¬¦åˆ\\\\(G(x)\\\\)çš„Guassianåˆ†å¸ƒï¼Œåˆ†ç±»å¹³é¢å°†ç©ºé—´åˆ’åˆ†æˆpositive region\\\\(\\cal R^{+} \\_{w}\\\\)å’Œnegative region\\\\(\\cal R^{-} \\_{w}\\\\)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png)\nå›¾ä¸­\\\\(w^{ \\*}\\\\)æ˜¯ç†æƒ³çš„åˆ†å‰²å¹³é¢ï¼Œ\\\\(w^{ \\*}\\\\) åº”è¯¥æ˜¯ä½¿lossæœ€å°çš„å–å€¼ï¼Œå³\n$$w^{*}= \\arg\\underset{w}{\\min} \\cal L^{*}(w)$$\nå¯¹äºlosså€¼ï¼Œå…¶å®å°±æ˜¯åˆ†ç±»ä¸­è¢«é”™åˆ†çš„fn(false negative)å’Œfp(false positive)çš„æœŸæœ›å€¼ï¼Œæ˜¾ç„¶ï¼Œé€šè¿‡minimunè¯¥losså¾—åˆ°çš„ä¼šæ˜¯å›¾ä¸­çš„\\\\(w^{*}\\\\)ï¼Œå› ä¸ºè¿™ä¸ªåˆ†ç±»å¹³é¢æ‰€å¸¦æ¥çš„erroræ˜æ˜¾æ˜¯æœ€å°‘çš„ã€‚\n$$\\cal L^{*}(w) = \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\nå¯¹äºæ•´ä¸ªæ•°æ®é›†\\\\(\\cal D \\\\)æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é‡è¾ƒå°‘çš„ä¸€ç±»(paperä¸­è®¾å®špositiveç±»è¾ƒå°‘)æ‰€å æ¯”ä¾‹ä¸º\\\\(\\pi\\\\)(å°äº0.5)ï¼Œé‚£ä¹ˆå¯¹äºå¸¦æœ‰æ¯”ä¾‹\\\\(\\pi\\\\)çš„æ•°æ®é›†\\\\(\\cal D_{\\pi}\\\\)ï¼Œå…¨å±€æœŸæœ›æ˜¯\n$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} _{-}} \\it P(x)dx + (1- \\pi) \\cal C_{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$\næ­¤å¤„ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œåœ¨ä¸¤ç±»æ•°æ®å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¨å±€æƒ…å†µä¸‹çš„æœŸæœ›å…¶å®æ˜¯å’Œä¸Šé¢çš„lossç­‰ä»·çš„ï¼Œä½†æ˜¯imbalanced dataå¸¦æ¥äº†ä¸å‡è¡¡çš„å› å­\\\\(\\pi\\\\)ï¼Œå› æ­¤ï¼Œä¸¤ä¸ªå…¬å¼ä¸å†ç­‰ä»·ã€‚\n\nOKï¼Œæ—¢ç„¶ä¸ç­‰ä»·ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼Œpaperä¸Šè¯´ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æœŸæœ›è·å¾—çš„\\\\(\\hat w\\\\)ï¼Œæ˜¯å‘ç€è¾ƒå°‘æ•°é‡ç±»åˆ«çš„æ ·æœ¬å€¾æ–œï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€å¹…å›¾ä¸­ï¼Œå‘è¾ƒå°‘çš„postiveé‚£è¾¹skewedï¼ŒåŸå› æ˜¯å› ä¸º\\\\( \\cal R \\_{+} ^{ \\hat w} < \\cal R \\_{+} ^{w^{\\*}}\\\\), ä¹Ÿå°±æ˜¯è¯´ï¼Œ\\\\(\\hat w\\\\)åˆ†å‰²çš„positive regioné¢ç§¯å°äº\\\\(w^{\\*}\\\\)åˆ†å‰²å‡ºçš„é¢ç§¯ï¼Œé¢ç§¯çš„å‡å°åŠ¿å¿…å¯¼è‡´åˆ†å‰²å¹³é¢å‘positiveç±»åˆ«æ–¹å‘åç§»ã€‚\n\né—æ†¾çš„æ˜¯ï¼Œå…³äºé¢ç§¯çš„è¯æ˜æˆ‘å®åœ¨çœ‹ä¸æ˜ç™½ï¼Œä¹Ÿemailäº†ä¸€äº›äººï¼Œä¹Ÿæ²¡æœ‰å¾—åˆ°ä¸€ä¸ªæ»¡æ„çš„ç­”æ¡ˆï¼Œå¦‚æœæœ‰æœ‹å‹çœ‹æ˜ç™½äº†çš„è¯ï¼Œ**è®°å¾—ç•™è¨€æˆ–è€…emailæˆ‘ï¼**\n\nåˆ°äº†è¿™é‡Œï¼Œpaperå¤§æ¦‚ä»‹ç»äº†undersamplingçš„è£¨ç›Šï¼Œundersamplingçš„æ ¸å¿ƒå…¶å®å°±æ˜¯æ¶ˆé™¤å‰é¢æåˆ°çš„æ¯”ä¾‹\\\\(\\pi\\\\)ï¼Œè®©å®ƒè¶‹è¿‘äº0.5åï¼Œåˆ†ç±»å¹³é¢\\\\(\\hat w\\\\)å°±ä¼šè¶‹è¿‘äºç†æƒ³åˆ†ç±»å¹³é¢\\\\(w^{*}\\\\)ã€‚\n\nè¿™é‡Œï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªbaggingæ–¹æ³•ï¼Œå°±æ˜¯å¤šæ¬¡åšundersamplingï¼Œæœ€åæœ€ç»“æœåšbaggingå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¦‚ä¸‹å›¾\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png)\npaperè¿˜å¯¹æ¯”äº†å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Weighted Empirical Cost Minimization(å¦‚weighted SVM)å’ŒSMOTEæ–¹æ³•æ•ˆæœä¸å¦‚bagging undersamplingï¼Œæˆ‘ä¸Šä¸€å¹…å›¾è¯´æ˜ä¸‹SMOTEçš„ç¼ºç‚¹ï¼Œæ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥è¯¦ç»†çœ‹çœ‹paperï¼Œå¦‚å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png)\nSMOTEæ–¹æ³•æ˜¯éšæœºé€‰æ‹©æ–¹å‘ç”Ÿæˆæ–°çš„sampleï¼Œä½†æ˜¯å¦‚æœæ–°çš„sampleäº§ç”Ÿäº†å›¾ä¸­ä½ç½®ï¼Œåˆ™æ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚\n\nOKï¼Œä»Šå¤©å°±è¿™ä¹ˆå¤šï¼Œè®°å¾—çœ‹æ˜ç™½äº†ä¸­é—´çš„æ¨å¯¼ä¸€èµ·åˆ†äº«å•Šï¼\n## Reference\n* [Wallace, Byron C., et al. \"Class imbalance, redux.\" Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.](https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf)\n* [PPT-Class Imbalance, Redux](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog)\n","slug":"paper-imbalance","published":1,"updated":"2018-11-19T06:19:16.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pg000ztr8lgliwrab7","content":"<p>å†æ¬¡æ„Ÿè°¢ä¼˜ç”·ï¼Œå‘æˆ‘æå‡ºäº†åˆä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œä½¿å¾—æˆ‘æœ‰æœºä¼šæ€è€ƒå’Œç ”ç©¶ï¼Œå¹¶ä¸”æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ç¯‡paperï¼Œå¹¶ä¸”æœ€åå¯ä»¥åˆ†äº«ç»™å¤§å®¶ã€‚</p>\n<p>æˆ‘ä¸ªäººåœ¨å·¥ä½œä¹‹ä¸­é‡åˆ°è¿‡imbalanced dataçš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯ç›´è§‚çš„æ„Ÿå—åˆ°ï¼Œimbalanced dataçš„æœ€åæ•ˆæœå¾€å¾€ä¸æ˜¯å¾ˆæ£’ï¼Œç½‘ä¸Šä¹Ÿåªæ˜¯ç»™å‡ºäº†oversamplingå’Œundersamplingçš„å»ºè®®ï¼Œå¹¶æ²¡æœ‰æåŠè¿™å…¶ä¸­çš„ä¸€äº›ç¼˜æ•…ï¼Œä»Šå¤©æˆ‘ä»¬ä¸€èµ·é€šè¿‡è¿™ç¯‡paperæ¥å­¦ä¹ å­¦ä¹ ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>æˆ‘ä»¬å‡è®¾æœ‰positiveå’Œnegativeä¸¤ç±»sampleï¼Œå…¶ä¸­positive samplesç¬¦åˆ\\(P(x)\\)çš„Guassianåˆ†å¸ƒï¼Œnegative samplesç¬¦åˆ\\(G(x)\\)çš„Guassianåˆ†å¸ƒï¼Œåˆ†ç±»å¹³é¢å°†ç©ºé—´åˆ’åˆ†æˆpositive region\\(\\cal R^{+} _{w}\\)å’Œnegative region\\(\\cal R^{-} _{w}\\)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png\" alt=\"\"><br>å›¾ä¸­\\(w^{ *}\\)æ˜¯ç†æƒ³çš„åˆ†å‰²å¹³é¢ï¼Œ\\(w^{ *}\\) åº”è¯¥æ˜¯ä½¿lossæœ€å°çš„å–å€¼ï¼Œå³<br>$$w^{<em>}= \\arg\\underset{w}{\\min} \\cal L^{</em>}(w)$$<br>å¯¹äºlosså€¼ï¼Œå…¶å®å°±æ˜¯åˆ†ç±»ä¸­è¢«é”™åˆ†çš„fn(false negative)å’Œfp(false positive)çš„æœŸæœ›å€¼ï¼Œæ˜¾ç„¶ï¼Œé€šè¿‡minimunè¯¥losså¾—åˆ°çš„ä¼šæ˜¯å›¾ä¸­çš„\\(w^{<em>}\\)ï¼Œå› ä¸ºè¿™ä¸ªåˆ†ç±»å¹³é¢æ‰€å¸¦æ¥çš„erroræ˜æ˜¾æ˜¯æœ€å°‘çš„ã€‚<br>$$\\cal L^{</em>}(w) = \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + \\cal C</em>{fp} \\int _{\\cal R^{w} <em>{+}} \\it G(x)dx$$<br>å¯¹äºæ•´ä¸ªæ•°æ®é›†\\(\\cal D \\)æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é‡è¾ƒå°‘çš„ä¸€ç±»(paperä¸­è®¾å®špositiveç±»è¾ƒå°‘)æ‰€å æ¯”ä¾‹ä¸º\\(\\pi\\)(å°äº0.5)ï¼Œé‚£ä¹ˆå¯¹äºå¸¦æœ‰æ¯”ä¾‹\\(\\pi\\)çš„æ•°æ®é›†\\(\\cal D</em>{\\pi}\\)ï¼Œå…¨å±€æœŸæœ›æ˜¯<br>$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + (1- \\pi) \\cal C</em>{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$<br>æ­¤å¤„ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œåœ¨ä¸¤ç±»æ•°æ®å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¨å±€æƒ…å†µä¸‹çš„æœŸæœ›å…¶å®æ˜¯å’Œä¸Šé¢çš„lossç­‰ä»·çš„ï¼Œä½†æ˜¯imbalanced dataå¸¦æ¥äº†ä¸å‡è¡¡çš„å› å­\\(\\pi\\)ï¼Œå› æ­¤ï¼Œä¸¤ä¸ªå…¬å¼ä¸å†ç­‰ä»·ã€‚</p>\n<p>OKï¼Œæ—¢ç„¶ä¸ç­‰ä»·ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼Œpaperä¸Šè¯´ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æœŸæœ›è·å¾—çš„\\(\\hat w\\)ï¼Œæ˜¯å‘ç€è¾ƒå°‘æ•°é‡ç±»åˆ«çš„æ ·æœ¬å€¾æ–œï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€å¹…å›¾ä¸­ï¼Œå‘è¾ƒå°‘çš„postiveé‚£è¾¹skewedï¼ŒåŸå› æ˜¯å› ä¸º\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), ä¹Ÿå°±æ˜¯è¯´ï¼Œ\\(\\hat w\\)åˆ†å‰²çš„positive regioné¢ç§¯å°äº\\(w^{*}\\)åˆ†å‰²å‡ºçš„é¢ç§¯ï¼Œé¢ç§¯çš„å‡å°åŠ¿å¿…å¯¼è‡´åˆ†å‰²å¹³é¢å‘positiveç±»åˆ«æ–¹å‘åç§»ã€‚</p>\n<p>é—æ†¾çš„æ˜¯ï¼Œå…³äºé¢ç§¯çš„è¯æ˜æˆ‘å®åœ¨çœ‹ä¸æ˜ç™½ï¼Œä¹Ÿemailäº†ä¸€äº›äººï¼Œä¹Ÿæ²¡æœ‰å¾—åˆ°ä¸€ä¸ªæ»¡æ„çš„ç­”æ¡ˆï¼Œå¦‚æœæœ‰æœ‹å‹çœ‹æ˜ç™½äº†çš„è¯ï¼Œ<strong>è®°å¾—ç•™è¨€æˆ–è€…emailæˆ‘ï¼</strong></p>\n<p>åˆ°äº†è¿™é‡Œï¼Œpaperå¤§æ¦‚ä»‹ç»äº†undersamplingçš„è£¨ç›Šï¼Œundersamplingçš„æ ¸å¿ƒå…¶å®å°±æ˜¯æ¶ˆé™¤å‰é¢æåˆ°çš„æ¯”ä¾‹\\(\\pi\\)ï¼Œè®©å®ƒè¶‹è¿‘äº0.5åï¼Œåˆ†ç±»å¹³é¢\\(\\hat w\\)å°±ä¼šè¶‹è¿‘äºç†æƒ³åˆ†ç±»å¹³é¢\\(w^{*}\\)ã€‚</p>\n<p>è¿™é‡Œï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªbaggingæ–¹æ³•ï¼Œå°±æ˜¯å¤šæ¬¡åšundersamplingï¼Œæœ€åæœ€ç»“æœåšbaggingå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¦‚ä¸‹å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png\" alt=\"\"><br>paperè¿˜å¯¹æ¯”äº†å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Weighted Empirical Cost Minimization(å¦‚weighted SVM)å’ŒSMOTEæ–¹æ³•æ•ˆæœä¸å¦‚bagging undersamplingï¼Œæˆ‘ä¸Šä¸€å¹…å›¾è¯´æ˜ä¸‹SMOTEçš„ç¼ºç‚¹ï¼Œæ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥è¯¦ç»†çœ‹çœ‹paperï¼Œå¦‚å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png\" alt=\"\"><br>SMOTEæ–¹æ³•æ˜¯éšæœºé€‰æ‹©æ–¹å‘ç”Ÿæˆæ–°çš„sampleï¼Œä½†æ˜¯å¦‚æœæ–°çš„sampleäº§ç”Ÿäº†å›¾ä¸­ä½ç½®ï¼Œåˆ™æ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚</p>\n<p>OKï¼Œä»Šå¤©å°±è¿™ä¹ˆå¤šï¼Œè®°å¾—çœ‹æ˜ç™½äº†ä¸­é—´çš„æ¨å¯¼ä¸€èµ·åˆ†äº«å•Šï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"noopener\">Wallace, Byron C., et al. â€œClass imbalance, redux.â€ Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"noopener\">PPT-Class Imbalance, Redux</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"imbalanced data","path":"tags/imbalanced-data/"},{"name":"undersampling","path":"tags/undersampling/"},{"name":"bagging","path":"tags/bagging/"}],"excerpt":"<p>å†æ¬¡æ„Ÿè°¢ä¼˜ç”·ï¼Œå‘æˆ‘æå‡ºäº†åˆä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œä½¿å¾—æˆ‘æœ‰æœºä¼šæ€è€ƒå’Œç ”ç©¶ï¼Œå¹¶ä¸”æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ç¯‡paperï¼Œå¹¶ä¸”æœ€åå¯ä»¥åˆ†äº«ç»™å¤§å®¶ã€‚</p>\n<p>æˆ‘ä¸ªäººåœ¨å·¥ä½œä¹‹ä¸­é‡åˆ°è¿‡imbalanced dataçš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯ç›´è§‚çš„æ„Ÿå—åˆ°ï¼Œimbalanced dataçš„æœ€åæ•ˆæœå¾€å¾€ä¸æ˜¯å¾ˆæ£’ï¼Œç½‘ä¸Šä¹Ÿåªæ˜¯ç»™å‡ºäº†oversamplingå’Œundersamplingçš„å»ºè®®ï¼Œå¹¶æ²¡æœ‰æåŠè¿™å…¶ä¸­çš„ä¸€äº›ç¼˜æ•…ï¼Œä»Šå¤©æˆ‘ä»¬ä¸€èµ·é€šè¿‡è¿™ç¯‡paperæ¥å­¦ä¹ å­¦ä¹ ã€‚<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>æˆ‘ä»¬å‡è®¾æœ‰positiveå’Œnegativeä¸¤ç±»sampleï¼Œå…¶ä¸­positive samplesç¬¦åˆ\\(P(x)\\)çš„Guassianåˆ†å¸ƒï¼Œnegative samplesç¬¦åˆ\\(G(x)\\)çš„Guassianåˆ†å¸ƒï¼Œåˆ†ç±»å¹³é¢å°†ç©ºé—´åˆ’åˆ†æˆpositive region\\(\\cal R^{+} _{w}\\)å’Œnegative region\\(\\cal R^{-} _{w}\\)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-1.png\" alt=\"\"><br>å›¾ä¸­\\(w^{ *}\\)æ˜¯ç†æƒ³çš„åˆ†å‰²å¹³é¢ï¼Œ\\(w^{ *}\\) åº”è¯¥æ˜¯ä½¿lossæœ€å°çš„å–å€¼ï¼Œå³<br>$$w^{<em>}= \\arg\\underset{w}{\\min} \\cal L^{</em>}(w)$$<br>å¯¹äºlosså€¼ï¼Œå…¶å®å°±æ˜¯åˆ†ç±»ä¸­è¢«é”™åˆ†çš„fn(false negative)å’Œfp(false positive)çš„æœŸæœ›å€¼ï¼Œæ˜¾ç„¶ï¼Œé€šè¿‡minimunè¯¥losså¾—åˆ°çš„ä¼šæ˜¯å›¾ä¸­çš„\\(w^{<em>}\\)ï¼Œå› ä¸ºè¿™ä¸ªåˆ†ç±»å¹³é¢æ‰€å¸¦æ¥çš„erroræ˜æ˜¾æ˜¯æœ€å°‘çš„ã€‚<br>$$\\cal L^{</em>}(w) = \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + \\cal C</em>{fp} \\int _{\\cal R^{w} <em>{+}} \\it G(x)dx$$<br>å¯¹äºæ•´ä¸ªæ•°æ®é›†\\(\\cal D \\)æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é‡è¾ƒå°‘çš„ä¸€ç±»(paperä¸­è®¾å®špositiveç±»è¾ƒå°‘)æ‰€å æ¯”ä¾‹ä¸º\\(\\pi\\)(å°äº0.5)ï¼Œé‚£ä¹ˆå¯¹äºå¸¦æœ‰æ¯”ä¾‹\\(\\pi\\)çš„æ•°æ®é›†\\(\\cal D</em>{\\pi}\\)ï¼Œå…¨å±€æœŸæœ›æ˜¯<br>$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} <em>{-}} \\it P(x)dx + (1- \\pi) \\cal C</em>{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$<br>æ­¤å¤„ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œåœ¨ä¸¤ç±»æ•°æ®å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¨å±€æƒ…å†µä¸‹çš„æœŸæœ›å…¶å®æ˜¯å’Œä¸Šé¢çš„lossç­‰ä»·çš„ï¼Œä½†æ˜¯imbalanced dataå¸¦æ¥äº†ä¸å‡è¡¡çš„å› å­\\(\\pi\\)ï¼Œå› æ­¤ï¼Œä¸¤ä¸ªå…¬å¼ä¸å†ç­‰ä»·ã€‚</p>\n<p>OKï¼Œæ—¢ç„¶ä¸ç­‰ä»·ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼Œpaperä¸Šè¯´ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æœŸæœ›è·å¾—çš„\\(\\hat w\\)ï¼Œæ˜¯å‘ç€è¾ƒå°‘æ•°é‡ç±»åˆ«çš„æ ·æœ¬å€¾æ–œï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€å¹…å›¾ä¸­ï¼Œå‘è¾ƒå°‘çš„postiveé‚£è¾¹skewedï¼ŒåŸå› æ˜¯å› ä¸º\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), ä¹Ÿå°±æ˜¯è¯´ï¼Œ\\(\\hat w\\)åˆ†å‰²çš„positive regioné¢ç§¯å°äº\\(w^{*}\\)åˆ†å‰²å‡ºçš„é¢ç§¯ï¼Œé¢ç§¯çš„å‡å°åŠ¿å¿…å¯¼è‡´åˆ†å‰²å¹³é¢å‘positiveç±»åˆ«æ–¹å‘åç§»ã€‚</p>\n<p>é—æ†¾çš„æ˜¯ï¼Œå…³äºé¢ç§¯çš„è¯æ˜æˆ‘å®åœ¨çœ‹ä¸æ˜ç™½ï¼Œä¹Ÿemailäº†ä¸€äº›äººï¼Œä¹Ÿæ²¡æœ‰å¾—åˆ°ä¸€ä¸ªæ»¡æ„çš„ç­”æ¡ˆï¼Œå¦‚æœæœ‰æœ‹å‹çœ‹æ˜ç™½äº†çš„è¯ï¼Œ<strong>è®°å¾—ç•™è¨€æˆ–è€…emailæˆ‘ï¼</strong></p>\n<p>åˆ°äº†è¿™é‡Œï¼Œpaperå¤§æ¦‚ä»‹ç»äº†undersamplingçš„è£¨ç›Šï¼Œundersamplingçš„æ ¸å¿ƒå…¶å®å°±æ˜¯æ¶ˆé™¤å‰é¢æåˆ°çš„æ¯”ä¾‹\\(\\pi\\)ï¼Œè®©å®ƒè¶‹è¿‘äº0.5åï¼Œåˆ†ç±»å¹³é¢\\(\\hat w\\)å°±ä¼šè¶‹è¿‘äºç†æƒ³åˆ†ç±»å¹³é¢\\(w^{*}\\)ã€‚</p>\n<p>è¿™é‡Œï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªbaggingæ–¹æ³•ï¼Œå°±æ˜¯å¤šæ¬¡åšundersamplingï¼Œæœ€åæœ€ç»“æœåšbaggingå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¦‚ä¸‹å›¾<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-2.png\" alt=\"\"><br>paperè¿˜å¯¹æ¯”äº†å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Weighted Empirical Cost Minimization(å¦‚weighted SVM)å’ŒSMOTEæ–¹æ³•æ•ˆæœä¸å¦‚bagging undersamplingï¼Œæˆ‘ä¸Šä¸€å¹…å›¾è¯´æ˜ä¸‹SMOTEçš„ç¼ºç‚¹ï¼Œæ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥è¯¦ç»†çœ‹çœ‹paperï¼Œå¦‚å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/5/5-3.png\" alt=\"\"><br>SMOTEæ–¹æ³•æ˜¯éšæœºé€‰æ‹©æ–¹å‘ç”Ÿæˆæ–°çš„sampleï¼Œä½†æ˜¯å¦‚æœæ–°çš„sampleäº§ç”Ÿäº†å›¾ä¸­ä½ç½®ï¼Œåˆ™æ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚</p>\n<p>OKï¼Œä»Šå¤©å°±è¿™ä¹ˆå¤šï¼Œè®°å¾—çœ‹æ˜ç™½äº†ä¸­é—´çš„æ¨å¯¼ä¸€èµ·åˆ†äº«å•Šï¼</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://pdfs.semanticscholar.org/a8ef/5a810099178b70d1490a4e6fc4426b642cde.pdf\" target=\"_blank\" rel=\"noopener\">Wallace, Byron C., et al. â€œClass imbalance, redux.â€ Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011.</a></li>\n<li><a href=\"https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiVmqXlkprWAhVEsJQKHYJiCf4QFgg8MAI&amp;url=https%3A%2F%2Fcourse.ccs.neu.edu%2Fcs6140sp15%2F4_boosting%2Fslides%2Fwallace_imbalance_icdm_11_for_class_2012_final.pptx&amp;usg=AFQjCNG6GpjKeinzCsXrZWWY1edtbBMgog\" target=\"_blank\" rel=\"noopener\">PPT-Class Imbalance, Redux</a></li>\n</ul>"},{"title":"Reading Notes-Swishï¼šA Self-gated Activation Function","date":"2017-10-22T08:13:30.000Z","_content":"Hi allï¼Œä»Šå¤©å’Œå¤§å®¶åˆ†äº«ä¸€ç¯‡æ¯”è¾ƒæ–°çš„paperï¼Œæ˜¯å…³äºä¸€ç§æ–°çš„activation functionï¼Œå…³äºæˆ‘ä»¬çŸ¥é“çš„activation functionï¼Œæœ‰sigmoidï¼Œtanhï¼ŒReLUä»¥åŠReLUçš„ä¸€äº›å˜ç§ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©æ¥çœ‹çœ‹è¿™ç§æ–°æå‡ºçš„activation functionåˆ°åº•æœ‰ä»€ä¹ˆç‰¹è‰²ã€‚\n<!--more-->\n## Notes\né¦–å…ˆå®šä¹‰ï¼Œswish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)ï¼Œå…¶ä¸­\\\\(\\sigma(x)\\\\)æ˜¯sigmoid functionï¼Œä¹Ÿå°±æ˜¯\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functinçš„å›¾åƒå¦‚å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png)\næˆ‘ä»¬å†æ¥çœ‹ä¸‹swish functionçš„1st and 2nd derivativesï¼Œ\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png)\nä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥é›†ä¸­çœ‹çœ‹swish functionçš„ä¼˜ç‚¹éƒ½æœ‰ä»€ä¹ˆï¼Œä½œè€…ç»™å‡ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼šå‡½æ•°å€¼æ²¡æœ‰ä¸Šé™ï¼Œå‡½æ•°å€¼æœ‰ä¸‹é™ï¼Œå‡½æ•°ä¸å•è°ƒï¼Œå‡½æ•°å…‰æ»‘è¿ç»­ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š\n### Unbounded above\nUnbounded aboveçš„å®è´¨ï¼Œæ˜¯é˜²æ­¢activation functionåœ¨bounded valueå¤„å‘ç”Ÿsaturation. bounded above å¸¦æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯è¶Šæ¥è¿‘bounded valueçš„æ—¶å€™ï¼Œfunction gradientå°±ä¼šè¶Šå°ï¼Œé€æ¸æ¥è¿‘0ï¼Œè¿™å°±å¯¼è‡´gradient descentå¼‚å¸¸ç¼“æ…¢ç”šè‡³æ— æ³•convergeã€‚ä¾‹å¦‚sigmoid å’Œtanh functionï¼Œä»–ä»¬éƒ½æ˜¯bounded below and aboveï¼Œå½“æˆ‘ä»¬é‡‡ç”¨è¿™ä¸¤ç§activation functionçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¿…é¡»è°¨æ…å°å¿ƒçš„è®©åˆå§‹å€¼å°½é‡åœ¨functionçš„æ¥è¿‘linerçš„éƒ¨åˆ†æ¥é¿å…ä¸Šé¢é—®é¢˜çš„äº§ç”Ÿï¼Œå› æ­¤ï¼Œunbounded aboveæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜ç‚¹ï¼Œä¾‹å¦‚ReLUåŠå…¶å˜ç§éƒ½é‡‡ç”¨äº†è¿™ä¸€åŸåˆ™ã€‚\n\n### Bounded below & non-monotonicity\nBounded belowå…¶å®ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¹Ÿæœ‰activation functionå·²ç»é‡‡ç”¨äº†ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ‰€æœ‰è´Ÿæ•°inputéƒ½ä¼šå¾—åˆ°ç›¸å·®æ— å‡ çš„activation valueï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ-1000å’Œ-1çš„å€¼å‡ ä¹æ²¡æœ‰åŒºåˆ«ï¼ŒæŒ‰ç…§authorçš„è¯æ¥è®²ï¼Œå°±æ˜¯æˆ‘ä»¬å°†\n> make large negative input \"fogotten\"\n\nè¿™å…¶å®ä¹Ÿæ˜¯regularzationçš„ä¸€ç§æ€æƒ³ï¼Œè¿™ç§æ–¹æ³•åœ¨ReLUç­‰æ–¹æ³•ä¸­ä¹Ÿæœ‰ä½“ç°ï¼Œä½†æ˜¯ï¼Œswishå¯ä»¥é€šè¿‡è‡ªèº«çš„éå•è°ƒæ€§è´¨ï¼Œå°†æ¯”è¾ƒå°çš„negative inputä»ç„¶ä»¥negative valueè¾“å‡ºï¼Œnon-monotonicityæä¾›äº†æ›´å¥½çš„gradient flow.\n\n### Smothness\nå…³äºsmoothnessçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png)\n \n æ€»è€Œè¨€ä¹‹ï¼Œä¸ªäººæ„Ÿè§‰swishåº”è¯¥ç®—æ˜¯ä¸€ä¸ªä¸é”™çš„activationï¼Œæœ¬äººç”±äºæ—¶é—´åŸå› ï¼Œè¿˜æ²¡æœ‰æ¥å¾—åŠè‡ªå·±æµ‹è¯•å®ƒï¼Œä½†æ˜¯æ®æˆ‘æ‰€çœ‹åˆ°çš„è®¨è®ºï¼Œswishçš„å®é™…æ•ˆæœè²Œä¼¼ä¸æ˜¯ååˆ†ç¨³å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒä¿ç•™æ„è§ï¼Œè¿›ä¸€æ­¥è§‚å¯Ÿå®ƒçš„è¡¨ç°ã€‚\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swishï¼ša Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","source":"_posts/paper-swish.md","raw":"---\ntitle: Reading Notes-Swishï¼šA Self-gated Activation Function\ndate: 2017-10-22 16:13:30\ntags: \n\t- activtion function\ncategories: reading notes\n---\nHi allï¼Œä»Šå¤©å’Œå¤§å®¶åˆ†äº«ä¸€ç¯‡æ¯”è¾ƒæ–°çš„paperï¼Œæ˜¯å…³äºä¸€ç§æ–°çš„activation functionï¼Œå…³äºæˆ‘ä»¬çŸ¥é“çš„activation functionï¼Œæœ‰sigmoidï¼Œtanhï¼ŒReLUä»¥åŠReLUçš„ä¸€äº›å˜ç§ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©æ¥çœ‹çœ‹è¿™ç§æ–°æå‡ºçš„activation functionåˆ°åº•æœ‰ä»€ä¹ˆç‰¹è‰²ã€‚\n<!--more-->\n## Notes\né¦–å…ˆå®šä¹‰ï¼Œswish activation function \\\\(f(x)=x \\cdot \\sigma (x)\\\\)ï¼Œå…¶ä¸­\\\\(\\sigma(x)\\\\)æ˜¯sigmoid functionï¼Œä¹Ÿå°±æ˜¯\\\\( \\sigma(x)=1/(1+ e^{-x})\\\\).Swish functinçš„å›¾åƒå¦‚å›¾æ‰€ç¤ºï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png)\næˆ‘ä»¬å†æ¥çœ‹ä¸‹swish functionçš„1st and 2nd derivativesï¼Œ\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png)\nä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥é›†ä¸­çœ‹çœ‹swish functionçš„ä¼˜ç‚¹éƒ½æœ‰ä»€ä¹ˆï¼Œä½œè€…ç»™å‡ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼šå‡½æ•°å€¼æ²¡æœ‰ä¸Šé™ï¼Œå‡½æ•°å€¼æœ‰ä¸‹é™ï¼Œå‡½æ•°ä¸å•è°ƒï¼Œå‡½æ•°å…‰æ»‘è¿ç»­ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š\n### Unbounded above\nUnbounded aboveçš„å®è´¨ï¼Œæ˜¯é˜²æ­¢activation functionåœ¨bounded valueå¤„å‘ç”Ÿsaturation. bounded above å¸¦æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯è¶Šæ¥è¿‘bounded valueçš„æ—¶å€™ï¼Œfunction gradientå°±ä¼šè¶Šå°ï¼Œé€æ¸æ¥è¿‘0ï¼Œè¿™å°±å¯¼è‡´gradient descentå¼‚å¸¸ç¼“æ…¢ç”šè‡³æ— æ³•convergeã€‚ä¾‹å¦‚sigmoid å’Œtanh functionï¼Œä»–ä»¬éƒ½æ˜¯bounded below and aboveï¼Œå½“æˆ‘ä»¬é‡‡ç”¨è¿™ä¸¤ç§activation functionçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¿…é¡»è°¨æ…å°å¿ƒçš„è®©åˆå§‹å€¼å°½é‡åœ¨functionçš„æ¥è¿‘linerçš„éƒ¨åˆ†æ¥é¿å…ä¸Šé¢é—®é¢˜çš„äº§ç”Ÿï¼Œå› æ­¤ï¼Œunbounded aboveæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜ç‚¹ï¼Œä¾‹å¦‚ReLUåŠå…¶å˜ç§éƒ½é‡‡ç”¨äº†è¿™ä¸€åŸåˆ™ã€‚\n\n### Bounded below & non-monotonicity\nBounded belowå…¶å®ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¹Ÿæœ‰activation functionå·²ç»é‡‡ç”¨äº†ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ‰€æœ‰è´Ÿæ•°inputéƒ½ä¼šå¾—åˆ°ç›¸å·®æ— å‡ çš„activation valueï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ-1000å’Œ-1çš„å€¼å‡ ä¹æ²¡æœ‰åŒºåˆ«ï¼ŒæŒ‰ç…§authorçš„è¯æ¥è®²ï¼Œå°±æ˜¯æˆ‘ä»¬å°†\n> make large negative input \"fogotten\"\n\nè¿™å…¶å®ä¹Ÿæ˜¯regularzationçš„ä¸€ç§æ€æƒ³ï¼Œè¿™ç§æ–¹æ³•åœ¨ReLUç­‰æ–¹æ³•ä¸­ä¹Ÿæœ‰ä½“ç°ï¼Œä½†æ˜¯ï¼Œswishå¯ä»¥é€šè¿‡è‡ªèº«çš„éå•è°ƒæ€§è´¨ï¼Œå°†æ¯”è¾ƒå°çš„negative inputä»ç„¶ä»¥negative valueè¾“å‡ºï¼Œnon-monotonicityæä¾›äº†æ›´å¥½çš„gradient flow.\n\n### Smothness\nå…³äºsmoothnessçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png)\n \n æ€»è€Œè¨€ä¹‹ï¼Œä¸ªäººæ„Ÿè§‰swishåº”è¯¥ç®—æ˜¯ä¸€ä¸ªä¸é”™çš„activationï¼Œæœ¬äººç”±äºæ—¶é—´åŸå› ï¼Œè¿˜æ²¡æœ‰æ¥å¾—åŠè‡ªå·±æµ‹è¯•å®ƒï¼Œä½†æ˜¯æ®æˆ‘æ‰€çœ‹åˆ°çš„è®¨è®ºï¼Œswishçš„å®é™…æ•ˆæœè²Œä¼¼ä¸æ˜¯ååˆ†ç¨³å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒä¿ç•™æ„è§ï¼Œè¿›ä¸€æ­¥è§‚å¯Ÿå®ƒçš„è¡¨ç°ã€‚\n \n## Reference\n* [Ramachandran P, Zoph B, Le Q V. Swishï¼ša Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.](https://arxiv.org/pdf/1710.05941.pdf)\n","slug":"paper-swish","published":1,"updated":"2018-11-19T06:16:52.121Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pi0014tr8lyrirv5ns","content":"<p>Hi allï¼Œä»Šå¤©å’Œå¤§å®¶åˆ†äº«ä¸€ç¯‡æ¯”è¾ƒæ–°çš„paperï¼Œæ˜¯å…³äºä¸€ç§æ–°çš„activation functionï¼Œå…³äºæˆ‘ä»¬çŸ¥é“çš„activation functionï¼Œæœ‰sigmoidï¼Œtanhï¼ŒReLUä»¥åŠReLUçš„ä¸€äº›å˜ç§ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©æ¥çœ‹çœ‹è¿™ç§æ–°æå‡ºçš„activation functionåˆ°åº•æœ‰ä»€ä¹ˆç‰¹è‰²ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>é¦–å…ˆå®šä¹‰ï¼Œswish activation function \\(f(x)=x \\cdot \\sigma (x)\\)ï¼Œå…¶ä¸­\\(\\sigma(x)\\)æ˜¯sigmoid functionï¼Œä¹Ÿå°±æ˜¯\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functinçš„å›¾åƒå¦‚å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png\" alt=\"\"><br>æˆ‘ä»¬å†æ¥çœ‹ä¸‹swish functionçš„1st and 2nd derivativesï¼Œ<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png\" alt=\"\"><br>ä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥é›†ä¸­çœ‹çœ‹swish functionçš„ä¼˜ç‚¹éƒ½æœ‰ä»€ä¹ˆï¼Œä½œè€…ç»™å‡ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼šå‡½æ•°å€¼æ²¡æœ‰ä¸Šé™ï¼Œå‡½æ•°å€¼æœ‰ä¸‹é™ï¼Œå‡½æ•°ä¸å•è°ƒï¼Œå‡½æ•°å…‰æ»‘è¿ç»­ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded aboveçš„å®è´¨ï¼Œæ˜¯é˜²æ­¢activation functionåœ¨bounded valueå¤„å‘ç”Ÿsaturation. bounded above å¸¦æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯è¶Šæ¥è¿‘bounded valueçš„æ—¶å€™ï¼Œfunction gradientå°±ä¼šè¶Šå°ï¼Œé€æ¸æ¥è¿‘0ï¼Œè¿™å°±å¯¼è‡´gradient descentå¼‚å¸¸ç¼“æ…¢ç”šè‡³æ— æ³•convergeã€‚ä¾‹å¦‚sigmoid å’Œtanh functionï¼Œä»–ä»¬éƒ½æ˜¯bounded below and aboveï¼Œå½“æˆ‘ä»¬é‡‡ç”¨è¿™ä¸¤ç§activation functionçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¿…é¡»è°¨æ…å°å¿ƒçš„è®©åˆå§‹å€¼å°½é‡åœ¨functionçš„æ¥è¿‘linerçš„éƒ¨åˆ†æ¥é¿å…ä¸Šé¢é—®é¢˜çš„äº§ç”Ÿï¼Œå› æ­¤ï¼Œunbounded aboveæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜ç‚¹ï¼Œä¾‹å¦‚ReLUåŠå…¶å˜ç§éƒ½é‡‡ç”¨äº†è¿™ä¸€åŸåˆ™ã€‚</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded belowå…¶å®ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¹Ÿæœ‰activation functionå·²ç»é‡‡ç”¨äº†ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ‰€æœ‰è´Ÿæ•°inputéƒ½ä¼šå¾—åˆ°ç›¸å·®æ— å‡ çš„activation valueï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ-1000å’Œ-1çš„å€¼å‡ ä¹æ²¡æœ‰åŒºåˆ«ï¼ŒæŒ‰ç…§authorçš„è¯æ¥è®²ï¼Œå°±æ˜¯æˆ‘ä»¬å°†</p>\n<blockquote>\n<p>make large negative input â€œfogottenâ€</p>\n</blockquote>\n<p>è¿™å…¶å®ä¹Ÿæ˜¯regularzationçš„ä¸€ç§æ€æƒ³ï¼Œè¿™ç§æ–¹æ³•åœ¨ReLUç­‰æ–¹æ³•ä¸­ä¹Ÿæœ‰ä½“ç°ï¼Œä½†æ˜¯ï¼Œswishå¯ä»¥é€šè¿‡è‡ªèº«çš„éå•è°ƒæ€§è´¨ï¼Œå°†æ¯”è¾ƒå°çš„negative inputä»ç„¶ä»¥negative valueè¾“å‡ºï¼Œnon-monotonicityæä¾›äº†æ›´å¥½çš„gradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>å…³äºsmoothnessçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png\" alt=\"\"></p>\n<p> æ€»è€Œè¨€ä¹‹ï¼Œä¸ªäººæ„Ÿè§‰swishåº”è¯¥ç®—æ˜¯ä¸€ä¸ªä¸é”™çš„activationï¼Œæœ¬äººç”±äºæ—¶é—´åŸå› ï¼Œè¿˜æ²¡æœ‰æ¥å¾—åŠè‡ªå·±æµ‹è¯•å®ƒï¼Œä½†æ˜¯æ®æˆ‘æ‰€çœ‹åˆ°çš„è®¨è®ºï¼Œswishçš„å®é™…æ•ˆæœè²Œä¼¼ä¸æ˜¯ååˆ†ç¨³å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒä¿ç•™æ„è§ï¼Œè¿›ä¸€æ­¥è§‚å¯Ÿå®ƒçš„è¡¨ç°ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"noopener\">Ramachandran P, Zoph B, Le Q V. Swishï¼ša Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"reading notes","path":"categories/reading-notes/"}],"_tags":[{"name":"activtion function","path":"tags/activtion-function/"}],"excerpt":"<p>Hi allï¼Œä»Šå¤©å’Œå¤§å®¶åˆ†äº«ä¸€ç¯‡æ¯”è¾ƒæ–°çš„paperï¼Œæ˜¯å…³äºä¸€ç§æ–°çš„activation functionï¼Œå…³äºæˆ‘ä»¬çŸ¥é“çš„activation functionï¼Œæœ‰sigmoidï¼Œtanhï¼ŒReLUä»¥åŠReLUçš„ä¸€äº›å˜ç§ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©æ¥çœ‹çœ‹è¿™ç§æ–°æå‡ºçš„activation functionåˆ°åº•æœ‰ä»€ä¹ˆç‰¹è‰²ã€‚<br></p>","more":"</p>\n<h2 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h2><p>é¦–å…ˆå®šä¹‰ï¼Œswish activation function \\(f(x)=x \\cdot \\sigma (x)\\)ï¼Œå…¶ä¸­\\(\\sigma(x)\\)æ˜¯sigmoid functionï¼Œä¹Ÿå°±æ˜¯\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functinçš„å›¾åƒå¦‚å›¾æ‰€ç¤ºï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-1.png\" alt=\"\"><br>æˆ‘ä»¬å†æ¥çœ‹ä¸‹swish functionçš„1st and 2nd derivativesï¼Œ<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-2.png\" alt=\"\"><br>ä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥é›†ä¸­çœ‹çœ‹swish functionçš„ä¼˜ç‚¹éƒ½æœ‰ä»€ä¹ˆï¼Œä½œè€…ç»™å‡ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼šå‡½æ•°å€¼æ²¡æœ‰ä¸Šé™ï¼Œå‡½æ•°å€¼æœ‰ä¸‹é™ï¼Œå‡½æ•°ä¸å•è°ƒï¼Œå‡½æ•°å…‰æ»‘è¿ç»­ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š</p>\n<h3 id=\"Unbounded-above\"><a href=\"#Unbounded-above\" class=\"headerlink\" title=\"Unbounded above\"></a>Unbounded above</h3><p>Unbounded aboveçš„å®è´¨ï¼Œæ˜¯é˜²æ­¢activation functionåœ¨bounded valueå¤„å‘ç”Ÿsaturation. bounded above å¸¦æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯è¶Šæ¥è¿‘bounded valueçš„æ—¶å€™ï¼Œfunction gradientå°±ä¼šè¶Šå°ï¼Œé€æ¸æ¥è¿‘0ï¼Œè¿™å°±å¯¼è‡´gradient descentå¼‚å¸¸ç¼“æ…¢ç”šè‡³æ— æ³•convergeã€‚ä¾‹å¦‚sigmoid å’Œtanh functionï¼Œä»–ä»¬éƒ½æ˜¯bounded below and aboveï¼Œå½“æˆ‘ä»¬é‡‡ç”¨è¿™ä¸¤ç§activation functionçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¿…é¡»è°¨æ…å°å¿ƒçš„è®©åˆå§‹å€¼å°½é‡åœ¨functionçš„æ¥è¿‘linerçš„éƒ¨åˆ†æ¥é¿å…ä¸Šé¢é—®é¢˜çš„äº§ç”Ÿï¼Œå› æ­¤ï¼Œunbounded aboveæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜ç‚¹ï¼Œä¾‹å¦‚ReLUåŠå…¶å˜ç§éƒ½é‡‡ç”¨äº†è¿™ä¸€åŸåˆ™ã€‚</p>\n<h3 id=\"Bounded-below-amp-non-monotonicity\"><a href=\"#Bounded-below-amp-non-monotonicity\" class=\"headerlink\" title=\"Bounded below &amp; non-monotonicity\"></a>Bounded below &amp; non-monotonicity</h3><p>Bounded belowå…¶å®ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¹Ÿæœ‰activation functionå·²ç»é‡‡ç”¨äº†ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ‰€æœ‰è´Ÿæ•°inputéƒ½ä¼šå¾—åˆ°ç›¸å·®æ— å‡ çš„activation valueï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ-1000å’Œ-1çš„å€¼å‡ ä¹æ²¡æœ‰åŒºåˆ«ï¼ŒæŒ‰ç…§authorçš„è¯æ¥è®²ï¼Œå°±æ˜¯æˆ‘ä»¬å°†</p>\n<blockquote>\n<p>make large negative input â€œfogottenâ€</p>\n</blockquote>\n<p>è¿™å…¶å®ä¹Ÿæ˜¯regularzationçš„ä¸€ç§æ€æƒ³ï¼Œè¿™ç§æ–¹æ³•åœ¨ReLUç­‰æ–¹æ³•ä¸­ä¹Ÿæœ‰ä½“ç°ï¼Œä½†æ˜¯ï¼Œswishå¯ä»¥é€šè¿‡è‡ªèº«çš„éå•è°ƒæ€§è´¨ï¼Œå°†æ¯”è¾ƒå°çš„negative inputä»ç„¶ä»¥negative valueè¾“å‡ºï¼Œnon-monotonicityæä¾›äº†æ›´å¥½çš„gradient flow.</p>\n<h3 id=\"Smothness\"><a href=\"#Smothness\" class=\"headerlink\" title=\"Smothness\"></a>Smothness</h3><p>å…³äºsmoothnessçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/11/11-3.png\" alt=\"\"></p>\n<p> æ€»è€Œè¨€ä¹‹ï¼Œä¸ªäººæ„Ÿè§‰swishåº”è¯¥ç®—æ˜¯ä¸€ä¸ªä¸é”™çš„activationï¼Œæœ¬äººç”±äºæ—¶é—´åŸå› ï¼Œè¿˜æ²¡æœ‰æ¥å¾—åŠè‡ªå·±æµ‹è¯•å®ƒï¼Œä½†æ˜¯æ®æˆ‘æ‰€çœ‹åˆ°çš„è®¨è®ºï¼Œswishçš„å®é™…æ•ˆæœè²Œä¼¼ä¸æ˜¯ååˆ†ç¨³å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒä¿ç•™æ„è§ï¼Œè¿›ä¸€æ­¥è§‚å¯Ÿå®ƒçš„è¡¨ç°ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://arxiv.org/pdf/1710.05941.pdf\" target=\"_blank\" rel=\"noopener\">Ramachandran P, Zoph B, Le Q V. Swishï¼ša Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.</a></li>\n</ul>"},{"title":"Catalyst Optimization in Spark SQL","date":"2018-09-25T11:52:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg)\nSpark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let's talk about Catalyst today.\n<!--more-->\nCatalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.\n## Trees And Rules\nWe will have a quick review of trees and rules. You can learn more about them by the references. \n### Trees\nThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let's have an example, the tree for expression x+(1+2) could be translated in Scala as:\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png)\n```scala\nAdd(Attribute(x),Add(Literal(1),Literal(2)))\n```\nActually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What's more, the data can be thrown to every node of the tree by the query plan iteratively. That's why tree datatype is used and introduced firstly in Catalyst.\n\n### Rules\nTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let's see an example.\n```Scala\ntree.transform {\n  case Add(Literal(c1),Literal(c2)) => Literal(c1+c2)\n  case Add(left, Literal(0)) => left\n  case Add(Literal(0), right) => right\n}\n```\n\n## Catalyst\nFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png)\n### Parser\nThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst,  by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API.\n### Analysis\nReturned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst.\n### Logcial Optimization\nLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.\n**Predicate pushdown** can reduce the computation of join operation by filtering unnecessary data before join.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png)\n**Constant folding** avoids calculating the same operation between constants for each record.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png)\n**Column Pruning** makes Spark SQL only load data which would be used in the table.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png)\n### Physical Planning\nSince we get the Logical Plan, Spark still doesn't know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark *map()* transformation.\n\n### Code Generation\nGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate \nava bytecode. \n> We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nIn Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from [here](https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark).\n\nAt last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:\n```Scala\n// for Logical Plan\nspark.sql(\"your SQL\").queryExecution\n// for Physical Plan\nspark.sql(\"your SQL\").explain\n```\n\n## References\n* [Deep Dive into Spark SQLâ€™s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n* [Spark SQL Optimization â€“ Understanding the Catalyst Optimizer](https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/)\n* [Catalyst Source Code](https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst)\n* [Quasiquotes Introduction](https://docs.scala-lang.org/overviews/quasiquotes/intro.html)\n","source":"_posts/spark-catalyst-optimization.md","raw":"---\ntitle: Catalyst Optimization in Spark SQL \ndate: 2018-09-25 19:52:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg)\nSpark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Let's talk about Catalyst today.\n<!--more-->\nCatalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.\n## Trees And Rules\nWe will have a quick review of trees and rules. You can learn more about them by the references. \n### Trees\nThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Let's have an example, the tree for expression x+(1+2) could be translated in Scala as:\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png)\n```scala\nAdd(Attribute(x),Add(Literal(1),Literal(2)))\n```\nActually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. What's more, the data can be thrown to every node of the tree by the query plan iteratively. That's why tree datatype is used and introduced firstly in Catalyst.\n\n### Rules\nTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Let's see an example.\n```Scala\ntree.transform {\n  case Add(Literal(c1),Literal(c2)) => Literal(c1+c2)\n  case Add(left, Literal(0)) => left\n  case Add(Literal(0), right) => right\n}\n```\n\n## Catalyst\nFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png)\n### Parser\nThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst,  by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API.\n### Analysis\nReturned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst.\n### Logcial Optimization\nLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.\n**Predicate pushdown** can reduce the computation of join operation by filtering unnecessary data before join.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png)\n**Constant folding** avoids calculating the same operation between constants for each record.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png)\n**Column Pruning** makes Spark SQL only load data which would be used in the table.\n\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png)\n### Physical Planning\nSince we get the Logical Plan, Spark still doesn't know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark *map()* transformation.\n\n### Code Generation\nGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate \nava bytecode. \n> We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.\n\nIn Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from [here](https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark).\n\nAt last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:\n```Scala\n// for Logical Plan\nspark.sql(\"your SQL\").queryExecution\n// for Physical Plan\nspark.sql(\"your SQL\").explain\n```\n\n## References\n* [Deep Dive into Spark SQLâ€™s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n* [Spark SQL Optimization â€“ Understanding the Catalyst Optimizer](https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/)\n* [Catalyst Source Code](https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst)\n* [Quasiquotes Introduction](https://docs.scala-lang.org/overviews/quasiquotes/intro.html)\n","slug":"spark-catalyst-optimization","published":1,"updated":"2018-11-20T03:24:36.205Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pj0017tr8lmw643g23","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg\" alt=\"\"><br>Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Letâ€™s talk about Catalyst today.<br><a id=\"more\"></a><br>Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.</p>\n<h2 id=\"Trees-And-Rules\"><a href=\"#Trees-And-Rules\" class=\"headerlink\" title=\"Trees And Rules\"></a>Trees And Rules</h2><p>We will have a quick review of trees and rules. You can learn more about them by the references. </p>\n<h3 id=\"Trees\"><a href=\"#Trees\" class=\"headerlink\" title=\"Trees\"></a>Trees</h3><p>The tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Letâ€™s have an example, the tree for expression x+(1+2) could be translated in Scala as:</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png\" alt=\"\"><br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Attribute</span>(x),<span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">1</span>),<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">2</span>)))</span><br></pre></td></tr></table></figure></p>\n<p>Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. Whatâ€™s more, the data can be thrown to every node of the tree by the query plan iteratively. Thatâ€™s why tree datatype is used and introduced firstly in Catalyst.</p>\n<h3 id=\"Rules\"><a href=\"#Rules\" class=\"headerlink\" title=\"Rules\"></a>Rules</h3><p>Trees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Letâ€™s see an example.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree.transform &#123;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(c1),<span class=\"hljs-type\">Literal</span>(c2)) =&gt; <span class=\"hljs-type\">Literal</span>(c1+c2)</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(left, <span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">0</span>)) =&gt; left</span><br><span class=\"line\">  <span class=\"hljs-keyword\">case</span> <span class=\"hljs-type\">Add</span>(<span class=\"hljs-type\">Literal</span>(<span class=\"hljs-number\">0</span>), right) =&gt; right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Catalyst\"><a href=\"#Catalyst\" class=\"headerlink\" title=\"Catalyst\"></a>Catalyst</h2><p>From this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png\" alt=\"\"></p>\n<h3 id=\"Parser\"><a href=\"#Parser\" class=\"headerlink\" title=\"Parser\"></a>Parser</h3><p>The first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst,  by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API.</p>\n<h3 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h3><p>Returned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst.</p>\n<h3 id=\"Logcial-Optimization\"><a href=\"#Logcial-Optimization\" class=\"headerlink\" title=\"Logcial Optimization\"></a>Logcial Optimization</h3><p>Logical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.<br><strong>Predicate pushdown</strong> can reduce the computation of join operation by filtering unnecessary data before join.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png\" alt=\"\"><br><strong>Constant folding</strong> avoids calculating the same operation between constants for each record.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png\" alt=\"\"><br><strong>Column Pruning</strong> makes Spark SQL only load data which would be used in the table.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png\" alt=\"\"></p>\n<h3 id=\"Physical-Planning\"><a href=\"#Physical-Planning\" class=\"headerlink\" title=\"Physical Planning\"></a>Physical Planning</h3><p>Since we get the Logical Plan, Spark still doesnâ€™t know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark <em>map()</em> transformation.</p>\n<h3 id=\"Code-Generation\"><a href=\"#Code-Generation\" class=\"headerlink\" title=\"Code Generation\"></a>Code Generation</h3><p>Getting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate<br>ava bytecode. </p>\n<blockquote>\n<p>We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.</p>\n</blockquote>\n<p>In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from <a href=\"https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark\">here</a>.</p>\n<p>At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// for Logical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"hljs-string\">\"your SQL\"</span>).queryExecution</span><br><span class=\"line\"><span class=\"hljs-comment\">// for Physical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"hljs-string\">\"your SQL\"</span>).explain</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\" rel=\"noopener\">Deep Dive into Spark SQLâ€™s Catalyst Optimizer</a></li>\n<li><a href=\"https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/\" target=\"_blank\" rel=\"noopener\">Spark SQL Optimization â€“ Understanding the Catalyst Optimizer</a></li>\n<li><a href=\"https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst\" target=\"_blank\" rel=\"noopener\">Catalyst Source Code</a></li>\n<li><a href=\"https://docs.scala-lang.org/overviews/quasiquotes/intro.html\" target=\"_blank\" rel=\"noopener\">Quasiquotes Introduction</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/app-applications-apps.jpg\" alt=\"\"><br>Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Letâ€™s talk about Catalyst today.<br></p>","more":"<br>Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers.</p>\n<h2 id=\"Trees-And-Rules\"><a href=\"#Trees-And-Rules\" class=\"headerlink\" title=\"Trees And Rules\"></a>Trees And Rules</h2><p>We will have a quick review of trees and rules. You can learn more about them by the references. </p>\n<h3 id=\"Trees\"><a href=\"#Trees\" class=\"headerlink\" title=\"Trees\"></a>Trees</h3><p>The tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Letâ€™s have an example, the tree for expression x+(1+2) could be translated in Scala as:</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-3.png\" alt=\"\"><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">Add</span>(<span class=\"type\">Attribute</span>(x),<span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(<span class=\"number\">1</span>),<span class=\"type\">Literal</span>(<span class=\"number\">2</span>)))</span><br></pre></td></tr></table></figure></p>\n<p>Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. Whatâ€™s more, the data can be thrown to every node of the tree by the query plan iteratively. Thatâ€™s why tree datatype is used and introduced firstly in Catalyst.</p>\n<h3 id=\"Rules\"><a href=\"#Rules\" class=\"headerlink\" title=\"Rules\"></a>Rules</h3><p>Trees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Letâ€™s see an example.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree.transform &#123;</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(c1),<span class=\"type\">Literal</span>(c2)) =&gt; <span class=\"type\">Literal</span>(c1+c2)</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(left, <span class=\"type\">Literal</span>(<span class=\"number\">0</span>)) =&gt; left</span><br><span class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Add</span>(<span class=\"type\">Literal</span>(<span class=\"number\">0</span>), right) =&gt; right</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Catalyst\"><a href=\"#Catalyst\" class=\"headerlink\" title=\"Catalyst\"></a>Catalyst</h2><p>From this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-2.png\" alt=\"\"></p>\n<h3 id=\"Parser\"><a href=\"#Parser\" class=\"headerlink\" title=\"Parser\"></a>Parser</h3><p>The first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst,  by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API.</p>\n<h3 id=\"Analysis\"><a href=\"#Analysis\" class=\"headerlink\" title=\"Analysis\"></a>Analysis</h3><p>Returned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst.</p>\n<h3 id=\"Logcial-Optimization\"><a href=\"#Logcial-Optimization\" class=\"headerlink\" title=\"Logcial Optimization\"></a>Logcial Optimization</h3><p>Logical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.<br><strong>Predicate pushdown</strong> can reduce the computation of join operation by filtering unnecessary data before join.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-4.png\" alt=\"\"><br><strong>Constant folding</strong> avoids calculating the same operation between constants for each record.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-5.png\" alt=\"\"><br><strong>Column Pruning</strong> makes Spark SQL only load data which would be used in the table.</p>\n<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/18/18-6.png\" alt=\"\"></p>\n<h3 id=\"Physical-Planning\"><a href=\"#Physical-Planning\" class=\"headerlink\" title=\"Physical Planning\"></a>Physical Planning</h3><p>Since we get the Logical Plan, Spark still doesnâ€™t know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark <em>map()</em> transformation.</p>\n<h3 id=\"Code-Generation\"><a href=\"#Code-Generation\" class=\"headerlink\" title=\"Code Generation\"></a>Code Generation</h3><p>Getting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generate<br>ava bytecode. </p>\n<blockquote>\n<p>We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code.</p>\n</blockquote>\n<p>In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from <a href=\"https://joeasir.github.io/2018/11/14/spark-second-generation-tungsten-in-spark\">here</a>.</p>\n<p>At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// for Logical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"string\">\"your SQL\"</span>).queryExecution</span><br><span class=\"line\"><span class=\"comment\">// for Physical Plan</span></span><br><span class=\"line\">spark.sql(<span class=\"string\">\"your SQL\"</span>).explain</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\" rel=\"noopener\">Deep Dive into Spark SQLâ€™s Catalyst Optimizer</a></li>\n<li><a href=\"https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/\" target=\"_blank\" rel=\"noopener\">Spark SQL Optimization â€“ Understanding the Catalyst Optimizer</a></li>\n<li><a href=\"https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst\" target=\"_blank\" rel=\"noopener\">Catalyst Source Code</a></li>\n<li><a href=\"https://docs.scala-lang.org/overviews/quasiquotes/intro.html\" target=\"_blank\" rel=\"noopener\">Quasiquotes Introduction</a></li>\n</ul>"},{"title":"From Spark RDD to DataFrame/Dataset","date":"2018-09-22T08:49:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg)\nThis article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What's the differences between them and how to decide which API to be imported, let's have a quick look.\n<!--more-->\n## RDD\nRDD (aka Resilient Distributed Dataset) is the most fundamental API, it's so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let's have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let's take a look and learn about the details one by one.\n### Distributed data abstraction\nThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That's really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.\n### Resilient and immutable\nRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that's why RDD is resilient.\nAs for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. \n### Compile-time type-safe\nRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.\n### Unstructured/Structured data\nThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it's good for those data without structures. Also, RDD can manipulate structured data, though it doesn't understand the different kinds of types and all depends on how you parse the data.\n### Lazy evaluation\nLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. \n## DataFrame/Dataset\nDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for *Dataset[Row]*, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.\nThere are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.\n### Static-typing and runtime type-safety\nDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing *form* rather than *from*, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png)\n### Nice performance\nDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let's have a look at the example.\n```scala\nrdd.filter{case(project, page, numRequests) => project=='en'}.\n    map{case(_,page,numRequests) => (page, numRequests)}.\n    reduceByKey(_+_).\n    filter{case(page,_) => !isSpecialPage(page)}.\n    take(100).foreach {case (project, requests) => println(s\"projec:$requests\"\")}\n```\nThe code above can be run perfectly without any bug. But think about it, the RDD execute a *filter* followed by *reduceByKey* transformation, which means we filter some data after shuffling the entire data. That's really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.\n## When to Use\nSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.\n### When to use RDD\n* When you want more about the low-level control of dataset\n* When you are dealing with some unstructred data\n* When you prefer manipulate data with lambda function\n* When you don't care about schema or structure of data\n\n### When to use DataFrame/Dataset\n* When you are dealing with structured data\n* When you want more code optimization and better performance\n\nAll in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. \n\n## References\n* [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n* [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n","source":"_posts/spark-from-rdd-to-dataframe-dataset.md","raw":"---\ntitle: From Spark RDD to DataFrame/Dataset\ndate: 2018-09-22 16:49:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg)\nThis article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. What's the differences between them and how to decide which API to be imported, let's have a quick look.\n<!--more-->\n## RDD\nRDD (aka Resilient Distributed Dataset) is the most fundamental API, it's so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Let's have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Let's take a look and learn about the details one by one.\n### Distributed data abstraction\nThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. That's really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.\n### Resilient and immutable\nRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and that's why RDD is resilient.\nAs for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. \n### Compile-time type-safe\nRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.\n### Unstructured/Structured data\nThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, it's good for those data without structures. Also, RDD can manipulate structured data, though it doesn't understand the different kinds of types and all depends on how you parse the data.\n### Lazy evaluation\nLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. \n## DataFrame/Dataset\nDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for *Dataset[Row]*, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.\nThere are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.\n### Static-typing and runtime type-safety\nDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing *form* rather than *from*, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png)\n### Nice performance\nDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Let's have a look at the example.\n```scala\nrdd.filter{case(project, page, numRequests) => project=='en'}.\n    map{case(_,page,numRequests) => (page, numRequests)}.\n    reduceByKey(_+_).\n    filter{case(page,_) => !isSpecialPage(page)}.\n    take(100).foreach {case (project, requests) => println(s\"projec:$requests\"\")}\n```\nThe code above can be run perfectly without any bug. But think about it, the RDD execute a *filter* followed by *reduceByKey* transformation, which means we filter some data after shuffling the entire data. That's really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.\n## When to Use\nSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.\n### When to use RDD\n* When you want more about the low-level control of dataset\n* When you are dealing with some unstructred data\n* When you prefer manipulate data with lambda function\n* When you don't care about schema or structure of data\n\n### When to use DataFrame/Dataset\n* When you are dealing with structured data\n* When you want more code optimization and better performance\n\nAll in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. \n\n## References\n* [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n* [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n","slug":"spark-from-rdd-to-dataframe-dataset","published":1,"updated":"2018-11-19T13:49:38.613Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pk001ctr8l2nhwqjph","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg\" alt=\"\"><br>This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. Whatâ€™s the differences between them and how to decide which API to be imported, letâ€™s have a quick look.<br><a id=\"more\"></a></p>\n<h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h2><p>RDD (aka Resilient Distributed Dataset) is the most fundamental API, itâ€™s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Letâ€™s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Letâ€™s take a look and learn about the details one by one.</p>\n<h3 id=\"Distributed-data-abstraction\"><a href=\"#Distributed-data-abstraction\" class=\"headerlink\" title=\"Distributed data abstraction\"></a>Distributed data abstraction</h3><p>The first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. Thatâ€™s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.</p>\n<h3 id=\"Resilient-and-immutable\"><a href=\"#Resilient-and-immutable\" class=\"headerlink\" title=\"Resilient and immutable\"></a>Resilient and immutable</h3><p>RDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and thatâ€™s why RDD is resilient.<br>As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. </p>\n<h3 id=\"Compile-time-type-safe\"><a href=\"#Compile-time-type-safe\" class=\"headerlink\" title=\"Compile-time type-safe\"></a>Compile-time type-safe</h3><p>RDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.</p>\n<h3 id=\"Unstructured-Structured-data\"><a href=\"#Unstructured-Structured-data\" class=\"headerlink\" title=\"Unstructured/Structured data\"></a>Unstructured/Structured data</h3><p>The fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, itâ€™s good for those data without structures. Also, RDD can manipulate structured data, though it doesnâ€™t understand the different kinds of types and all depends on how you parse the data.</p>\n<h3 id=\"Lazy-evaluation\"><a href=\"#Lazy-evaluation\" class=\"headerlink\" title=\"Lazy evaluation\"></a>Lazy evaluation</h3><p>Lazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. </p>\n<h2 id=\"DataFrame-Dataset\"><a href=\"#DataFrame-Dataset\" class=\"headerlink\" title=\"DataFrame/Dataset\"></a>DataFrame/Dataset</h2><p>DataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for <em>Dataset[Row]</em>, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.<br>There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.</p>\n<h3 id=\"Static-typing-and-runtime-type-safety\"><a href=\"#Static-typing-and-runtime-type-safety\" class=\"headerlink\" title=\"Static-typing and runtime type-safety\"></a>Static-typing and runtime type-safety</h3><p>DataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing <em>form</em> rather than <em>from</em>, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png\" alt=\"\"></p>\n<h3 id=\"Nice-performance\"><a href=\"#Nice-performance\" class=\"headerlink\" title=\"Nice performance\"></a>Nice performance</h3><p>DataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Letâ€™s have a look at the example.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rdd.filter&#123;<span class=\"hljs-keyword\">case</span>(project, page, numRequests) =&gt; project==<span class=\"hljs-symbol\">'e</span>n'&#125;.</span><br><span class=\"line\">    map&#123;<span class=\"hljs-keyword\">case</span>(_,page,numRequests) =&gt; (page, numRequests)&#125;.</span><br><span class=\"line\">    reduceByKey(_+_).</span><br><span class=\"line\">    filter&#123;<span class=\"hljs-keyword\">case</span>(page,_) =&gt; !isSpecialPage(page)&#125;.</span><br><span class=\"line\">    take(<span class=\"hljs-number\">100</span>).foreach &#123;<span class=\"hljs-keyword\">case</span> (project, requests) =&gt; println(<span class=\"hljs-string\">s\"projec:<span class=\"hljs-subst\">$requests</span>\"</span><span class=\"hljs-string\">\")&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>The code above can be run perfectly without any bug. But think about it, the RDD execute a <em>filter</em> followed by <em>reduceByKey</em> transformation, which means we filter some data after shuffling the entire data. Thatâ€™s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.</p>\n<h2 id=\"When-to-Use\"><a href=\"#When-to-Use\" class=\"headerlink\" title=\"When to Use\"></a>When to Use</h2><p>Since we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.</p>\n<h3 id=\"When-to-use-RDD\"><a href=\"#When-to-use-RDD\" class=\"headerlink\" title=\"When to use RDD\"></a>When to use RDD</h3><ul>\n<li>When you want more about the low-level control of dataset</li>\n<li>When you are dealing with some unstructred data</li>\n<li>When you prefer manipulate data with lambda function</li>\n<li>When you donâ€™t care about schema or structure of data</li>\n</ul>\n<h3 id=\"When-to-use-DataFrame-Dataset\"><a href=\"#When-to-use-DataFrame-Dataset\" class=\"headerlink\" title=\"When to use DataFrame/Dataset\"></a>When to use DataFrame/Dataset</h3><ul>\n<li>When you are dealing with structured data</li>\n<li>When you want more code optimization and better performance</li>\n</ul>\n<p>All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\" rel=\"noopener\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li>\n<li><a href=\"https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\" target=\"_blank\" rel=\"noopener\">Apache Spark RDD vs DataFrame vs DataSet</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blur-close-up-code.jpg\" alt=\"\"><br>This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. Whatâ€™s the differences between them and how to decide which API to be imported, letâ€™s have a quick look.<br></p>","more":"</p>\n<h2 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h2><p>RDD (aka Resilient Distributed Dataset) is the most fundamental API, itâ€™s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Letâ€™s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Letâ€™s take a look and learn about the details one by one.</p>\n<h3 id=\"Distributed-data-abstraction\"><a href=\"#Distributed-data-abstraction\" class=\"headerlink\" title=\"Distributed data abstraction\"></a>Distributed data abstraction</h3><p>The first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. Thatâ€™s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors.</p>\n<h3 id=\"Resilient-and-immutable\"><a href=\"#Resilient-and-immutable\" class=\"headerlink\" title=\"Resilient and immutable\"></a>Resilient and immutable</h3><p>RDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and thatâ€™s why RDD is resilient.<br>As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. </p>\n<h3 id=\"Compile-time-type-safe\"><a href=\"#Compile-time-type-safe\" class=\"headerlink\" title=\"Compile-time type-safe\"></a>Compile-time type-safe</h3><p>RDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time.</p>\n<h3 id=\"Unstructured-Structured-data\"><a href=\"#Unstructured-Structured-data\" class=\"headerlink\" title=\"Unstructured/Structured data\"></a>Unstructured/Structured data</h3><p>The fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, itâ€™s good for those data without structures. Also, RDD can manipulate structured data, though it doesnâ€™t understand the different kinds of types and all depends on how you parse the data.</p>\n<h3 id=\"Lazy-evaluation\"><a href=\"#Lazy-evaluation\" class=\"headerlink\" title=\"Lazy evaluation\"></a>Lazy evaluation</h3><p>Lazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. </p>\n<h2 id=\"DataFrame-Dataset\"><a href=\"#DataFrame-Dataset\" class=\"headerlink\" title=\"DataFrame/Dataset\"></a>DataFrame/Dataset</h2><p>DataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for <em>Dataset[Row]</em>, and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.<br>There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome.</p>\n<h3 id=\"Static-typing-and-runtime-type-safety\"><a href=\"#Static-typing-and-runtime-type-safety\" class=\"headerlink\" title=\"Static-typing and runtime type-safety\"></a>Static-typing and runtime type-safety</h3><p>DataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing <em>form</em> rather than <em>from</em>, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/17/17-1.png\" alt=\"\"></p>\n<h3 id=\"Nice-performance\"><a href=\"#Nice-performance\" class=\"headerlink\" title=\"Nice performance\"></a>Nice performance</h3><p>DataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Letâ€™s have a look at the example.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rdd.filter&#123;<span class=\"keyword\">case</span>(project, page, numRequests) =&gt; project==<span class=\"symbol\">'e</span>n'&#125;.</span><br><span class=\"line\">    map&#123;<span class=\"keyword\">case</span>(_,page,numRequests) =&gt; (page, numRequests)&#125;.</span><br><span class=\"line\">    reduceByKey(_+_).</span><br><span class=\"line\">    filter&#123;<span class=\"keyword\">case</span>(page,_) =&gt; !isSpecialPage(page)&#125;.</span><br><span class=\"line\">    take(<span class=\"number\">100</span>).foreach &#123;<span class=\"keyword\">case</span> (project, requests) =&gt; println(<span class=\"string\">s\"projec:<span class=\"subst\">$requests</span>\"</span><span class=\"string\">\")&#125;</span></span><br></pre></td></tr></table></figure></p>\n<p>The code above can be run perfectly without any bug. But think about it, the RDD execute a <em>filter</em> followed by <em>reduceByKey</em> transformation, which means we filter some data after shuffling the entire data. Thatâ€™s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD.</p>\n<h2 id=\"When-to-Use\"><a href=\"#When-to-Use\" class=\"headerlink\" title=\"When to Use\"></a>When to Use</h2><p>Since we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset.</p>\n<h3 id=\"When-to-use-RDD\"><a href=\"#When-to-use-RDD\" class=\"headerlink\" title=\"When to use RDD\"></a>When to use RDD</h3><ul>\n<li>When you want more about the low-level control of dataset</li>\n<li>When you are dealing with some unstructred data</li>\n<li>When you prefer manipulate data with lambda function</li>\n<li>When you donâ€™t care about schema or structure of data</li>\n</ul>\n<h3 id=\"When-to-use-DataFrame-Dataset\"><a href=\"#When-to-use-DataFrame-Dataset\" class=\"headerlink\" title=\"When to use DataFrame/Dataset\"></a>When to use DataFrame/Dataset</h3><ul>\n<li>When you are dealing with structured data</li>\n<li>When you want more code optimization and better performance</li>\n</ul>\n<p>All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. </p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\" target=\"_blank\" rel=\"noopener\">A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets</a></li>\n<li><a href=\"https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\" target=\"_blank\" rel=\"noopener\">Apache Spark RDD vs DataFrame vs DataSet</a></li>\n</ul>"},{"title":"Second Generation Tungsten Engine in Spark 2.x","date":"2018-11-14T07:41:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg)\nThis article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let's take a look!\n<!--more-->\n## Project Tungsten\nIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:\n> * Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection\n* Cache-aware computation: algorithms and data structures to exploit memory hierarchy\n* Code generation: using code generation to exploit modern compilers and CPUs\n\nAs we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.\n## WholeStageCodeGen\nAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.\n### Volcano Iterator Model\nWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png)\nAlthough Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. \n### Bottom-up Model\nIn [this blog](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html), a hand-written code is proposed to implement the query in the figure above, it's just a so simple for-loop that even a college freshman can complete, which is:\n```scala\nvar count = 0\nfor (ss_item_sk in store_sales) {\n  if (ss_item_sk == 1000) {\n      count += 1\n  }\n}        \n```\nEven though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png)\nBut why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:\n* Too many virtual functions calls:\nIn Volcano Iterator Model, when one operator call for the next operator, a virtual function **next()** would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.\n* Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:\nAs one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.\n* Volcano Iterator Model don't take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:\nAs Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.\n\n#### Loop-pipelining \nIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png)\n#### Loop-unrolling\nLoop-unrolling is another technique to exploit parallelism between loop iterations. Let's learn about it by the code:\n```java\n// without loop-unrolling\nint sum=0;\nfor (int i=0; i<10; i++) {\n    sum+=a[i];\n}\n// with loop-unrolling\nint sum = 0;\nfor (int i=0; i<10; i+=2) {\n    sum += a[i];\n    sum += a[i+1];\n}\n```\nAs shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. \n\n### Whole Stage Code Generation\nFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.\n## Vectorization\nAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What's more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so **Vector Processing** and **Column Format** are used in 2nd generation Tungsten engine.\n### Vector Processing\n> In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.\n\nThe following figure presents the differences between Scalar and Vector Processing. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png)\n\nAnd we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)\n> Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.\n\nLet me show one figure to show what's SIMD breifly.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png)\nAs presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.\n\n### Column Format\nColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png)\n\n## Summary\nWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.\n## References\n* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n* [Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop\nDeep dive into the new Tungsten execution engine](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)\n* [Spark 2.x - 2nd generation Tungsten Engine](https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html)\n* [Loop Pipelining and Loop Unrolling](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#)\n* [Vectorization: Ranger to Stampede Transition](http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf)\n","source":"_posts/spark-second-generation-tungsten-in-spark.md","raw":"---\ntitle: Second Generation Tungsten Engine in Spark 2.x\ndate: 2018-11-14 15:41:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg)\nThis article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Let's take a look!\n<!--more-->\n## Project Tungsten\nIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:\n> * Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection\n* Cache-aware computation: algorithms and data structures to exploit memory hierarchy\n* Code generation: using code generation to exploit modern compilers and CPUs\n\nAs we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.\n## WholeStageCodeGen\nAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.\n### Volcano Iterator Model\nWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png)\nAlthough Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. \n### Bottom-up Model\nIn [this blog](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html), a hand-written code is proposed to implement the query in the figure above, it's just a so simple for-loop that even a college freshman can complete, which is:\n```scala\nvar count = 0\nfor (ss_item_sk in store_sales) {\n  if (ss_item_sk == 1000) {\n      count += 1\n  }\n}        \n```\nEven though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png)\nBut why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:\n* Too many virtual functions calls:\nIn Volcano Iterator Model, when one operator call for the next operator, a virtual function **next()** would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.\n* Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:\nAs one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.\n* Volcano Iterator Model don't take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:\nAs Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.\n\n#### Loop-pipelining \nIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png)\n#### Loop-unrolling\nLoop-unrolling is another technique to exploit parallelism between loop iterations. Let's learn about it by the code:\n```java\n// without loop-unrolling\nint sum=0;\nfor (int i=0; i<10; i++) {\n    sum+=a[i];\n}\n// with loop-unrolling\nint sum = 0;\nfor (int i=0; i<10; i+=2) {\n    sum += a[i];\n    sum += a[i+1];\n}\n```\nAs shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. \n\n### Whole Stage Code Generation\nFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.\n## Vectorization\nAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. What's more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so **Vector Processing** and **Column Format** are used in 2nd generation Tungsten engine.\n### Vector Processing\n> In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.\n\nThe following figure presents the differences between Scalar and Vector Processing. \n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png)\n\nAnd we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)\n> Single instruction, multiple data (SIMD) is a class of parallel computers in Flynn's taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.\n\nLet me show one figure to show what's SIMD breifly.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png)\nAs presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.\n\n### Column Format\nColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png)\n\n## Summary\nWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.\n## References\n* [Project Tungsten: Bringing Apache Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n* [Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop\nDeep dive into the new Tungsten execution engine](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)\n* [Spark 2.x - 2nd generation Tungsten Engine](https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html)\n* [Loop Pipelining and Loop Unrolling](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#)\n* [Vectorization: Ranger to Stampede Transition](http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf)\n","slug":"spark-second-generation-tungsten-in-spark","published":1,"updated":"2018-11-20T04:45:36.619Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pl001dtr8l7hp9uwri","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg\" alt=\"\"><br>This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Letâ€™s take a look!<br><a id=\"more\"></a></p>\n<h2 id=\"Project-Tungsten\"><a href=\"#Project-Tungsten\" class=\"headerlink\" title=\"Project Tungsten\"></a>Project Tungsten</h2><p>In the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:</p>\n<blockquote>\n<ul>\n<li>Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n<li>Cache-aware computation: algorithms and data structures to exploit memory hierarchy</li>\n<li>Code generation: using code generation to exploit modern compilers and CPUs</li>\n</ul>\n</blockquote>\n<p>As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.</p>\n<h2 id=\"WholeStageCodeGen\"><a href=\"#WholeStageCodeGen\" class=\"headerlink\" title=\"WholeStageCodeGen\"></a>WholeStageCodeGen</h2><p>As the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.</p>\n<h3 id=\"Volcano-Iterator-Model\"><a href=\"#Volcano-Iterator-Model\" class=\"headerlink\" title=\"Volcano Iterator Model\"></a>Volcano Iterator Model</h3><p>What a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png\" alt=\"\"><br>Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. </p>\n<h3 id=\"Bottom-up-Model\"><a href=\"#Bottom-up-Model\" class=\"headerlink\" title=\"Bottom-up Model\"></a>Bottom-up Model</h3><p>In <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">this blog</a>, a hand-written code is proposed to implement the query in the figure above, itâ€™s just a so simple for-loop that even a college freshman can complete, which is:<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">var</span> count = <span class=\"hljs-number\">0</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (ss_item_sk in store_sales) &#123;</span><br><span class=\"line\">  <span class=\"hljs-keyword\">if</span> (ss_item_sk == <span class=\"hljs-number\">1000</span>) &#123;</span><br><span class=\"line\">      count += <span class=\"hljs-number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Even though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png\" alt=\"\"><br>But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:</p>\n<ul>\n<li>Too many virtual functions calls:<br>In Volcano Iterator Model, when one operator call for the next operator, a virtual function <strong>next()</strong> would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.</li>\n<li>Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:<br>As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.</li>\n<li>Volcano Iterator Model donâ€™t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:<br>As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.</li>\n</ul>\n<h4 id=\"Loop-pipelining\"><a href=\"#Loop-pipelining\" class=\"headerlink\" title=\"Loop-pipelining\"></a>Loop-pipelining</h4><p>In a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png\" alt=\"\"></p>\n<h4 id=\"Loop-unrolling\"><a href=\"#Loop-unrolling\" class=\"headerlink\" title=\"Loop-unrolling\"></a>Loop-unrolling</h4><p>Loop-unrolling is another technique to exploit parallelism between loop iterations. Letâ€™s learn about it by the code:<br><figure class=\"highlight java hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// without loop-unrolling</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">int</span> sum=<span class=\"hljs-number\">0</span>;</span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i=<span class=\"hljs-number\">0</span>; i&lt;<span class=\"hljs-number\">10</span>; i++) &#123;</span><br><span class=\"line\">    sum+=a[i];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"hljs-comment\">// with loop-unrolling</span></span><br><span class=\"line\"><span class=\"hljs-keyword\">int</span> sum = <span class=\"hljs-number\">0</span>;</span><br><span class=\"line\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i=<span class=\"hljs-number\">0</span>; i&lt;<span class=\"hljs-number\">10</span>; i+=<span class=\"hljs-number\">2</span>) &#123;</span><br><span class=\"line\">    sum += a[i];</span><br><span class=\"line\">    sum += a[i+<span class=\"hljs-number\">1</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. </p>\n<h3 id=\"Whole-Stage-Code-Generation\"><a href=\"#Whole-Stage-Code-Generation\" class=\"headerlink\" title=\"Whole Stage Code Generation\"></a>Whole Stage Code Generation</h3><p>Fusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Although the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. Whatâ€™s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so <strong>Vector Processing</strong> and <strong>Column Format</strong> are used in 2nd generation Tungsten engine.</p>\n<h3 id=\"Vector-Processing\"><a href=\"#Vector-Processing\" class=\"headerlink\" title=\"Vector Processing\"></a>Vector Processing</h3><blockquote>\n<p>In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.</p>\n</blockquote>\n<p>The following figure presents the differences between Scalar and Vector Processing.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png\" alt=\"\"></p>\n<p>And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)</p>\n<blockquote>\n<p>Single instruction, multiple data (SIMD) is a class of parallel computers in Flynnâ€™s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.</p>\n</blockquote>\n<p>Let me show one figure to show whatâ€™s SIMD breifly.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png\" alt=\"\"><br>As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.</p>\n<h3 id=\"Column-Format\"><a href=\"#Column-Format\" class=\"headerlink\" title=\"Column Format\"></a>Column Format</h3><p>Column Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png\" alt=\"\"></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>WholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop<br>Deep dive into the new Tungsten execution engine</a></li>\n<li><a href=\"https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html\" target=\"_blank\" rel=\"noopener\">Spark 2.x - 2nd generation Tungsten Engine</a></li>\n<li><a href=\"https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#\" target=\"_blank\" rel=\"noopener\">Loop Pipelining and Loop Unrolling</a></li>\n<li><a href=\"http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf\" target=\"_blank\" rel=\"noopener\">Vectorization: Ranger to Stampede Transition</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/adventure-climb-691668.jpg\" alt=\"\"><br>This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Letâ€™s take a look!<br></p>","more":"</p>\n<h2 id=\"Project-Tungsten\"><a href=\"#Project-Tungsten\" class=\"headerlink\" title=\"Project Tungsten\"></a>Project Tungsten</h2><p>In the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including:</p>\n<blockquote>\n<ul>\n<li>Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection</li>\n<li>Cache-aware computation: algorithms and data structures to exploit memory hierarchy</li>\n<li>Code generation: using code generation to exploit modern compilers and CPUs</li>\n</ul>\n</blockquote>\n<p>As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster.</p>\n<h2 id=\"WholeStageCodeGen\"><a href=\"#WholeStageCodeGen\" class=\"headerlink\" title=\"WholeStageCodeGen\"></a>WholeStageCodeGen</h2><p>As the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine.</p>\n<h3 id=\"Volcano-Iterator-Model\"><a href=\"#Volcano-Iterator-Model\" class=\"headerlink\" title=\"Volcano Iterator Model\"></a>Volcano Iterator Model</h3><p>What a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-1.png\" alt=\"\"><br>Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. </p>\n<h3 id=\"Bottom-up-Model\"><a href=\"#Bottom-up-Model\" class=\"headerlink\" title=\"Bottom-up Model\"></a>Bottom-up Model</h3><p>In <a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">this blog</a>, a hand-written code is proposed to implement the query in the figure above, itâ€™s just a so simple for-loop that even a college freshman can complete, which is:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> count = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (ss_item_sk in store_sales) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (ss_item_sk == <span class=\"number\">1000</span>) &#123;</span><br><span class=\"line\">      count += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Even though the code is pretty simple, the comparison of performance  between Volcano Iterator Model and Bottom-up Model will do shake you.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-2.png\" alt=\"\"><br>But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model:</p>\n<ul>\n<li>Too many virtual functions calls:<br>In Volcano Iterator Model, when one operator call for the next operator, a virtual function <strong>next()</strong> would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function.</li>\n<li>Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:<br>As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers.</li>\n<li>Volcano Iterator Model donâ€™t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:<br>As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling.</li>\n</ul>\n<h4 id=\"Loop-pipelining\"><a href=\"#Loop-pipelining\" class=\"headerlink\" title=\"Loop-pipelining\"></a>Loop-pipelining</h4><p>In a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-3.png\" alt=\"\"></p>\n<h4 id=\"Loop-unrolling\"><a href=\"#Loop-unrolling\" class=\"headerlink\" title=\"Loop-unrolling\"></a>Loop-unrolling</h4><p>Loop-unrolling is another technique to exploit parallelism between loop iterations. Letâ€™s learn about it by the code:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// without loop-unrolling</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> sum=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    sum+=a[i];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// with loop-unrolling</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;<span class=\"number\">10</span>; i+=<span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">    sum += a[i];</span><br><span class=\"line\">    sum += a[i+<span class=\"number\">1</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. </p>\n<h3 id=\"Whole-Stage-Code-Generation\"><a href=\"#Whole-Stage-Code-Generation\" class=\"headerlink\" title=\"Whole Stage Code Generation\"></a>Whole Stage Code Generation</h3><p>Fusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application.</p>\n<h2 id=\"Vectorization\"><a href=\"#Vectorization\" class=\"headerlink\" title=\"Vectorization\"></a>Vectorization</h2><p>Although the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. Whatâ€™s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so <strong>Vector Processing</strong> and <strong>Column Format</strong> are used in 2nd generation Tungsten engine.</p>\n<h3 id=\"Vector-Processing\"><a href=\"#Vector-Processing\" class=\"headerlink\" title=\"Vector Processing\"></a>Vector Processing</h3><blockquote>\n<p>In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items.</p>\n</blockquote>\n<p>The following figure presents the differences between Scalar and Vector Processing.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-4.png\" alt=\"\"></p>\n<p>And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data)</p>\n<blockquote>\n<p>Single instruction, multiple data (SIMD) is a class of parallel computers in Flynnâ€™s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously.</p>\n</blockquote>\n<p>Let me show one figure to show whatâ€™s SIMD breifly.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-5.png\" alt=\"\"><br>As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark.</p>\n<h3 id=\"Column-Format\"><a href=\"#Column-Format\" class=\"headerlink\" title=\"Column Format\"></a>Column Format</h3><p>Column Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format.<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/19/19-6.png\" alt=\"\"></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>WholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" target=\"_blank\" rel=\"noopener\">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</a></li>\n<li><a href=\"https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html\" target=\"_blank\" rel=\"noopener\">Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop<br>Deep dive into the new Tungsten execution engine</a></li>\n<li><a href=\"https://spoddutur.github.io/spark-notes/second_generation_tungsten_engine.html\" target=\"_blank\" rel=\"noopener\">Spark 2.x - 2nd generation Tungsten Engine</a></li>\n<li><a href=\"https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#\" target=\"_blank\" rel=\"noopener\">Loop Pipelining and Loop Unrolling</a></li>\n<li><a href=\"http://www.cac.cornell.edu/education/training/ParallelFall2012/Vectorization.pdf\" target=\"_blank\" rel=\"noopener\">Vectorization: Ranger to Stampede Transition</a></li>\n</ul>"},{"title":"Sparkå·¥ä½œæµç¨‹ç®€æ","date":"2018-01-07T10:44:37.000Z","_content":"Helloï¼Œæœ‰ä¸€ä¸ªæœˆæ²¡å†™blogäº†æ„Ÿè§‰å¾ˆè‡ªè´£ï¼Œå¿…é¡»æ•´èµ·æ¥ï¼æœ€è¿‘ç”±äºå·¥ä½œä¸Šé‡åˆ°çš„ä¸€äº›è°ƒä¼˜å›°éš¾ï¼Œè®©æˆ‘å¯¹Sparkæœ‰äº›æ•¬ç•ï¼Œæ‰€ä»¥é›†ä¸­çš„ç ”ç©¶äº†ä¸‹é¬¼é­…ç„å­¦Sparkï¼Œå’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹ã€‚é¦–å…ˆå…ˆæ¥çœ‹çœ‹sparkçš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚\n<!--more-->\n## Work Flow\nå’Œhadoopä¸€æ ·ï¼Œsparkä¹Ÿæ˜¯master-slaveæœºåˆ¶ï¼ŒSparké€šè¿‡driverè¿›ç¨‹ï¼Œå°†taskåˆ†å‘åˆ°å¤šä¸ªexecutorsä¸Šå¹¶å‘è¿›è¡Œè®¡ç®—ã€‚æ•´ä¸ªdriverå’Œæ‰€æœ‰çš„executorsç»„æˆäº†ä¸€ä¸ªspark applicationï¼Œæ¯ä¸€ä¸ªapplicationæ˜¯è¿è¡Œåœ¨cluster managerä¸Šçš„ï¼ŒSparkæœ¬èº«é›†æˆäº†standalone clusterï¼Œå½“ç„¶ï¼ŒSparkè¿˜å¯ä»¥è¿è¡Œåœ¨èµ«èµ«æœ‰åçš„YARNå’ŒMesosä¸Šã€‚æˆ‘å¹³æ—¶ä½¿ç”¨çš„å…¬å¸é›†ç¾¤éƒ½æ˜¯åŸºäºYARN cluster managerçš„ï¼Œå› æ­¤æœ¬æ–‡é‡ç‚¹æ¢è®¨åŸºäºYARNçš„sparkã€‚\n\nä¸‹å›¾å°±æ˜¯sparkåœ¨cluster managerä¸‹çš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png)\n\n## The Driver\nDriveræ˜¯æ•´ä¸ªapplicationæœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä»–è¿è¡Œçš„æ˜¯applicationçš„mainæ–¹æ³•ï¼Œå®ƒä¼´éšè¿™æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼Œdriverè¿›ç¨‹çš„ç»“æŸå°±ä¼šå¸¦æ¥æ•´ä¸ªapplicationçš„ç»“æŸã€‚\n\nå¯¹äºæ‰€æœ‰çš„Sparkä»»åŠ¡ï¼Œä»–ä»¬å…¶å®éƒ½æ˜¯å®ç°RDDçš„transformationå’Œactionæ“ä½œï¼Œè€Œè¿™äº›æ“ä½œï¼Œæœ€åæ˜¯éœ€è¦driverå°†ä»–ä»¬è½¬åŒ–å’Œåˆ†å‘æˆtasksï¼Œç„¶åæ‰å¯ä»¥å»æ‰§è¡Œã€‚æ‰€æœ‰çš„user programéƒ½ä¼šè¢«driveré€šè¿‡DAG(directed acyclic graph)è½¬åŒ–æˆå®é™…çš„tasksæ‰§è¡Œè®¡åˆ’ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œdriverè¿˜ä¼šåœ¨tasksæ‰§è¡Œçš„æœŸé—´ï¼Œç›‘æ§executorä¸Šçš„tasksï¼Œå¹¶ä¸”ä¿è¯ä»–ä»¬æ‹¥æœ‰å¥åº·è€Œåˆç†çš„èµ„æºã€‚\n\n## Executors\nExecutorsæ˜¯Spark applicationçš„æ‰§è¡Œè€…ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä¼´éšç€applicationçš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨çš„ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ**Spark jobåœ¨executorsæ‰§è¡Œå¤±è´¥çš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥ç»§ç»­è¿›è¡Œ**ã€‚Executorsä¼šå¯¹å…·ä½“çš„tasksçš„æ‰§è¡Œç»“æœè¿”å›ç»™driverï¼ŒåŒæ—¶ç»™ç¼“å­˜çš„RDDæä¾›å­˜å‚¨ç©ºé—´ã€‚\n\n## Some terms\n* Job: Jobæ˜¯executorå±‚é¢æœ€å¤§çš„æ‰§è¡Œå•å…ƒï¼Œjobé€šè¿‡RDDçš„actionæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯ä¸€ä¸ªactionæ“ä½œå°±ä¼šè¿›è¡Œä¸€æ¬¡jobçš„åˆ’åˆ†ï¼›\n* Stage: Stageæ˜¯åŒ…å«åœ¨jobä¸­çš„æ‰§è¡Œå•å…ƒï¼Œstageé€šè¿‡RDDçš„shuffleæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯è¿›è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡stageçš„åˆ’åˆ†ï¼›\n* Task: Taskæ˜¯executoræ‰§è¡Œä¸­æœ€ç»†çš„æ‰§è¡Œå•å…ƒï¼Œtaskçš„æ•°ç›®å–å’Œparent RDDçš„partitionæ•°ç›®æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚\n\n## Spark on Yarn-cluster\nä¸‹é¢ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æ•´ä¸ªSpark applicationä¸­ï¼Œdriverå’Œexecutorsçš„éƒ½ä¼šèµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚æˆ‘ä»¥åŸºäºyarn-clusterçš„YARNçš„Sparkä½œä¸ºä¾‹å­æ¥ç®€è¿°æ•´ä¸ªæµç¨‹ï¼Œå…ˆçœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png)\né¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä¸€äº›YARNçš„æ¦‚å¿µï¼ŒYARNæ˜¯ä¸master-slaverçš„ä¸€ä¸ªCluster Managerï¼Œ åœ¨YARNä¸­ï¼ŒRM(ResourseManager)è´Ÿè´£æ•´ä¸ªè°ƒåº¦åˆ†å‘ï¼Œå³æˆ‘ä»¬å¸¸è¯´çš„masterï¼›è€ŒNM(NodeManager)ä»»åŠ¡åˆ†å‘çš„æ¥å—è€…ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„workerã€‚è¿™äº›æ¦‚å¿µåç»­æˆ‘ä¸“é—¨ä»‹ç»YARNçš„æ—¶å€™ä¼šè¯¦ç»†çš„è¯´æ˜ï¼Œä»–ä»¬çš„ä½œç”¨éƒ½æ˜¯å®ç°sparkå’ŒYARNä¹‹é—´è¯¸å¦‚èµ„æºç”³è¯·ç­‰æ“ä½œã€‚\n\né¦–å…ˆClientå‘ResourceManagerå‘å‡ºæäº¤applicationçš„è¯·æ±‚ï¼ŒResourseManagerä¼šåœ¨æŸä¸€ä¸ªNodeManagerä¸Šå¯åŠ¨AppManagerè¿›ç¨‹ï¼ŒAppManagerä¼šéšåå¯åŠ¨driverï¼Œå¹¶å°†driverç”³è¯·containersèµ„æºçš„ä¿¡æ¯å‘ç»™ResourceManagerï¼Œç”³è¯·å®Œæˆåï¼ŒResourceManagerå°†èµ„æºåˆ†é…æ¶ˆæ¯ä¼ é€’ç»™AppManagerå¹¶ç”±å®ƒå¯åŠ¨containerï¼Œæ¯ä¸€ä¸ªcontainerä¸­åªè¿è¡Œä¸€ä¸ªspark executorï¼Œç”±æ­¤å®Œæˆäº†èµ„æºçš„ç”³è¯·å’Œåˆ†é…ã€‚\n\nç„¶åæ•´ä¸ªapplicationå¼€å§‹æ‰§è¡Œï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®RDDçš„transformationæˆ–è€…actionï¼ŒdriveræŠŠè¿™äº›ä»»åŠ¡ä»¥tasksçš„å½¢å¼ï¼Œæºæºä¸æ–­çš„ä¼ é€ç»™executorsï¼Œäºæ˜¯executorsä¸åœåœ°è¿›è¡Œè®¡ç®—å’Œå­˜å‚¨çš„ä»»åŠ¡ã€‚å½“driverç»“æŸçš„æ—¶å€™ï¼Œä»–ä¼šç»“æŸæ‰executorså¹¶ä¸”é‡Šæ”¾æ‰èµ„æºã€‚è¿™å°±æ˜¯yarn-clusterä¸Šsparkçš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚\n\né™¤äº†yarn-clusterï¼Œè¿˜æœ‰ä¸€ç§yarn-clientçš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–çš„driverå¹¶éè¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Šï¼Œè€Œæ˜¯ä¸€ç›´è¿è¡Œåœ¨clientä¸­ã€‚è¿™æ ·çš„é—®é¢˜å°±æ˜¯clientä¸€æ—¦å…³é—­ï¼Œé‚£ä¹ˆæ•´ä¸ªä»»åŠ¡ä¹Ÿå°±éšä¹‹åœæ­¢æ‰§è¡Œã€‚å› æ­¤ç›¸è¾ƒè€Œè¨€ï¼Œyarn-clusteræ›´é€‚åˆçº¿ä¸Šä»»åŠ¡ï¼Œè€Œyarn-clientæ›´é€‚åˆè°ƒè¯•æ¨¡å¼ã€‚\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-clusterå’ŒYarn-clientåŒºåˆ«ä¸è”ç³»](https://www.iteblog.com/archives/1223.html)\n","source":"_posts/spark-spark-workflow.md","raw":"---\ntitle: Sparkå·¥ä½œæµç¨‹ç®€æ\ndate: 2018-01-07 18:44:37\ntags: spark\ncategories: spark\n---\nHelloï¼Œæœ‰ä¸€ä¸ªæœˆæ²¡å†™blogäº†æ„Ÿè§‰å¾ˆè‡ªè´£ï¼Œå¿…é¡»æ•´èµ·æ¥ï¼æœ€è¿‘ç”±äºå·¥ä½œä¸Šé‡åˆ°çš„ä¸€äº›è°ƒä¼˜å›°éš¾ï¼Œè®©æˆ‘å¯¹Sparkæœ‰äº›æ•¬ç•ï¼Œæ‰€ä»¥é›†ä¸­çš„ç ”ç©¶äº†ä¸‹é¬¼é­…ç„å­¦Sparkï¼Œå’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹ã€‚é¦–å…ˆå…ˆæ¥çœ‹çœ‹sparkçš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚\n<!--more-->\n## Work Flow\nå’Œhadoopä¸€æ ·ï¼Œsparkä¹Ÿæ˜¯master-slaveæœºåˆ¶ï¼ŒSparké€šè¿‡driverè¿›ç¨‹ï¼Œå°†taskåˆ†å‘åˆ°å¤šä¸ªexecutorsä¸Šå¹¶å‘è¿›è¡Œè®¡ç®—ã€‚æ•´ä¸ªdriverå’Œæ‰€æœ‰çš„executorsç»„æˆäº†ä¸€ä¸ªspark applicationï¼Œæ¯ä¸€ä¸ªapplicationæ˜¯è¿è¡Œåœ¨cluster managerä¸Šçš„ï¼ŒSparkæœ¬èº«é›†æˆäº†standalone clusterï¼Œå½“ç„¶ï¼ŒSparkè¿˜å¯ä»¥è¿è¡Œåœ¨èµ«èµ«æœ‰åçš„YARNå’ŒMesosä¸Šã€‚æˆ‘å¹³æ—¶ä½¿ç”¨çš„å…¬å¸é›†ç¾¤éƒ½æ˜¯åŸºäºYARN cluster managerçš„ï¼Œå› æ­¤æœ¬æ–‡é‡ç‚¹æ¢è®¨åŸºäºYARNçš„sparkã€‚\n\nä¸‹å›¾å°±æ˜¯sparkåœ¨cluster managerä¸‹çš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png)\n\n## The Driver\nDriveræ˜¯æ•´ä¸ªapplicationæœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä»–è¿è¡Œçš„æ˜¯applicationçš„mainæ–¹æ³•ï¼Œå®ƒä¼´éšè¿™æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼Œdriverè¿›ç¨‹çš„ç»“æŸå°±ä¼šå¸¦æ¥æ•´ä¸ªapplicationçš„ç»“æŸã€‚\n\nå¯¹äºæ‰€æœ‰çš„Sparkä»»åŠ¡ï¼Œä»–ä»¬å…¶å®éƒ½æ˜¯å®ç°RDDçš„transformationå’Œactionæ“ä½œï¼Œè€Œè¿™äº›æ“ä½œï¼Œæœ€åæ˜¯éœ€è¦driverå°†ä»–ä»¬è½¬åŒ–å’Œåˆ†å‘æˆtasksï¼Œç„¶åæ‰å¯ä»¥å»æ‰§è¡Œã€‚æ‰€æœ‰çš„user programéƒ½ä¼šè¢«driveré€šè¿‡DAG(directed acyclic graph)è½¬åŒ–æˆå®é™…çš„tasksæ‰§è¡Œè®¡åˆ’ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œdriverè¿˜ä¼šåœ¨tasksæ‰§è¡Œçš„æœŸé—´ï¼Œç›‘æ§executorä¸Šçš„tasksï¼Œå¹¶ä¸”ä¿è¯ä»–ä»¬æ‹¥æœ‰å¥åº·è€Œåˆç†çš„èµ„æºã€‚\n\n## Executors\nExecutorsæ˜¯Spark applicationçš„æ‰§è¡Œè€…ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä¼´éšç€applicationçš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨çš„ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ**Spark jobåœ¨executorsæ‰§è¡Œå¤±è´¥çš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥ç»§ç»­è¿›è¡Œ**ã€‚Executorsä¼šå¯¹å…·ä½“çš„tasksçš„æ‰§è¡Œç»“æœè¿”å›ç»™driverï¼ŒåŒæ—¶ç»™ç¼“å­˜çš„RDDæä¾›å­˜å‚¨ç©ºé—´ã€‚\n\n## Some terms\n* Job: Jobæ˜¯executorå±‚é¢æœ€å¤§çš„æ‰§è¡Œå•å…ƒï¼Œjobé€šè¿‡RDDçš„actionæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯ä¸€ä¸ªactionæ“ä½œå°±ä¼šè¿›è¡Œä¸€æ¬¡jobçš„åˆ’åˆ†ï¼›\n* Stage: Stageæ˜¯åŒ…å«åœ¨jobä¸­çš„æ‰§è¡Œå•å…ƒï¼Œstageé€šè¿‡RDDçš„shuffleæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯è¿›è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡stageçš„åˆ’åˆ†ï¼›\n* Task: Taskæ˜¯executoræ‰§è¡Œä¸­æœ€ç»†çš„æ‰§è¡Œå•å…ƒï¼Œtaskçš„æ•°ç›®å–å’Œparent RDDçš„partitionæ•°ç›®æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚\n\n## Spark on Yarn-cluster\nä¸‹é¢ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æ•´ä¸ªSpark applicationä¸­ï¼Œdriverå’Œexecutorsçš„éƒ½ä¼šèµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚æˆ‘ä»¥åŸºäºyarn-clusterçš„YARNçš„Sparkä½œä¸ºä¾‹å­æ¥ç®€è¿°æ•´ä¸ªæµç¨‹ï¼Œå…ˆçœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png)\né¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä¸€äº›YARNçš„æ¦‚å¿µï¼ŒYARNæ˜¯ä¸master-slaverçš„ä¸€ä¸ªCluster Managerï¼Œ åœ¨YARNä¸­ï¼ŒRM(ResourseManager)è´Ÿè´£æ•´ä¸ªè°ƒåº¦åˆ†å‘ï¼Œå³æˆ‘ä»¬å¸¸è¯´çš„masterï¼›è€ŒNM(NodeManager)ä»»åŠ¡åˆ†å‘çš„æ¥å—è€…ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„workerã€‚è¿™äº›æ¦‚å¿µåç»­æˆ‘ä¸“é—¨ä»‹ç»YARNçš„æ—¶å€™ä¼šè¯¦ç»†çš„è¯´æ˜ï¼Œä»–ä»¬çš„ä½œç”¨éƒ½æ˜¯å®ç°sparkå’ŒYARNä¹‹é—´è¯¸å¦‚èµ„æºç”³è¯·ç­‰æ“ä½œã€‚\n\né¦–å…ˆClientå‘ResourceManagerå‘å‡ºæäº¤applicationçš„è¯·æ±‚ï¼ŒResourseManagerä¼šåœ¨æŸä¸€ä¸ªNodeManagerä¸Šå¯åŠ¨AppManagerè¿›ç¨‹ï¼ŒAppManagerä¼šéšåå¯åŠ¨driverï¼Œå¹¶å°†driverç”³è¯·containersèµ„æºçš„ä¿¡æ¯å‘ç»™ResourceManagerï¼Œç”³è¯·å®Œæˆåï¼ŒResourceManagerå°†èµ„æºåˆ†é…æ¶ˆæ¯ä¼ é€’ç»™AppManagerå¹¶ç”±å®ƒå¯åŠ¨containerï¼Œæ¯ä¸€ä¸ªcontainerä¸­åªè¿è¡Œä¸€ä¸ªspark executorï¼Œç”±æ­¤å®Œæˆäº†èµ„æºçš„ç”³è¯·å’Œåˆ†é…ã€‚\n\nç„¶åæ•´ä¸ªapplicationå¼€å§‹æ‰§è¡Œï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®RDDçš„transformationæˆ–è€…actionï¼ŒdriveræŠŠè¿™äº›ä»»åŠ¡ä»¥tasksçš„å½¢å¼ï¼Œæºæºä¸æ–­çš„ä¼ é€ç»™executorsï¼Œäºæ˜¯executorsä¸åœåœ°è¿›è¡Œè®¡ç®—å’Œå­˜å‚¨çš„ä»»åŠ¡ã€‚å½“driverç»“æŸçš„æ—¶å€™ï¼Œä»–ä¼šç»“æŸæ‰executorså¹¶ä¸”é‡Šæ”¾æ‰èµ„æºã€‚è¿™å°±æ˜¯yarn-clusterä¸Šsparkçš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚\n\né™¤äº†yarn-clusterï¼Œè¿˜æœ‰ä¸€ç§yarn-clientçš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–çš„driverå¹¶éè¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Šï¼Œè€Œæ˜¯ä¸€ç›´è¿è¡Œåœ¨clientä¸­ã€‚è¿™æ ·çš„é—®é¢˜å°±æ˜¯clientä¸€æ—¦å…³é—­ï¼Œé‚£ä¹ˆæ•´ä¸ªä»»åŠ¡ä¹Ÿå°±éšä¹‹åœæ­¢æ‰§è¡Œã€‚å› æ­¤ç›¸è¾ƒè€Œè¨€ï¼Œyarn-clusteræ›´é€‚åˆçº¿ä¸Šä»»åŠ¡ï¼Œè€Œyarn-clientæ›´é€‚åˆè°ƒè¯•æ¨¡å¼ã€‚\n\n## Reference\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n* [Spark:Yarn-clusterå’ŒYarn-clientåŒºåˆ«ä¸è”ç³»](https://www.iteblog.com/archives/1223.html)\n","slug":"spark-spark-workflow","published":1,"updated":"2018-11-19T04:51:30.519Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pm001htr8law39ktsd","content":"<p>Helloï¼Œæœ‰ä¸€ä¸ªæœˆæ²¡å†™blogäº†æ„Ÿè§‰å¾ˆè‡ªè´£ï¼Œå¿…é¡»æ•´èµ·æ¥ï¼æœ€è¿‘ç”±äºå·¥ä½œä¸Šé‡åˆ°çš„ä¸€äº›è°ƒä¼˜å›°éš¾ï¼Œè®©æˆ‘å¯¹Sparkæœ‰äº›æ•¬ç•ï¼Œæ‰€ä»¥é›†ä¸­çš„ç ”ç©¶äº†ä¸‹é¬¼é­…ç„å­¦Sparkï¼Œå’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹ã€‚é¦–å…ˆå…ˆæ¥çœ‹çœ‹sparkçš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚<br><a id=\"more\"></a></p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>å’Œhadoopä¸€æ ·ï¼Œsparkä¹Ÿæ˜¯master-slaveæœºåˆ¶ï¼ŒSparké€šè¿‡driverè¿›ç¨‹ï¼Œå°†taskåˆ†å‘åˆ°å¤šä¸ªexecutorsä¸Šå¹¶å‘è¿›è¡Œè®¡ç®—ã€‚æ•´ä¸ªdriverå’Œæ‰€æœ‰çš„executorsç»„æˆäº†ä¸€ä¸ªspark applicationï¼Œæ¯ä¸€ä¸ªapplicationæ˜¯è¿è¡Œåœ¨cluster managerä¸Šçš„ï¼ŒSparkæœ¬èº«é›†æˆäº†standalone clusterï¼Œå½“ç„¶ï¼ŒSparkè¿˜å¯ä»¥è¿è¡Œåœ¨èµ«èµ«æœ‰åçš„YARNå’ŒMesosä¸Šã€‚æˆ‘å¹³æ—¶ä½¿ç”¨çš„å…¬å¸é›†ç¾¤éƒ½æ˜¯åŸºäºYARN cluster managerçš„ï¼Œå› æ­¤æœ¬æ–‡é‡ç‚¹æ¢è®¨åŸºäºYARNçš„sparkã€‚</p>\n<p>ä¸‹å›¾å°±æ˜¯sparkåœ¨cluster managerä¸‹çš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png\" alt=\"\"></p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driveræ˜¯æ•´ä¸ªapplicationæœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä»–è¿è¡Œçš„æ˜¯applicationçš„mainæ–¹æ³•ï¼Œå®ƒä¼´éšè¿™æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼Œdriverè¿›ç¨‹çš„ç»“æŸå°±ä¼šå¸¦æ¥æ•´ä¸ªapplicationçš„ç»“æŸã€‚</p>\n<p>å¯¹äºæ‰€æœ‰çš„Sparkä»»åŠ¡ï¼Œä»–ä»¬å…¶å®éƒ½æ˜¯å®ç°RDDçš„transformationå’Œactionæ“ä½œï¼Œè€Œè¿™äº›æ“ä½œï¼Œæœ€åæ˜¯éœ€è¦driverå°†ä»–ä»¬è½¬åŒ–å’Œåˆ†å‘æˆtasksï¼Œç„¶åæ‰å¯ä»¥å»æ‰§è¡Œã€‚æ‰€æœ‰çš„user programéƒ½ä¼šè¢«driveré€šè¿‡DAG(directed acyclic graph)è½¬åŒ–æˆå®é™…çš„tasksæ‰§è¡Œè®¡åˆ’ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œdriverè¿˜ä¼šåœ¨tasksæ‰§è¡Œçš„æœŸé—´ï¼Œç›‘æ§executorä¸Šçš„tasksï¼Œå¹¶ä¸”ä¿è¯ä»–ä»¬æ‹¥æœ‰å¥åº·è€Œåˆç†çš„èµ„æºã€‚</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>Executorsæ˜¯Spark applicationçš„æ‰§è¡Œè€…ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä¼´éšç€applicationçš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨çš„ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ<strong>Spark jobåœ¨executorsæ‰§è¡Œå¤±è´¥çš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥ç»§ç»­è¿›è¡Œ</strong>ã€‚Executorsä¼šå¯¹å…·ä½“çš„tasksçš„æ‰§è¡Œç»“æœè¿”å›ç»™driverï¼ŒåŒæ—¶ç»™ç¼“å­˜çš„RDDæä¾›å­˜å‚¨ç©ºé—´ã€‚</p>\n<h2 id=\"Some-terms\"><a href=\"#Some-terms\" class=\"headerlink\" title=\"Some terms\"></a>Some terms</h2><ul>\n<li>Job: Jobæ˜¯executorå±‚é¢æœ€å¤§çš„æ‰§è¡Œå•å…ƒï¼Œjobé€šè¿‡RDDçš„actionæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯ä¸€ä¸ªactionæ“ä½œå°±ä¼šè¿›è¡Œä¸€æ¬¡jobçš„åˆ’åˆ†ï¼›</li>\n<li>Stage: Stageæ˜¯åŒ…å«åœ¨jobä¸­çš„æ‰§è¡Œå•å…ƒï¼Œstageé€šè¿‡RDDçš„shuffleæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯è¿›è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡stageçš„åˆ’åˆ†ï¼›</li>\n<li>Task: Taskæ˜¯executoræ‰§è¡Œä¸­æœ€ç»†çš„æ‰§è¡Œå•å…ƒï¼Œtaskçš„æ•°ç›®å–å’Œparent RDDçš„partitionæ•°ç›®æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚</li>\n</ul>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>ä¸‹é¢ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æ•´ä¸ªSpark applicationä¸­ï¼Œdriverå’Œexecutorsçš„éƒ½ä¼šèµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚æˆ‘ä»¥åŸºäºyarn-clusterçš„YARNçš„Sparkä½œä¸ºä¾‹å­æ¥ç®€è¿°æ•´ä¸ªæµç¨‹ï¼Œå…ˆçœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png\" alt=\"\"><br>é¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä¸€äº›YARNçš„æ¦‚å¿µï¼ŒYARNæ˜¯ä¸master-slaverçš„ä¸€ä¸ªCluster Managerï¼Œ åœ¨YARNä¸­ï¼ŒRM(ResourseManager)è´Ÿè´£æ•´ä¸ªè°ƒåº¦åˆ†å‘ï¼Œå³æˆ‘ä»¬å¸¸è¯´çš„masterï¼›è€ŒNM(NodeManager)ä»»åŠ¡åˆ†å‘çš„æ¥å—è€…ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„workerã€‚è¿™äº›æ¦‚å¿µåç»­æˆ‘ä¸“é—¨ä»‹ç»YARNçš„æ—¶å€™ä¼šè¯¦ç»†çš„è¯´æ˜ï¼Œä»–ä»¬çš„ä½œç”¨éƒ½æ˜¯å®ç°sparkå’ŒYARNä¹‹é—´è¯¸å¦‚èµ„æºç”³è¯·ç­‰æ“ä½œã€‚</p>\n<p>é¦–å…ˆClientå‘ResourceManagerå‘å‡ºæäº¤applicationçš„è¯·æ±‚ï¼ŒResourseManagerä¼šåœ¨æŸä¸€ä¸ªNodeManagerä¸Šå¯åŠ¨AppManagerè¿›ç¨‹ï¼ŒAppManagerä¼šéšåå¯åŠ¨driverï¼Œå¹¶å°†driverç”³è¯·containersèµ„æºçš„ä¿¡æ¯å‘ç»™ResourceManagerï¼Œç”³è¯·å®Œæˆåï¼ŒResourceManagerå°†èµ„æºåˆ†é…æ¶ˆæ¯ä¼ é€’ç»™AppManagerå¹¶ç”±å®ƒå¯åŠ¨containerï¼Œæ¯ä¸€ä¸ªcontainerä¸­åªè¿è¡Œä¸€ä¸ªspark executorï¼Œç”±æ­¤å®Œæˆäº†èµ„æºçš„ç”³è¯·å’Œåˆ†é…ã€‚</p>\n<p>ç„¶åæ•´ä¸ªapplicationå¼€å§‹æ‰§è¡Œï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®RDDçš„transformationæˆ–è€…actionï¼ŒdriveræŠŠè¿™äº›ä»»åŠ¡ä»¥tasksçš„å½¢å¼ï¼Œæºæºä¸æ–­çš„ä¼ é€ç»™executorsï¼Œäºæ˜¯executorsä¸åœåœ°è¿›è¡Œè®¡ç®—å’Œå­˜å‚¨çš„ä»»åŠ¡ã€‚å½“driverç»“æŸçš„æ—¶å€™ï¼Œä»–ä¼šç»“æŸæ‰executorså¹¶ä¸”é‡Šæ”¾æ‰èµ„æºã€‚è¿™å°±æ˜¯yarn-clusterä¸Šsparkçš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚</p>\n<p>é™¤äº†yarn-clusterï¼Œè¿˜æœ‰ä¸€ç§yarn-clientçš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–çš„driverå¹¶éè¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Šï¼Œè€Œæ˜¯ä¸€ç›´è¿è¡Œåœ¨clientä¸­ã€‚è¿™æ ·çš„é—®é¢˜å°±æ˜¯clientä¸€æ—¦å…³é—­ï¼Œé‚£ä¹ˆæ•´ä¸ªä»»åŠ¡ä¹Ÿå°±éšä¹‹åœæ­¢æ‰§è¡Œã€‚å› æ­¤ç›¸è¾ƒè€Œè¨€ï¼Œyarn-clusteræ›´é€‚åˆçº¿ä¸Šä»»åŠ¡ï¼Œè€Œyarn-clientæ›´é€‚åˆè°ƒè¯•æ¨¡å¼ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"noopener\">Spark:Yarn-clusterå’ŒYarn-clientåŒºåˆ«ä¸è”ç³»</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p>Helloï¼Œæœ‰ä¸€ä¸ªæœˆæ²¡å†™blogäº†æ„Ÿè§‰å¾ˆè‡ªè´£ï¼Œå¿…é¡»æ•´èµ·æ¥ï¼æœ€è¿‘ç”±äºå·¥ä½œä¸Šé‡åˆ°çš„ä¸€äº›è°ƒä¼˜å›°éš¾ï¼Œè®©æˆ‘å¯¹Sparkæœ‰äº›æ•¬ç•ï¼Œæ‰€ä»¥é›†ä¸­çš„ç ”ç©¶äº†ä¸‹é¬¼é­…ç„å­¦Sparkï¼Œå’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹ã€‚é¦–å…ˆå…ˆæ¥çœ‹çœ‹sparkçš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚<br></p>","more":"</p>\n<h2 id=\"Work-Flow\"><a href=\"#Work-Flow\" class=\"headerlink\" title=\"Work Flow\"></a>Work Flow</h2><p>å’Œhadoopä¸€æ ·ï¼Œsparkä¹Ÿæ˜¯master-slaveæœºåˆ¶ï¼ŒSparké€šè¿‡driverè¿›ç¨‹ï¼Œå°†taskåˆ†å‘åˆ°å¤šä¸ªexecutorsä¸Šå¹¶å‘è¿›è¡Œè®¡ç®—ã€‚æ•´ä¸ªdriverå’Œæ‰€æœ‰çš„executorsç»„æˆäº†ä¸€ä¸ªspark applicationï¼Œæ¯ä¸€ä¸ªapplicationæ˜¯è¿è¡Œåœ¨cluster managerä¸Šçš„ï¼ŒSparkæœ¬èº«é›†æˆäº†standalone clusterï¼Œå½“ç„¶ï¼ŒSparkè¿˜å¯ä»¥è¿è¡Œåœ¨èµ«èµ«æœ‰åçš„YARNå’ŒMesosä¸Šã€‚æˆ‘å¹³æ—¶ä½¿ç”¨çš„å…¬å¸é›†ç¾¤éƒ½æ˜¯åŸºäºYARN cluster managerçš„ï¼Œå› æ­¤æœ¬æ–‡é‡ç‚¹æ¢è®¨åŸºäºYARNçš„sparkã€‚</p>\n<p>ä¸‹å›¾å°±æ˜¯sparkåœ¨cluster managerä¸‹çš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-1.png\" alt=\"\"></p>\n<h2 id=\"The-Driver\"><a href=\"#The-Driver\" class=\"headerlink\" title=\"The Driver\"></a>The Driver</h2><p>Driveræ˜¯æ•´ä¸ªapplicationæœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä»–è¿è¡Œçš„æ˜¯applicationçš„mainæ–¹æ³•ï¼Œå®ƒä¼´éšè¿™æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼Œdriverè¿›ç¨‹çš„ç»“æŸå°±ä¼šå¸¦æ¥æ•´ä¸ªapplicationçš„ç»“æŸã€‚</p>\n<p>å¯¹äºæ‰€æœ‰çš„Sparkä»»åŠ¡ï¼Œä»–ä»¬å…¶å®éƒ½æ˜¯å®ç°RDDçš„transformationå’Œactionæ“ä½œï¼Œè€Œè¿™äº›æ“ä½œï¼Œæœ€åæ˜¯éœ€è¦driverå°†ä»–ä»¬è½¬åŒ–å’Œåˆ†å‘æˆtasksï¼Œç„¶åæ‰å¯ä»¥å»æ‰§è¡Œã€‚æ‰€æœ‰çš„user programéƒ½ä¼šè¢«driveré€šè¿‡DAG(directed acyclic graph)è½¬åŒ–æˆå®é™…çš„tasksæ‰§è¡Œè®¡åˆ’ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œdriverè¿˜ä¼šåœ¨tasksæ‰§è¡Œçš„æœŸé—´ï¼Œç›‘æ§executorä¸Šçš„tasksï¼Œå¹¶ä¸”ä¿è¯ä»–ä»¬æ‹¥æœ‰å¥åº·è€Œåˆç†çš„èµ„æºã€‚</p>\n<h2 id=\"Executors\"><a href=\"#Executors\" class=\"headerlink\" title=\"Executors\"></a>Executors</h2><p>Executorsæ˜¯Spark applicationçš„æ‰§è¡Œè€…ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä¼´éšç€applicationçš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨çš„ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ<strong>Spark jobåœ¨executorsæ‰§è¡Œå¤±è´¥çš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥ç»§ç»­è¿›è¡Œ</strong>ã€‚Executorsä¼šå¯¹å…·ä½“çš„tasksçš„æ‰§è¡Œç»“æœè¿”å›ç»™driverï¼ŒåŒæ—¶ç»™ç¼“å­˜çš„RDDæä¾›å­˜å‚¨ç©ºé—´ã€‚</p>\n<h2 id=\"Some-terms\"><a href=\"#Some-terms\" class=\"headerlink\" title=\"Some terms\"></a>Some terms</h2><ul>\n<li>Job: Jobæ˜¯executorå±‚é¢æœ€å¤§çš„æ‰§è¡Œå•å…ƒï¼Œjobé€šè¿‡RDDçš„actionæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯ä¸€ä¸ªactionæ“ä½œå°±ä¼šè¿›è¡Œä¸€æ¬¡jobçš„åˆ’åˆ†ï¼›</li>\n<li>Stage: Stageæ˜¯åŒ…å«åœ¨jobä¸­çš„æ‰§è¡Œå•å…ƒï¼Œstageé€šè¿‡RDDçš„shuffleæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯è¿›è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡stageçš„åˆ’åˆ†ï¼›</li>\n<li>Task: Taskæ˜¯executoræ‰§è¡Œä¸­æœ€ç»†çš„æ‰§è¡Œå•å…ƒï¼Œtaskçš„æ•°ç›®å–å’Œparent RDDçš„partitionæ•°ç›®æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚</li>\n</ul>\n<h2 id=\"Spark-on-Yarn-cluster\"><a href=\"#Spark-on-Yarn-cluster\" class=\"headerlink\" title=\"Spark on Yarn-cluster\"></a>Spark on Yarn-cluster</h2><p>ä¸‹é¢ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æ•´ä¸ªSpark applicationä¸­ï¼Œdriverå’Œexecutorsçš„éƒ½ä¼šèµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚æˆ‘ä»¥åŸºäºyarn-clusterçš„YARNçš„Sparkä½œä¸ºä¾‹å­æ¥ç®€è¿°æ•´ä¸ªæµç¨‹ï¼Œå…ˆçœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/15/15-2.png\" alt=\"\"><br>é¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä¸€äº›YARNçš„æ¦‚å¿µï¼ŒYARNæ˜¯ä¸master-slaverçš„ä¸€ä¸ªCluster Managerï¼Œ åœ¨YARNä¸­ï¼ŒRM(ResourseManager)è´Ÿè´£æ•´ä¸ªè°ƒåº¦åˆ†å‘ï¼Œå³æˆ‘ä»¬å¸¸è¯´çš„masterï¼›è€ŒNM(NodeManager)ä»»åŠ¡åˆ†å‘çš„æ¥å—è€…ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„workerã€‚è¿™äº›æ¦‚å¿µåç»­æˆ‘ä¸“é—¨ä»‹ç»YARNçš„æ—¶å€™ä¼šè¯¦ç»†çš„è¯´æ˜ï¼Œä»–ä»¬çš„ä½œç”¨éƒ½æ˜¯å®ç°sparkå’ŒYARNä¹‹é—´è¯¸å¦‚èµ„æºç”³è¯·ç­‰æ“ä½œã€‚</p>\n<p>é¦–å…ˆClientå‘ResourceManagerå‘å‡ºæäº¤applicationçš„è¯·æ±‚ï¼ŒResourseManagerä¼šåœ¨æŸä¸€ä¸ªNodeManagerä¸Šå¯åŠ¨AppManagerè¿›ç¨‹ï¼ŒAppManagerä¼šéšåå¯åŠ¨driverï¼Œå¹¶å°†driverç”³è¯·containersèµ„æºçš„ä¿¡æ¯å‘ç»™ResourceManagerï¼Œç”³è¯·å®Œæˆåï¼ŒResourceManagerå°†èµ„æºåˆ†é…æ¶ˆæ¯ä¼ é€’ç»™AppManagerå¹¶ç”±å®ƒå¯åŠ¨containerï¼Œæ¯ä¸€ä¸ªcontainerä¸­åªè¿è¡Œä¸€ä¸ªspark executorï¼Œç”±æ­¤å®Œæˆäº†èµ„æºçš„ç”³è¯·å’Œåˆ†é…ã€‚</p>\n<p>ç„¶åæ•´ä¸ªapplicationå¼€å§‹æ‰§è¡Œï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®RDDçš„transformationæˆ–è€…actionï¼ŒdriveræŠŠè¿™äº›ä»»åŠ¡ä»¥tasksçš„å½¢å¼ï¼Œæºæºä¸æ–­çš„ä¼ é€ç»™executorsï¼Œäºæ˜¯executorsä¸åœåœ°è¿›è¡Œè®¡ç®—å’Œå­˜å‚¨çš„ä»»åŠ¡ã€‚å½“driverç»“æŸçš„æ—¶å€™ï¼Œä»–ä¼šç»“æŸæ‰executorså¹¶ä¸”é‡Šæ”¾æ‰èµ„æºã€‚è¿™å°±æ˜¯yarn-clusterä¸Šsparkçš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚</p>\n<p>é™¤äº†yarn-clusterï¼Œè¿˜æœ‰ä¸€ç§yarn-clientçš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–çš„driverå¹¶éè¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Šï¼Œè€Œæ˜¯ä¸€ç›´è¿è¡Œåœ¨clientä¸­ã€‚è¿™æ ·çš„é—®é¢˜å°±æ˜¯clientä¸€æ—¦å…³é—­ï¼Œé‚£ä¹ˆæ•´ä¸ªä»»åŠ¡ä¹Ÿå°±éšä¹‹åœæ­¢æ‰§è¡Œã€‚å› æ­¤ç›¸è¾ƒè€Œè¨€ï¼Œyarn-clusteræ›´é€‚åˆçº¿ä¸Šä»»åŠ¡ï¼Œè€Œyarn-clientæ›´é€‚åˆè°ƒè¯•æ¨¡å¼ã€‚</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015.</a></li>\n<li><a href=\"https://www.iteblog.com/archives/1223.html\" target=\"_blank\" rel=\"noopener\">Spark:Yarn-clusterå’ŒYarn-clientåŒºåˆ«ä¸è”ç³»</a></li>\n</ul>"},{"title":"Spark Tuning","date":"2018-02-23T05:10:32.000Z","_content":"Hi, all, æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶spark tuningæ–¹é¢çš„é—®é¢˜ï¼Œæ·±æ„Ÿè¿™æ˜¯ä¸€ä¸ªç»éªŒæ´»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŠ€æœ¯æ´»ï¼ŒæŸ¥é˜…å’Œå¾ˆå¤šèµ„æ–™ï¼Œåœ¨è¿™é‡Œmarkä¸€ä¸‹ã€‚\n<!--more-->\nä¸Šæ¬¡æˆ‘ä»¬reviewäº†ä¸€ä¸‹sparkçš„work-flowï¼Œä¸»è¦æ˜¯åŸºäºspark on yarnçš„ï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢è®¨çš„ä¹Ÿä¸»è¦æ˜¯åŸºäºspark on yarnã€‚\n## Resource Allocation\n### Some Configuration\nResource allocationæ˜¯sparkä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç¯èŠ‚ï¼Œç»™äºˆä¸€ä¸ªapplicationè¿‡å°‘çš„resourceä¼šå¸¦äº†æ‰§è¡Œæ•ˆç‡çš„ä½ä¸‹å’Œæ‰§è¡Œé€Ÿåº¦çš„ç¼“æ…¢ï¼›ç›¸åï¼Œè¿‡å¤šçš„resourceåˆ™ä¼šå¸¦æ¥èµ„æºæµªè´¹ï¼Œå½±å“clusterä¸Šå…¶ä»–appllicationçš„è¿è¡Œï¼Œå› æ­¤ï¼Œä¸€ä¸ªåˆé€‚çš„resource allocationæ˜¯éå¸¸éå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ¯”è¾ƒé‡è¦çš„parameterï¼š\n* num-executors: è¡¨æ˜sparkç”³è¯·executorsçš„æ•°ç›®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®spark.dynamicAllocation.enabledæ¥è®©sparkæ ¹æ®æ•°æ®åŠ¨æ€çš„åˆ†é…executorsï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜èµ„æºåˆ©ç”¨ç‡ï¼›\n* executor-cores: æŒ‡å®šæ¯ä¸€ä¸ªexecutorçš„coreæ•°ç›®ï¼Œcoreæ•°ç›®å†³å®šäº†æ¯ä¸ªexecutorçš„æœ€å¤§å¹¶è¡Œtaskæ•°ç›®\n* executor-memory: æŒ‡å®šåˆ†é…ç»™æ¯ä¸€ä¸ªexecutorçš„å†…å­˜å¤§å°ã€‚\n\n### Some Tips\n* å¯¹äºexecutoræ¥è¯´ï¼Œåœ¨è¿‡äºå¤§çš„memoryä¸Šè¿è¡Œå¯èƒ½ä¼šå¸¦æ¥æ¯”è¾ƒé«˜çš„GC(gabage collection) timeï¼Œå¯¹äºä¸€ä¸ªexecutoræ¥è¯´ï¼Œå»ºè®®ç»™å‡ºçš„ä¸Šé™memoryæ˜¯64Gï¼›\n* ç”±äºHDFSåœ¨å¹¶è¡Œè¯»å†™çš„æ—¶å€™å­˜åœ¨ä¸€äº›ç“¶é¢ˆï¼Œå› æ­¤æ¯ä¸€ä¸ªexecutorä¸­æœ€å¥½ä¸è¦è¶…è¿‡5ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå³coresæ•°ä¸è¦è¶…è¿‡5ä¸ªï¼Œæœ‰å®éªŒå¯ä»¥è¯æ˜ï¼Œsparkåœ¨å¤šexecutorå°‘coreçš„é…ç½®ä¸‹æ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼›\n* ç›¸åçš„ï¼Œå¯¹äºexecutoræ¥è¯´ï¼Œè¿‡åˆ†å°‘çš„coreï¼Œä¾‹å¦‚1ä¸ªï¼Œå°†ä¼šä½¿å¾—executorsæ•°ç›®å˜å¤šï¼Œä¾‹å¦‚æŸä¸ªbroadcastè¿‡ç¨‹ï¼Œéœ€è¦ä¼ æ’­åˆ°æ‰€æœ‰çš„executorsä¸Šï¼Œé‚£ä¹ˆè¿‡åˆ†å¤šçš„executorsä¼šé™ä½æ‰§è¡Œçš„æ•ˆç‡ã€‚\n\n## Memory Mangement\nå…³äºsparkä¸­çš„memory managementï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png)\nåœ¨å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒsparkæŠŠmemoryåˆ†æˆäº†ä¸‰éƒ¨åˆ†ï¼Œå³spark memoryã€user memoryå’Œreserved memoryï¼Œæˆ‘ä»¬é¡ºæ¬¡æ¥çœ‹çœ‹ï¼š\n### Reserved Memory\næ‰€è°“reserved memoryï¼Œå®ƒå°±æ˜¯ç³»ç»Ÿé¢„ç•™ä¸‹çš„ä¸€éƒ¨åˆ†memoryï¼Œç”¨äºå­˜å‚¨sparkçš„å†…éƒ¨å¯¹è±¡ï¼Œé»˜è®¤å¤§å°ä¸º300mï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¸ä¼šä¿®æ”¹è¿™äº›å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“executorè¢«åˆ†é…çš„memoryå°äº1.5å€çš„reserved memoryæ—¶ï¼Œå°†ä¼šæŠ›å‡ºâ€œplease use larger heap sizeâ€çš„é”™è¯¯ã€‚\n### User Memory\nUser memoryç”¨äºå‚¨å­˜sparkçš„transfermationçš„ä¸€äº›ä¿¡æ¯ï¼Œæ¯”å¦‚RDDä¹‹é—´çš„ä¾èµ–ä¿¡æ¯ç­‰ç­‰ï¼Œè¿™éƒ¨åˆ†å†…å­˜é»˜è®¤å¤§å°ä¸º(Java Heap - 300M)*0.25ï¼Œå…¶ä¸­çš„300Må…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„reserved memory.å…·ä½“çš„å¤§å°è¦ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†user å’Œ ä¸‹é¢è¦è®²åˆ°çš„spark memoryçš„åˆ†é…æ¯”ä¾‹ã€‚\n### Spark Memory\nä¸Šæ–‡å·²ç»åˆ°äº†ï¼Œspark memoryä¸»è¦æ˜¯sparkè‡ªå·±ä½¿ç”¨çš„memoryéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†çš„å¤§å°ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œå³(Java Heap - 300M)*spark.memory.fractionï¼Œå…¶ä¸­fractionçš„defaultä¸º0.75ã€‚\n\nSpark memoryä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼Œä¸€æ˜¯ç”¨äºsparkçš„shuffleç­‰æ“ä½œï¼Œè€Œæ˜¯ç”¨æ¥cache sparkä¸­çš„RDDï¼Œå› æ­¤spark memoryä¹Ÿè‡ªç„¶è€Œç„¶çš„åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼Œå³è´Ÿè´£shuffleæ“ä½œçš„execution memoryå’Œè´Ÿè´£cacheçš„storage memoryï¼Œä¸¤è€…çš„å¤§å°é€šè¿‡spark.memory.storageFractionå‚æ•°æ¥åˆ†å‰²ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚\n\nåœ¨spark memoryä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼Œé‚£å°±æ˜¯storage å’Œ execution memoryçš„å…±äº«æœºåˆ¶ï¼Œè¯´çš„ç®€å•ä¸€äº›å°±æ˜¯ï¼Œå½“ä¸€è¾¹å†…å­˜ç©ºé—²è€Œå¦ä¸€æ–¹å†…å­˜ç´§å¼ çš„æ—¶å€™ï¼Œå¯ä»¥å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œæˆ‘ä»¬ä¸‹é¢çœ‹çœ‹åœ¨å†…å­˜å‡ºç°å†²çªçš„æ—¶å€™ï¼Œsparkæ€ä¹ˆåè°ƒï¼š\n* å½“storageå ç”¨execution memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿexecution memoryä½¿ç”¨ç´§å¼ çš„æƒ…å†µæ—¶ï¼Œå¼ºåˆ¶å°†storageå æœ‰çš„å†…å­˜é‡Šæ”¾å¹¶å½’è¿˜executionï¼Œä¸¢å¤±çš„æ•°æ®å°†ä¼šåç»­é‡æ–°è®¡ç®—ï¼›\n* å½“executionå ç”¨storage memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿstorage memoryç´§å¼ çš„æƒ…å†µï¼Œè¢«å ç”¨çš„å†…å­˜ä¸ä¼šè¢«å¼ºåˆ¶é‡Šæ”¾ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥ä»»åŠ¡ä¸¢å¤±ï¼Œstorageä¼šè€å¿ƒç­‰å¾…çŸ¥é“executionæ‰§è¡Œå®Œé‡Šæ”¾å‡ºå†…å­˜ã€‚\n\n## Data Serialization\nåœ¨æ•´ä¸ªsparkä»»åŠ¡ä¸­ï¼Œæ•°æ®ä¼ è¾“éƒ½æ˜¯ç»è¿‡åºåˆ—åŒ–å(serialization)ä¹‹åä¼ è¾“çš„ï¼Œå› æ­¤æ•°æ®çš„åºåˆ—åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå†—ä½™çš„åºåˆ—åŒ–è¿‡ç¨‹ä¼šè®©æ•´ä¸ªsparkä»»åŠ¡å˜æ…¢ï¼Œsparkæä¾›ä¸¤ç§åºåˆ—åŒ–æ–¹å¼ï¼š\n* Java serializationï¼šè¿™æ˜¯sparké»˜è®¤çš„åºåˆ—åŒ–æ–¹å¼ï¼Œjavaåºåˆ—åŒ–æ˜¯ä¸€ç§å¾ˆç»å…¸å’Œç¨³å®šçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä½†æ˜¯æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯â€”â€”æ…¢ï¼\n* Kryo serializationï¼šKryo åºåˆ—åŒ–å¯ä»¥è®©sparkä»»åŠ¡æ›´åŠ å¿«é€Ÿï¼Œç”šè‡³10å€äºjavaåºåˆ—åŒ–ï¼›ä½†æ˜¯å®ƒä¸æ”¯æŒæ‰€æœ‰çš„Serializableç±»å‹ï¼ŒåŒæ—¶éœ€è¦ä¸ºç”¨æˆ·è‡ªå·±å¼€å‘çš„classè¿›è¡Œæ³¨å†Œåï¼Œæ‰å¯ä»¥ä½¿ç”¨Kyo.\n\nå…³äºKryoçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹[spark documentation](https://spark.apache.org/docs/latest/tuning.html#data-serialization)ï¼Œæˆ–è€…[Kryo documentation](https://github.com/EsotericSoftware/kryo)\n\n## Summary\nå…³äºsparkè°ƒä¼˜çš„é—®é¢˜ï¼Œæœ‰å¾ˆå¤šå› ç´ ï¼Œæˆ‘ä¹Ÿæ˜¯ç®€å•çš„åšäº†ä¸€äº›äº†è§£å¹¶åˆ†äº«ç»™å¤§å®¶ï¼Œé™¤äº†æˆ‘æåˆ°çš„ï¼Œè¿˜æœ‰è¯¸å¦‚GCç­‰ç­‰å› ç´ ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æˆ‘ç»™å‡ºçš„referencesåšè¿›ä¸€æ­¥çš„äº†è§£ã€‚\n\n## References\n* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n* [Spark Documentation-Tuning](https://spark.apache.org/docs/latest/tuning.html)\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n","source":"_posts/spark-spark-tune.md","raw":"---\ntitle: Spark Tuning\ndate: 2018-02-23 13:10:32\ntags: spark\ncategories: spark\n---\nHi, all, æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶spark tuningæ–¹é¢çš„é—®é¢˜ï¼Œæ·±æ„Ÿè¿™æ˜¯ä¸€ä¸ªç»éªŒæ´»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŠ€æœ¯æ´»ï¼ŒæŸ¥é˜…å’Œå¾ˆå¤šèµ„æ–™ï¼Œåœ¨è¿™é‡Œmarkä¸€ä¸‹ã€‚\n<!--more-->\nä¸Šæ¬¡æˆ‘ä»¬reviewäº†ä¸€ä¸‹sparkçš„work-flowï¼Œä¸»è¦æ˜¯åŸºäºspark on yarnçš„ï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢è®¨çš„ä¹Ÿä¸»è¦æ˜¯åŸºäºspark on yarnã€‚\n## Resource Allocation\n### Some Configuration\nResource allocationæ˜¯sparkä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç¯èŠ‚ï¼Œç»™äºˆä¸€ä¸ªapplicationè¿‡å°‘çš„resourceä¼šå¸¦äº†æ‰§è¡Œæ•ˆç‡çš„ä½ä¸‹å’Œæ‰§è¡Œé€Ÿåº¦çš„ç¼“æ…¢ï¼›ç›¸åï¼Œè¿‡å¤šçš„resourceåˆ™ä¼šå¸¦æ¥èµ„æºæµªè´¹ï¼Œå½±å“clusterä¸Šå…¶ä»–appllicationçš„è¿è¡Œï¼Œå› æ­¤ï¼Œä¸€ä¸ªåˆé€‚çš„resource allocationæ˜¯éå¸¸éå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ¯”è¾ƒé‡è¦çš„parameterï¼š\n* num-executors: è¡¨æ˜sparkç”³è¯·executorsçš„æ•°ç›®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®spark.dynamicAllocation.enabledæ¥è®©sparkæ ¹æ®æ•°æ®åŠ¨æ€çš„åˆ†é…executorsï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜èµ„æºåˆ©ç”¨ç‡ï¼›\n* executor-cores: æŒ‡å®šæ¯ä¸€ä¸ªexecutorçš„coreæ•°ç›®ï¼Œcoreæ•°ç›®å†³å®šäº†æ¯ä¸ªexecutorçš„æœ€å¤§å¹¶è¡Œtaskæ•°ç›®\n* executor-memory: æŒ‡å®šåˆ†é…ç»™æ¯ä¸€ä¸ªexecutorçš„å†…å­˜å¤§å°ã€‚\n\n### Some Tips\n* å¯¹äºexecutoræ¥è¯´ï¼Œåœ¨è¿‡äºå¤§çš„memoryä¸Šè¿è¡Œå¯èƒ½ä¼šå¸¦æ¥æ¯”è¾ƒé«˜çš„GC(gabage collection) timeï¼Œå¯¹äºä¸€ä¸ªexecutoræ¥è¯´ï¼Œå»ºè®®ç»™å‡ºçš„ä¸Šé™memoryæ˜¯64Gï¼›\n* ç”±äºHDFSåœ¨å¹¶è¡Œè¯»å†™çš„æ—¶å€™å­˜åœ¨ä¸€äº›ç“¶é¢ˆï¼Œå› æ­¤æ¯ä¸€ä¸ªexecutorä¸­æœ€å¥½ä¸è¦è¶…è¿‡5ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå³coresæ•°ä¸è¦è¶…è¿‡5ä¸ªï¼Œæœ‰å®éªŒå¯ä»¥è¯æ˜ï¼Œsparkåœ¨å¤šexecutorå°‘coreçš„é…ç½®ä¸‹æ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼›\n* ç›¸åçš„ï¼Œå¯¹äºexecutoræ¥è¯´ï¼Œè¿‡åˆ†å°‘çš„coreï¼Œä¾‹å¦‚1ä¸ªï¼Œå°†ä¼šä½¿å¾—executorsæ•°ç›®å˜å¤šï¼Œä¾‹å¦‚æŸä¸ªbroadcastè¿‡ç¨‹ï¼Œéœ€è¦ä¼ æ’­åˆ°æ‰€æœ‰çš„executorsä¸Šï¼Œé‚£ä¹ˆè¿‡åˆ†å¤šçš„executorsä¼šé™ä½æ‰§è¡Œçš„æ•ˆç‡ã€‚\n\n## Memory Mangement\nå…³äºsparkä¸­çš„memory managementï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png)\nåœ¨å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒsparkæŠŠmemoryåˆ†æˆäº†ä¸‰éƒ¨åˆ†ï¼Œå³spark memoryã€user memoryå’Œreserved memoryï¼Œæˆ‘ä»¬é¡ºæ¬¡æ¥çœ‹çœ‹ï¼š\n### Reserved Memory\næ‰€è°“reserved memoryï¼Œå®ƒå°±æ˜¯ç³»ç»Ÿé¢„ç•™ä¸‹çš„ä¸€éƒ¨åˆ†memoryï¼Œç”¨äºå­˜å‚¨sparkçš„å†…éƒ¨å¯¹è±¡ï¼Œé»˜è®¤å¤§å°ä¸º300mï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¸ä¼šä¿®æ”¹è¿™äº›å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“executorè¢«åˆ†é…çš„memoryå°äº1.5å€çš„reserved memoryæ—¶ï¼Œå°†ä¼šæŠ›å‡ºâ€œplease use larger heap sizeâ€çš„é”™è¯¯ã€‚\n### User Memory\nUser memoryç”¨äºå‚¨å­˜sparkçš„transfermationçš„ä¸€äº›ä¿¡æ¯ï¼Œæ¯”å¦‚RDDä¹‹é—´çš„ä¾èµ–ä¿¡æ¯ç­‰ç­‰ï¼Œè¿™éƒ¨åˆ†å†…å­˜é»˜è®¤å¤§å°ä¸º(Java Heap - 300M)*0.25ï¼Œå…¶ä¸­çš„300Må…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„reserved memory.å…·ä½“çš„å¤§å°è¦ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†user å’Œ ä¸‹é¢è¦è®²åˆ°çš„spark memoryçš„åˆ†é…æ¯”ä¾‹ã€‚\n### Spark Memory\nä¸Šæ–‡å·²ç»åˆ°äº†ï¼Œspark memoryä¸»è¦æ˜¯sparkè‡ªå·±ä½¿ç”¨çš„memoryéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†çš„å¤§å°ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œå³(Java Heap - 300M)*spark.memory.fractionï¼Œå…¶ä¸­fractionçš„defaultä¸º0.75ã€‚\n\nSpark memoryä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼Œä¸€æ˜¯ç”¨äºsparkçš„shuffleç­‰æ“ä½œï¼Œè€Œæ˜¯ç”¨æ¥cache sparkä¸­çš„RDDï¼Œå› æ­¤spark memoryä¹Ÿè‡ªç„¶è€Œç„¶çš„åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼Œå³è´Ÿè´£shuffleæ“ä½œçš„execution memoryå’Œè´Ÿè´£cacheçš„storage memoryï¼Œä¸¤è€…çš„å¤§å°é€šè¿‡spark.memory.storageFractionå‚æ•°æ¥åˆ†å‰²ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚\n\nåœ¨spark memoryä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼Œé‚£å°±æ˜¯storage å’Œ execution memoryçš„å…±äº«æœºåˆ¶ï¼Œè¯´çš„ç®€å•ä¸€äº›å°±æ˜¯ï¼Œå½“ä¸€è¾¹å†…å­˜ç©ºé—²è€Œå¦ä¸€æ–¹å†…å­˜ç´§å¼ çš„æ—¶å€™ï¼Œå¯ä»¥å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œæˆ‘ä»¬ä¸‹é¢çœ‹çœ‹åœ¨å†…å­˜å‡ºç°å†²çªçš„æ—¶å€™ï¼Œsparkæ€ä¹ˆåè°ƒï¼š\n* å½“storageå ç”¨execution memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿexecution memoryä½¿ç”¨ç´§å¼ çš„æƒ…å†µæ—¶ï¼Œå¼ºåˆ¶å°†storageå æœ‰çš„å†…å­˜é‡Šæ”¾å¹¶å½’è¿˜executionï¼Œä¸¢å¤±çš„æ•°æ®å°†ä¼šåç»­é‡æ–°è®¡ç®—ï¼›\n* å½“executionå ç”¨storage memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿstorage memoryç´§å¼ çš„æƒ…å†µï¼Œè¢«å ç”¨çš„å†…å­˜ä¸ä¼šè¢«å¼ºåˆ¶é‡Šæ”¾ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥ä»»åŠ¡ä¸¢å¤±ï¼Œstorageä¼šè€å¿ƒç­‰å¾…çŸ¥é“executionæ‰§è¡Œå®Œé‡Šæ”¾å‡ºå†…å­˜ã€‚\n\n## Data Serialization\nåœ¨æ•´ä¸ªsparkä»»åŠ¡ä¸­ï¼Œæ•°æ®ä¼ è¾“éƒ½æ˜¯ç»è¿‡åºåˆ—åŒ–å(serialization)ä¹‹åä¼ è¾“çš„ï¼Œå› æ­¤æ•°æ®çš„åºåˆ—åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå†—ä½™çš„åºåˆ—åŒ–è¿‡ç¨‹ä¼šè®©æ•´ä¸ªsparkä»»åŠ¡å˜æ…¢ï¼Œsparkæä¾›ä¸¤ç§åºåˆ—åŒ–æ–¹å¼ï¼š\n* Java serializationï¼šè¿™æ˜¯sparké»˜è®¤çš„åºåˆ—åŒ–æ–¹å¼ï¼Œjavaåºåˆ—åŒ–æ˜¯ä¸€ç§å¾ˆç»å…¸å’Œç¨³å®šçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä½†æ˜¯æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯â€”â€”æ…¢ï¼\n* Kryo serializationï¼šKryo åºåˆ—åŒ–å¯ä»¥è®©sparkä»»åŠ¡æ›´åŠ å¿«é€Ÿï¼Œç”šè‡³10å€äºjavaåºåˆ—åŒ–ï¼›ä½†æ˜¯å®ƒä¸æ”¯æŒæ‰€æœ‰çš„Serializableç±»å‹ï¼ŒåŒæ—¶éœ€è¦ä¸ºç”¨æˆ·è‡ªå·±å¼€å‘çš„classè¿›è¡Œæ³¨å†Œåï¼Œæ‰å¯ä»¥ä½¿ç”¨Kyo.\n\nå…³äºKryoçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹[spark documentation](https://spark.apache.org/docs/latest/tuning.html#data-serialization)ï¼Œæˆ–è€…[Kryo documentation](https://github.com/EsotericSoftware/kryo)\n\n## Summary\nå…³äºsparkè°ƒä¼˜çš„é—®é¢˜ï¼Œæœ‰å¾ˆå¤šå› ç´ ï¼Œæˆ‘ä¹Ÿæ˜¯ç®€å•çš„åšäº†ä¸€äº›äº†è§£å¹¶åˆ†äº«ç»™å¤§å®¶ï¼Œé™¤äº†æˆ‘æåˆ°çš„ï¼Œè¿˜æœ‰è¯¸å¦‚GCç­‰ç­‰å› ç´ ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æˆ‘ç»™å‡ºçš„referencesåšè¿›ä¸€æ­¥çš„äº†è§£ã€‚\n\n## References\n* [How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n* [Spark Documentation-Tuning](https://spark.apache.org/docs/latest/tuning.html)\n* [Karau, Holden, et al. Learning spark: lightning-fast big data analysis. \" O'Reilly Media, Inc.\", 2015.](http://shop.oreilly.com/product/0636920028512.do)\n","slug":"spark-spark-tune","published":1,"updated":"2018-11-19T04:50:38.728Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pn001jtr8l5u5ay5ik","content":"<p>Hi, all, æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶spark tuningæ–¹é¢çš„é—®é¢˜ï¼Œæ·±æ„Ÿè¿™æ˜¯ä¸€ä¸ªç»éªŒæ´»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŠ€æœ¯æ´»ï¼ŒæŸ¥é˜…å’Œå¾ˆå¤šèµ„æ–™ï¼Œåœ¨è¿™é‡Œmarkä¸€ä¸‹ã€‚<br><a id=\"more\"></a><br>ä¸Šæ¬¡æˆ‘ä»¬reviewäº†ä¸€ä¸‹sparkçš„work-flowï¼Œä¸»è¦æ˜¯åŸºäºspark on yarnçš„ï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢è®¨çš„ä¹Ÿä¸»è¦æ˜¯åŸºäºspark on yarnã€‚</p>\n<h2 id=\"Resource-Allocation\"><a href=\"#Resource-Allocation\" class=\"headerlink\" title=\"Resource Allocation\"></a>Resource Allocation</h2><h3 id=\"Some-Configuration\"><a href=\"#Some-Configuration\" class=\"headerlink\" title=\"Some Configuration\"></a>Some Configuration</h3><p>Resource allocationæ˜¯sparkä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç¯èŠ‚ï¼Œç»™äºˆä¸€ä¸ªapplicationè¿‡å°‘çš„resourceä¼šå¸¦äº†æ‰§è¡Œæ•ˆç‡çš„ä½ä¸‹å’Œæ‰§è¡Œé€Ÿåº¦çš„ç¼“æ…¢ï¼›ç›¸åï¼Œè¿‡å¤šçš„resourceåˆ™ä¼šå¸¦æ¥èµ„æºæµªè´¹ï¼Œå½±å“clusterä¸Šå…¶ä»–appllicationçš„è¿è¡Œï¼Œå› æ­¤ï¼Œä¸€ä¸ªåˆé€‚çš„resource allocationæ˜¯éå¸¸éå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ¯”è¾ƒé‡è¦çš„parameterï¼š</p>\n<ul>\n<li>num-executors: è¡¨æ˜sparkç”³è¯·executorsçš„æ•°ç›®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®spark.dynamicAllocation.enabledæ¥è®©sparkæ ¹æ®æ•°æ®åŠ¨æ€çš„åˆ†é…executorsï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜èµ„æºåˆ©ç”¨ç‡ï¼›</li>\n<li>executor-cores: æŒ‡å®šæ¯ä¸€ä¸ªexecutorçš„coreæ•°ç›®ï¼Œcoreæ•°ç›®å†³å®šäº†æ¯ä¸ªexecutorçš„æœ€å¤§å¹¶è¡Œtaskæ•°ç›®</li>\n<li>executor-memory: æŒ‡å®šåˆ†é…ç»™æ¯ä¸€ä¸ªexecutorçš„å†…å­˜å¤§å°ã€‚</li>\n</ul>\n<h3 id=\"Some-Tips\"><a href=\"#Some-Tips\" class=\"headerlink\" title=\"Some Tips\"></a>Some Tips</h3><ul>\n<li>å¯¹äºexecutoræ¥è¯´ï¼Œåœ¨è¿‡äºå¤§çš„memoryä¸Šè¿è¡Œå¯èƒ½ä¼šå¸¦æ¥æ¯”è¾ƒé«˜çš„GC(gabage collection) timeï¼Œå¯¹äºä¸€ä¸ªexecutoræ¥è¯´ï¼Œå»ºè®®ç»™å‡ºçš„ä¸Šé™memoryæ˜¯64Gï¼›</li>\n<li>ç”±äºHDFSåœ¨å¹¶è¡Œè¯»å†™çš„æ—¶å€™å­˜åœ¨ä¸€äº›ç“¶é¢ˆï¼Œå› æ­¤æ¯ä¸€ä¸ªexecutorä¸­æœ€å¥½ä¸è¦è¶…è¿‡5ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå³coresæ•°ä¸è¦è¶…è¿‡5ä¸ªï¼Œæœ‰å®éªŒå¯ä»¥è¯æ˜ï¼Œsparkåœ¨å¤šexecutorå°‘coreçš„é…ç½®ä¸‹æ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼›</li>\n<li>ç›¸åçš„ï¼Œå¯¹äºexecutoræ¥è¯´ï¼Œè¿‡åˆ†å°‘çš„coreï¼Œä¾‹å¦‚1ä¸ªï¼Œå°†ä¼šä½¿å¾—executorsæ•°ç›®å˜å¤šï¼Œä¾‹å¦‚æŸä¸ªbroadcastè¿‡ç¨‹ï¼Œéœ€è¦ä¼ æ’­åˆ°æ‰€æœ‰çš„executorsä¸Šï¼Œé‚£ä¹ˆè¿‡åˆ†å¤šçš„executorsä¼šé™ä½æ‰§è¡Œçš„æ•ˆç‡ã€‚</li>\n</ul>\n<h2 id=\"Memory-Mangement\"><a href=\"#Memory-Mangement\" class=\"headerlink\" title=\"Memory Mangement\"></a>Memory Mangement</h2><p>å…³äºsparkä¸­çš„memory managementï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png\" alt=\"\"><br>åœ¨å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒsparkæŠŠmemoryåˆ†æˆäº†ä¸‰éƒ¨åˆ†ï¼Œå³spark memoryã€user memoryå’Œreserved memoryï¼Œæˆ‘ä»¬é¡ºæ¬¡æ¥çœ‹çœ‹ï¼š</p>\n<h3 id=\"Reserved-Memory\"><a href=\"#Reserved-Memory\" class=\"headerlink\" title=\"Reserved Memory\"></a>Reserved Memory</h3><p>æ‰€è°“reserved memoryï¼Œå®ƒå°±æ˜¯ç³»ç»Ÿé¢„ç•™ä¸‹çš„ä¸€éƒ¨åˆ†memoryï¼Œç”¨äºå­˜å‚¨sparkçš„å†…éƒ¨å¯¹è±¡ï¼Œé»˜è®¤å¤§å°ä¸º300mï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¸ä¼šä¿®æ”¹è¿™äº›å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“executorè¢«åˆ†é…çš„memoryå°äº1.5å€çš„reserved memoryæ—¶ï¼Œå°†ä¼šæŠ›å‡ºâ€œplease use larger heap sizeâ€çš„é”™è¯¯ã€‚</p>\n<h3 id=\"User-Memory\"><a href=\"#User-Memory\" class=\"headerlink\" title=\"User Memory\"></a>User Memory</h3><p>User memoryç”¨äºå‚¨å­˜sparkçš„transfermationçš„ä¸€äº›ä¿¡æ¯ï¼Œæ¯”å¦‚RDDä¹‹é—´çš„ä¾èµ–ä¿¡æ¯ç­‰ç­‰ï¼Œè¿™éƒ¨åˆ†å†…å­˜é»˜è®¤å¤§å°ä¸º(Java Heap - 300M)*0.25ï¼Œå…¶ä¸­çš„300Må…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„reserved memory.å…·ä½“çš„å¤§å°è¦ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†user å’Œ ä¸‹é¢è¦è®²åˆ°çš„spark memoryçš„åˆ†é…æ¯”ä¾‹ã€‚</p>\n<h3 id=\"Spark-Memory\"><a href=\"#Spark-Memory\" class=\"headerlink\" title=\"Spark Memory\"></a>Spark Memory</h3><p>ä¸Šæ–‡å·²ç»åˆ°äº†ï¼Œspark memoryä¸»è¦æ˜¯sparkè‡ªå·±ä½¿ç”¨çš„memoryéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†çš„å¤§å°ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œå³(Java Heap - 300M)*spark.memory.fractionï¼Œå…¶ä¸­fractionçš„defaultä¸º0.75ã€‚</p>\n<p>Spark memoryä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼Œä¸€æ˜¯ç”¨äºsparkçš„shuffleç­‰æ“ä½œï¼Œè€Œæ˜¯ç”¨æ¥cache sparkä¸­çš„RDDï¼Œå› æ­¤spark memoryä¹Ÿè‡ªç„¶è€Œç„¶çš„åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼Œå³è´Ÿè´£shuffleæ“ä½œçš„execution memoryå’Œè´Ÿè´£cacheçš„storage memoryï¼Œä¸¤è€…çš„å¤§å°é€šè¿‡spark.memory.storageFractionå‚æ•°æ¥åˆ†å‰²ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚</p>\n<p>åœ¨spark memoryä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼Œé‚£å°±æ˜¯storage å’Œ execution memoryçš„å…±äº«æœºåˆ¶ï¼Œè¯´çš„ç®€å•ä¸€äº›å°±æ˜¯ï¼Œå½“ä¸€è¾¹å†…å­˜ç©ºé—²è€Œå¦ä¸€æ–¹å†…å­˜ç´§å¼ çš„æ—¶å€™ï¼Œå¯ä»¥å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œæˆ‘ä»¬ä¸‹é¢çœ‹çœ‹åœ¨å†…å­˜å‡ºç°å†²çªçš„æ—¶å€™ï¼Œsparkæ€ä¹ˆåè°ƒï¼š</p>\n<ul>\n<li>å½“storageå ç”¨execution memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿexecution memoryä½¿ç”¨ç´§å¼ çš„æƒ…å†µæ—¶ï¼Œå¼ºåˆ¶å°†storageå æœ‰çš„å†…å­˜é‡Šæ”¾å¹¶å½’è¿˜executionï¼Œä¸¢å¤±çš„æ•°æ®å°†ä¼šåç»­é‡æ–°è®¡ç®—ï¼›</li>\n<li>å½“executionå ç”¨storage memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿstorage memoryç´§å¼ çš„æƒ…å†µï¼Œè¢«å ç”¨çš„å†…å­˜ä¸ä¼šè¢«å¼ºåˆ¶é‡Šæ”¾ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥ä»»åŠ¡ä¸¢å¤±ï¼Œstorageä¼šè€å¿ƒç­‰å¾…çŸ¥é“executionæ‰§è¡Œå®Œé‡Šæ”¾å‡ºå†…å­˜ã€‚</li>\n</ul>\n<h2 id=\"Data-Serialization\"><a href=\"#Data-Serialization\" class=\"headerlink\" title=\"Data Serialization\"></a>Data Serialization</h2><p>åœ¨æ•´ä¸ªsparkä»»åŠ¡ä¸­ï¼Œæ•°æ®ä¼ è¾“éƒ½æ˜¯ç»è¿‡åºåˆ—åŒ–å(serialization)ä¹‹åä¼ è¾“çš„ï¼Œå› æ­¤æ•°æ®çš„åºåˆ—åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå†—ä½™çš„åºåˆ—åŒ–è¿‡ç¨‹ä¼šè®©æ•´ä¸ªsparkä»»åŠ¡å˜æ…¢ï¼Œsparkæä¾›ä¸¤ç§åºåˆ—åŒ–æ–¹å¼ï¼š</p>\n<ul>\n<li>Java serializationï¼šè¿™æ˜¯sparké»˜è®¤çš„åºåˆ—åŒ–æ–¹å¼ï¼Œjavaåºåˆ—åŒ–æ˜¯ä¸€ç§å¾ˆç»å…¸å’Œç¨³å®šçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä½†æ˜¯æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯â€”â€”æ…¢ï¼</li>\n<li>Kryo serializationï¼šKryo åºåˆ—åŒ–å¯ä»¥è®©sparkä»»åŠ¡æ›´åŠ å¿«é€Ÿï¼Œç”šè‡³10å€äºjavaåºåˆ—åŒ–ï¼›ä½†æ˜¯å®ƒä¸æ”¯æŒæ‰€æœ‰çš„Serializableç±»å‹ï¼ŒåŒæ—¶éœ€è¦ä¸ºç”¨æˆ·è‡ªå·±å¼€å‘çš„classè¿›è¡Œæ³¨å†Œåï¼Œæ‰å¯ä»¥ä½¿ç”¨Kyo.</li>\n</ul>\n<p>å…³äºKryoçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹<a href=\"https://spark.apache.org/docs/latest/tuning.html#data-serialization\" target=\"_blank\" rel=\"noopener\">spark documentation</a>ï¼Œæˆ–è€…<a href=\"https://github.com/EsotericSoftware/kryo\" target=\"_blank\" rel=\"noopener\">Kryo documentation</a></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>å…³äºsparkè°ƒä¼˜çš„é—®é¢˜ï¼Œæœ‰å¾ˆå¤šå› ç´ ï¼Œæˆ‘ä¹Ÿæ˜¯ç®€å•çš„åšäº†ä¸€äº›äº†è§£å¹¶åˆ†äº«ç»™å¤§å®¶ï¼Œé™¤äº†æˆ‘æåˆ°çš„ï¼Œè¿˜æœ‰è¯¸å¦‚GCç­‰ç­‰å› ç´ ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æˆ‘ç»™å‡ºçš„referencesåšè¿›ä¸€æ­¥çš„äº†è§£ã€‚</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\" target=\"_blank\" rel=\"noopener\">How-to: Tune Your Apache Spark Jobs (Part 2)</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/tuning.html\" target=\"_blank\" rel=\"noopener\">Spark Documentation-Tuning</a></li>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015.</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p>Hi, all, æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶spark tuningæ–¹é¢çš„é—®é¢˜ï¼Œæ·±æ„Ÿè¿™æ˜¯ä¸€ä¸ªç»éªŒæ´»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŠ€æœ¯æ´»ï¼ŒæŸ¥é˜…å’Œå¾ˆå¤šèµ„æ–™ï¼Œåœ¨è¿™é‡Œmarkä¸€ä¸‹ã€‚<br></p>","more":"<br>ä¸Šæ¬¡æˆ‘ä»¬reviewäº†ä¸€ä¸‹sparkçš„work-flowï¼Œä¸»è¦æ˜¯åŸºäºspark on yarnçš„ï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢è®¨çš„ä¹Ÿä¸»è¦æ˜¯åŸºäºspark on yarnã€‚</p>\n<h2 id=\"Resource-Allocation\"><a href=\"#Resource-Allocation\" class=\"headerlink\" title=\"Resource Allocation\"></a>Resource Allocation</h2><h3 id=\"Some-Configuration\"><a href=\"#Some-Configuration\" class=\"headerlink\" title=\"Some Configuration\"></a>Some Configuration</h3><p>Resource allocationæ˜¯sparkä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç¯èŠ‚ï¼Œç»™äºˆä¸€ä¸ªapplicationè¿‡å°‘çš„resourceä¼šå¸¦äº†æ‰§è¡Œæ•ˆç‡çš„ä½ä¸‹å’Œæ‰§è¡Œé€Ÿåº¦çš„ç¼“æ…¢ï¼›ç›¸åï¼Œè¿‡å¤šçš„resourceåˆ™ä¼šå¸¦æ¥èµ„æºæµªè´¹ï¼Œå½±å“clusterä¸Šå…¶ä»–appllicationçš„è¿è¡Œï¼Œå› æ­¤ï¼Œä¸€ä¸ªåˆé€‚çš„resource allocationæ˜¯éå¸¸éå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ¯”è¾ƒé‡è¦çš„parameterï¼š</p>\n<ul>\n<li>num-executors: è¡¨æ˜sparkç”³è¯·executorsçš„æ•°ç›®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®spark.dynamicAllocation.enabledæ¥è®©sparkæ ¹æ®æ•°æ®åŠ¨æ€çš„åˆ†é…executorsï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜èµ„æºåˆ©ç”¨ç‡ï¼›</li>\n<li>executor-cores: æŒ‡å®šæ¯ä¸€ä¸ªexecutorçš„coreæ•°ç›®ï¼Œcoreæ•°ç›®å†³å®šäº†æ¯ä¸ªexecutorçš„æœ€å¤§å¹¶è¡Œtaskæ•°ç›®</li>\n<li>executor-memory: æŒ‡å®šåˆ†é…ç»™æ¯ä¸€ä¸ªexecutorçš„å†…å­˜å¤§å°ã€‚</li>\n</ul>\n<h3 id=\"Some-Tips\"><a href=\"#Some-Tips\" class=\"headerlink\" title=\"Some Tips\"></a>Some Tips</h3><ul>\n<li>å¯¹äºexecutoræ¥è¯´ï¼Œåœ¨è¿‡äºå¤§çš„memoryä¸Šè¿è¡Œå¯èƒ½ä¼šå¸¦æ¥æ¯”è¾ƒé«˜çš„GC(gabage collection) timeï¼Œå¯¹äºä¸€ä¸ªexecutoræ¥è¯´ï¼Œå»ºè®®ç»™å‡ºçš„ä¸Šé™memoryæ˜¯64Gï¼›</li>\n<li>ç”±äºHDFSåœ¨å¹¶è¡Œè¯»å†™çš„æ—¶å€™å­˜åœ¨ä¸€äº›ç“¶é¢ˆï¼Œå› æ­¤æ¯ä¸€ä¸ªexecutorä¸­æœ€å¥½ä¸è¦è¶…è¿‡5ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå³coresæ•°ä¸è¦è¶…è¿‡5ä¸ªï¼Œæœ‰å®éªŒå¯ä»¥è¯æ˜ï¼Œsparkåœ¨å¤šexecutorå°‘coreçš„é…ç½®ä¸‹æ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼›</li>\n<li>ç›¸åçš„ï¼Œå¯¹äºexecutoræ¥è¯´ï¼Œè¿‡åˆ†å°‘çš„coreï¼Œä¾‹å¦‚1ä¸ªï¼Œå°†ä¼šä½¿å¾—executorsæ•°ç›®å˜å¤šï¼Œä¾‹å¦‚æŸä¸ªbroadcastè¿‡ç¨‹ï¼Œéœ€è¦ä¼ æ’­åˆ°æ‰€æœ‰çš„executorsä¸Šï¼Œé‚£ä¹ˆè¿‡åˆ†å¤šçš„executorsä¼šé™ä½æ‰§è¡Œçš„æ•ˆç‡ã€‚</li>\n</ul>\n<h2 id=\"Memory-Mangement\"><a href=\"#Memory-Mangement\" class=\"headerlink\" title=\"Memory Mangement\"></a>Memory Mangement</h2><p>å…³äºsparkä¸­çš„memory managementï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼š<br><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/16/16-1.png\" alt=\"\"><br>åœ¨å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒsparkæŠŠmemoryåˆ†æˆäº†ä¸‰éƒ¨åˆ†ï¼Œå³spark memoryã€user memoryå’Œreserved memoryï¼Œæˆ‘ä»¬é¡ºæ¬¡æ¥çœ‹çœ‹ï¼š</p>\n<h3 id=\"Reserved-Memory\"><a href=\"#Reserved-Memory\" class=\"headerlink\" title=\"Reserved Memory\"></a>Reserved Memory</h3><p>æ‰€è°“reserved memoryï¼Œå®ƒå°±æ˜¯ç³»ç»Ÿé¢„ç•™ä¸‹çš„ä¸€éƒ¨åˆ†memoryï¼Œç”¨äºå­˜å‚¨sparkçš„å†…éƒ¨å¯¹è±¡ï¼Œé»˜è®¤å¤§å°ä¸º300mï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¸ä¼šä¿®æ”¹è¿™äº›å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“executorè¢«åˆ†é…çš„memoryå°äº1.5å€çš„reserved memoryæ—¶ï¼Œå°†ä¼šæŠ›å‡ºâ€œplease use larger heap sizeâ€çš„é”™è¯¯ã€‚</p>\n<h3 id=\"User-Memory\"><a href=\"#User-Memory\" class=\"headerlink\" title=\"User Memory\"></a>User Memory</h3><p>User memoryç”¨äºå‚¨å­˜sparkçš„transfermationçš„ä¸€äº›ä¿¡æ¯ï¼Œæ¯”å¦‚RDDä¹‹é—´çš„ä¾èµ–ä¿¡æ¯ç­‰ç­‰ï¼Œè¿™éƒ¨åˆ†å†…å­˜é»˜è®¤å¤§å°ä¸º(Java Heap - 300M)*0.25ï¼Œå…¶ä¸­çš„300Må…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„reserved memory.å…·ä½“çš„å¤§å°è¦ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†user å’Œ ä¸‹é¢è¦è®²åˆ°çš„spark memoryçš„åˆ†é…æ¯”ä¾‹ã€‚</p>\n<h3 id=\"Spark-Memory\"><a href=\"#Spark-Memory\" class=\"headerlink\" title=\"Spark Memory\"></a>Spark Memory</h3><p>ä¸Šæ–‡å·²ç»åˆ°äº†ï¼Œspark memoryä¸»è¦æ˜¯sparkè‡ªå·±ä½¿ç”¨çš„memoryéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†çš„å¤§å°ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œå³(Java Heap - 300M)*spark.memory.fractionï¼Œå…¶ä¸­fractionçš„defaultä¸º0.75ã€‚</p>\n<p>Spark memoryä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼Œä¸€æ˜¯ç”¨äºsparkçš„shuffleç­‰æ“ä½œï¼Œè€Œæ˜¯ç”¨æ¥cache sparkä¸­çš„RDDï¼Œå› æ­¤spark memoryä¹Ÿè‡ªç„¶è€Œç„¶çš„åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼Œå³è´Ÿè´£shuffleæ“ä½œçš„execution memoryå’Œè´Ÿè´£cacheçš„storage memoryï¼Œä¸¤è€…çš„å¤§å°é€šè¿‡spark.memory.storageFractionå‚æ•°æ¥åˆ†å‰²ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚</p>\n<p>åœ¨spark memoryä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼Œé‚£å°±æ˜¯storage å’Œ execution memoryçš„å…±äº«æœºåˆ¶ï¼Œè¯´çš„ç®€å•ä¸€äº›å°±æ˜¯ï¼Œå½“ä¸€è¾¹å†…å­˜ç©ºé—²è€Œå¦ä¸€æ–¹å†…å­˜ç´§å¼ çš„æ—¶å€™ï¼Œå¯ä»¥å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œæˆ‘ä»¬ä¸‹é¢çœ‹çœ‹åœ¨å†…å­˜å‡ºç°å†²çªçš„æ—¶å€™ï¼Œsparkæ€ä¹ˆåè°ƒï¼š</p>\n<ul>\n<li>å½“storageå ç”¨execution memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿexecution memoryä½¿ç”¨ç´§å¼ çš„æƒ…å†µæ—¶ï¼Œå¼ºåˆ¶å°†storageå æœ‰çš„å†…å­˜é‡Šæ”¾å¹¶å½’è¿˜executionï¼Œä¸¢å¤±çš„æ•°æ®å°†ä¼šåç»­é‡æ–°è®¡ç®—ï¼›</li>\n<li>å½“executionå ç”¨storage memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿstorage memoryç´§å¼ çš„æƒ…å†µï¼Œè¢«å ç”¨çš„å†…å­˜ä¸ä¼šè¢«å¼ºåˆ¶é‡Šæ”¾ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥ä»»åŠ¡ä¸¢å¤±ï¼Œstorageä¼šè€å¿ƒç­‰å¾…çŸ¥é“executionæ‰§è¡Œå®Œé‡Šæ”¾å‡ºå†…å­˜ã€‚</li>\n</ul>\n<h2 id=\"Data-Serialization\"><a href=\"#Data-Serialization\" class=\"headerlink\" title=\"Data Serialization\"></a>Data Serialization</h2><p>åœ¨æ•´ä¸ªsparkä»»åŠ¡ä¸­ï¼Œæ•°æ®ä¼ è¾“éƒ½æ˜¯ç»è¿‡åºåˆ—åŒ–å(serialization)ä¹‹åä¼ è¾“çš„ï¼Œå› æ­¤æ•°æ®çš„åºåˆ—åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå†—ä½™çš„åºåˆ—åŒ–è¿‡ç¨‹ä¼šè®©æ•´ä¸ªsparkä»»åŠ¡å˜æ…¢ï¼Œsparkæä¾›ä¸¤ç§åºåˆ—åŒ–æ–¹å¼ï¼š</p>\n<ul>\n<li>Java serializationï¼šè¿™æ˜¯sparké»˜è®¤çš„åºåˆ—åŒ–æ–¹å¼ï¼Œjavaåºåˆ—åŒ–æ˜¯ä¸€ç§å¾ˆç»å…¸å’Œç¨³å®šçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä½†æ˜¯æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯â€”â€”æ…¢ï¼</li>\n<li>Kryo serializationï¼šKryo åºåˆ—åŒ–å¯ä»¥è®©sparkä»»åŠ¡æ›´åŠ å¿«é€Ÿï¼Œç”šè‡³10å€äºjavaåºåˆ—åŒ–ï¼›ä½†æ˜¯å®ƒä¸æ”¯æŒæ‰€æœ‰çš„Serializableç±»å‹ï¼ŒåŒæ—¶éœ€è¦ä¸ºç”¨æˆ·è‡ªå·±å¼€å‘çš„classè¿›è¡Œæ³¨å†Œåï¼Œæ‰å¯ä»¥ä½¿ç”¨Kyo.</li>\n</ul>\n<p>å…³äºKryoçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹<a href=\"https://spark.apache.org/docs/latest/tuning.html#data-serialization\" target=\"_blank\" rel=\"noopener\">spark documentation</a>ï¼Œæˆ–è€…<a href=\"https://github.com/EsotericSoftware/kryo\" target=\"_blank\" rel=\"noopener\">Kryo documentation</a></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>å…³äºsparkè°ƒä¼˜çš„é—®é¢˜ï¼Œæœ‰å¾ˆå¤šå› ç´ ï¼Œæˆ‘ä¹Ÿæ˜¯ç®€å•çš„åšäº†ä¸€äº›äº†è§£å¹¶åˆ†äº«ç»™å¤§å®¶ï¼Œé™¤äº†æˆ‘æåˆ°çš„ï¼Œè¿˜æœ‰è¯¸å¦‚GCç­‰ç­‰å› ç´ ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æˆ‘ç»™å‡ºçš„referencesåšè¿›ä¸€æ­¥çš„äº†è§£ã€‚</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\" target=\"_blank\" rel=\"noopener\">How-to: Tune Your Apache Spark Jobs (Part 2)</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/tuning.html\" target=\"_blank\" rel=\"noopener\">Spark Documentation-Tuning</a></li>\n<li><a href=\"http://shop.oreilly.com/product/0636920028512.do\" target=\"_blank\" rel=\"noopener\">Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015.</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-2","date":"2018-10-13T01:53:02.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg)\nThis article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I've tried a lot to learn about Apache Spark but can't know the detail of every part of it. I'd appreciate it if you figure out the mistakes in this article.\n<!--more-->\n## Coalesce\nChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, *repartition()* and *coalesce()* is proposed in Apache Spark. \nBefore talking about the detail about *coalesce()*, let's review the concept of transformation with wide-dependencies and narrow-dependencies.\n> * Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.\n* Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.\n\nAccording to the definition above, *repartition()* is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about *coalesce()*? To find out more about it, let's see the definition first.\n```scala\n// coalesce() for RDD is defined in org.apache.spark.rdd.RDD\ndef coalesce(numPartitions: Int, shuffle: Boolean = false,\n    partitionCoalescer: Option[PartitionCoalescer] = Option.empty) \n    (implicit ord: Ordering[T] = null)\n   : RDD[T] = withScope {...}\n\n// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset\ndef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n}\n```\nFor RDDs, *coalesce()* has a boolean typed parameter called *shuffle*. The *coalesce()* can be treated as *repartition()* as *shuffle* is set to *True*, which is a transformation with wide-dependencies. In contrast, when *shuffle* is *False*, *coalesce()* is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter *numPartitions*. As for DataFrame/Dataset API, *coalesce()* is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, *coalesce()* cannot increase the number of DataFrame/Dataset's partitions and can only be used to reduce DataFrame/Dataset's partitions.\nAfter understanding the above, there is a crucial tip for you. When you use *coalesce()* and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by *coalesce()*. To avoid this problem, you can set *shuffle=True* for RDDs or use *repartition()* instead for DataFrame/Dataset to split the whole stage by a shuffle.\n## Read ORC Table\nReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.\n> ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. \n\nGenerally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That's caused by ORC split strategy set by *hive.exec.orc.split.strategy*, which determines what strategy ORC should use to create splits for execution. The available option includes \"BI\", \"ETL\" and \"HYBRID\"\n> The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.\n\nAs results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it's better for us to decide by the actual situation.  \n\n## Reference\n* [Managing Spark Partitions with Coalesce and Repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4)\n* [High Performence Spark](http://opencarts.org/sachlaptrinh/pdf/28044.pdf)\n* [Apache ORC](https://orc.apache.org/)\n* [Apache ORC Configuration Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy)\n","source":"_posts/spark-sumup-part-2.md","raw":"---\ntitle: Spark Tips Sum-up Part-2\ndate: 2018-10-13 09:53:02\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg)\nThis article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. I've tried a lot to learn about Apache Spark but can't know the detail of every part of it. I'd appreciate it if you figure out the mistakes in this article.\n<!--more-->\n## Coalesce\nChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, *repartition()* and *coalesce()* is proposed in Apache Spark. \nBefore talking about the detail about *coalesce()*, let's review the concept of transformation with wide-dependencies and narrow-dependencies.\n> * Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.\n* Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.\n\nAccording to the definition above, *repartition()* is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about *coalesce()*? To find out more about it, let's see the definition first.\n```scala\n// coalesce() for RDD is defined in org.apache.spark.rdd.RDD\ndef coalesce(numPartitions: Int, shuffle: Boolean = false,\n    partitionCoalescer: Option[PartitionCoalescer] = Option.empty) \n    (implicit ord: Ordering[T] = null)\n   : RDD[T] = withScope {...}\n\n// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset\ndef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n}\n```\nFor RDDs, *coalesce()* has a boolean typed parameter called *shuffle*. The *coalesce()* can be treated as *repartition()* as *shuffle* is set to *True*, which is a transformation with wide-dependencies. In contrast, when *shuffle* is *False*, *coalesce()* is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter *numPartitions*. As for DataFrame/Dataset API, *coalesce()* is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, *coalesce()* cannot increase the number of DataFrame/Dataset's partitions and can only be used to reduce DataFrame/Dataset's partitions.\nAfter understanding the above, there is a crucial tip for you. When you use *coalesce()* and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by *coalesce()*. To avoid this problem, you can set *shuffle=True* for RDDs or use *repartition()* instead for DataFrame/Dataset to split the whole stage by a shuffle.\n## Read ORC Table\nReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.\n> ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. \n\nGenerally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. That's caused by ORC split strategy set by *hive.exec.orc.split.strategy*, which determines what strategy ORC should use to create splits for execution. The available option includes \"BI\", \"ETL\" and \"HYBRID\"\n> The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.\n\nAs results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and it's better for us to decide by the actual situation.  \n\n## Reference\n* [Managing Spark Partitions with Coalesce and Repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4)\n* [High Performence Spark](http://opencarts.org/sachlaptrinh/pdf/28044.pdf)\n* [Apache ORC](https://orc.apache.org/)\n* [Apache ORC Configuration Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy)\n","slug":"spark-sumup-part-2","published":1,"updated":"2018-11-19T04:48:53.715Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4po001otr8lqs108ij8","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg\" alt=\"\"><br>This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. Iâ€™ve tried a lot to learn about Apache Spark but canâ€™t know the detail of every part of it. Iâ€™d appreciate it if you figure out the mistakes in this article.<br><a id=\"more\"></a></p>\n<h2 id=\"Coalesce\"><a href=\"#Coalesce\" class=\"headerlink\" title=\"Coalesce\"></a>Coalesce</h2><p>Changing the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, <em>repartition()</em> and <em>coalesce()</em> is proposed in Apache Spark.<br>Before talking about the detail about <em>coalesce()</em>, letâ€™s review the concept of transformation with wide-dependencies and narrow-dependencies.</p>\n<blockquote>\n<ul>\n<li>Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.</li>\n<li>Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.</li>\n</ul>\n</blockquote>\n<p>According to the definition above, <em>repartition()</em> is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about <em>coalesce()</em>? To find out more about it, letâ€™s see the definition first.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-comment\">// coalesce() for RDD is defined in org.apache.spark.rdd.RDD</span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">coalesce</span></span>(numPartitions: <span class=\"hljs-type\">Int</span>, shuffle: <span class=\"hljs-type\">Boolean</span> = <span class=\"hljs-literal\">false</span>,</span><br><span class=\"line\">    partitionCoalescer: <span class=\"hljs-type\">Option</span>[<span class=\"hljs-type\">PartitionCoalescer</span>] = <span class=\"hljs-type\">Option</span>.empty) </span><br><span class=\"line\">    (<span class=\"hljs-keyword\">implicit</span> ord: <span class=\"hljs-type\">Ordering</span>[<span class=\"hljs-type\">T</span>] = <span class=\"hljs-literal\">null</span>)</span><br><span class=\"line\">   : <span class=\"hljs-type\">RDD</span>[<span class=\"hljs-type\">T</span>] = withScope &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"hljs-comment\">// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset</span></span><br><span class=\"line\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">coalesce</span></span>(numPartitions: <span class=\"hljs-type\">Int</span>): <span class=\"hljs-type\">Dataset</span>[<span class=\"hljs-type\">T</span>] = withTypedPlan &#123;</span><br><span class=\"line\">    <span class=\"hljs-type\">Repartition</span>(numPartitions, shuffle = <span class=\"hljs-literal\">false</span>, logicalPlan)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>For RDDs, <em>coalesce()</em> has a boolean typed parameter called <em>shuffle</em>. The <em>coalesce()</em> can be treated as <em>repartition()</em> as <em>shuffle</em> is set to <em>True</em>, which is a transformation with wide-dependencies. In contrast, when <em>shuffle</em> is <em>False</em>, <em>coalesce()</em> is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter <em>numPartitions</em>. As for DataFrame/Dataset API, <em>coalesce()</em> is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, <em>coalesce()</em> cannot increase the number of DataFrame/Datasetâ€™s partitions and can only be used to reduce DataFrame/Datasetâ€™s partitions.<br>After understanding the above, there is a crucial tip for you. When you use <em>coalesce()</em> and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by <em>coalesce()</em>. To avoid this problem, you can set <em>shuffle=True</em> for RDDs or use <em>repartition()</em> instead for DataFrame/Dataset to split the whole stage by a shuffle.</p>\n<h2 id=\"Read-ORC-Table\"><a href=\"#Read-ORC-Table\" class=\"headerlink\" title=\"Read ORC Table\"></a>Read ORC Table</h2><p>Reading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.</p>\n<blockquote>\n<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. </p>\n</blockquote>\n<p>Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. Thatâ€™s caused by ORC split strategy set by <em>hive.exec.orc.split.strategy</em>, which determines what strategy ORC should use to create splits for execution. The available option includes â€œBIâ€, â€œETLâ€ and â€œHYBRIDâ€</p>\n<blockquote>\n<p>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p>\n</blockquote>\n<p>As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and itâ€™s better for us to decide by the actual situation.  </p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\" target=\"_blank\" rel=\"noopener\">Managing Spark Partitions with Coalesce and Repartition</a></li>\n<li><a href=\"http://opencarts.org/sachlaptrinh/pdf/28044.pdf\" target=\"_blank\" rel=\"noopener\">High Performence Spark</a></li>\n<li><a href=\"https://orc.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache ORC</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy\" target=\"_blank\" rel=\"noopener\">Apache ORC Configuration Properties</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/boulders-calm-conifers-426894.jpg\" alt=\"\"><br>This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. Iâ€™ve tried a lot to learn about Apache Spark but canâ€™t know the detail of every part of it. Iâ€™d appreciate it if you figure out the mistakes in this article.<br></p>","more":"</p>\n<h2 id=\"Coalesce\"><a href=\"#Coalesce\" class=\"headerlink\" title=\"Coalesce\"></a>Coalesce</h2><p>Changing the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, <em>repartition()</em> and <em>coalesce()</em> is proposed in Apache Spark.<br>Before talking about the detail about <em>coalesce()</em>, letâ€™s review the concept of transformation with wide-dependencies and narrow-dependencies.</p>\n<blockquote>\n<ul>\n<li>Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.</li>\n<li>Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD.</li>\n</ul>\n</blockquote>\n<p>According to the definition above, <em>repartition()</em> is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about <em>coalesce()</em>? To find out more about it, letâ€™s see the definition first.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// coalesce() for RDD is defined in org.apache.spark.rdd.RDD</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coalesce</span></span>(numPartitions: <span class=\"type\">Int</span>, shuffle: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>,</span><br><span class=\"line\">    partitionCoalescer: <span class=\"type\">Option</span>[<span class=\"type\">PartitionCoalescer</span>] = <span class=\"type\">Option</span>.empty) </span><br><span class=\"line\">    (<span class=\"keyword\">implicit</span> ord: <span class=\"type\">Ordering</span>[<span class=\"type\">T</span>] = <span class=\"literal\">null</span>)</span><br><span class=\"line\">   : <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;...&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// coalesce() for Dataset is defined in org.apache.spark.sql.Dataset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coalesce</span></span>(numPartitions: <span class=\"type\">Int</span>): <span class=\"type\">Dataset</span>[<span class=\"type\">T</span>] = withTypedPlan &#123;</span><br><span class=\"line\">    <span class=\"type\">Repartition</span>(numPartitions, shuffle = <span class=\"literal\">false</span>, logicalPlan)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>For RDDs, <em>coalesce()</em> has a boolean typed parameter called <em>shuffle</em>. The <em>coalesce()</em> can be treated as <em>repartition()</em> as <em>shuffle</em> is set to <em>True</em>, which is a transformation with wide-dependencies. In contrast, when <em>shuffle</em> is <em>False</em>, <em>coalesce()</em> is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter <em>numPartitions</em>. As for DataFrame/Dataset API, <em>coalesce()</em> is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, <em>coalesce()</em> cannot increase the number of DataFrame/Datasetâ€™s partitions and can only be used to reduce DataFrame/Datasetâ€™s partitions.<br>After understanding the above, there is a crucial tip for you. When you use <em>coalesce()</em> and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by <em>coalesce()</em>. To avoid this problem, you can set <em>shuffle=True</em> for RDDs or use <em>repartition()</em> instead for DataFrame/Dataset to split the whole stage by a shuffle.</p>\n<h2 id=\"Read-ORC-Table\"><a href=\"#Read-ORC-Table\" class=\"headerlink\" title=\"Read ORC Table\"></a>Read ORC Table</h2><p>Reading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table.</p>\n<blockquote>\n<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. </p>\n</blockquote>\n<p>Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. Thatâ€™s caused by ORC split strategy set by <em>hive.exec.orc.split.strategy</em>, which determines what strategy ORC should use to create splits for execution. The available option includes â€œBIâ€, â€œETLâ€ and â€œHYBRIDâ€</p>\n<blockquote>\n<p>The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.</p>\n</blockquote>\n<p>As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and itâ€™s better for us to decide by the actual situation.  </p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\" target=\"_blank\" rel=\"noopener\">Managing Spark Partitions with Coalesce and Repartition</a></li>\n<li><a href=\"http://opencarts.org/sachlaptrinh/pdf/28044.pdf\" target=\"_blank\" rel=\"noopener\">High Performence Spark</a></li>\n<li><a href=\"https://orc.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache ORC</a></li>\n<li><a href=\"https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.split.strategy\" target=\"_blank\" rel=\"noopener\">Apache ORC Configuration Properties</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-1","date":"2018-09-15T14:15:40.000Z","toc":"ture","_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg)\nThis article is about things I learned about Apache Spark recently. I've been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I've learned from my work. This article is part 1 and here we go.\n<!--more-->\n## RDD vs DataFrame Partition Number in Shuffle \nShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.\nDuring my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.\nAfter searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration *spark.sql.shuffle.partitions* with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration *spark.sql.shuffle.partitions* when you want to modify the DataFrame partition number. \nWhat if I want to modify the partition number of a RDD? Actually, the configuration *spark.default.parallelism*, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling *repartition( )* or *coalese( )*, which is effective for both DataFrames and RDDs.\n\n## Smart Action\nAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.\nWhen I'm tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action *show( )* takes shorter time than *createOrReplaceTempView( )*, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the *show( )* action than those for *createOrReplaceTempView( )*.\n\n## Broadcast Joins\nBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by *spark.sql.autoBroadcastJoinThreshold*, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the *broadcast* function must be imported or Spark wouldn't broadcast data even if the size is below the threshold.\n```scala\nval df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\")\n```\nAlso, you can enlarge the value of *spark.sql.autoBroadcastJoinThreshold* so that larger table can also be broadcast, but the memory of your application should be paid attention. \nBroadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!\n\n## References\n* [Spark SQL Programming guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options)\n* [Mastering Spark SQL](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html)\n","source":"_posts/spark-sumup-part-1.md","raw":"---\ntitle: Spark Tips Sum-up Part-1\ndate: 2018-09-15 22:15:40\ntags: spark\ncategories: spark\ntoc: ture\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg)\nThis article is about things I learned about Apache Spark recently. I've been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips I've learned from my work. This article is part 1 and here we go.\n<!--more-->\n## RDD vs DataFrame Partition Number in Shuffle \nShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.\nDuring my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.\nAfter searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration *spark.sql.shuffle.partitions* with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration *spark.sql.shuffle.partitions* when you want to modify the DataFrame partition number. \nWhat if I want to modify the partition number of a RDD? Actually, the configuration *spark.default.parallelism*, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling *repartition( )* or *coalese( )*, which is effective for both DataFrames and RDDs.\n\n## Smart Action\nAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.\nWhen I'm tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action *show( )* takes shorter time than *createOrReplaceTempView( )*, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the *show( )* action than those for *createOrReplaceTempView( )*.\n\n## Broadcast Joins\nBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by *spark.sql.autoBroadcastJoinThreshold*, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the *broadcast* function must be imported or Spark wouldn't broadcast data even if the size is below the threshold.\n```scala\nval df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\")\n```\nAlso, you can enlarge the value of *spark.sql.autoBroadcastJoinThreshold* so that larger table can also be broadcast, but the memory of your application should be paid attention. \nBroadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!\n\n## References\n* [Spark SQL Programming guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options)\n* [Mastering Spark SQL](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html)\n","slug":"spark-sumup-part-1","published":1,"updated":"2018-11-19T13:49:25.169Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pp001ptr8lov9md1mw","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg\" alt=\"\"><br>This article is about things I learned about Apache Spark recently. Iâ€™ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips Iâ€™ve learned from my work. This article is part 1 and here we go.<br><a id=\"more\"></a></p>\n<h2 id=\"RDD-vs-DataFrame-Partition-Number-in-Shuffle\"><a href=\"#RDD-vs-DataFrame-Partition-Number-in-Shuffle\" class=\"headerlink\" title=\"RDD vs DataFrame Partition Number in Shuffle\"></a>RDD vs DataFrame Partition Number in Shuffle</h2><p>Shuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.<br>During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.<br>After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration <em>spark.sql.shuffle.partitions</em> with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration <em>spark.sql.shuffle.partitions</em> when you want to modify the DataFrame partition number.<br>What if I want to modify the partition number of a RDD? Actually, the configuration <em>spark.default.parallelism</em>, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling <em>repartition( )</em> or <em>coalese( )</em>, which is effective for both DataFrames and RDDs.</p>\n<h2 id=\"Smart-Action\"><a href=\"#Smart-Action\" class=\"headerlink\" title=\"Smart Action\"></a>Smart Action</h2><p>As we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.<br>When Iâ€™m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action <em>show( )</em> takes shorter time than <em>createOrReplaceTempView( )</em>, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the <em>show( )</em> action than those for <em>createOrReplaceTempView( )</em>.</p>\n<h2 id=\"Broadcast-Joins\"><a href=\"#Broadcast-Joins\" class=\"headerlink\" title=\"Broadcast Joins\"></a>Broadcast Joins</h2><p>Broadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by <em>spark.sql.autoBroadcastJoinThreshold</em>, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the <em>broadcast</em> function must be imported or Spark wouldnâ€™t broadcast data even if the size is below the threshold.<br><figure class=\"highlight scala hljs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"hljs-keyword\">val</span> df = largeDF.join(broadcast(smallDF),<span class=\"hljs-type\">Seq</span>(<span class=\"hljs-string\">\"col1\"</span>,<span class=\"hljs-string\">\"col2\"</span>),<span class=\"hljs-string\">\"left\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Also, you can enlarge the value of <em>spark.sql.autoBroadcastJoinThreshold</em> so that larger table can also be broadcast, but the memory of your application should be paid attention.<br>Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options\" target=\"_blank\" rel=\"noopener\">Spark SQL Programming guide</a></li>\n<li><a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html\" target=\"_blank\" rel=\"noopener\">Mastering Spark SQL</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/blurred-background-close-up-coffee-cup.jpg\" alt=\"\"><br>This article is about things I learned about Apache Spark recently. Iâ€™ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips Iâ€™ve learned from my work. This article is part 1 and here we go.<br></p>","more":"</p>\n<h2 id=\"RDD-vs-DataFrame-Partition-Number-in-Shuffle\"><a href=\"#RDD-vs-DataFrame-Partition-Number-in-Shuffle\" class=\"headerlink\" title=\"RDD vs DataFrame Partition Number in Shuffle\"></a>RDD vs DataFrame Partition Number in Shuffle</h2><p>Shuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.<br>During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.<br>After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration <em>spark.sql.shuffle.partitions</em> with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration <em>spark.sql.shuffle.partitions</em> when you want to modify the DataFrame partition number.<br>What if I want to modify the partition number of a RDD? Actually, the configuration <em>spark.default.parallelism</em>, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling <em>repartition( )</em> or <em>coalese( )</em>, which is effective for both DataFrames and RDDs.</p>\n<h2 id=\"Smart-Action\"><a href=\"#Smart-Action\" class=\"headerlink\" title=\"Smart Action\"></a>Smart Action</h2><p>As we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.<br>When Iâ€™m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action <em>show( )</em> takes shorter time than <em>createOrReplaceTempView( )</em>, which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the <em>show( )</em> action than those for <em>createOrReplaceTempView( )</em>.</p>\n<h2 id=\"Broadcast-Joins\"><a href=\"#Broadcast-Joins\" class=\"headerlink\" title=\"Broadcast Joins\"></a>Broadcast Joins</h2><p>Broadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by <em>spark.sql.autoBroadcastJoinThreshold</em>, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the <em>broadcast</em> function must be imported or Spark wouldnâ€™t broadcast data even if the size is below the threshold.<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> df = largeDF.join(broadcast(smallDF),<span class=\"type\">Seq</span>(<span class=\"string\">\"col1\"</span>,<span class=\"string\">\"col2\"</span>),<span class=\"string\">\"left\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>Also, you can enlarge the value of <em>spark.sql.autoBroadcastJoinThreshold</em> so that larger table can also be broadcast, but the memory of your application should be paid attention.<br>Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try!</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options\" target=\"_blank\" rel=\"noopener\">Spark SQL Programming guide</a></li>\n<li><a href=\"https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins-broadcast.html\" target=\"_blank\" rel=\"noopener\">Mastering Spark SQL</a></li>\n</ul>"},{"title":"Spark Tips Sum-up Part-3","date":"2019-03-06T10:24:15.000Z","toc":true,"_content":"![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg)\nThis blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I'll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!  \n<!--more-->\n## Explode\nSpark SQL provides a varority of functions in *org.apache.spark.sql.functions* for you to restruct your data, one of which is the *explode()* function. Since Spark 2.3, *explode()* function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, [this issue](https://issues.apache.org/jira/browse/SPARK-21657) may help you a lot.\nHowever, what I want to share about is the number of partitions when you use *explode()*. It's easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by *repartition()*, especially when the rows explode more than 10 times than previous, each task would process much more data and that's possible to get an OOM error, or high GC time.\n\n## Foreach vs ForeachPartition, Map vs MapPartition\nYeah, *foreach()* vs *foreachPartition()* and *map()* vs *mapPartition()*, these four method do confuse me for a long time and let me share you my understanding about them. \nFirst of all, *foreach()* and *foreachPartition()* are actiona in Spark, while *map()* and *mapPartition()* are transformations. If you have no ideas about the defination of action and transformation, it's better to read about my previous blog or just ask help for dear *google*. *foreach()* and *foreachPartition()* are often used for writing data to external database while *map()* and *mapPartition()* are used to modify the data of each row in the RDD, also DataFrame or DataSet.\nSeondly, *foreachPartition()* and *mapPartitionn()* are respectively based on *foreach()* and *map()*. Instead of invoking function for each element, *foreachPartition()* and *mapPartition()* calls for each partition and provide an iterator to invoke the function. So what's the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, *foreach()* will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, *foreachPartition()* could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn't it!\n\n## Reading ORC Table\nActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let's have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!\nTo enbale the vectotized ORC reader, you just need to set these configuration:\n* --conf spark.sql.orc.impl=native \n* --conf spark.sql.orc.enableVectorizedReader=true \n* --conf spark.sql.hive.convertMetastoreOrc=true\n\nFor more information, you can read the [Spark Doc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n\n## References\n* [Apache Spark - foreach Vs foreachPartitions When to use What?](https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what)\n* [Spark SQL Guide - ORC File](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n","source":"_posts/spark-sumup-part-3.md","raw":"---\ntitle: Spark Tips Sum-up Part-3\ndate: 2019-03-06 18:24:15\ntags: spark\ncategories: spark\ntoc: true\n---\n![](https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg)\nThis blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today I'll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!  \n<!--more-->\n## Explode\nSpark SQL provides a varority of functions in *org.apache.spark.sql.functions* for you to restruct your data, one of which is the *explode()* function. Since Spark 2.3, *explode()* function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, [this issue](https://issues.apache.org/jira/browse/SPARK-21657) may help you a lot.\nHowever, what I want to share about is the number of partitions when you use *explode()*. It's easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by *repartition()*, especially when the rows explode more than 10 times than previous, each task would process much more data and that's possible to get an OOM error, or high GC time.\n\n## Foreach vs ForeachPartition, Map vs MapPartition\nYeah, *foreach()* vs *foreachPartition()* and *map()* vs *mapPartition()*, these four method do confuse me for a long time and let me share you my understanding about them. \nFirst of all, *foreach()* and *foreachPartition()* are actiona in Spark, while *map()* and *mapPartition()* are transformations. If you have no ideas about the defination of action and transformation, it's better to read about my previous blog or just ask help for dear *google*. *foreach()* and *foreachPartition()* are often used for writing data to external database while *map()* and *mapPartition()* are used to modify the data of each row in the RDD, also DataFrame or DataSet.\nSeondly, *foreachPartition()* and *mapPartitionn()* are respectively based on *foreach()* and *map()*. Instead of invoking function for each element, *foreachPartition()* and *mapPartition()* calls for each partition and provide an iterator to invoke the function. So what's the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, *foreach()* will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, *foreachPartition()* could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isn't it!\n\n## Reading ORC Table\nActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Let's have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!\nTo enbale the vectotized ORC reader, you just need to set these configuration:\n* --conf spark.sql.orc.impl=native \n* --conf spark.sql.orc.enableVectorizedReader=true \n* --conf spark.sql.hive.convertMetastoreOrc=true\n\nFor more information, you can read the [Spark Doc](https://spark.apache.org/docs/latest/sql-data-sources-orc.html).\n\n## References\n* [Apache Spark - foreach Vs foreachPartitions When to use What?](https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what)\n* [Spark SQL Guide - ORC File](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n","slug":"spark-sumup-part-3","published":1,"updated":"2019-03-23T11:16:41.637Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjtlef4pq001ttr8lko2ig7hv","content":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg\" alt=\"\"><br>This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today Iâ€™ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!<br><a id=\"more\"></a></p>\n<h2 id=\"Explode\"><a href=\"#Explode\" class=\"headerlink\" title=\"Explode\"></a>Explode</h2><p>Spark SQL provides a varority of functions in <em>org.apache.spark.sql.functions</em> for you to restruct your data, one of which is the <em>explode()</em> function. Since Spark 2.3, <em>explode()</em> function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, <a href=\"https://issues.apache.org/jira/browse/SPARK-21657\" target=\"_blank\" rel=\"noopener\">this issue</a> may help you a lot.<br>However, what I want to share about is the number of partitions when you use <em>explode()</em>. Itâ€™s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by <em>repartition()</em>, especially when the rows explode more than 10 times than previous, each task would process much more data and thatâ€™s possible to get an OOM error, or high GC time.</p>\n<h2 id=\"Foreach-vs-ForeachPartition-Map-vs-MapPartition\"><a href=\"#Foreach-vs-ForeachPartition-Map-vs-MapPartition\" class=\"headerlink\" title=\"Foreach vs ForeachPartition, Map vs MapPartition\"></a>Foreach vs ForeachPartition, Map vs MapPartition</h2><p>Yeah, <em>foreach()</em> vs <em>foreachPartition()</em> and <em>map()</em> vs <em>mapPartition()</em>, these four method do confuse me for a long time and let me share you my understanding about them.<br>First of all, <em>foreach()</em> and <em>foreachPartition()</em> are actiona in Spark, while <em>map()</em> and <em>mapPartition()</em> are transformations. If you have no ideas about the defination of action and transformation, itâ€™s better to read about my previous blog or just ask help for dear <em>google</em>. <em>foreach()</em> and <em>foreachPartition()</em> are often used for writing data to external database while <em>map()</em> and <em>mapPartition()</em> are used to modify the data of each row in the RDD, also DataFrame or DataSet.<br>Seondly, <em>foreachPartition()</em> and <em>mapPartitionn()</em> are respectively based on <em>foreach()</em> and <em>map()</em>. Instead of invoking function for each element, <em>foreachPartition()</em> and <em>mapPartition()</em> calls for each partition and provide an iterator to invoke the function. So whatâ€™s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, <em>foreach()</em> will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, <em>foreachPartition()</em> could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isnâ€™t it!</p>\n<h2 id=\"Reading-ORC-Table\"><a href=\"#Reading-ORC-Table\" class=\"headerlink\" title=\"Reading ORC Table\"></a>Reading ORC Table</h2><p>Actually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Letâ€™s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!<br>To enbale the vectotized ORC reader, you just need to set these configuration:</p>\n<ul>\n<li>â€“conf spark.sql.orc.impl=native </li>\n<li>â€“conf spark.sql.orc.enableVectorizedReader=true </li>\n<li>â€“conf spark.sql.hive.convertMetastoreOrc=true</li>\n</ul>\n<p>For more information, you can read the <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark Doc</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what\" target=\"_blank\" rel=\"noopener\">Apache Spark - foreach Vs foreachPartitions When to use What?</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark SQL Guide - ORC File</a></li>\n</ul>\n","site":{"data":{}},"_categories":[{"name":"spark","path":"categories/spark/"}],"_tags":[{"name":"spark","path":"tags/spark/"}],"excerpt":"<p><img src=\"https://github.com/JoeAsir/blog-image/raw/master/blog/background/beach-dawn-dusk-185927.jpg\" alt=\"\"><br>This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today Iâ€™ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect!<br></p>","more":"</p>\n<h2 id=\"Explode\"><a href=\"#Explode\" class=\"headerlink\" title=\"Explode\"></a>Explode</h2><p>Spark SQL provides a varority of functions in <em>org.apache.spark.sql.functions</em> for you to restruct your data, one of which is the <em>explode()</em> function. Since Spark 2.3, <em>explode()</em> function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, <a href=\"https://issues.apache.org/jira/browse/SPARK-21657\" target=\"_blank\" rel=\"noopener\">this issue</a> may help you a lot.<br>However, what I want to share about is the number of partitions when you use <em>explode()</em>. Itâ€™s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by <em>repartition()</em>, especially when the rows explode more than 10 times than previous, each task would process much more data and thatâ€™s possible to get an OOM error, or high GC time.</p>\n<h2 id=\"Foreach-vs-ForeachPartition-Map-vs-MapPartition\"><a href=\"#Foreach-vs-ForeachPartition-Map-vs-MapPartition\" class=\"headerlink\" title=\"Foreach vs ForeachPartition, Map vs MapPartition\"></a>Foreach vs ForeachPartition, Map vs MapPartition</h2><p>Yeah, <em>foreach()</em> vs <em>foreachPartition()</em> and <em>map()</em> vs <em>mapPartition()</em>, these four method do confuse me for a long time and let me share you my understanding about them.<br>First of all, <em>foreach()</em> and <em>foreachPartition()</em> are actiona in Spark, while <em>map()</em> and <em>mapPartition()</em> are transformations. If you have no ideas about the defination of action and transformation, itâ€™s better to read about my previous blog or just ask help for dear <em>google</em>. <em>foreach()</em> and <em>foreachPartition()</em> are often used for writing data to external database while <em>map()</em> and <em>mapPartition()</em> are used to modify the data of each row in the RDD, also DataFrame or DataSet.<br>Seondly, <em>foreachPartition()</em> and <em>mapPartitionn()</em> are respectively based on <em>foreach()</em> and <em>map()</em>. Instead of invoking function for each element, <em>foreachPartition()</em> and <em>mapPartition()</em> calls for each partition and provide an iterator to invoke the function. So whatâ€™s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, <em>foreach()</em> will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, <em>foreachPartition()</em> could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isnâ€™t it!</p>\n<h2 id=\"Reading-ORC-Table\"><a href=\"#Reading-ORC-Table\" class=\"headerlink\" title=\"Reading ORC Table\"></a>Reading ORC Table</h2><p>Actually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Letâ€™s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!<br>To enbale the vectotized ORC reader, you just need to set these configuration:</p>\n<ul>\n<li>â€“conf spark.sql.orc.impl=native </li>\n<li>â€“conf spark.sql.orc.enableVectorizedReader=true </li>\n<li>â€“conf spark.sql.hive.convertMetastoreOrc=true</li>\n</ul>\n<p>For more information, you can read the <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark Doc</a>.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://stackoverflow.com/questions/30484701/apache-spark-foreach-vs-foreachpartitions-when-to-use-what\" target=\"_blank\" rel=\"noopener\">Apache Spark - foreach Vs foreachPartitions When to use What?</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-orc.html\" target=\"_blank\" rel=\"noopener\">Spark SQL Guide - ORC File</a></li>\n</ul>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjtlef4p00008tr8l0klebrfx","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4p5000etr8lg36u3ec1"},{"post_id":"cjtlef4os0001tr8lamqrp5a4","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4p8000itr8lyhnglvb8"},{"post_id":"cjtlef4p10009tr8l8cxyfnhz","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4pa000ltr8l8ap3z2xa"},{"post_id":"cjtlef4p3000ctr8lkate1mlw","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4pc000qtr8l6eiq48c7"},{"post_id":"cjtlef4ov0003tr8lwnny342t","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4pd000str8ljby0xw5k"},{"post_id":"cjtlef4p4000dtr8lxso8020o","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4pe000wtr8lzb0cjbma"},{"post_id":"cjtlef4oz0007tr8lafvbj814","category_id":"cjtlef4ow0004tr8lo9flocbb","_id":"cjtlef4pf000ytr8l66dca99w"},{"post_id":"cjtlef4p6000htr8l2853x7ox","category_id":"cjtlef4pa000otr8lhr1q0vw5","_id":"cjtlef4ph0011tr8lzpsxftnk"},{"post_id":"cjtlef4pe000utr8l8jqdu3gd","category_id":"cjtlef4pa000otr8lhr1q0vw5","_id":"cjtlef4pi0015tr8linz3fdz4"},{"post_id":"cjtlef4p9000ktr8l7t81sfvg","category_id":"cjtlef4pa000otr8lhr1q0vw5","_id":"cjtlef4pj0019tr8lmw1bj06q"},{"post_id":"cjtlef4pb000ptr8lfpv0o4d4","category_id":"cjtlef4pa000otr8lhr1q0vw5","_id":"cjtlef4pl001etr8l0qdq74g5"},{"post_id":"cjtlef4pc000rtr8lpp3mfyfw","category_id":"cjtlef4pj0018tr8lotbkdqm2","_id":"cjtlef4po001ltr8lmbi3lfm5"},{"post_id":"cjtlef4pf000xtr8l0ewy67ma","category_id":"cjtlef4pl001ftr8lcybt42rw","_id":"cjtlef4pq001rtr8lucupy0sj"},{"post_id":"cjtlef4pg000ztr8lgliwrab7","category_id":"cjtlef4pl001ftr8lcybt42rw","_id":"cjtlef4ps001wtr8l3ut9tj3x"},{"post_id":"cjtlef4pi0014tr8lyrirv5ns","category_id":"cjtlef4pl001ftr8lcybt42rw","_id":"cjtlef4ps001ztr8l119ev4xv"},{"post_id":"cjtlef4pj0017tr8lmw643g23","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4pt0023tr8lg4hgy1pv"},{"post_id":"cjtlef4pk001ctr8l2nhwqjph","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4pv0027tr8lwi4wrldr"},{"post_id":"cjtlef4pl001dtr8l7hp9uwri","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4pw002ctr8lthgobayg"},{"post_id":"cjtlef4pm001htr8law39ktsd","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4px002htr8l86jewncn"},{"post_id":"cjtlef4pn001jtr8l5u5ay5ik","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4py002ktr8lpola37vv"},{"post_id":"cjtlef4po001otr8lqs108ij8","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4py002ntr8lfv0nakoz"},{"post_id":"cjtlef4pp001ptr8lov9md1mw","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4pz002rtr8l0sps3frf"},{"post_id":"cjtlef4pq001ttr8lko2ig7hv","category_id":"cjtlef4pr001vtr8lne0a1t2v","_id":"cjtlef4q0002utr8lr02v0r7l"}],"PostTag":[{"post_id":"cjtlef4os0001tr8lamqrp5a4","tag_id":"cjtlef4oy0005tr8lakp09ii6","_id":"cjtlef4p9000jtr8lqs1ebeg9"},{"post_id":"cjtlef4os0001tr8lamqrp5a4","tag_id":"cjtlef4p2000btr8lcl64np0u","_id":"cjtlef4pa000mtr8lsen29bqm"},{"post_id":"cjtlef4ov0003tr8lwnny342t","tag_id":"cjtlef4p5000gtr8lwt23s1xc","_id":"cjtlef4ph0013tr8l7lwn79rd"},{"post_id":"cjtlef4ov0003tr8lwnny342t","tag_id":"cjtlef4pa000ntr8l3mb7r1p8","_id":"cjtlef4pi0016tr8lt0lnghx0"},{"post_id":"cjtlef4ov0003tr8lwnny342t","tag_id":"cjtlef4pd000ttr8lza2l1mjo","_id":"cjtlef4pk001btr8lmtbccufd"},{"post_id":"cjtlef4oz0007tr8lafvbj814","tag_id":"cjtlef4p2000btr8lcl64np0u","_id":"cjtlef4pm001itr8lid9ggj8y"},{"post_id":"cjtlef4oz0007tr8lafvbj814","tag_id":"cjtlef4pj001atr8l7wrudi1y","_id":"cjtlef4pn001ktr8l6slcmb9c"},{"post_id":"cjtlef4p00008tr8l0klebrfx","tag_id":"cjtlef4pm001gtr8lb4i08vip","_id":"cjtlef4pr001utr8l7htg1lwp"},{"post_id":"cjtlef4p00008tr8l0klebrfx","tag_id":"cjtlef4po001ntr8lwydgjejo","_id":"cjtlef4ps001xtr8lv9gvwfng"},{"post_id":"cjtlef4p10009tr8l8cxyfnhz","tag_id":"cjtlef4pm001gtr8lb4i08vip","_id":"cjtlef4pu0025tr8ld27vh92t"},{"post_id":"cjtlef4p10009tr8l8cxyfnhz","tag_id":"cjtlef4ps001ytr8llf4csp58","_id":"cjtlef4pv0028tr8l4a8blmxm"},{"post_id":"cjtlef4p10009tr8l8cxyfnhz","tag_id":"cjtlef4pt0021tr8l9cdcah19","_id":"cjtlef4pv002atr8leuyhqpej"},{"post_id":"cjtlef4p3000ctr8lkate1mlw","tag_id":"cjtlef4pt0024tr8locv0eu2y","_id":"cjtlef4pw002dtr8l8yxh8emv"},{"post_id":"cjtlef4p4000dtr8lxso8020o","tag_id":"cjtlef4pt0024tr8locv0eu2y","_id":"cjtlef4pw002ftr8lyanc1w3v"},{"post_id":"cjtlef4p6000htr8l2853x7ox","tag_id":"cjtlef4pw002etr8lyfte0koz","_id":"cjtlef4py002otr8lw6rdaaow"},{"post_id":"cjtlef4p6000htr8l2853x7ox","tag_id":"cjtlef4p2000btr8lcl64np0u","_id":"cjtlef4pz002ptr8lmexo1tjl"},{"post_id":"cjtlef4p6000htr8l2853x7ox","tag_id":"cjtlef4px002itr8l6e2z9xr4","_id":"cjtlef4pz002str8lf1gub5qc"},{"post_id":"cjtlef4p9000ktr8l7t81sfvg","tag_id":"cjtlef4py002ltr8lrc8rdsyz","_id":"cjtlef4q0002vtr8lo7ayqb2o"},{"post_id":"cjtlef4p9000ktr8l7t81sfvg","tag_id":"cjtlef4p2000btr8lcl64np0u","_id":"cjtlef4q0002wtr8litkglv2x"},{"post_id":"cjtlef4p9000ktr8l7t81sfvg","tag_id":"cjtlef4px002itr8l6e2z9xr4","_id":"cjtlef4q0002ytr8lfc6szzq9"},{"post_id":"cjtlef4pb000ptr8lfpv0o4d4","tag_id":"cjtlef4oy0005tr8lakp09ii6","_id":"cjtlef4q10031tr8lhcsmokpa"},{"post_id":"cjtlef4pb000ptr8lfpv0o4d4","tag_id":"cjtlef4pz002ttr8lq8hq7eyn","_id":"cjtlef4q10032tr8lv80r5eji"},{"post_id":"cjtlef4pb000ptr8lfpv0o4d4","tag_id":"cjtlef4q0002xtr8l3d43kvsx","_id":"cjtlef4q10034tr8lou6nl6ee"},{"post_id":"cjtlef4pb000ptr8lfpv0o4d4","tag_id":"cjtlef4q0002ztr8l1zsw82e2","_id":"cjtlef4q10035tr8lg3gukg5t"},{"post_id":"cjtlef4pc000rtr8lpp3mfyfw","tag_id":"cjtlef4q00030tr8l13801wvk","_id":"cjtlef4q10037tr8letsj1kja"},{"post_id":"cjtlef4pe000utr8l8jqdu3gd","tag_id":"cjtlef4q10033tr8ldrppqi6n","_id":"cjtlef4q20038tr8l8ml3qped"},{"post_id":"cjtlef4pf000xtr8l0ewy67ma","tag_id":"cjtlef4q10036tr8lgk06kxv6","_id":"cjtlef4q2003btr8l4s61g5gx"},{"post_id":"cjtlef4pf000xtr8l0ewy67ma","tag_id":"cjtlef4q20039tr8lqjqaktns","_id":"cjtlef4q2003ctr8lzsmmmhsp"},{"post_id":"cjtlef4pf000xtr8l0ewy67ma","tag_id":"cjtlef4p2000btr8lcl64np0u","_id":"cjtlef4q3003etr8l5xab7u05"},{"post_id":"cjtlef4pg000ztr8lgliwrab7","tag_id":"cjtlef4q10033tr8ldrppqi6n","_id":"cjtlef4q4003htr8lskamgj1j"},{"post_id":"cjtlef4pg000ztr8lgliwrab7","tag_id":"cjtlef4q3003dtr8lcw73ew54","_id":"cjtlef4q4003itr8l24hl447a"},{"post_id":"cjtlef4pg000ztr8lgliwrab7","tag_id":"cjtlef4q3003ftr8lc3h4sns7","_id":"cjtlef4q4003ktr8l828u7bwv"},{"post_id":"cjtlef4pi0014tr8lyrirv5ns","tag_id":"cjtlef4q3003gtr8lvug8qqj7","_id":"cjtlef4q5003ltr8lngadek9l"},{"post_id":"cjtlef4pj0017tr8lmw643g23","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q5003ntr8lup20eca7"},{"post_id":"cjtlef4pk001ctr8l2nhwqjph","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q6003ptr8lxv66ruzl"},{"post_id":"cjtlef4pl001dtr8l7hp9uwri","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q6003rtr8l1ouwhtzc"},{"post_id":"cjtlef4pm001htr8law39ktsd","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q7003ttr8lbv1hkoyb"},{"post_id":"cjtlef4pn001jtr8l5u5ay5ik","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q7003vtr8lqr3cbjqh"},{"post_id":"cjtlef4po001otr8lqs108ij8","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q8003xtr8lyi2690re"},{"post_id":"cjtlef4pp001ptr8lov9md1mw","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q9003ztr8l1c3ojlig"},{"post_id":"cjtlef4pq001ttr8lko2ig7hv","tag_id":"cjtlef4q4003jtr8lq98ldq7t","_id":"cjtlef4q90040tr8lju36n5q4"}],"Tag":[{"name":"regularization","_id":"cjtlef4oy0005tr8lakp09ii6"},{"name":"gradient descent","_id":"cjtlef4p2000btr8lcl64np0u"},{"name":"hyperparameter","_id":"cjtlef4p5000gtr8lwt23s1xc"},{"name":"batch norm","_id":"cjtlef4pa000ntr8l3mb7r1p8"},{"name":"covariate shift","_id":"cjtlef4pd000ttr8lza2l1mjo"},{"name":"moving averages","_id":"cjtlef4pj001atr8l7wrudi1y"},{"name":"learning strategy","_id":"cjtlef4pm001gtr8lb4i08vip"},{"name":"orthogonalization","_id":"cjtlef4po001ntr8lwydgjejo"},{"name":"transfer learning","_id":"cjtlef4ps001ytr8llf4csp58"},{"name":"multi-task learning","_id":"cjtlef4pt0021tr8l9cdcah19"},{"name":"CNN","_id":"cjtlef4pt0024tr8locv0eu2y"},{"name":"convex optimization","_id":"cjtlef4pw002etr8lyfte0koz"},{"name":"newton's method","_id":"cjtlef4px002itr8l6e2z9xr4"},{"name":"unconstrained optimization","_id":"cjtlef4py002ltr8lrc8rdsyz"},{"name":"MAP","_id":"cjtlef4pz002ttr8lq8hq7eyn"},{"name":"ridge regression","_id":"cjtlef4q0002xtr8l3d43kvsx"},{"name":"lasso regression","_id":"cjtlef4q0002ztr8l1zsw82e2"},{"name":"life","_id":"cjtlef4q00030tr8l13801wvk"},{"name":"imbalanced data","_id":"cjtlef4q10033tr8ldrppqi6n"},{"name":"gbt","_id":"cjtlef4q10036tr8lgk06kxv6"},{"name":"logistic regression","_id":"cjtlef4q20039tr8lqjqaktns"},{"name":"undersampling","_id":"cjtlef4q3003dtr8lcw73ew54"},{"name":"bagging","_id":"cjtlef4q3003ftr8lc3h4sns7"},{"name":"activtion function","_id":"cjtlef4q3003gtr8lvug8qqj7"},{"name":"spark","_id":"cjtlef4q4003jtr8lq98ldq7t"}]}}