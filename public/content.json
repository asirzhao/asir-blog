{"pages":[{"title":"About","text":"Welome to superAsirâ€™s Notes.All the cover images are downloaded from PEXELS. ğŸ“– M.E. from BUAA ğŸ’» Software Developer/Data Scientist ğŸ‡¨ğŸ‡³ Beijing, China ğŸ’¡ Machine learning, deep learning and distributed computing ğŸ”´ COME ON YOU GOONERS! ğŸ¸ Punkâ€™s not dead ğŸ¥ƒ Prefer rum to vodka ğŸº Prefer Tsingtao to Corona â˜•ï¸ Couldâ€™t love CaffÃ¨ Americano more","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Learning Notes-Deep Learning, course2, week1","text":"å¤§å®¶å¥½ï¼Œæœ€è¿‘åœ¨å­¦ä¹ Andrew Ngçš„Deep learningè¯¾ç¨‹ï¼Œäºæ˜¯å†³å®šå†™ä¸€äº›learning notesæ¥recapå’Œmarkä¸€ä¸‹å­¦åˆ°çš„çŸ¥è¯†ï¼Œé¿å…é—å¿˜ã€‚ç”±äºè¯¥è¯¾ç¨‹çš„course1æ¯”è¾ƒåŸºç¡€ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰markçš„å¿…è¦ï¼Œæ‰€ä»¥ä»course2å¼€å§‹ï¼ŒæŒ‰ç…§weekæ¥mark. Data setåœ¨machine learningä¸­ï¼Œdata setå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŒºåˆ«äºä¼ ç»Ÿmachine learningï¼Œdeep learningä¸­çš„data setåˆ†å¸ƒæ›´ä¾§é‡äºtrainingï¼ŒNgå»ºè®®æˆ‘ä»¬è®²data setåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š training setâ€”â€”è®­ç»ƒæ•°æ®é›† dev/validation setâ€”â€”æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒæ•´ï¼Œæ³›åŒ–èƒ½åŠ›æµ‹è¯• testing setâ€”â€”æ¨¡å‹æ•ˆæœæµ‹è¯•ä¸€å®šæœ‰å¾ˆå¤šäººå¯¹äºdevå’Œtesting setæœ‰ä¸€äº›ç–‘é—®ï¼Œæœ€å¼€å§‹æˆ‘ä¹Ÿæ˜¯æ‡µé€¼çš„ï¼Œæ¥çœ‹çœ‹ä¸‹é¢è¿™æ®µè¯ Dev/Validation Set: this data set is used to minimize overfitting. Youâ€™re not adjusting the weights of the network with this data set, youâ€™re just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasnâ€™t trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then youâ€™re overfitting your neural network and you should stop training.Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network. è¿™ä¸‰è€…çš„æ¯”ä¾‹åˆ™æ˜¯/99.5%/2.5%/2.5%/ï¼Œè¿™æ ·çš„åŸå› æ˜¯å› ä¸ºdeep learningä¸­ï¼Œæ•°æ®é‡è¶³å¤Ÿå¤§è€Œä¸”deep learningçš„å­¦ä¹ èƒ½åŠ›å¾ˆå¼ºï¼Œå¤§å®¶ä¸€å®šæ³¨æ„è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœå®åœ¨æ²¡æœ‰test setï¼Œä½†æ˜¯æœ‰dev setä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚ Bias and varianceä»€ä¹ˆæ˜¯biaså’Œvariancebias &amp; varianceæ˜¯machine learning é¢†åŸŸä¸€ä¸ªç»å…¸çš„è¾©è¯é—®é¢˜ï¼Œåœ¨Ngç»å…¸çš„CS229ä¸­å°±é‡ç‚¹çš„è®²è¿°è¿‡ï¼Œå…·ä½“çš„å®šä¹‰æˆ‘ä¸å¤ªæƒ³ç»™å‡ºäº†ï¼Œåç»­æœ‰æ—¶é—´å¯ä»¥ä¸“é—¨å†™ä¸€ç¯‡ï¼Œåé¢ä¼šç»™å‡ºä¸€äº›èµ„æ–™é“¾æ¥ã€‚æˆ‘ä»¬ç®€å•çš„çœ‹ä¸€å¹…å›¾å·¦å›¾å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„high bias situationï¼Œæ¨¡å‹æ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ‹Ÿåˆæ•°æ®ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„under fittingï¼Œå³å›¾åˆ™æ˜¯å…¸å‹çš„high variance situationï¼Œæ¨¡å‹è¿‡åˆ†çš„æ‹Ÿåˆäº†training setï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æœ€éœ€è¦é˜²èŒƒçš„over fitting.å½“ç„¶ï¼Œä¸­é—´çš„åˆ™æ˜¯æ¯”è¾ƒç†æƒ³çš„çŠ¶å†µã€‚ Solutionåœ¨å®é™…çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåˆ†æè‡ªå·±æ¨¡å‹çš„biaså’Œvarianceæƒ…å†µå‘¢ï¼ŒNgç»™äº†æˆ‘ä»¬ä¸€ä¸ªæµç¨‹å›¾ï¼Œå¦‚ä¸‹ï¼šé¦–å…ˆæ£€éªŒæ˜¯å¦å­˜åœ¨high bias æƒ…å†µï¼Œå…·ä½“æ–¹æ³•æ˜¯åœ¨training set å’Œ dev setä¸Šè®¡ç®—errorï¼Œå¯¹æ¯”training errorå’Œdev errorï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆé«˜ï¼Œé‚£ä¹ˆå°±æ˜¯high biasï¼Œå¦‚æœtraining errorå¾ˆå°è€Œdev errorå¾ˆé«˜ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯high varianceï¼Œå¦‚æœä¸¤è€…éƒ½å¾ˆå¤§ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å·®çš„æƒ…å†µäº†æ—¢high biasåˆhigh variance å¯¹äºhigh biasï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œã€æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œæ›´å¼ºçš„ç½‘ç»œç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›å¯¹äºhigh varianceï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´å¤šçš„æ•°æ®ï¼Œregularizationçš„æ–¹æ³•æ¥è§£å†³ã€‚ RegularizationL1&amp;L2 regularizationè¿™éƒ¨åˆ†å†…å®¹æˆ‘å°±ä¸å¤šè¯´äº†ï¼Œæˆ‘ä¹‹å‰ä¸“é—¨è¯¦ç»†æ·±å…¥çš„è®²è¿°è¿‡L1å’ŒL2 regularizationï¼Œå¤§å®¶å¯ä»¥å»çœ‹ä¸€çœ‹ã€‚ å”¯ä¸€éœ€è¦æ˜ç¡®çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨åŠ å…¥L1æˆ–è€…L2 regularizationä¹‹åï¼Œåœ¨è§‚æµ‹cost function convergence çš„æ—¶å€™ï¼Œä¸€å®šè¦å¸¦ä¸Šregularization itemï¼Œå¦åˆ™ç»“æœæ˜¯å¾ˆéš¾çœ‹åˆ°convergenceçš„ï¼Œè¿™å’Œregularizationæ€§è´¨æœ‰å¾ˆå¤§çš„å…³ç³»ã€‚ DropoutDropoutæ˜¯neural networkä¸­ä¸€ç§ç»å…¸çš„regularizationæ–¹æ³•ï¼Œç»å…¸åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Œæˆ‘å½“å¹´æ¯•è®¾è¯¾é¢˜ä¸­éƒ½ç”¨åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œè€Œä¸”æ•ˆæœè¶…èµ Dropoutæ–¹æ³•çš„å®è´¨æ˜¯æŒ‰æ¯”ä¾‹éšæœºéšè—æ‰neural networkä¸­layeré‡Œçš„æŸäº›unitsï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå†ä¸€æ¬¡epochä¸­ï¼Œåªæœ‰ä¸€éƒ¨åˆ†çš„unitså¯¹åº”çš„weightså’Œbiasä¼šå¾—åˆ°æ›´æ–°ï¼Œè€Œä¸‹ä¸€æ¬¡epochä¸­ï¼Œåˆ™æ˜¯å¦ä¸€éƒ¨åˆ†unitså¯¹åº”çš„weightså’Œbiaså¾—åˆ°æ›´æ–°ï¼Œå¦‚ä¸‹å›¾é‚£ä¹ˆä¸ºä»€ä¹ˆDropoutå¯ä»¥å®ç°regularizationæ•ˆæœå‘¢ï¼ŒNgå‘Šè¯‰æˆ‘ä»¬ï¼š Intuition:Canâ€™t rely on any one feature, so have to spread out weights å¦‚ä½•ç†è§£å‘¢ï¼ŸåŠ å…¥dropoutåï¼Œæ¯ä¸ªunitå¯¹åº”çš„weightså’Œbiasä¸èƒ½å®Œå…¨ä¾èµ–ä¸Šå±‚unitsï¼Œå› ä¸ºä»–å¹¶ä¸æ˜¯æ¯ä¸€æ¬¡epochéƒ½å¯ä»¥work onï¼Œå› æ­¤åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œè§ç¬‘äº†over fittingçš„é£é™©ã€‚å®é™…ä¸Šï¼Œdropoutå¯ä»¥äº§ç”Ÿshrink weightsçš„æ•ˆæœï¼Œå’ŒL2 regularizationç›¸ä¼¼ï¼Œå› æ­¤ä¹Ÿæ˜¯ä¸€ç§regularizationæ–¹æ³•ã€‚ ä½†æ˜¯ï¼Œdropoutå’ŒL2 regularizationå”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–å¾ˆéš¾ç»™å‡ºä¸€ä¸ªregularization itemï¼Œæ‰€ä»¥ä½ æ²¡æœ‰åŠæ³•ç”»å‡ºcost function convergenceçš„è½¨è¿¹ã€‚ Other methodsé™¤äº†ç»å…¸çš„L1ã€L2 regularizationå’Œdropoutæ–¹æ³•ï¼Œè¿˜æœ‰ä¸€äº›é˜²æ­¢over fittingçš„æ–¹æ³•ï¼Œä¾‹å¦‚å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨data augmentationï¼Œæ—‹è½¬ï¼Œç¿»è½¬ï¼ŒåŠ å™ªå£°ç­‰æ–¹æ³•ã€‚ è¿˜æœ‰ä¸€ä¸ªearly stoppingæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œéšç€training çš„epochå¢å¤šï¼Œæ¨¡å‹å¯¹training setæ‹Ÿåˆä¼šè¶Šæ¥è¶Šå¥½ï¼Œéšä¹‹å¸¦æ¥çš„é—®é¢˜å°±æ˜¯å¯èƒ½over fittingï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡early stoppingï¼Œè®©æ¨¡å‹åœ¨æ²¡æœ‰äº§ç”Ÿover fittingçš„æ—¶å€™åœä¸‹æ¥ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚ Exploding/vanishing gradientä»€ä¹ˆæ˜¯exploding/vanishing gradientå¯¹äºdeep learningï¼Œæ›¾ç»æœ€ä¸ºæ£˜æ‰‹çš„é—®é¢˜å°±æ˜¯exploding/vanishing gradientï¼Œç”šè‡³æ˜¯é™åˆ¶deep learningå‘å±•çš„ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¯”è¾ƒæ·±çš„neural networkï¼Œå‡è®¾ä¸€å…±æœ‰\\(l\\)å±‚ï¼Œå¯¹åº”çš„weightsæ˜¯\\(W^{[1]}\\)åˆ°\\(W^{[l]}\\)ï¼Œbiasæ˜¯\\(b^{[1]}\\)åˆ°\\(b^{[l]}\\)ï¼Œæˆ‘ä»¬ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œå‡è®¾biaså‡ä¸º0ï¼Œactive functionä¸º\\(g(z)=z\\)ï¼Œé‚£ä¹ˆï¼Œ\\(y\\)å°±ç­‰äº$$\\hat{y}=W^{[l]}W^{[l-1]}W^{[l-2]} \\cdots W^{[3]}W^{[2]}W^{[1]}X$$å¤§å®¶æ„Ÿå…´è¶£çš„è¯å¯ä»¥éªŒè¯ä¸€ä¸‹ï¼Œå¾ˆç®€å•çš„ã€‚ é‚£ä¹ˆç°åœ¨é—®é¢˜æ¥äº†ï¼Œå½“\\(l\\)å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å¤§äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šéå¸¸éå¸¸å¤§ï¼Œç”šè‡³åˆ°æ— é™å¤§ï¼Œè¿™ç§æƒ…å†µå«exploding gradientï¼›ç›¸åº”çš„ï¼Œå¦‚æœ\\(W\\)å…ƒç´ éƒ½å°äº1ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœå°±ä¼šç‰¹åˆ«å°ï¼Œç”šè‡³ä¸ºé›¶ï¼Œè¿™å°±æ˜¯vanishing gradient. Solutionå¯¹äºä¸Šé¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èˆ¬åœ¨weightsåˆå§‹åŒ–çš„æ—¶å€™åšä¸€äº›å·¥ä½œæ¥è§£å†³å¯èƒ½å‡ºç°çš„exploding or vanishing gradientã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå¯¹äºactive function\\(g(z)\\)ï¼Œå‡è®¾biasä¸ºé›¶ï¼Œ$$z=w_{1}x_1 +w_{2}x_2+ \\cdots +w_{n}x_n$$æˆ‘ä»¬è¦æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(n\\)çš„å¢å¤§ï¼Œ\\(w\\)ä¼šå˜å°ï¼Œæˆ‘ä»¬è®©\\(w\\)å§‹ç»ˆä¿æŒä»¥0ä¸ºmeanï¼Œ1ä¸ºvaranceçš„Gaussian distributionä¸‹ï¼Œå°±å¯ä»¥å¾ˆå¥½çš„æ§åˆ¶\\(w\\)çš„å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\) åœ¨Ngçš„å»ºè®®ä¸­ï¼Œå¦‚æœactive functionæ˜¯sigmoidï¼Œæˆ‘ä»¬ä¸€èˆ¬å–\\(var(w)= \\frac{1}{n^{l-1}}\\)ï¼Œå¦‚æœæ˜¯reLuï¼Œæˆ‘ä»¬å–\\(var(w)= \\frac{2}{n^{l-1}}\\)ï¼Œå¯¹äºtanhï¼Œ\\(var(w)= \\frac{1}{n^{l-1}}\\)æˆ–è€…\\(var(w)= \\frac{2}{n^{l-1}+n^{l}}\\)ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¥½çš„é¿å…exploding or vanishing gradient. Gradient checkingGradient approximationåœ¨è°ƒè¯•neural networkçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç»å¸¸åšgradient checkçš„å·¥ä½œï¼Œä»¥ç¡®å®šæ•´ä¸ªnetworkæ­£å¸¸çš„è¿è¡Œï¼ŒNgåœ¨è¿™é‡Œå»ºè®®æˆ‘ä»¬ä½¿ç”¨åŒè¾¹é€¼è¿‘çš„æ–¹æ³•å»åšgradient checkï¼Œè¿™é‡Œæˆ‘ä¸åšå¤ªå¤šæè¿°ï¼Œä¸»è¦ä¸Šä¸€å¼ å›¾ï¼šé€šå¸¸æ¥è¯´ï¼ŒåŒè¾¹é€¼è¿‘çš„æ–¹æ³•è·å¾—ç»“æœæ›´åŠ å‡†ç¡®ã€‚ Gradient checking notes Donâ€™t use in training-only to debug(too slow) If algorithm fails grad check, look at components to try to identify bug Remeber regularization Dosenâ€™t wrok with dropout Run at random initialzation; perhaps again after some training. Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng Bias and variance","link":"/2017/09/24/course-deep-learning-course2-week1/"},{"title":"Learning Notes-Deep Learning, course2, week3","text":"ä¸çŸ¥ä¸è§‰æ¥åˆ°ç¬¬ä¸‰å‘¨çš„è¯¾ç¨‹äº†ï¼Œå¤§å®¶åŠ æ²¹ï¼è¿™å‘¨çš„ä¸»è¦å†…å®¹æ˜¯hyperparameter selectionå’Œbatch normalçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸€å‘¨çš„å†…å®¹ï¼ Hyperparameter selectionHyperparameter selectionåœ¨machine learningä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚gradient descentä¸­çš„learning rate \\(\\alpha\\)å°±æ˜¯å…³ä¹ç®—æ³•ç»“æœçš„é‡è¦hyperparameterï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆå»é€‰æ‹©å‘¢ï¼ŸNgç»™å‡ºäº†ä¸¤ä¸ªå»ºè®®ï¼š æ„å»ºå¤šä¸ªhyperparameteräº¤å‰ï¼Œéšæœºé€‰æ‹©å¤§å°ï¼Œé€‰æ‹©æ•ˆæœè¾ƒå¥½çš„èŒƒå›´ï¼Œç»§ç»­éšæœºé€‰æ‹©hyperparameterå¤§å°ï¼Œè§‚å¯Ÿç»“æœã€‚ é€‰æ‹©å‚æ•°çš„æ—¶å€™åˆ†æ®µé€‰æ‹©ï¼Œå¹¶ä¸”ä½¿ç”¨logåˆ†æ®µï¼Œä¾‹å¦‚åœ¨0.0001åˆ°1ä¹‹é—´é€‰æ‹©ï¼Œå°†æ•°è½´åˆ†æˆ0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1å’Œ1ï¼Œè¿™æ ·é€‰æ‹©å‡ºçš„ç»“æœæ›´å¥½ å¯¹äºæ•´ä½“æ¨¡å‹çš„hyperparameter selectionï¼ŒNgä¹Ÿå‡ºäº†å»ºè®®ï¼Œé‚£å°±æ˜¯babysittingå’Œparallelæ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹ä¸€ä¸ªæ¨¡å‹å¤šæ¬¡è°ƒæ•´ï¼Œä¸€ç§æ˜¯åŒæ—¶å¯åŠ¨å¤šä¸ªä¸åŒhyperparameterçš„æ¨¡å‹ï¼Œæœ€åå–æ•ˆæœæœ€å¥½çš„ã€‚ ä¸¤ç§æ–¹æ³•æ®Šé€”åŒå½’ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…·ä½“æƒ…å†µåšå‡ºé€‰æ‹©ã€‚ Batch normNormalizationç›¸ä¿¡å¤§å®¶éƒ½å¬è¯´è¿‡å¤§åé¼é¼çš„normalizationå§ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¾ˆå¥½çš„æå‡æ•°æ®å¤„ç†ï¼ˆä¾‹å¦‚gradient descentï¼‰çš„é€Ÿåº¦å’Œæ•ˆæœï¼Œåœ¨å¼•å…¥batch normä¹‹å‰ï¼Œæˆ‘ä¹Ÿç¨å¾®æä¸€ä¸‹normalizationï¼Œä¸‹é¢ä¸Šå…¬å¼ï¼š å¯¹äºè¾“å…¥æ•°æ®æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹æ³•æ¥normalize$$ \\mu = \\frac{1}{m} \\sum_i x^{(i)}$$$$X = X- \\mu$$$$ \\sigma^2 = \\frac{1}{m} \\sum_i (x^{(i)})^2 $$$$ X = X/ \\sigma ^2$$è¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠè¾“å…¥æ•°æ®è½¬åŒ–æˆäº†ç¬¦åˆæœŸæœ›ä¸º0ï¼Œæ–¹å·®ä¸º1çš„Gaussian distributionçš„æ•°æ®ã€‚ å½“ç„¶ï¼Œè¿™åªæ˜¯normaliztionä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯è¢«ç§°ä½œz-scoreæ–¹æ³•ã€‚ Batch normä¸Šé¢è¯´çš„normalizationæ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°neural networksä¸­ï¼Œå¯¹äºnerual networksä¸­çš„æŸä¸€ä¸ªlayeræ¥è¯´ï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªå­¤ç«‹çš„è®¡ç®—è¿‡ç¨‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥normalizationï¼Œå¯¹äº\\(z^{(i)}\\)æ¥è¯´ï¼š$$ \\mu = \\frac{1}{m} \\sum_i z^{(i)}$$$$ \\sigma ^2= \\frac{1}{m} \\sum_i (z^{(i)}- \\mu)^2 $$$$z^{(i)}{norm}= \\frac{z^{(i)}- \\mu}{ \\sqrt{ \\sigma^2 + \\epsilon}}$$$$z^{N(i)}= \\gamma z^{(i)}{norm} + \\beta$$ç„¶åæˆ‘ä»¬ç”¨æœ€ç»ˆçš„\\(z^{N[l](i)}\\)æ¥æ›¿æ¢\\(z^{[l](i)}\\) å°±å¯ä»¥ï¼Œå…¶ä¸­\\( \\gamma\\)å’Œ\\(\\beta\\)æ˜¯ä¸¤ä¸ªparameterï¼Œå¯ä»¥é€šè¿‡gradient descentæ¥æ›´æ–°ï¼Œè¿™ä¸¤ä¸ªparameterå­˜åœ¨çš„æ„ä¹‰ï¼Œå°±æ˜¯å¯ä»¥è°ƒæ•´normalizationæ˜ å°„çš„Gaussian distributionï¼Œè€Œä¸æ˜¯ç»Ÿç»Ÿæ˜ å°„åˆ°Normal distributionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(\\epsilon\\)æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œç”¨æ¥é¿å…åˆ†æ¯åˆ†0çš„æƒ…å†µã€‚ å¦‚æœ\\(\\gamma = \\sqrt{ \\sigma^2 + \\epsilon}\\)ä¸”\\( \\beta = \\mu\\)çš„è¯ï¼Œé‚£ä¹ˆå…¶å®\\(z^{N(i)}=z^(i)\\)çš„ï¼Œå¤§å®¶å¯ä»¥ç®—ç®—ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œå°±æ˜¯ç›¸å½“äºæ²¡åšnormalization. Batch norm on neural networkså¯¹äºneural networksï¼Œè¾“å…¥\\(X\\)é€šè¿‡parameter\\(w^{[1]}\\)å’Œ\\(b^{[1]}\\)å¾—åˆ°\\(z^{[1]}\\)ï¼Œé€šè¿‡\\(\\beta\\)å’Œ\\(\\gamma\\)è·å¾—\\(z^{N[1]}\\)ï¼Œç»è¿‡active functionåè·å¾—\\(a^{[1]}\\)ï¼Œé€šè¿‡\\(w^{[2]}\\)å’Œ\\(b^{[2]}\\)è·å¾—\\(z^{[2]}\\)ï¼Œå¦‚æ­¤ä¸‹å»ï¼Œä¸€ç›´åˆ°æœ€åçš„è¾“å‡ºå±‚ï¼Œå®Œæˆforward propagation. åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±æœ‰å››ä¸ªparametersï¼Œåˆ†åˆ«æ˜¯\\(w^{[l]}\\)ï¼Œ\\(b^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\)ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼š$$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$$ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨åšbatch normalçš„æ—¶å€™ï¼Œé¦–å…ˆä¼šæŠŠ\\(z^{[l]}\\)æ˜ å°„åˆ°æœŸæœ›ä¸º1æ–¹å·®ä¸º0çš„Gaussian distributionä¸Šï¼Œè¿™å°±æ„å‘³ç€\\(b^{[l]}\\)æ˜¯å¯ä»¥å¿½ç•¥æ‰çš„ï¼Œå› ä¸ºå³ä½¿ä¿ç•™ï¼Œåœ¨batch normalçš„æ—¶å€™ä¹Ÿä¼šè¢«å‡å»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„parameteråªæœ‰ä¸‰ä¸ªï¼Œå³ï¼š\\(w^{[l]}\\)ï¼Œ\\( \\beta^{[l]}\\)ï¼Œ\\( \\gamma^{[l]}\\) åœ¨backforwardçš„æ—¶å€™ï¼Œæˆ‘ä»¬å’Œæ™®é€šçš„neural networksä¸€æ ·ï¼Œåªæ˜¯å¯ä»¥ä¸ç”¨å†å»è®¡ç®—\\(db\\) Solve covariate shiftä»€ä¹ˆæ˜¯covariate shiftï¼Ÿç®€å•çš„ç†è§£ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦éšç€æ ·æœ¬çš„å˜åŒ–è€Œå˜åŒ–ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆç›´è§‚ï¼Œåœ¨çŒ«è„¸è¯•éªŒä¸­ï¼Œå‡è®¾training seté‡Œéƒ½æ˜¯é»‘çŒ«ï¼Œè¿™æ ·è·å¾—çš„æ¨¡å‹ï¼Œå¯¹äºèŠ±çŒ«è¯†åˆ«å°±æ˜¯ä¸é€‚ç”¨çš„ï¼Œè¿™å°±å«covariate shift. å…¶å®ï¼Œbatch normå¯ä»¥æ”¹å–„neural networksæ•ˆæœçš„åŸå› ï¼Œå°±å¯ä»¥ç†è§£ä¸ºsolve covariate shiftçš„è¿‡ç¨‹ã€‚ OKï¼Œæˆ‘ä»¬æ¥è¯¦ç»†çœ‹çœ‹åŸå› ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¦‚å›¾çš„neural networksï¼šåœ¨æ ‡ç¤ºå‡ºçš„ä½ç½®ï¼Œæœ‰parameter\\(w^{[3]}\\)å’Œ\\(b^{[3]}\\)ï¼Œå¦‚æœæˆ‘ä»¬ç›–ä½å‰é¢çš„éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†è·å¾—å¦‚å›¾çš„neural networkså¯¹äºneural networksæ¥è¯´ï¼Œç›¸å½“äºè·å¾—äº†é»‘ç®±è¾“å‡ºçš„\\(a^{[2]}\\)ï¼Œè€Œ\\(a^{[2]}\\)çš„å€¼å…¶å®å¹¶ä¸æ˜¯æ˜¯å›ºå®šçš„ï¼Œæ¯ä¸€æ¬¡iterationåéƒ½æœ‰ä¸ä¸€æ ·çš„\\(a^{[2]}\\)ï¼Œè¿™å°±äº§ç”Ÿäº†covariate shifté—®é¢˜ã€‚ ä½†æ˜¯ï¼Œbatch normå¯ä»¥å°†\\(a^{[2]}\\)çš„æœŸæœ›å’Œæ–¹å·®é™åˆ¶åˆ°\\(\\beta\\)å’Œ\\(gamma\\)æ§åˆ¶çš„èŒƒå›´å†…ï¼Œä»¥æ­¤æå¤§é™åº¦çš„ç¼“è§£äº†covariate shiftç°è±¡ã€‚ å¦å¤–ï¼Œbatch normè¿˜å¯ä»¥æœ‰ä¸€äº›regularizationçš„ä½œç”¨ï¼Œç”±äºæ¯æ¬¡mini-batch gradient descentä¸­batch normä½œç”¨çš„sampleä¸ä¸€æ ·ï¼Œç±»ä¼¼äºdropoutçš„æ•ˆæœï¼Œä¼šç»™å¯¹åº”layeråŠ å…¥ä¸€äº›å™ªå£°ï¼Œä»¥æ­¤äº§ç”Ÿä¸€äº›regularizationçš„æ•ˆæœã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šæŠŠbatch normåˆ—å…¥regularizationèŒƒç•´å†…ã€‚ Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng","link":"/2017/09/30/course-deep-learning-course2-week3/"},{"title":"Learning Notes-Deep Learning, course2, week2","text":"å¤§å®¶å¥½ï¼Œè¯¾ç¨‹æ¥åˆ°äº†ç¬¬äºŒå‘¨ï¼Œè¿™å‘¨ä¸»è¦æ˜¯ä¸€äº›ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿å¾—æ•´ä¸ªneural networkså¯ä»¥æ›´å¿«æ›´å¥½çš„å·¥ä½œï¼Œæˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹ã€‚ Mini-batch gradient descentè¿™ä¸€èŠ‚æˆ‘å°±ä¸æ‰“ç®—å†™äº†ï¼Œæ¯”è¾ƒåŸºç¡€ï¼Œå…¶å®mini-batch gradient descentæ˜¯batch gradientå’Œstochastic gradient descentç»¼åˆåçš„ç»“æœï¼Œä¸€æ–¹é¢è§£å†³äº†batch gradientè®¡ç®—é‡å¤§ï¼Œå®¹æ˜“convergeåˆ°local minimumçš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†stochastic gradient descent epochå¤ªå¤šçš„å¼Šç«¯ï¼Œæ˜¯ç°åœ¨gradient descentä½¿ç”¨æœ€å¹¿æ³›çš„æ–¹æ³•ã€‚ å¯¹äºmini-batch gradient descentï¼ŒNgå»ºè®®batchå¤§å°å–å†³äºæ•°æ®é‡å¤šå°‘ï¼Œåœ¨å°æ•°æ®é‡ä¸Šå®Œå…¨æ²¡æœ‰å¿…è¦åšmini-batchï¼Œç›´æ¥ä½¿ç”¨batch gradient descentå°±å¯ä»¥ï¼Œå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼Œæœ€å¥½é€‰æ‹©2çš„ä¹˜æ–¹ï¼Œå¦‚64,128,256,512æ¥ä½œä¸ºbatch size. å½“ç„¶ï¼Œbatch sizeä¹Ÿè¦æ»¡è¶³CPUå’ŒGPUçš„å†…å­˜å¤§å°ã€‚ Exponentially weighted averagesExponentially weighted averagesExponentially weighted averagesï¼Œä¹Ÿè¢«ç§°ä¸ºmoving averagesï¼Œæ˜¯ä¸€ç§ç»¼åˆå†å²æ•°æ®çš„åŠ æƒå¹³å‡æ–¹æ³•ï¼Œè¯¾ç¨‹ä¸­Ngç”¨äº†ä¼¦æ•¦ä¸€å¹´çš„æ°”æ¸©å˜åŒ–æ›²çº¿ä½œä¸ºä¾‹å­ï¼Œå¯¹äºå›ºæœ‰å˜é‡\\(\\theta\\)æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ±‚çš„å¹³å‡å€¼\\(v\\)åº”è¯¥æ˜¯$$v_0 = 0$$$$v_1 = \\beta v_0 + (1- \\beta) \\theta_1 $$$$v_2 = \\beta v_1 + (1- \\beta) \\theta_2$$$$v_3= \\beta v_2 + (1- \\beta) \\theta_3$$$$\\cdots$$$$v_t= \\beta v_{t-1} + (1- \\beta) \\theta_t$$å…¶ä¸­\\(\\beta\\)æ˜¯ä¸€ä¸ªå› å­ï¼Œå®ƒå†³å®šäº†moving averageså¤§çº¦å‘å‰å¹³å‡äº†\\(\\frac{1}{1- \\beta}\\)ä¸ª\\(\\theta\\)å€¼ï¼Œä¾‹å¦‚\\(\\beta = 0.9\\)ï¼Œé‚£ä¹ˆå¤§çº¦å‘å‰å¹³å‡äº†10ä¸ªå€¼ï¼Œå¹¶ä¸”æ˜¯å‘å‰æŒ‰æŒ‡æ•°è¡°å‡åŠ æƒè·å¾—çš„å¹³å‡å€¼ã€‚ Bias correctionåœ¨exponentially weighted averagesä¸­ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¾ˆå°–é”ï¼Œé‚£å°±æ˜¯åœ¨æœ€åˆçš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œç”±äº\\(v_0=0\\)ï¼Œå¯¼è‡´å‰é¢çš„æ•°å­—ç»“æœè·ç¦»æ­£ç¡®ç»“æœè¾ƒå°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç´«è‰²æ›²çº¿æ˜¯è·å¾—çš„ç»“æœï¼Œè€Œåœ¨èµ·å§‹ä½ç½®çš„å€¼æ˜æ˜¾æ˜¯åå°çš„ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥bias correctionï¼ŒåŸç†ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯æ­¤å¤„æˆ‘ä»¬ä¸ä½¿ç”¨\\(v_t\\)ä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œè€Œæ˜¯ä½¿ç”¨\\( \\frac{v_t}{1- \\beta^{t}}\\)ä½œä¸ºæœ€åçš„ç»“æœï¼Œé€šè¿‡bias correctionï¼Œæˆ‘ä»¬ä¼šè·å¾—ç»¿è‰²çš„æ›²çº¿ã€‚ æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèµ·å§‹ç»¿è‰²é’±å’Œç´«è‰²æ›²çº¿åœ¨æœ€ååŸºæœ¬æ²¡æœ‰å·®åˆ«ï¼Œå‡ ä¹é‡åˆï¼Œä½†æ˜¯åœ¨æ›²çº¿å¼€å§‹çš„æ—¶å€™ï¼Œç»¿è‰²æ›²çº¿æ¯”ç´«è‰²æ›²çº¿æ›´åŠ é€¼è¿‘çœŸå®æƒ…å†µï¼Œå› æ­¤ï¼ŒNgç»™æˆ‘ä»¬ä»¥ä¸‹å»ºè®®ï¼š å½“æˆ‘ä»¬ä¸å…³æ³¨moving averages initial valueå¤§å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ä½¿ç”¨bias correction Bias correctionå¯¹äºinitial valueæ•ˆæœæ›´å¥½ Gradient descent optimizationmomentumåœ¨gradient descentä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ç§æƒ…å†µï¼Œå¦‚å›¾æ‰€ç¤ºï¼šåœ¨æ°´å¹³æ–¹å‘ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›æ›´å¿«çš„ä¸‹é™ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ä¸Šæˆ‘ä»¬å¸Œæœ›æ›´å°çš„ä¸‹é™é€Ÿç‡ï¼Œä»¥é¿å…è¿‡å¤šçš„iterationï¼Œé’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è®²moving averagesçš„æ€æƒ³å¸¦å…¥è¿›æ¥ï¼Œè¿™å°±æ˜¯momentumæ–¹æ³•ã€‚ åœ¨momentumä¸­ï¼Œæˆ‘ä»¬çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼š$$v_{dW}= \\beta v_{dW}+(1- \\beta)dW$$$$v_{db}= \\beta v_{db}+(1- \\beta)db$$$$W:=W- \\alpha v_{dW}$$$$b:=b - \\alpha v_{db}$$åœ¨å¾ˆå¤šçš„æ–‡çŒ®ä¸­ï¼Œä¸Šé¢çš„\\(1- \\beta\\)é¡¹è¢«çœç•¥æ‰äº†ï¼Œè¿™æ ·åšåªæ˜¯å°†ç­‰å¼ç­‰é‡åšäº†ç¼©æ”¾ï¼Œå¹¶ä¸å½±å“å®é™…çš„æ•ˆæœï¼ŒNgè¡¨ç¤ºï¼Œä¸¤ç§æ–¹å¼çš„momentumå‡ ä¹æ²¡æœ‰å·®åˆ«ï¼Œå¤§å®¶å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚ å®é™…ä¸Šï¼Œmomentumå¯ä»¥ç†è§£ä¸ºå°†æ•°æ¬¡ä¹‹å‰è¿­ä»£è¿‡ç¨‹ä¸­çš„gradientå˜åŒ–ä¹Ÿå¸¦å…¥åˆ°äº†è¿™æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºå°±æ˜¯å¸¦å…¥äº†ä¸€ç§gradientå˜åŒ–çš„è¶‹åŠ¿ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„æ§åˆ¶gradient descentçš„æ–¹å‘å’Œå¤§å°ã€‚ä¾‹å¦‚ä¸Šå›¾ä¸­çš„çºµå‘æ–¹å‘ä¸­ï¼ŒåŠ å…¥momentumåå¯ä»¥è½»æ¾çš„å°†çºµå‘æ¢¯åº¦æ­£è´ŸæŠµæ¶ˆåˆ°è¿‘ä¼¼0ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘åœ¨gradient descentåœ¨çºµå‘çš„åå¤è¿­ä»£ï¼Œå› ä¸ºï¼Œé‚£æ—¢æ˜¯æ— ç”¨åŠŸï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¸æ„¿æ„çœ‹åˆ°çš„æƒ…å†µã€‚ å¦å¤–ï¼ŒNgç»™æˆ‘ä»¬äº†ä¸€ä¸ª\\(\\beta\\)çš„ç†æƒ³å–å€¼ï¼Œæ—¢0.9ï¼Œè¿™ä¸ªå€¼å¤§çº¦å–äº†å‰10æ¬¡è¿­ä»£ç»“æœçš„moving averagesã€‚å¦å¤–ï¼ŒNgè¡¨ç¤ºï¼Œåœ¨momenä¸­å¾ˆå°‘ä½¿ç”¨bias correctionï¼Œå› ä¸º10æ¬¡è¿­ä»£ä¹‹åï¼Œè¿™ç§é—®é¢˜å¾ˆå¿«å°±ä¼šæ¶ˆé™¤ï¼Œè€Œä¸€èˆ¬çš„gradient descentï¼Œiterationæ¬¡æ•°è¿œè¿œå¤§äº10æ¬¡ã€‚ RMSpropé™¤äº†momentumï¼Œè¿˜æœ‰ä¸€äº›ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤§åé¼é¼çš„RMSprop(root means square prop)ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦ä½“ç°äº†squareçš„åº”ç”¨ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œåœ¨æ¯æ¬¡çš„è¿­ä»£ä¸­ï¼š$$S_{dW}= \\beta S_{dW} + (1- \\beta)(dW)^2$$$$S_{db}= \\beta S_{db}+(1- \\beta)(db)^2$$$$W:=W- \\alpha \\frac{dW}{\\sqrt {S_{dW}}+ \\epsilon}$$$$b:=b- \\alpha \\frac{db}{\\sqrt {S_{db}} + \\epsilon}$$\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\) å‡è®¾åœ¨gradient descentä¸­ï¼Œ\\(W\\)ä¸‹é™é€Ÿç‡å¤ªä½ï¼Œä¹Ÿå°±æ˜¯\\(dW\\)å¤ªå°ï¼Œé‚£ä¹ˆåœ¨RMSpropçš„è¿­ä»£ä¸­ï¼Œ\\(dW\\)å°†ä¼šé™¤ä»¥ä¸€ä¸ªå¾ˆå°çš„å€¼\\( \\sqrt{S_{dW}}\\)ï¼Œä¹Ÿå°±æ˜¯\\(W\\)å°†ä¼šå‡å»ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œåä¹‹äº¦ç„¶ã€‚ é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ç¼“è§£äº†ä¸Šå›¾æ‰€ç¤ºçš„æƒ…å†µï¼Œæ”¹å–„äº†gradient descentçš„åˆç†æ€§ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„\\(\\alpha\\)å»å®ç°æ›´å¿«çš„gradient descent. Adamæˆ‘ä»¬çœ‹åˆ°äº†momentumå’ŒRMSpropä¼˜åŒ–æ–¹æ³•çš„å‰å®³ä¹‹å¤„ï¼Œç°åœ¨Adamæ–¹æ³•æ¨ªç©ºå‡ºä¸–ï¼Œä»–èåˆäº†momentumå’ŒRMSpropï¼Œä»–æ˜¯å¦‚ä½•èåˆçš„å‘¢ï¼Œæˆ‘ä»¬æŠŠmomentumä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_1\\)ï¼ŒæŠŠRMSpropä¸­çš„\\( \\beta\\)å‘½åä¸º\\( \\beta_2\\)ï¼Œåœ¨ç¬¬\\(t\\)æ¬¡è¿­ä»£ä¸­ï¼š$$v_{dW}= \\beta_1 v_{dW}+(1- \\beta_1)dW, \\qquad v_{db}= \\beta_1 v_{db}+(1- \\beta_1)db$$$$s_{dW}= \\beta_2 s_{dW}+(1- \\beta_2)(dW)^2, \\qquad s_{db}= \\beta_2 s_{db}+(1- \\beta_2)(db)^2$$åŠ ä¸Šbias correctionå$$v^{corrected}{dW}= \\frac{v{dW}}{1- \\beta^t_1}, \\qquad v^{corrected}{db}= \\frac{v{db}}{1- \\beta^t_1}$$$$s^{corrected}{dW}= \\frac{s{dW}}{1- \\beta^t_2}, \\qquad s^{corrected}{db}= \\frac{s{db}}{1- \\beta^t_2}$$$$W:=W- \\alpha \\frac{v^{corrected}{dW}}{ \\sqrt{s^{corrected}{dW}}+ \\epsilon}$$$$W:=W- \\alpha \\frac{v^{corrected}{db}}{ \\sqrt{s^{corrected}{db}}+ \\epsilon}$$å…¶ä¸­ï¼Œ\\(\\epsilon\\)æ˜¯ä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0çš„ä¸€ä¸ªitemï¼Œå–å€¼å»ºè®®ä¸º\\(10^{-8}\\)ï¼Œ\\(\\beta_1\\)å»ºè®®å–å€¼0.9ï¼Œ\\(\\beta_2\\)å»ºè®®å–å€¼0.999ã€‚ Learning rate decayåœ¨gradient descentä¸­ï¼Œéšç€è¿­ä»£çš„æ·±åº¦ï¼Œè¶Šæ¥è¶Šé è¿‘minimumï¼Œæˆ‘ä»¬éœ€è¦æ›´å°çš„learning rateï¼Œä»¥é¿å…è¶Šè¿‡minimumï¼Œå¸¸ç”¨çš„learning decayæ–¹æ³•æœ‰ï¼š$$\\alpha = \\frac{1}{1+decayRateepochNum} \\alpha_0$$$$\\alpha = 0.95^{epochNum} \\alpha_0$$$$\\alpha = \\frac{k}{\\sqrt{epochNum}} \\alpha_0$$è¿™äº›æ–¹æ³•éƒ½å¯ä»¥è®©\\(\\alpha\\)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œæ…¢æ…¢å˜å°ï¼Œå¯ä»¥æ›´å¥½çš„é€¼è¿‘minimum. Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng An overview of gradient descent optimization algorithms","link":"/2017/09/27/course-deep-learning-course2-week2/"},{"title":"Learning Notes-Deep Learning, course3, week1","text":"è¯¾ç¨‹3ä¸»è¦è®²çš„æ˜¯deep learningä¸­çš„ä¸€äº›strategyï¼Œè¿™äº›strategyå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿçš„åˆ†ææ¨¡å‹æ‰€å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æœ‰åå·®è€Œå¯¼è‡´çš„äººåŠ›ä»¥åŠæ—¶é—´çš„æµªè´¹ï¼Œè¿™ä¸€ç‚¹å¯¹äºå›¢é˜Ÿå°¤å…¶é‡è¦ã€‚ æˆ‘ä»¬ä¸€èµ·æ¥recapä¸€ä¸‹week1çš„è¯¾ç¨‹ Orthogonalizationå¯¹äºML taskæ¥è¯´ï¼Œæœ‰ä¼—å¤šå› ç´ å½±å“æœ€ç»ˆçš„æ•ˆæœï¼Œè¿™äº›å› ç´ ç›¸äº’çŠ¬ç‰™äº¤é”™ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æå‡æ¨¡å‹æ•ˆæœçš„æ—¶å€™ï¼Œä¸€å®šè¦æŠŠæ‰€æœ‰çš„å› ç´ orthogonalizationä¸€ä¸‹ï¼ŒNgä¸¾çš„ä¾‹å­å°±å¾ˆå½¢è±¡ï¼Œå°±åƒæ˜¾ç¤ºå™¨çš„æ§åˆ¶æŒ‰é’®ä¸€æ ·ï¼Œæ¯ä¸ªæŒ‰é’®å„å¸å…¶èŒï¼Œä¸€ä¸ªæ§åˆ¶é«˜åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å®½åº¦ï¼Œä¸€ä¸ªæ§åˆ¶å¤§å°ï¼Œä¸€ä¸ªæ§åˆ¶æ¢¯åº¦ï¼Œé€šè¿‡å„è‡ªè°ƒæ•´æ¯ä¸€ä¸ªæŒ‰é’®ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„å®Œæˆç”»é¢è°ƒæ•´ã€‚ å¯¹äºorthogonalizationä¼˜åŒ–æ¨¡å‹ï¼ŒNgç»™å‡ºäº†4æ–¹é¢çš„å»ºè®®ï¼š Fit training set well in cost function Fit development set well on cost function Fit test set well on cost function Performs well in real world æˆ‘ä»¬è¯¦ç»†æ¥çœ‹çœ‹è¿™å››æ¡ï¼š å¯¹äºç¬¬ä¸€æ¡ï¼Œé¦–å…ˆæ¨¡å‹å¿…é¡»è¦å¯¹äºtraining setæœ‰è‰¯å¥½çš„æ‹Ÿåˆæ•ˆæœï¼Œå¦‚æœè¿™ç‚¹è¾¾ä¸åˆ°çš„è¯ï¼Œæ¨¡å‹ä¸€å®šæ˜¯high biasï¼Œä¹Ÿå°±æ˜¯under fittingäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å°è¯•é€šè¿‡more complexçš„æ¨¡å‹ï¼Œbigger neural networksæˆ–è€…æ˜¯longer training timeå»æ›´å……åˆ†çš„æ‹Ÿåˆtraining set. å¯¹äºç¬¬äºŒæ¡ï¼Œåœ¨å¾ˆå¥½çš„æ‹Ÿåˆtraining setçš„å‰æä¸‹ï¼Œæˆ‘ä»¬å°±è¦çœ‹çœ‹development setçš„æ•ˆæœäº†ï¼Œå¦‚æœå¯¹äºdevelopment set fitæ•ˆæœä¸å¥½çš„è¯ï¼Œé‚£åŸºæœ¬ä¸Šå°±æ˜¯high varienceï¼Œä¹Ÿå°±æ˜¯over fittingçš„é—®é¢˜äº†ï¼Œè¿™æ—¶å€™ï¼Œregularizationæˆ–è€…more training dataå¯ä»¥è§£å†³è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ å¯¹äºç¬¬ä¸‰æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸¤æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨test setä¸Šè¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤§çš„development setå»æ¶µç›–æ›´å¤šçš„æƒ…å†µï¼Œå¹¶é€šè¿‡æ‰©å……åçš„development seté‡å¤ç¬¬äºŒæ¡çš„æ£€éªŒ å¯¹äºæœ€åä¸€æ¡ï¼Œåœ¨ç¬¦åˆä¸Šä¸‰æ¡çš„å‰æä¸‹ï¼Œå¦‚æœæ¨¡å‹åœ¨real worldä¸­è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºæˆ‘ä»¬çš„development å’Œtest setä¸real worldç›¸å·®æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚çŒ«è„¸æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬çš„data setéƒ½æ˜¯é«˜æ¸…çš„å›¾åƒï¼Œä½†æ˜¯real world ä¸­ï¼Œéƒ½æ˜¯åƒç´ å¾ˆä½çš„å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®©development å’Œtest setæ›´æ¥è¿‘ä¸real worldï¼Œç„¶åé‡å¤ä¸Šé¢çš„æ­¥éª¤ã€‚ é€šè¿‡è¿™å››ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡orthogonalizationæ¥å®Œæˆæ¨¡å‹çš„è°ƒæ•´ MetricSingle number evaluationMetricæ— ç–‘æ˜¯ML taskä¸­å¾ˆé‡è¦çš„ç¯èŠ‚ï¼Œé€šè¿‡metricï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„ä¼˜è‰¯å·®å¼‚ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©å‡ºæœ€ç†æƒ³çš„æ¨¡å‹ã€‚ ä½†æ˜¯metricæŒ‡æ ‡ç³éƒæ»¡ç›®ï¼Œä¾‹å¦‚å¯¹äºä¸¤ä¸ªæ¨¡å‹ï¼Œæ¨¡å‹Açš„precisioné«˜äºBçš„ï¼Œä½†æ˜¯Açš„recallåˆä½äºBï¼Œè¿™æ—¶å€™å°±ä¸å¤ªå¥½è¯„ä»·ä¸¤ä¸ªæ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨å•ä¸€çš„æ•°å­—è¯„ä»·æŒ‡æ ‡ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥ç”¨F1-scoreæ¥è¿›è¡Œè¯„ä¼°ï¼Œsingle number evaluation metricæ˜¯æˆ‘ä»¬åšmetricsæ—¶ä¸€å®šè¦æ³¨æ„çš„ Satisficing and optimizingåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚æˆ‘ä»¬ä¸ä»…ä»…è¦æ±‚æ¨¡å‹çš„æŒ‡æ ‡ï¼Œè¿˜å¯¹å…¶ä»–çš„ï¼Œä¾‹å¦‚æ¨¡å‹æ—¶é—´ä¼šæœ‰è¦æ±‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰å¾ˆé«˜çš„æ¨¡å‹accuracyï¼Œä½†æ˜¯å´å¾ˆè€—è´¹æ—¶é—´ï¼Œé‚£æ˜¯æˆ‘ä»¬ä¸èƒ½æ¥å—çš„ï¼Œå¦‚ä¸‹å›¾ä¾‹å­ï¼šå›¾ä¸­çš„accuracyæ˜¯optimizing metricï¼Œé€šå¸¸æ›´é«˜çš„accuracyå°±ä»£è¡¨è¿™classifieræ›´åŠ çš„ä¼˜ç§€ï¼›ä½†æ˜¯ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå¿…é¡»ä½äº100ms çš„running timeä½œä¸ºsatisficing metricï¼Œé€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬æœ‰\\(N\\)ä¸ªmetricsï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„optimizing metricå¿…é¡»åªæœ‰ä¸€ä¸ªï¼Œå‰©ä¸‹çš„\\(N-1\\)metrics éƒ½æ˜¯satisficing metricsï¼Œåªè¦ä»¥thresholdå½¢å¼è¿›è¡Œé™å®šå°±å¯ä»¥äº†ã€‚ Data setDistributionså¯¹äºtraining setï¼Œdev setå’Œtest setæ¥è¯´ï¼Œæ‰€æœ‰dataä¸€å®šè¦ä¿è¯æœä»åŒä¸€ä¸ªdata distributionï¼Œä¾‹å¦‚çŒ«è„¸å®éªŒï¼Œå¦‚æœtraining setæ˜¯é«˜æ¸…å¤§å›¾è€Œdev setæ˜¯æ¨¡ç³Šå›¾åƒï¼Œé‚£ä¹ˆæœ€ç»ˆä¸€å®šå¾ˆéš¾è·å¾—ç†æƒ³çš„metric. æœ€é‡è¦çš„æ˜¯ï¼Œä½ æ‰€æ„å»ºæ¨¡å‹çš„dataï¼Œä¸€å®šå’Œæ¨¡å‹åº”ç”¨åœºæ™¯çš„dataåœ¨åŒæ ·çš„distributionä¸‹ï¼ŒNgç»™å‡ºçš„guidelineæ˜¯ Choose a development set and test set to reflect data you expect to get in the future and consider important to do well. Size of data setä¼ ç»Ÿçš„ML taskä¸­ï¼Œdatasetçš„åˆ†å¸ƒä¸€èˆ¬å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šä½†æ˜¯åœ¨big dataæ—¶ä»£ï¼Œä¸€èˆ¬é‡‡ç”¨ä¸‹å›¾ï¼šNgåŒæ ·ç»™å‡ºäº†guidelineï¼š Set uop the size of test set to give a high confidence in the overall performance of the system. Test set helps evaluate the proformance of the final classifier which could be less than 30% of the whole data set The development set has to be big enough to evaluate different ideas Change data set and metricChange data setè¿™ä¸ªé—®é¢˜çš„åŸå› ï¼Œå…¶å®å°±æ˜¯data setä¸åœ¨åŒä¸€distributionä¸‹çš„é—®é¢˜ï¼Œå¦‚æœæ¨¡å‹åœ¨devå’Œtest set ä¸Šéƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°å’Œmetricï¼Œä½†æ˜¯åœ¨real worldä¸­æ•ˆæœå¹¶ä¸å¥½ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„ä¸€å®šå°±æ˜¯æ”¹å˜data setï¼Œè®©dev/test setä¸real worldåœ¨åŒä¸€distributionä¸‹ Change metricæˆ‘ä»¬ä½¿ç”¨MLæ¥è§£å†³ç°å®ä¸­çš„é—®é¢˜æ—¶ï¼Œmetricä¹Ÿä¸æ˜¯ä¸€æˆä¸å˜çš„ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„æƒ…å†µåšå‡ºä¸€äº›æ”¹å˜ï¼Œä¾‹å¦‚ï¼Œè‰²æƒ…å›¾åƒè¯†åˆ«ä¸­ï¼Œæˆ‘ä»¬è¯¯è®²éè‰²æƒ…è¯†åˆ«ç»´è‰²æƒ…å›¾åƒï¼Œæ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†æ˜¯å°†è‰²æƒ…å›¾åƒè¯†åˆ«æˆéè‰²æƒ…å›¾åƒåˆ™æ˜¯ä¸å¯æ¥å—çš„ï¼Œç›¸ä¼¼çš„ï¼Œé£æ§ç³»ç»Ÿä¸­ï¼Œå°†é£é™©ç”¨æˆ·åˆ†ç±»ä¸ºæ­£å¸¸ç”¨æˆ·çš„é”™è¯¯ï¼Œæ¯”æŠŠæ­£å¸¸ç”¨æˆ·åˆ†ç±»ä¸ºé£é™©ç”¨æˆ·çš„é”™è¯¯è¦ä¸¥é‡å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨ç±»ä¼¼çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„metricéœ€è¦éšç€ä¸šåŠ¡åœºæ™¯åšå‡ºä¸€äº›æ”¹å˜ã€‚ æ­£å¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ¨¡å‹erroræ˜¯ï¼š$$Error = \\frac{1}{m_{dev}} \\sum^{m_{dev}}{i=1} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$ä½†æ˜¯åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŠ å…¥ä¸€ä¸ªæƒé‡ï¼Œæ¥å¯¹ä¸¤ç§ä¸åŒé”™è¯¯åŠ ä»¥åŒºåˆ†$$ w^{(i)}=\\left{\\begin{aligned}1 \\quad x^{(i)}notpron \\10 \\quad x^{(i)} pron \\\\end{aligned}\\right.$$$$Error = \\frac{1}{ \\sum w^{(i)}} \\sum^{m{dev}}_{i=1} w^{(i)} \\mathcal { \\hat{y}^{(i)} \\neq y^{(i)} }$$è¿™æ ·å°±æŠŠä¸¤ç§ä¸åŒçš„é—®é¢˜åŒºåˆ†å¼€äº†ã€‚ Improve model performanceæ¨¡å‹performanceçš„æå‡æ˜¯æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•ç¡®å®šæ¨¡å‹è°ƒæ•´çš„å¤§ä½“æ–¹å‘ï¼ŒNgç»™å‡ºäº†å¦‚ä¸‹çš„å›¾æˆ‘ä»¬é»˜è®¤human-levelæ˜¯å¾ˆæ¥è¿‘ç†è®ºè¯¯å·®ï¼Œä¹Ÿå°±æ˜¯Bayes errorï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒhuman-levelï¼Œtraining errorå’Œdev errorè¿™ä¸‰è€…ä¹‹é—´çš„å…³ç³»ï¼Œhuman-levelå’Œtraining errorä¹‹é—´çš„å·®å€¼æ›´å¤§çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦å»å‡å°biasï¼Œåä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å»å‡å°‘varianceï¼Œå…·ä½“çš„æ–¹æ³•ï¼Œè¿˜æ˜¯æˆ‘ä»¬ä¹‹å‰çš„è€å¥—è·¯ã€‚ Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng","link":"/2017/10/12/course-deep-learning-course3-week1/"},{"title":"Learning Notes-Deep Learning, course3, week2","text":"Hi allï¼Œcourse3æ¥åˆ°äº†week2ï¼Œæœ¬å‘¨çš„è¯¾ç¨‹ä¾ç„¶ä¸»è¦æ˜¯å…³äºä¸€äº›learning strategyï¼Œè¿™äº›æ–¹æ³•ç›¸å½“å®ç”¨ã€‚è™½ç„¶ä¸æ˜¯ä»€ä¹ˆå…·ä½“çš„ç®—æ³•ï¼Œä½†éƒ½éƒ½æ˜¯Ngåœ¨ç§‘ç ”å’Œå·¥ä½œä¸­ç§¯ç´¯ä¸‹æ¥çš„å®è´µç»éªŒï¼Œå¯¹äºå®é™…é—®é¢˜ååˆ†æœ‰æ•ˆã€‚ æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ã€‚ Error analysisCarry out error analysisæŒ‰ç…§é€šå¸¸çš„æµç¨‹ï¼Œåœ¨è¿›è¡Œtrainingè¿‡ç¨‹åï¼Œæˆ‘ä»¬åœ¨dev setä¼šè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ï¼Œå¦‚æœdev erroræ¯”training errorå¤§å¾ˆå¤šçš„è¯ï¼Œæˆ‘ä»¬åº”è¯¥å»æ’æŸ¥é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨å‘¢ï¼ŸNgç»™å‡ºäº†solution ä¾‹å¦‚åœ¨cat recognitionä¸­ï¼Œæˆ‘ä»¬å‘ç°é”™åˆ†çš„sampleæœ‰å¾ˆå¤šdogå›¾åƒï¼Œè¿˜æœ‰å¾ˆå¤šçŒ«ç§‘åŠ¨ç‰©çš„å›¾åƒï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ¨¡ç³Šçš„catå›¾åƒã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶è€Œç„¶çš„æƒ³åˆ°ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š è§£å†³ç‹—é”™åˆ†ä¸ºçŒ«çš„é—®é¢˜ è§£å†³çŒ«ç§‘åŠ¨ç‰©è¢«é”™åˆ†æˆçŒ«çš„é—®é¢˜ æå‡æ¨¡ç³Šå›¾åƒè¢«è¯¯åˆ†çš„é—®é¢˜ å¯æ˜¯ç”±äºæˆ‘ä»¬ç²¾åŠ›å’Œæ—¶é—´éƒ½æœ‰é™ï¼Œéœ€è¦æ‰¾å‡ºè¯¯åˆ†æœ€ä¸»è¦çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯æŠŠæ‰€æœ‰é”™åˆ†çš„å›¾åƒç½—åˆ—å‡ºæ¥ï¼Œæˆ–è€…éšæœºæŠ½æ ·ä¸€å®šçš„å›¾åƒï¼Œåˆ†ææ¯ç§é”™è¯¯å®ƒæœ‰å¤šå°‘ï¼Œå é”™åˆ†å›¾åƒå¤šå°‘æ¯”ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹æˆªå›¾æ¯ä¸€ä¸ªé”™åˆ†çš„å›¾åƒéƒ½ä¼šè¿›è¡Œæ ‡ç­¾åŒ–çš„ç»Ÿè®¡ï¼Œæœ€åé€šè¿‡ç»Ÿè®¡æ¯ä¸€ä¸ªæ ‡ç­¾ï¼Œæ‰¾å‡ºå½±å“é”™åˆ†æœ€ä¸¥é‡çš„å› ç´ ï¼Œä½œä¸ºæˆ‘ä»¬çš„æ”¹è¿›æ–¹å‘ã€‚ Clean up incorrectly labeled dataåœ¨å¸¸è§çš„é”™è¯¯ä¸­ï¼Œé”™è¯¯çš„labelæ˜¯ä¸€ç§å¾ˆå¸¸è§çš„é—®é¢˜ï¼Œè¿™ç§é—®é¢˜å¾€å¾€æ¥è‡ªäºæ ‡æ³¨æ—¶å€™ï¼Œé”™è¯¯çš„labelä¼šå¯¹trainingé€ æˆè¯¯å¯¼ã€‚ é¦–å…ˆï¼Œå¯¹äºtraining setï¼Œæ¥è¯´ï¼Œincorrectly labeled dataåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿé¦–å…ˆï¼ŒNgå‘Šè¯‰äº†æˆ‘ä»¬ä¸€ä¸ªæ€§è´¨ï¼š DL algorithms are quite robust to random errors in the training set DLå› ä¸ºå…¶è‡ªèº«çš„robustæ€§è´¨ï¼Œå½“training setä¸­æœ‰å°‘è®¸çš„ï¼Œéšæœºäº§ç”Ÿçš„incorrectly labeled dataæ—¶ï¼Œæ•ˆæœå¹¶ä¸ä¼šæœ‰å¤šå·®ï¼Œæˆ‘ä»¬å®Œå…¨ä¸éœ€è¦å»ç®¡ä»–ã€‚ä½†æ˜¯ï¼Œå½“è¿™incorrectly labeled dataå¾ˆå¤šæ—¶å°±ä¸è¡Œäº†ï¼Œå› ä¸ºå®ƒä»¬å¸¦æ¥çš„æ˜¯systematic errorsï¼Œæç«¯çš„æƒ³ï¼Œå¦‚æœæŠŠæ‰€æœ‰çš„ç™½ç‹—éƒ½é”™è¯¯çš„æ ‡æ³¨æˆäº†çŒ«ï¼Œé‚£ä¹ˆè¿™ä¸ªcat recognitionç³»ç»Ÿä¸€å®šä¸ä¼šå¥½ï¼Œå› ä¸ºå®ƒä¸€å®šä¼šæŠŠç™½è‰²çš„ç‹—åˆ¤æ–­æˆä¸ºçŒ«ã€‚ å†æ¥çœ‹çœ‹dev/test setä¸­çš„incorrectly labeled dataï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼Œè¯„ä¼°incorrectly labeled dataå¯¹dev errorå¸¦æ¥äº†å¤šå°‘è´¡çŒ®ï¼Œè§£å†³çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œæ¥çœ‹æˆªå›¾ï¼šæˆ‘ä»¬æŠŠincorrectly labeledä¹Ÿä½œä¸ºä¸€ä¸ªè¦ç´ æˆ–æ ‡ç­¾ï¼Œæ”¾åœ¨é”™åˆ†å›¾åƒåˆ†æçš„è¿‡ç¨‹ä¸­ï¼Œçœ‹çœ‹æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœï¼Œå†å†³å®šincorrectly labeled dataæ˜¯ä¸æ˜¯å½±å“dev errorçš„ä¸»è¦åŸå› ï¼Œæ˜¯å¦å€¼å¾—æˆ‘ä»¬å»fix it up. æœ€åï¼Œå…³äºcorrecting incorrect dev/test set exampleï¼ŒNgç»™å‡ºäº†ä¸€äº›å»ºè®®ï¼š Apply same proces to your dev and test sets to make sure thry continue to come from the same distribution. åœ¨ä¿®æ­£çš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦ä¿è¯devå’Œtest setåŒæ—¶è¢«ä¿®æ­£ï¼Œå¦‚æœä»–ä»¬ä¸å†ç¬¦åˆåŒä¸€distributionï¼Œé‚£ä¹ˆä¼šå¯¹äºåç»­çš„è¯„ä»·å¸¦æ¥ä¸€äº›é—®é¢˜ã€‚ Consider examining examples your algorithm got right as well as ones it got wrong. æˆ‘ä»¬åœ¨æ›´æ­£çš„æ—¶å€™ï¼Œä¸èƒ½åªæ˜¯çœ‹è¢«é”™åˆ†çš„å›¾åƒï¼Œå¯¹äºè¢«æ­£ç¡®åˆ†ç±»çš„ï¼Œä¹Ÿæœ‰å¯èƒ½å­˜åœ¨incorrect labeled çš„æƒ…å†µã€‚ Tran and dev/test data may now come from slightly different distribution æ­£å¦‚åˆšæ‰è®²çš„ï¼ŒDLå¯¹äºtrainingæœ‰ä¸€å®šç¨‹åº¦çš„robustæ€§ï¼Œincorrect labeled dataå¯èƒ½ä¸ä¼šå¯¹training setå¸¦æ¥è¿™äº›é—®é¢˜ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ç”¨å»æ›´æ­£training setï¼Œè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥æ¥å—çš„ã€‚ Build up quickly and iterateæœ€åNgç”¨ä¸€ä¸ªspeech recognitionä½œä¸ºä¾‹å­ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åˆ†æå‡ºå¯èƒ½å½±å“æ•ˆæœçš„ä¸€äº›å› ç´ ï¼š Noisy background Accented speech Far from microphone Young childrenâ€™s speech â€¦é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•æ„é€ æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼ŒNgç»™å‡ºäº†å»ºè®® Set up dev/test set and metric Build initial system quickly Use bias/variance analysis &amp; error analysis to prioritize next steps. æ€»è€Œè¨€ä¹‹ï¼Œguidelineæ˜¯ Build your first system quickly, then iterate. Mismatched training and dev/test dataTraining and testing on different distributionsä¹‹å‰æˆ‘ä»¬å†ä¸‰å¼ºè°ƒè¿‡ä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯training/dev/test setä¸€å®šè¦åœ¨åŒä¸€ä¸ªdistributionä¸‹ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ„¿æœ›æ€»æ˜¯ç¾å¥½çš„è€Œç°å®å¾ˆæ®‹é…·ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šé¢å¯¹ä¸€äº›training and testing on different distributioné—®é¢˜ã€‚ ä¾‹å¦‚åœ¨çŒ«è¯†åˆ«çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹éƒ¨ç½²åˆ°æ‰‹æœºappä¸Šï¼Œæˆ‘ä»¬æ‰‹ä¸Šçš„æ•°æ®åªæœ‰10kæ˜¯ä»æ‰‹æœºæ‹æ‘„è·å¾—çš„ï¼Œè€Œæœ‰200kçš„æ•°æ®æ˜¯ä»ç½‘ç»œä¸Šè·å¾—çš„ï¼Œè¿™ä¸¤ç§å›¾åƒæ˜¾ç„¶ä¸å±äºåŒä¸€distributionï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ é¦–å…ˆæ¥çœ‹option1ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„210kæ•°æ®å……åˆ†æ··åˆåœ¨ä¸€èµ·ï¼Œå…¶ä¸­205kä½œä¸ºtraining setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtest setã€‚è¿™æ ·çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„æ–¹æ³•ï¼Œä½†æ˜¯ï¼Œç¡®å®å¾ˆä¸å¥½çš„ä¸€ä¸ªæ–¹æ³•ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œdev/test setå…¶å®æ‰®æ¼”äº†ä¸€ä¸ªéå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬çš„targetï¼Œä¹Ÿå°±æ˜¯æ•´ä½“çš„ä¼˜åŒ–æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦ä¼˜åŒ–çš„æ–¹å‘æ˜¯appä¸Šçš„å›¾åƒï¼Œè€Œè¿™ç§data setåˆ†å‰²æ–¹æ³•å’Œæˆ‘ä»¬çš„task targetå¹¶ä¸ç¬¦åˆï¼Œå› æ­¤å¹¶ä¸ä¼˜ç§€ã€‚ æˆ‘ä»¬å†æ¥çœ‹option2ï¼Œæˆ‘ä»¬å°†200kçš„æ¥è‡ªç½‘ç»œçš„å›¾ç‰‡å…¨éƒ¨æ”¾å…¥training setï¼Œç„¶åå°†10kçš„appæ•°æ®ï¼Œ5kæ”¾å…¥training setï¼Œ2.5kä½œä¸ºdevï¼Œ2.5ä½œä¸ºtestï¼Œè¿™æ ·åšçš„è¯ï¼Œdev/testå†³å®šçš„target å’Œæˆ‘ä»¬çš„task targetæ˜¯ä¸€è‡´çš„ï¼Œæ‰€ä»¥é•¿è¿œæ¥çœ‹ï¼Œè™½ç„¶option2çš„training/dev setå¹¶ä¸æ˜¯åŒä¸€distributionï¼Œä½†æ˜¯ä»é•¿è¿œçœ‹å®ƒçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚ Bias &amp; variance with mismatched data distributionåœ¨training/dev/test setç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒtraining errorå’Œdev errorå°±å¯ä»¥å®šæ€§æ˜¯å¦å­˜åœ¨high varianceçš„é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“training setå’Œdev setä¸ç¬¦åˆåŒä¸€distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ¤æ–­å°±æ˜¾å¾—æœ‰äº›å›°éš¾äº†ã€‚æˆ‘ä»¬åº”è¯¥æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ è¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä»training setä¸­å–å‡ºä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œå‘½åä¸ºtraining-dev setï¼Œè¿™éƒ¨åˆ†æ•°æ®å°†ä¸å†è¿›è¡Œtrainingï¼Œè€Œæ˜¯ä½œä¸ºè¯„åˆ¤trainingæ•ˆæœçš„ä¸€ä¸ªsetï¼Œæ­¤æ—¶æˆ‘ä»¬å°±æœ‰äº†training errorï¼Œtraining-dev errorå’Œdev errorä¸‰ä¸ªerrorï¼Œå†ç»“åˆhuman errorï¼Œtraining errorå’Œtraining-dev errorä¹‹é—´çš„å·®å€¼å¯ä»¥åæ˜ å‡ºæ¨¡å‹æ˜¯å¦æœ‰high biasæˆ–è€…varianceï¼Œè¿™æ ·å¯ä»¥æ›´ç§‘å­¦çš„æ¥è¯„åˆ¤æ¨¡å‹æ•ˆæœã€‚ç›¸åº”çš„ï¼Œtraining-dev errorå’Œdev errorç›¸å·®è¶Šå¤šï¼Œdata mismatchçš„ç¨‹åº¦è¶Šå¤§ã€‚ Addressing data mismatchæˆ‘ä»¬å¦‚ä½•addressing data mismatchå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹Ngçš„ä¸¤æ¡guidelineï¼š Carry out manual error analysis to try to understand difference between training and dev/test sets Make training data more similar; or collect more data similar to dev/test sets ç†è§£ä¸€ä¸‹ï¼Œé¦–å…ˆæˆ‘ä»¬è¦é€šè¿‡äººå·¥çš„analysiså»åˆ†æå‡ºé€ æˆtraining setå’Œdev setä¹‹é—´distributionä¸åŒçš„åŸå› ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ— æ±½è½¦å™ªå£°ç­‰ç­‰ï¼›ç„¶åæˆ‘ä»¬éœ€è¦æ ¹æ®è¿™äº›å·®åˆ«ï¼Œè®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œç”šè‡³ç›¸é€šã€‚ ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œè¦é¿å…å‡ºç°overfittingçš„æƒ…å†µå‡ºç°ï¼Œä¾‹å¦‚Ngä¸¾å‡ºçš„ä¾‹å­ï¼Œåœ¨è¯†åˆ«è½¦å†…çš„äººå£°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥çš„åˆæˆæ±½è½¦å£°éŸ³ä¸äººçš„å£°éŸ³è®©training setå’Œdev setæ›´åŠ çš„ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„åªç”¨ä¸€æ®µæ±½è½¦å™ªéŸ³å¾ªç¯å¾€å¤çš„å»åšåˆæˆï¼Œä¾‹å¦‚å§1minçš„æ±½è½¦å™ªå£°å¾ªç¯çš„åˆæˆåˆ°1hçš„äººå£°ä¸­ï¼Œé‚£ç»“æœä¸€å®šæ˜¯ä¸å°½å¦‚äººæ„çš„ï¼Œå› ä¸ºå‡ºç°äº†overfitting. Transfer learningä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¤§åé¼é¼çš„transfer learningï¼Œæ‰€è°“transferï¼Œå°±æ˜¯å­˜åœ¨ä¸€ç§ä»Aåˆ°Bçš„è½¬æ¢ï¼Œè€Œä¸”è¿™ç§æƒ…å†µå¾€å¾€æ˜¯Bçš„æ•°æ®é‡å¾ˆå°‘ï¼Œéœ€è¦é€šè¿‡Aæ¥åšä¸€ä¸ªpre-trainingè¿‡ç¨‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹çš„neural networkså‡è®¾è¿™ä¸ªæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªneural networksè®­ç»ƒäº†ä¸€ä¸ªimage recognitionæ¨¡å‹ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°†æœ€åçš„outputï¼Œä»¥åŠoutputå¯¹åº”çš„çš„\\(w\\)å’Œ\\(b\\)ä¹Ÿåˆ é™¤ï¼Œæ›´æ¢æˆä¾‹å¦‚æ”¾å°„æ•°æ®å†è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾ï¼šæˆ‘ä»¬ä¸ä»…ä»…å¯ä»¥æŠŠoutputå±‚æ›´æ¢æˆä¸€ä¸ªæ–°çš„outputå±‚ï¼Œè¿˜å¯ä»¥å°†outputå±‚æ›´æ¢æˆå‡ ä¸ªæ–°å±‚ã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥å°†transferä¹‹å‰çš„è®­ç»ƒè®¤ä¸ºæ˜¯ä¸€ç§pre-trainingï¼Œä½†æ˜¯transfer trainingéœ€è¦æœ‰å‡ ä¸ªæ¡ä»¶ï¼š Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. Multi-task learningç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶çš„åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è§†é¢‘ä¸­è¯†åˆ«è¡Œäººã€è½¦è¾†ã€åœè½¦æ ‡å¿—å’Œçº¢ç»¿ç¯ï¼ŒæŒ‰ç…§å¸¸ç†æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å•ç‹¬çš„æ„å»º4ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™4ä¸ªæ¨¡å‹çš„ç‰¹å¾åœºæ™¯éƒ½æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ„å»º4ä¸ªæ¨¡å‹ç¨å¾®æœ‰ä¸€äº›æµªè´¹ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠè¿™å››ä¸ªä»»åŠ¡åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯Multi-task learning. åœ¨è¿™é‡Œæˆ‘ä»¬çš„æ ‡ç­¾\\(y\\)ï¼Œå°±ä¸å†æ˜¯ä¸€ä¸ªmÃ—1çš„çŸ©é˜µäº†ï¼Œè€Œæ˜¯ä¸€ä¸ªmÃ—4çš„çŸ©é˜µï¼Œå¯¹äºmulti-taskæ¥è¯´ï¼Œåœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ˜¯å¯è¡Œçš„ï¼š Training on a set of tasks that could benefit from having shared lower-level features. Usually: Amount of data you have for eachtask is quite similar. Can train a big enough neural network to do well on all the tasks. End to end learningEnd to end learningæ˜¯éšç€DLå…´èµ·åè€Œäº§ç”Ÿçš„ä¸€ç§learningæ–¹å¼ï¼Œåœ¨end2endä¸­ï¼Œæˆ‘ä»¬ä¸å†å…³æ³¨ä¸€äº›ä¸­é—´çš„æ­¥éª¤ï¼Œä¾‹å¦‚feature selectionæˆ–è€…image processingï¼Œæˆ‘ä»¬åªæ˜¯æŠŠåŸå§‹çš„æ•°æ®å’Œæœ€åçš„ç»“æœå‘Šè¯‰DLï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»çš„å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚ å½“ç„¶end2end ä¹Ÿæ˜¯æœ‰ä¸€äº›ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼šProsï¼š Let the data speak. Less hand-desgining of components needed. Cons: May need large amount of data. Excludes potentially userful hand-designed components. æ€»ä¹‹ï¼Œå¯¹äºend2endæ¥è¯´ï¼Œå¤§æ•°æ®é‡ï¼Œä¸€å®šæ˜¯æœ€é‡è¦çš„å› ç´ ï¼ŒåŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰å¯ä»¥æ‘†è„±ä¼ ç»Ÿçš„ä¸­é—´æ­¥éª¤ï¼Œå½»åº•å®ç°end to end learning. Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng","link":"/2017/10/18/course-deep-learning-course3-week2/"},{"title":"Learning Notes-Deep Learning, course4, week2","text":"æˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹course4çš„week2ï¼ŒCNNçš„çŸ¥è¯†è¿˜æ˜¯è›®ä¸°å¯Œçš„ï¼Œæœ¬å‘¨ä¸»è¦è®²äº†ä¸€äº›ç»å…¸çš„CNNç»“æ„ä»¥åŠä¸€äº›computer visionçš„æŠ€å·§å’ŒçŸ¥è¯†ï¼Œä¸€èµ·recapä¸€ä¸‹ã€‚ Classic NetworksNgä¸€å…±ç»™æˆ‘ä»¬å¸¦æ¥äº†3ä¸ªæœ€ä¸ºç»å…¸çš„CNNç½‘ç»œï¼Œè¿™é‡Œæˆ‘ä¼šç»™å‡ºç½‘ç»œçš„æˆªå›¾å’ŒpaperåŸæ–‡ï¼ŒæŠ½ç©ºæˆ‘ä¹Ÿä¼šçœ‹çœ‹åŸæ–‡ï¼Œå¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·æ¥çœ‹çœ‹ã€‚ LeNet-5LÃ©cun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324. AlexNetKrizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[J]. Communications of the Acm, 2012, 60(2):2012. VGG-16Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014. ä»¥ä¸Šå¯ä»¥è¯´æ˜¯æœ€ä¸ºç»å…¸çš„ä¸‰ä¸ªcnnç½‘ç»œäº†ï¼Œå¤§å®¶å¯ä»¥é€šè¿‡é˜…è¯»paperè·å¾—ä¸€äº›è¯¦ç»†çš„çŸ¥è¯†ï¼Œéƒ½æ˜¯ç»å…¸ä¹‹ä½œï¼Œæ¨èé˜…è¯»ã€‚ Residual Networks(ResNets)å¯¹äºresidual networksï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå…·ä½“çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„å…·ä½“åŸç†å¯ä»¥é€šè¿‡ä¸‹å›¾çš„residual blockæ¥çœ‹çœ‹ï¼šå…¶å®ï¼Œresidual blockæ˜¯æŠŠ\\(a^{[l]}\\)ç›´æ¥ä½œä¸º\\(a^{[l+2]}\\)è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$å…¶ä¸­\\(g\\)æ˜¯activation functionï¼Œå¦‚ReLUç­‰ã€‚è¿™ç§æ€æƒ³ä¹Ÿè¢«ç§°ä¸ºshort circuitæˆ–è€…skip connectionã€‚æŠŠä¸Šé¢çš„residual blockä¸²è”èµ·æ¥ï¼Œå°±å˜æˆäº†æˆ‘ä»¬çš„residual networksï¼Œå¦‚ä¸‹å›¾ï¼šResidual networksæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯ï¼Œæ™®é€šnetworkséšç€layerå¢å¤§ï¼Œtraining errorç†è®ºä¸Šæ˜¯ä¼šå˜å°ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šåœ¨æŸä¸ªæœ€å°ç‚¹åå¢å¤§ï¼Œä½†æ˜¯residual networksåˆ™ä¼šä¸¥æ ¼çš„éšç€layerå¢å¤šè€Œå‡å°training errorï¼Œä¸‹é¢æ˜¯åŸæ–‡ï¼šHe K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778. Network in Network and 1Ã—1 Convolutionsé€šå¸¸æˆ‘ä»¬ä½¿ç”¨çš„filterï¼Œéƒ½æ˜¯å¥‡æ•°çš„kernel matrixï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ1Ã—1çš„filterä¹Ÿä¼šè¢«æˆ‘ä»¬ä½¿ç”¨ï¼Œå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼šä»è¿™å¼ å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œ1Ã—1çš„filterå¯ä»¥å‹ç¼©inputçš„channel(depth)ï¼Œå› æ­¤1Ã—1filterè¿˜æ˜¯æœ‰ä¸€äº›æ„æ€çš„ã€‚ä¸‹é¢æ˜¯åŸæ–‡ï¼šLin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013. Inception Networkå…³äºinception networkï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼šå¯¹äºåŒä¸€ä¸ªinputï¼Œæˆ‘ä»¬åˆ†åˆ«é‡‡ç”¨ä¸åŒçš„filterï¼Œç”šè‡³max poolingï¼Œåœ¨ä¿è¯è¾“å‡ºçš„hightå’Œwidthä¸€æ ·çš„å‰æä¸‹ï¼Œå°†ç»“æœå †å èµ·æ¥ï¼Œä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºï¼Œè¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æŒ‘é€‰filterï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„å¯èƒ½éƒ½äº¤ç»™networkï¼Œè®©å®ƒæ¥å†³å®šå»é€‰æ‹©ä»€ä¹ˆæ ·å­çš„ç»“æ„ã€‚åŸæ–‡æ˜¯ï¼šSzegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]. Computer Vision and Pattern Recognition. IEEE, 2015:1-9.åŒæ—¶ï¼ŒNgåœ¨è¯¾ç¨‹ä¸Šè¯´æ˜ï¼Œinception network ä¸­å¤§é‡ä½¿ç”¨äº†1Ã—1filteræ¥é™ä½è®¡ç®—é‡ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹Inception å•å…ƒçš„å›¾è§£ï¼šè¿™ä¸€å‘¨çš„è¯¾ç¨‹æ„Ÿè§‰é‡å¾ˆå¤§ï¼Œä»‹ç»äº†å¾ˆå¤šçš„ç½‘ç»œï¼Œæˆ‘å‡†å¤‡ä¸‹é¢æ…¢æ…¢çš„çœ‹çœ‹è¿™äº›paperï¼Œç«™åœ¨å·¨äººçš„è‚©ä¸Šå»çœ‹ä¸–ç•Œï¼Œä¸€å®šä¼šæœ‰åˆ«æ ·çš„é£æ™¯ï¼ Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng","link":"/2017/11/29/course-deep-learning-course4-week2/"},{"title":"Learning Notes-Deep Learning, course4, week1","text":"Hi, all. æœ€è¿‘å¼€å§‹ä¼‘å‡äº†ï¼Œå¯ä»¥æœ‰ç©ºç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä¸€æ–¹é¢è¡¥ä¸€è¡¥å‰é¢çš„ä½œä¸šï¼Œä¸€æ–¹é¢ç»§ç»­è‡ªå·±çš„å­¦ä¹ ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åˆ°äº†course4ï¼Œä¹Ÿå°±æ˜¯convolutional neural networks çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ï¼ Convolutionåœ¨è¯¾ç¨‹ä¸­ï¼ŒNgä»edge detectionçš„è§’åº¦æ¥ç»™å¤§å®¶è®²äº†è®²convolutionï¼Œå› ä¸ºæœ¬äººæ˜¯image processingå‡ºèº«ï¼Œæ‰€ä»¥è®¤ä¸ºNgåœ¨è¿™é‡Œè®²çš„è¿˜æ˜¯å¾ˆæµ…æ˜¾æ˜“æ‡‚çš„ï¼Œæˆ‘å°±ä¸å†ä¸“é—¨çš„markdownã€‚ä¸»è¦æ¥çœ‹çœ‹convolutionä¸­çš„ä¸€äº›æŠ€å·§ã€‚ Paddingæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œåœ¨æœ€çº¯ç²¹çš„convolutionä¸­ï¼Œæˆ‘ä»¬å‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( (n-f+1) *(n-f+1)\\)ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»“æœçš„å°ºå¯¸å˜å°äº†ã€‚å¦‚æœæƒ³è®©è¾“å‡ºimageçš„å°ºå¯¸ä¸å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦ä½¿ç”¨å¤§åé¼é¼çš„paddingäº†ã€‚ Paddingå…¶å®å°±æ˜¯è¡¨ç¤ºï¼Œåœ¨åŸå§‹imageä¸­ï¼Œå‘å¤–æ‰©å¤§å¤šå°‘å°ºå¯¸ï¼Œä¸€èˆ¬æˆ‘ä»¬ä¼šä½¿ç”¨ç®€å•å¤åˆ¶ç›¸é‚»å…ƒç´ å€¼çš„æ–¹æ³•è¿›è¡Œæ‰©å……ã€‚å‡è®¾å¯¹äºä¸€ä¸ª\\(6 *6\\)çš„åŸå§‹imageï¼Œé‡‡ç”¨\\(3 *3\\)çš„filterï¼ŒåŠ ä¸Š\\(p=1\\)çš„paddingï¼Œé‚£ä¹ˆåŸå§‹å›¾åƒå°ºå¯¸å˜æˆäº†\\(8 * 8\\)ï¼Œç»“æœå˜æˆäº†\\(6 *6\\)ï¼ŒåŸå§‹imageå’Œç»“æœimageä¸€æ¨¡ä¸€æ ·ï¼äºæ˜¯åŠ å…¥äº†paddingçš„convolutionå…¬å¼å°±æˆäº†\\( (n+2p-f+1) *(n+2p-f+1)\\). åœ¨è¿™é‡ŒNgå¼•å…¥äº†ä¸¤ä¸ªæ¦‚å¿µï¼Œvalidå’Œsame convolutionsï¼Œæ‰€è°“valid convolutionï¼Œå°±æ˜¯æ²¡æœ‰padding çš„convolutionï¼›æ‰€è°“same convolutionï¼Œå°±æ˜¯è¾“å…¥è¾“å‡ºçš„å°ºå¯¸å®Œå…¨ä¸€æ ·ã€‚ Strideç»§paddingä¹‹åï¼Œè¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„å‚æ•°ï¼Œå°±æ˜¯æ­¥é•¿strideï¼Œæ­¥é•¿strideå†³å®šäº†filteråšconvolutionæ—¶å€™çš„æ­¥é•¿ï¼Œå¦‚æœstride=1ï¼Œé‚£ä¹ˆfilterå°±ä¼šæŒ¨ç€è®¡ç®—ï¼Œå¦‚æœstride=2ï¼Œé‚£ä¹ˆå°±ä¼šè·³è·ƒè¿™è¿›è¡Œè®¡ç®—ã€‚ æ€»ç»“ä¸€ä¸‹ï¼Œå‡è®¾åŸimageå°ºå¯¸æ˜¯\\(n *n\\)ï¼Œconvolution filterå°ºå¯¸æ˜¯\\(f *f\\)ï¼Œpaddingå€¼æ˜¯\\(p\\)ï¼Œstrideå€¼æ˜¯\\(s\\)é‚£ä¹ˆæœ€ç»ˆçš„ç»“æœimageå°ºå¯¸åº”è¯¥æ˜¯\\( ( \\frac {n+2p-f}{s}+1) *( \\frac {n+2p-f}{s}+1)\\)ï¼Œå¦‚æœé™¤ä¸å°½çš„è¯ï¼Œæˆ‘ä»¬é€‰æ‹©å‘ä¸‹å–æ•´ï¼Œä¹Ÿå°±æ˜¯ä¸è¶³ä»¥åšconvolutionçš„åŒºåŸŸï¼Œæˆ‘ä»¬é€‰æ‹©æ”¾å¼ƒã€‚ ##Convolution over Volumeå¯¹äºä¸€èˆ¬çš„å›¾åƒå¤„ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„éƒ½æ˜¯RGBå›¾åƒï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒRGBå›¾åƒæœ‰ä¸‰ä¸ªchannelï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œconvolutionåº”è¯¥å¦‚ä½•åšï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„å›¾ï¼šå‡è®¾æˆ‘ä»¬çš„å›¾åƒæ˜¯6Ã—6Ã—3ï¼Œä¹Ÿå°±æ˜¯hightÃ—widthÃ—channel(depth)ï¼Œå› æ­¤å¯¹åº”çš„filterä¹Ÿè¦æœ‰3çš„channel(depth)ï¼Œæœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª4Ã—4çš„ç»“æœã€‚ å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸æ­¢ä¸€ä¸ªfilterï¼Œå¦‚å›¾æˆ‘ä»¬åŠ å…¥äº†ä¸¤ä¸ªä¸åŒçš„filterï¼Œä»–ä»¬çš„å¤§å°éƒ½æ˜¯3Ã—3Ã—3ï¼Œäºæ˜¯æœ€ç»ˆçš„ç»“æœå°±æ˜¯4Ã—4Ã—2ï¼Œè¯·æ³¨æ„ï¼šç»“æœçš„channelæ•°ç›®å–å†³äºfilterçš„ä¸ªæ•°ï¼Œè€Œå’Œè¾“å…¥çš„channelæ²¡æœ‰ä»»ä½•å…³ç³»ã€‚ CNNConvolution Layerä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹CNNç½‘ç»œä¸­çš„ä¸€ä¸ªlayerçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Œé¦–å…ˆæ¥çœ‹æˆªå›¾ï¼šè¿™å¼ å›¾ååˆ†å¤æ‚ï¼Œæˆ‘ä»¬ä¸€èµ·ä»”ç»†çœ‹çœ‹è¿™å¼ å›¾ï¼Œå¯¹äºä¸€ä¸ª6Ã—6Ã—3çš„RGBå›¾åƒï¼Œæˆ‘ä»¬ç”¨äº†ä¸¤ä¸ª3Ã—3Ã—3çš„filterï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¾“å…¥imageçœ‹åš\\(x\\)ï¼Œä¹Ÿå°±æ˜¯\\(a ^{[0]}\\)ï¼Œfilterçœ‹åš\\(w ^{[1]}\\)ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯\\(w ^{[1]}a ^{[0]}\\)ï¼Œæˆ‘ä»¬å†åŠ ä¸Šä¸€ä¸ªbiasé¡¹\\(b^{[1]}\\)ï¼Œé‚£ä¹ˆå°±è·å¾—äº†ä¸€ä¸ªliner output\\(w ^{[1]}a ^{[0]}+b^{[1]}\\)ï¼Œæˆ‘ä»¬å†ä½¿ç”¨ä¸€ä¸ªnon-liner functionä¾‹å¦‚ReLUï¼Œå¦‚æ­¤è·å¾—ä¸€ä¸ª4Ã—4Ã—2çš„outputã€‚å¦‚æ­¤å°±æ˜¯CNNçš„ä¸€ä¸ªlayer. å¦‚æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒCNNå’Œä¹‹å‰çš„DNNå®è´¨ä¸Šéƒ½å­˜åœ¨ä¸€ç§liner functionåˆ°non-liner functionçš„è½¬åŒ–ï¼Œé€šè¿‡non-liner functionå»classifyçº¿æ€§ä¸å¯åˆ†çš„dataï¼Œå¦å¤–ï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸€ä¸ªfilterå°±å¯ä»¥è·å¾—ä¸€ä¸ªä¸åŒçš„featureï¼Œå¤šä¸ªfilterå¯ä»¥è®©æˆ‘ä»¬ä»å¤šä¸ªè§’åº¦å»classify data. å¦å¤–ï¼Œç›¸æ¯”è¾ƒäºfully connected çš„DNNï¼ŒCNNæ‰€éœ€è¦çš„parametersä¹Ÿå°‘äº†å¾ˆå¤šï¼Œè¿™ä¸€ç‚¹å€¼å¾—æˆ‘ä»¬æ³¨æ„ã€‚ PoolingPoolingåŸç†è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼šé¦–å…ˆæˆ‘ä»¬æ¥çœ‹çœ‹max poolingï¼Œå¦‚å›¾ï¼Œæˆ‘ä»¬å–filterå°ºå¯¸\\(f=2\\)ï¼Œstrideå¤§å°\\(s=2\\)ï¼Œå¯¹äºä¸€ä¸ªfilterä¸­çš„å…ƒç´ ï¼Œæˆ‘ä»¬å–maxä½œä¸ºè¾“å‡ºï¼›ç›¸å¯¹åº”çš„ï¼Œå¦‚æœæˆ‘ä»¬å–averageï¼Œé‚£ä¹ˆå°±æˆäº†average poolingï¼Œpoolingä¸­çš„hyperparameteråªæœ‰filterå°ºå¯¸\\(f\\)å’Œstrideå¤§å°\\(s\\)ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œpoolingè¿‡ç¨‹ä¸­ä¸å­˜åœ¨å­¦ä¹ è¿‡ç¨‹ï¼Œno parameters to learn! Fully Connected layerFully connected layeråœ¨CNNå…¶å®å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦å°†inputå±•å¼€ï¼ŒæŒ‰ç…§DNNçš„æ–¹æ³•è¿›è¡Œfully connectedå°±å¯ä»¥äº†ã€‚ ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰prameterå˜åŒ–çš„æ‰ç®—ä¸€å±‚ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸ºpoolingæ˜¯ä¸€ä¸ªlayerï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¸€ä¸ªæœ€ç®€å•çš„CNNåšä¾‹å­ï¼šCONV-POOL-CONV-POOL-FC-FC-Softmaxï¼Œæˆ‘ä»¬è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„5å±‚çš„CNNï¼Œåœ¨ä¸‹å‘¨çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»å…¸çš„CNNæ¡†æ¶ï¼Œè¿™é‡Œå°±ä¸å†å¤è¿°ã€‚ Why Convolutionså…³äºè¿™ä¸ªé—®é¢˜ï¼ŒNgç»™å‡ºäº†ä¸¤ä¸ªæ„è§ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š Parameter sharing: A feature detector (such as a vertical edge detetor) thatâ€™s useful in one part of image is probably useful in another part of the image.Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Reference Deep learning-Coursera Andrew Ng Deep learning-ç½‘æ˜“äº‘è¯¾å ‚ Andrew Ng","link":"/2017/11/26/course-deep-learning-course4-week1/"},{"title":"ä»å‡¸å‡½æ•°åˆ°æ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•","text":"è®°å¾—æˆ‘åœ¨å’Œä¼˜ç”·ä¸€èµ·ç ”ç©¶logistic regressionçš„æ—¶å€™ï¼Œä»–é—®äº†æˆ‘å‡ ä¸ªéå¸¸å°–é”çš„é—®é¢˜ï¼Œè®©æˆ‘é¡¿æ—¶å“‘å£æ— è¨€ æ€ä¹ˆä¿è¯logistic regressioné€šè¿‡gradient descentæ‰¾åˆ°çš„æ˜¯æœ€ä¼˜è§£ï¼› ä¸ºä»€ä¹ˆlogistic regressionå¯ä»¥ç”¨newtonâ€™s methodå‘¢ï¼Ÿ Newtonâ€™s methodä¸­Hessian matrixå¿…é¡»positive definiteæœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Œlog cost functionèƒ½ä¿è¯å—ï¼Ÿ è¿™äº›ç»†èŠ‚é—®é¢˜ï¼Œè¯´å®è¯æˆ‘ä¹Ÿæ²¡æœ‰è®¤çœŸçš„æƒ³è¿‡ã€‚åœ¨å¤¸å¥–ä»–ä¹‹ä½™ï¼Œæˆ‘ä»¬ä¹Ÿä¸€èµ·å¼€å§‹äº†ç ”ç©¶ï¼Œå¸Œæœ›ä»ä¸­å­¦ä¹ åˆ°ä¸€äº›æ›´æ·±å±‚çš„ä¸œè¥¿ï¼Œè¶ç€ç°åœ¨æœ‰ä¸ªblogåˆ†äº«ç»™å¤§å®¶ å‡¸å‡½æ•°(Convex function)åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºæœ¯è¯­çš„å€¡è®®ã€‚ä¸­æ–‡é‡Œçš„â€œå‡¸å‡½æ•°â€ï¼Œçœ‹ä¸Šå»æ˜¯å‡¹ä¸‹å»çš„ï¼Œå¯¹åº”çš„ï¼Œä¸­æ–‡é‡Œçš„â€œå‡¹å‡½æ•°â€çœ‹ä¸Šå»å‡¸èµ·æ¥çš„ï¼Œamazingå§ï¼Ÿè¿™æ˜¯æœ‰ä¸€å®šå†å²åŸå› çš„ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å»æŸ¥é˜…ä¸‹èµ„æ–™ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å†å¤è¿°ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è®©å¤§å®¶äº§ç”Ÿè¯¯è§£ï¼Œæˆ‘é¼“åŠ±å¤§å®¶ä½¿ç”¨è‹±æ–‡ï¼Œconvex functionå’Œconcave function.è¿™æ ·ä¼šé¿å…å¾ˆå¤šä¸å¿…è¦çš„éº»çƒ¦ã€‚ OKï¼Œæˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ï¼Œconvex function å¯¹äºä¸€ç»´å‡½æ•° \\(f(x)\\)æ¥è¯´ï¼Œåœ¨å®šä¹‰åŸŸå†…çš„ä»»æ„å€¼ \\(a\\)å’Œ\\(b\\)ï¼Œå¯¹äºä»»æ„çš„ \\( 0 \\leq \\theta \\leq 1\\)ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°ä¸ºconvex function$$f(\\theta a+(1-\\theta) b) \\leq \\theta f(a) + (1- \\theta)f(b)$$æˆ‘ä»¬å†ç”¨å›¾ç‰‡ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹ æ˜¾è€Œæ˜“è§çš„æ˜¯ï¼Œå½“å…¬å¼ä¸­ç­‰å·å»æ‰çš„æ—¶å€™ï¼Œå‡½æ•°å°±æ˜¯strictly convex function. Convex functionå…·æœ‰ä¸€å®šçš„æ€§è´¨ï¼Œæˆ‘ä»¬ç®€å•çš„æè¿°ä¸€ä¸‹ã€‚ First order conditionå¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…ä¸€é˜¶å¯å¯¼ï¼Œä¸”å¯¼æ•°ä¸º$$ \\nabla f=( \\frac{\\partial f(x)}{x_1}, \\frac{\\partial f(x)}{x_2},â€¦, \\frac{\\partial f(x)}{x_n})$$é‚£ä¹ˆ \\(f\\)æ˜¯convex functionçš„å……è¦æ¡ä»¶æ˜¯ï¼šå¯¹äºå®šä¹‰åŸŸå†…ä»»æ„ \\(x\\) å’Œ \\(y\\)$$ f(y) \\geq f(x) + \\nabla f(x)^T (y - x)$$OKï¼Œå†æ¥å¼ å›¾ç‰‡ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼š å…¶å®ç®€å•çš„æ¥è®²ï¼Œå°±æ˜¯å¯¹äºconvex function \\(f\\)ï¼Œå®ƒçš„å‡½æ•°å€¼æ°¸è¿œå¤§äºç­‰äºåˆ‡çº¿ä¸Šçš„å€¼ï¼ Second order conditionå¯¹äº function \\(f\\)ï¼Œåœ¨å®šä¹‰åŸŸå†…äºŒé˜¶å¯å¯¼ï¼Œä¸” \\(n\\) ç»´æ–¹é˜µHessian matrixçš„å…ƒç´ ä¸º$$ \\nabla ^2 f(x) = \\frac{ \\partial ^2 f(x)}{ \\partial x_i \\partial x_j}, \\quad i,j = 1,2,â€¦,n$$å½“ä¸”ä»…å½“Hessian matrix positive semi-defniteçš„æ—¶å€™ï¼Œ\\(f\\) æ˜¯convex functionã€‚ä»¥ä¸Šäº’ä¸ºå……è¦æ¡ä»¶ã€‚è¿™é‡Œçš„è¯æ˜æˆ‘ä¸æƒ³å±•å¼€è®²ï¼Œåœ¨åé¢æˆ‘ä¼šç»™å‡ºreferenceé“¾æ¥ã€‚ ä¸‹é¢ç»™å‡ºä¸€äº› \\(\\Bbb R\\) ç©ºé—´ä¸‹å¸¸è§çš„convex functionï¼š çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\) æŒ‡æ•°å‡½æ•°ï¼š\\(f(x)=e^ {ax}\\) è´Ÿç†µå‡½æ•°ï¼š \\(f(x)=xlogx \\quad on \\quad \\Bbb R_{++}\\) å¯¹åº”çš„ï¼Œä¸€äº›å¸¸è§çš„concave functionï¼š çº¿æ€§å‡½æ•°ï¼š\\(f(x) = ax+b\\) å¯¹æ•°å‡½æ•°ï¼š \\(f(x)=logx \\quad on \\quad \\Bbb R_{++}\\) å…¶ä¸­å¤§å®¶å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§å‡½æ•°æ—¢æ˜¯convexä¹Ÿæ˜¯concaveå‡½æ•°ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œè¿™å’Œå®ƒæœ¬èº«çš„first order conditionä¸ºå¸¸æ•°æœ‰å…³ã€‚ ä»¥ä¸Šå°±æ˜¯convex functionçš„ä¸€ä¸ªç®€å•ä»‹ç»ï¼Œä½ ä¹Ÿè®¸ä¼šé—®ï¼Œä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆå¤šåŠ›æ°”æ¥ä»‹ç»convex function. å…¶å®ï¼Œåœ¨machine learningä¸­ï¼Œconvex functionçš„ä¼˜åŒ–æ˜¯éå¸¸é‡è¦çš„ï¼Œå¾ˆå¤šç®—æ³•è¯´åˆ°åº•ï¼Œéƒ½æ˜¯è¦optimizeä¸€ä¸ªconvex functionï¼Œæˆ‘ä»¬ä¼šç”¨liner regressionå’Œlogistic regressionä¸ºä¾‹å­ï¼Œè¿›ä¸€æ­¥ä»convex functionçš„ç®€ä»‹è¿‡æ¸¡åˆ°gradient descentå’Œnewtonâ€™s method. æ¢¯åº¦ä¸‹é™æ³•(Gradient descent)å…³äºgradient descentï¼Œæˆ‘ä»¬ä½¿ç”¨liner regressionä½œä¸ºä¾‹å­æ¥è®¨è®ºã€‚Liner regressionç®—æ³•çš„å®è´¨æ˜¯least square methodï¼Œä»–çš„cost functionæ˜¯$$J( \\theta)= \\frac{1}{2} \\sum_{i=1} ^m (h_ \\theta (x ^{(i)})-y^{(i)})^2, \\quad h_ \\theta (x)= \\theta ^Tx $$å¯¹äºliner regressionæ¥è¯´ï¼Œç®—æ³•çš„å®è´¨å°±æ˜¯å»æ±‚å‡º\\( J( \\theta) \\) ä»¥\\( \\theta\\)ä¸ºå‚æ•°çš„minimumï¼Œgradient descentç®—æ³•çš„ä½œç”¨å°±æ˜¯å»å®ç°äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œgradient descentçš„åŸºç¡€çŸ¥è¯†è¯¦è§reference. é‚£ä¹ˆé’ˆå¯¹cost functionï¼Œgradient descentæ˜¯å¦‚ä½•ä¿è¯æ”¶æ•›çš„å‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹ å¯¹äº\\(J( \\theta)\\)ï¼Œæˆ‘ä»¬å°†å…¶å¸¦å…¥convex functionçš„å®šä¹‰å…¬å¼ä¸­ï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬çš„è‡ªå˜é‡æ˜¯\\( \\theta\\)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨å¯¼è¯æ˜è¯¥å¼æˆç«‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œleast square cost functionæ˜¯convex function. æ—¢ç„¶æœ‰è¿™ä¸ªç»“è®ºäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œleast square cost functionä½œä¸ºconvex functionï¼Œæ˜¯å­˜åœ¨å…¨å±€æœ€å°å€¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œgradient descentä¸ä¼šå‡ºç°é™·å…¥å±€éƒ¨æœ€ä¼˜æ— æ³•è‡ªæ‹”çš„ç°è±¡ï¼Œåªè¦gradient descentä¿è¯å‚æ•°è¶³å¤Ÿå¥½çš„æƒ…å†µä¸‹ï¼Œç†è®ºä¸Šï¼Œæ˜¯å®Œå…¨å¯ä»¥å¾ˆå¥½çš„é€¼è¿‘å…¨å±€æœ€ä¼˜çš„è§£çš„ã€‚ Gradien descentç®—æ³•æœ¬èº«å¹¶ä¸èƒ½ä¿è¯è·å¾—å…¨å±€æœ€å°å€¼ï¼Œåªæœ‰åœ¨objective functionæ˜¯convex functionçš„æ—¶å€™æ‰å¯ä»¥ä¿è¯ ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œå³è¾¹çš„object functionæ˜¯non-convex functionï¼Œå› è€Œå¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼æ— æ³•è‡ªæ‹”ï¼Œè€Œå·¦è¾¹çš„objective functionæ˜¯ä¸€ä¸ªæ ‡å‡†çš„convex functionï¼Œåœ¨gradient descentå‚æ•°åˆç†çš„å‰æä¸‹ï¼Œå¯ä»¥é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚ å½“ç„¶ï¼Œgradien descentçš„ä¸€äº›æ”¹è¿›æ–¹æ³•ï¼Œä¾‹å¦‚stochastic gradient descentåœ¨è§£å†³non-convex optimizationä¸Šæœ‰ä¸€äº›å¸®åŠ©ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œä¸åšè®¨è®ºï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†å†™ã€‚ ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼Œgradient descentä¸ä»…ä»…æ˜¯minimize liner regressionçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä¹Ÿæ˜¯convex optimizationçš„ä¸€ç§ç†æƒ³æ–¹æ³• ç‰›é¡¿æ³•(Newtonâ€™s method)Newtonâ€™s method è¿™å—å†…å®¹ï¼Œæˆ‘ä»¬å°†ä¼šç”¨logistic regressionä½œä¸ºä¾‹å­ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å…ˆæ¥å…³æ³¨ä¸‹log cost functionï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬å–labelä¸º-1å’Œ+1ï¼Œå› ä¸ºè¿™æ ·å¾—åˆ°çš„cost functionæ¯”0,1ä¸‹çš„è®¡ç®—æ›´åŠ ç®€å•$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})$$è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†-1å’Œ+1ä½œä¸ºæ ‡ç­¾å€¼ï¼Œå’Œå¤§å¤šæ•°æ•™æä¸­ä¸ä¸€æ ·ï¼Œå¤§å®¶å¯ä»¥ä¸‹æ¥è‡ªå·±æ¨å¯¼ä¸€ä¸‹\\(J( \\omega)\\)ï¼Œè¿™ç§å†™æ³•å¹¿æ³›çš„åº”ç”¨åœ¨äº†æ¯”è¾ƒlogistic regressionå’ŒSVMä¸¤å¤§åˆ†ç±»å™¨çš„æ–‡çŒ®ä¸­ï¼Œå¸Œæœ›å¤§å®¶ç†ŸçŸ¥ã€‚ æ­¤å¤„æˆ‘ä»¬å¯¹åŸå§‹çš„likehood functionåŠ ä¸Šäº† \\(- \\frac{1}{m}\\)çš„ç³»æ•°ï¼ŒåŒæ ·ï¼Œå½“æˆ‘ä»¬æŠŠ \\(J( \\omega)\\)å¸¦å…¥åˆ°convex functionçš„å®šä¹‰ä¸­ï¼Œå¯ä»¥éªŒè¯ä¸Šå¼ä¸ºconvex functionï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ\\(J( \\omega)\\)æ˜¯\\( \\omega\\)çš„å‡½æ•°ã€‚ å…¶å®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†log cost functionå±•å¼€åï¼Œåˆ©ç”¨æœ€åŸºæœ¬çš„å‡½æ•°convexå’Œconcaveæ€§è´¨æ¥è·å¾—ä¸Šå¼æ˜¯convex functionçš„ç»“è®ºï¼Œç¢äºå…¬å¼å®åœ¨å¤ªéš¾æ‰“ï¼Œå°±ç•™ç»™å¤§å®¶å»è¯æ˜å§ã€‚ OKï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œæˆ‘ä»¬ä¸€å®šæ˜¯å¯ä»¥ç”¨gradient descentå»æ±‚è§£çš„ã€‚é—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ç”¨newtonâ€™s methodå‘¢ï¼Ÿ Newtonâ€™s methodçš„åŸºæœ¬åŸç†è¯¦è§referenceï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæ—¢ç„¶log cost functionæ˜¯convex functionï¼Œé‚£ä¹ˆæ ¹æ®second order conditionå¯ä»¥çŸ¥é“ï¼Œå®ƒçš„Hessian matrixä¸€å®šæ˜¯positive semi-definiteçš„ã€‚å¦‚æœæˆ‘ä»¬åŠ ä¸Šäº†L2 regularizerï¼Œç”±äºL2 regularizeræœ¬èº«å°±æ˜¯ä¸€ä¸ªstrict convex functionï¼Œé‚£ä¹ˆlog cost functionå°±ä¸€å®šæ˜¯strict convex functionäº†ï¼Œä¹Ÿå°±æ˜¯ï¼š$$J( \\omega)= - \\frac{1}{m} \\sum_{i=1} ^{m} log(1+e^{-y^{(i)} \\omega^T x^{(i)}})+ \\frac{1}{2}|| \\omega||^2$$å› æ­¤ï¼Œåœ¨log cost functionä¸­ï¼ŒHessian matrixæ˜¯positive definiteçš„ï¼Œå®Œå…¨æ»¡è¶³newtonâ€™s method çš„è¦æ±‚ã€‚åŒæ ·ï¼Œç±»ä¼¼äºä¸Šä¸€éƒ¨åˆ†ï¼Œnewtonâ€™s methodä¹Ÿå¯ä»¥æ‰¾åˆ°log cost functionçš„å…¨å±€æœ€ä¼˜ã€‚ Sum upOKï¼Œæˆ‘ä»¬è¯´åˆ°è¿™é‡Œä¹Ÿç¡®å®è®²äº†ä¸å°‘ï¼Œè¿™ç¯‡blogæœ‰äº›å†—é•¿ï¼Œå¸Œæœ›æœ‹å‹ä»¬ä¸è¦ç„¦è™‘ã€‚æ€»ä½“æ¥è¯´ï¼Œæˆ‘æƒ³è¡¨è¾¾çš„æ˜¯ä»¥ä¸‹å‡ ä¸ªè§‚ç‚¹ï¼š Machine learningä¸­æˆ‘ä»¬å¯»æ±‚çš„å…¶å®å°±æ˜¯objective functionä¸€ä¸ªå…¨å±€æœ€ä¼˜å€¼ï¼Œè¿™äº›é—®é¢˜æ˜¯é€šè¿‡gradient descentç­‰æ–¹æ³•è§£å†³çš„ï¼› Gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œä»–ä»¬éƒ½å¯ä»¥å¯¹äºconvex functionè·å¾—å…¨å±€æœ€ä¼˜ï¼› å¯¹äºnon-convex optimizationé—®é¢˜ï¼Œstochastic gradient descentä¹Ÿå¾ˆæœ‰æ•ˆæœï¼Œæˆ‘ä»¬åç»­å†æ…¢æ…¢å­¦ä¹ ã€‚ å¥½äº†ï¼Œæ ¸å¿ƒæ€æƒ³å°±è¿™ä¸‰ç‚¹ï¼Œä»Šå¤©å…ˆè¯´è¿™ä¹ˆå¤šï¼ Reference EE364, Convex Optimization Stanford University Regularized Logistic Regression is Strictly Convex XinyiLIå¤§ç¥çš„blog Liner regression Logsitc regression Gradient descent Newtonâ€™s method","link":"/2017/08/02/ml-convex-opt/"},{"title":"å†æ·±å…¥èŠèŠæ¢¯åº¦ä¸‹é™å’Œç‰›é¡¿æ³•","text":"ä¸Šæ¬¡æˆ‘ä»¬ä¸€èµ·èŠåˆ°äº†gradient descentå’Œnewtonâ€™s methodï¼Œè€Œä¸”æˆ‘ä»¬å·²ç»çŸ¥é“äº†gradient descentå’Œnewtonâ€™s methodéƒ½æ˜¯convex optimizationçš„å¥½æ–¹æ³•ï¼Œè¿™æ¬¡æˆ‘ä»¬å°±è·³å‡ºconvex optimizationï¼Œä»æ›´å¤§çš„unconstrained optimizationè§’åº¦æ¥æ¢è®¨ä¸‹è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å…³è”å’ŒåŒºåˆ«ã€‚ å‡è®¾æˆ‘ä»¬ç°æœ‰ä¸€ä¸ªçš„optimization taskï¼Œè¦æ±‚objective function \\(f(x)\\)çš„æœ€å°å€¼ï¼Œæˆ‘ä»¬ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ¡ˆï¼š è€ƒè™‘åˆ°\\(f(x)\\)çš„æœ€å°å€¼å¾ˆæœ‰å¯èƒ½æ˜¯å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾\\( \\nabla f(x)=0\\)çš„ç‚¹æ¥ç¡®å®šæœ€å°å€¼ï¼Œè¿™å°±æ˜¯newtonâ€™s methodçš„æ€æƒ³ æ—¢ç„¶æˆ‘ä»¬è¦å¯»æ‰¾æœ€å°å€¼ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é¡ºç€ä¸€æ¡\\(f(x)\\)é€æ¸å‡å°çš„è·¯å¾„ï¼Œé¡ºç€è¿™æ¡è·¯å¾„ä¸€ç›´èµ°ä¸‹å»ï¼Œç›´åˆ°ä¸å†å˜å°ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ OKï¼Œç®€å•çš„å™è¿°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ­£é¢˜ï¼ æ³°å‹’çº§æ•°(Taylor series)é¦–å…ˆæˆ‘ä»¬éœ€è¦å›å¿†ä¸€ä¸‹é«˜ç­‰æ•°å­¦ä¸­é‡è¦çš„Taylor seriesï¼Œå¦‚æœ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)çš„é¢†åŸŸå†…å…·æœ‰\\(n+1\\)é˜¶å¯¼æ•°ï¼Œé‚£ä¹ˆï¼Œåœ¨è¯¥é¢†åŸŸå†…ï¼Œ\\( f(x)\\)å¯å±•å¼€æˆ\\(n\\)é˜¶Taylor seriesï¼Œå¿½ç•¥æ— é™å¤§æ¬¡é¡¹çš„å½¢å¼å°±æ˜¯$$f(x)=f(x_0)+ \\nabla f(x_0)(x-x_0) + \\frac{ \\nabla ^2 f(x_0)}{2!}(x-x_0)^2 +â€¦+\\frac{ \\nabla ^n f(x_0)}{n!}(x-x_0)^n $$å…¶å®åœ¨é«˜ç­‰æ•°å­¦ä¸­å­¦åˆ°Taylor seriesçš„æ—¶å€™ï¼Œæˆ‘æœ¬äººæ˜¯ååˆ†æ— æ„Ÿçš„ï¼Œæˆ‘å¹¶ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿åˆ°åº•æœ‰ä»€ä¹ˆç”¨å¤„ï¼Œç›¸ä¿¡å¾ˆå¤šäººå’Œæˆ‘æœ‰ç›¸ä¼¼çš„ç»å†ã€‚ In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functionâ€™s derivatives at a single point. äº‹å®ä¸Šï¼ŒTaylor seriesæ‰€è¡¨ç°çš„æ˜¯ï¼Œå¯¹äº\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„ä¸€ä¸ªä¼°è®¡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼Œæ ¹æ®\\( x_0\\)ç‚¹å¤„çš„å„é˜¶derivativesä¹‹å’Œæ„æˆä¸€ä¸ªæ–°çš„functionï¼Œè¿™ä¸ªfunctionå°±æ˜¯å¯¹\\(f(x)\\)çš„é€¼è¿‘å’Œæ‹Ÿåˆï¼Œè€Œä¸”è¿™ç§é€¼è¿‘å’Œæ‹Ÿåˆï¼Œéšç€Taylor seriesé˜¶æ•°å¢åŠ è€Œæ›´æ¥è¿‘äºçœŸå®çš„\\(f(x)\\)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨0é˜¶Taylor seriesæ¥é€¼è¿‘çš„è¯ï¼Œé‚£æˆ‘ä»¬å°±ç²—æš´çš„è®¤ä¸ºï¼Œ\\( f(x)\\)åœ¨ç‚¹\\( x_0\\)é™„è¿‘çš„å€¼å°±éƒ½æ˜¯\\(x_0\\)ï¼Œè¿™å½“ç„¶å¤ªç²—æš´ç›´æ¥äº†ï¼Œå“ˆå“ˆã€‚ æ—¢ç„¶è¿™å¤ªç²—æš´äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç”¨1st order Taylor seriesæ¥åšä¸€ä¸ªé€¼è¿‘å’Œä¼°è®¡ï¼Œè¿™å°±æ˜¯gradient descentçš„æ€æƒ³ï¼›å¦‚æœæˆ‘ä»¬ç”¨2nd order Taylor seriesæ¥ä¼°è®¡å‘¢ï¼Œé‚£å°±æˆäº†newtonâ€™s methodäº† OKï¼Œæˆ‘ä»¬ç»§ç»­å¨“å¨“é“æ¥ã€‚ 1st order Taylor series &amp; gradient descentå‡è®¾\\(x_k\\)æ˜¯ç¬¬kæ¬¡gradient descentè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„1st order Taylor series å°±æ˜¯$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k)$$å…¶ä¸­\\(x\\)æ˜¯è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ–¹å‘ï¼Œgradient descentçš„ç›®æ ‡å°±æ˜¯è®©\\(f(x)\\)è¾¾åˆ°å±€éƒ¨ç”šè‡³å…¨å±€æœ€å°å€¼ï¼Œé‚£ä¹ˆæ¯ä¸€æ¬¡è¿­ä»£ï¼Œä¹Ÿéœ€è¦å°½å¯èƒ½çš„å‡å°æ›´å¤šä»¥è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé‚£ä¹ˆ$$f(x_k)-f(x)=- \\nabla f(x_k)(x-x_k)$$æ˜¾ç„¶ï¼Œä¸Šå¼åº”è¯¥å°½å¯èƒ½çš„å¤§ï¼Œå³\\(- \\nabla f(x_k)(x-x_k)\\)è¶Šå¤§è¶Šå¥½ï¼Œæˆ‘ä»¬ç°åœ¨æŠŠ\\((x-x_k)\\)åšä¸€ä¸ªæ›¿æ¢ï¼Œç”¨å•ä½å‘é‡\\(\\vec g\\)å’Œæ ‡é‡\\( \\alpha\\)åˆ†åˆ«ä»£è¡¨æ–¹å‘å’Œå¤§å°ï¼Œç°åœ¨çš„ä»»åŠ¡å°±å˜æˆäº†$$ \\min \\nabla f(x_k)(x-x_k) = \\min( \\alpha \\vec{\\nabla f(x_k)}â‹… \\vec g)$$æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå¯¹äºä¸¤ä¸ªå‘é‡æ¥è¯´ï¼Œå½“ä»–ä»¬æ–¹å‘ç›¸åæ—¶ï¼Œä»–ä»¬çš„å†…ç§¯æ˜¯æœ€å°çš„ã€‚ æ¢¯åº¦æ–¹å‘çš„å®šä¹‰æ˜¯è¯¥ç‚¹æ¢¯åº¦åœ¨æ ‡é‡åœºå¢é•¿æœ€å¿«çš„æ–¹å‘ å› æ­¤å½“\\(\\vec g\\)çš„æ–¹å‘æ˜¯\\( \\vec{\\nabla f(x_k)}\\)çš„åæ–¹å‘æ—¶ï¼Œä¸Šå¼å¯ä»¥å–åˆ°æœ€å°å€¼ï¼Œäºæ˜¯å°±æœ‰$$x-x_k=- \\alpha \\nabla f(x_k)$$$$x:=x_k- \\alpha \\nabla f(x_k)$$åˆ°è¿™ä¸€æ­¥ï¼Œæ˜¯ä¸æ˜¯çœ‹åˆ°äº†ç†Ÿæ‚‰çš„gradient descentå‘¢ï¼Œyeah mateï¼We make it! 2nd order Taylor series &amp; newtonâ€™s methodå’Œä¸Šé¢çš„gradient descentç›¸ä¼¼ï¼Œå‡è®¾\\(x_k\\)æ˜¯ç¬¬\\(k\\)æ¬¡newtonâ€™s methodè¿­ä»£åçš„\\(x\\)å–å€¼ï¼Œé‚£æˆ‘ä»¬åœ¨æ­¤å¤„çš„2nd order Taylor series æ˜¯$$f(x)=f(x_k)+ \\nabla f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\nabla^2 f(x_k)(x-x_k) $$æˆ‘ä»¬å¯¹ç­‰å·ä¸¤è¾¹åŒæ—¶å¯¹\\(x\\)æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸ºé›¶$$ \\nabla f(x) = \\nabla f(x_k) + (x-x_k) \\nabla^2 f(x_k)=0$$ç”±äºnewtonâ€™s methodçš„åŸç†å°±æ˜¯é€šè¿‡\\(\\nabla f(x)=0\\)æ¥å¯»æ‰¾æœ€å°å€¼ï¼Œæ•…ä¸Šå¼ä¸ºé›¶çš„è§£\\(x\\)å…¶å®å°±æ˜¯newtonâ€™s methodåœ¨\\(k+1\\)æ¬¡è¿­ä»£åçš„æ–°çš„\\(x\\)å€¼ã€‚å…¶ä¸­\\(\\nabla f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„ä¸€é˜¶å¯¼æ•°ï¼Œ\\( \\nabla^2 f(x_k)\\)æ˜¯\\(x_k\\)å¤„çš„äºŒé˜¶å¯¼æ•°HessiançŸ©é˜µå…ƒç´  æˆ‘ä»¬ä»¤\\(\\nabla f(x_k)=g\\)ï¼Œ\\(\\nabla^2 f(x_k)=H\\)ï¼Œåˆ™ä¸Šå¼å˜æˆ$$g+H(x-x_k)=0$$è¿›ä¸€æ­¥çš„$$x=x_k-H^{-1}g$$ç”±äº\\(-g H^{-1} \\) æ˜¯ä¼˜åŒ–çš„å‰è¿›æ–¹å‘ï¼Œåœ¨å¯»æ‰¾æœ€å°å€¼çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯å’Œæ¢¯åº¦æ–¹å‘\\(g\\)ç›¸åæ‰å¯ä»¥æ›´å¿«çš„ä¸‹é™ï¼Œé‚£ä¹ˆå°±æœ‰\\( g^T H^{-1} g &gt; 0\\)ï¼Œè¿™ä¸å°±æ˜¯positive definiteçš„å®šä¹‰å—ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼ŒHessiançŸ©é˜µæ˜¯positive definiteçš„ã€‚ æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœHessianæ˜¯negative definiteçš„è¯ï¼Œå‚æ•°æ›´æ–°çš„æ–¹å‘å°±æˆäº†å’Œ\\(g\\)ç›¸åŒçš„æ–¹å‘ï¼Œnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œè¿™ä¸€ç‚¹ï¼Œä¹Ÿæ˜¯newtonâ€™s methodçš„ç¼ºç‚¹ã€‚åœ¨objective functionæ˜¯non-convex functionçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬\\(k\\)æ¬¡è¿­ä»£è·å¾—çš„\\(x_k\\)å¤„çš„Hessian matrix negative definiteï¼Œé‚£ä¹ˆnewtonâ€™s methodå°†ä¼šå‘æ•£ï¼Œä»è€Œå¯¼è‡´ä¸æ”¶æ•›ã€‚å½“ç„¶ï¼Œä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œåç»­æœ‰æ”¹è¿›çš„BFGSç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæš‚æ—¶ä¸è¯¦ç»†è®¨è®ºã€‚ Sum upä¸‹é¢æˆ‘ä»¬å†æ¥æ€»ç»“æ€§è´¨çš„å¯¹æ¯”ä¸€ä¸‹ä¸¤ç§æ–¹æ³•ï¼Œæ¥çœ‹ä¸€å¼ å›¾äº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½é‡‡ç”¨äº†ä¸€ç§é€¼è¿‘å’Œæ‹Ÿåˆçš„æ€æƒ³ã€‚å‡è®¾ç°åœ¨å¤„äºè¿­ä»£\\(k\\)æ¬¡ä¹‹åçš„\\(x_k\\)ç‚¹ï¼Œå¯¹äºobjective functionï¼Œæˆ‘ä»¬ç”¨\\(x_k\\)ç‚¹çš„Taylor series \\(f(x)\\)æ¥é€¼è¿‘å’Œæ‹Ÿåˆï¼Œå½“ç„¶äº†ï¼Œä¸Šå›¾æˆ‘ä»¬çœ‹åˆ°ï¼Œgradient descentæ˜¯ç”¨ä¸€æ¬¡functionè€Œnewtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡functionï¼Œè¿™æ˜¯äºŒè€…ä¹‹é—´æœ€æ˜¾è‘—çš„åŒºåˆ«ã€‚ å¯¹äºnewâ€™s methodï¼Œåœ¨æ‹Ÿåˆä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡\\( \\nabla f(x)=0\\)æ±‚å¾—çš„\\(x _{k+1}\\)ç‚¹ä½œä¸ºæ­¤æ¬¡è¿­ä»£çš„ç»“æœï¼Œä¸‹æ¬¡è¿­ä»£æ—¶å€™ï¼Œåˆåœ¨\\(x _{k+1}\\)å¤„æ¬¡è¿›è¡ŒäºŒæ¬¡functionçš„æ‹Ÿåˆï¼Œå¹¶å¦‚æ­¤è¿­ä»£ä¸‹å»ã€‚ Newtonâ€™s methodé‡‡ç”¨äºŒæ¬¡functionæ¥æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æ„Ÿæ€§çš„ç†è§£ä¸ºï¼Œnewtonâ€™s methodåœ¨å¯»æ‰¾ä¸‹é™çš„æ–¹å‘æ—¶å€™ï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯æ­¤å¤„objective function valueæ˜¯ä¸æ˜¯å‡å°(ä¸€é˜¶value)ï¼Œè¿˜å…³æ³¨æ­¤å¤„valueä¸‹é™çš„è¶‹åŠ¿å¦‚ä½•(äºŒé˜¶value)ï¼Œè€Œgradient descentåªå…³å¿ƒæ­¤å¤„function valueæ˜¯ä¸æ˜¯å‡å°ï¼Œå› æ­¤newtonâ€™s methodå¯ä»¥è¿­ä»£æ›´å°‘æ¬¡æ•°è·å¾—æœ€ä¼˜è§£ã€‚å¯¹äºæ ‡å‡†äºŒæ¬¡å‹çš„objective functionï¼Œnewtonâ€™s methodç”šè‡³å¯ä»¥ä¸€æ¬¡è¿­ä»£å°±æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚ ä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢æ‰€è¯´çš„æ ‡å‡†äºŒæ¬¡å‹functionï¼Œå®è´¨ä¸Šæ˜¯convex functionï¼Œåœ¨ä¸€èˆ¬çš„unconstrained optimizationä¸­ï¼Œæ›´å¤šçš„æƒ…å†µåˆ™æ˜¯non-convex optimizationï¼Œå¯¹äºä¸€èˆ¬çš„non-convex optimizationï¼Œnewtonâ€™s methodæ˜¯ç›¸å¯¹ä¸ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾ä¿è¯Hessian matrixçš„positive definiteã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬ä¼šåŠ å…¥æ­¥é•¿\\(\\lambda\\)é™åˆ¶ï¼Œé˜²æ­¢å…¶ä¸€æ¬¡è¿­ä»£è¿‡å¤§è€Œå¸¦æ¥è¿­ä»£åHessian matrix negative definiteçš„æƒ…å†µï¼Œå³$$x:=x- \\lambda H^{-1} g$$å¯¹äºè¿™ç§æ€æƒ³ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œæ˜¯åœ¨æ•´ä½“non-convex functionä¸­å¯»æ‰¾ä¸€ä¸ªå±€éƒ¨çš„convex functionï¼Œé€šè¿‡æ­¥é•¿å°†newtonâ€™s methodé™åˆ¶åœ¨è¿™ä¸ªå±€éƒ¨ä¸­ï¼Œæœ€åæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ä¸­ã€‚ç”±æ­¤å¯è§ï¼Œnewtonâ€™s mtehodåœ¨non-convexä¸­å—é™åˆ¶æ¯”è¾ƒå¤§ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºgradient descenté‡‡ç”¨çš„ä¸€æ¬¡functionåšæ‹Ÿåˆï¼Œåªéœ€è¦è€ƒè™‘æ²¿ç€æ¢¯åº¦åæ–¹å‘å¯»æ‰¾æœ€å°å€¼ï¼Œå› æ­¤gradient descenté€‚ç”¨äºå„ç§åœºæ™¯ï¼Œç”šè‡³æ˜¯non-convex optimizationï¼Œè™½ç„¶ä¸èƒ½ä¿è¯æ˜¯å…¨å±€æœ€ä¼˜ï¼Œä½†è‡³å°‘gradient descentæ˜¯å¯ä»¥å€¼å¾—ä¸€è¯•çš„æ–¹æ³•ã€‚ ä¸‹é¢æ¥æ€»ç»“ä¸€ä¸‹ï¼š Gradient descent å’Œ newtonâ€™s methodéƒ½æ˜¯åˆ©ç”¨Taylor serieså¯¹objective functionè¿›è¡Œæ‹Ÿåˆæ¥å®ç°è¿­ä»£çš„ï¼› Gradient descent é‡‡ç”¨ä¸€æ¬¡å‹functionæ‹Ÿåˆè€Œ newtonâ€™s methodé‡‡ç”¨çš„æ˜¯äºŒæ¬¡å‹functionï¼Œå› æ­¤newtonâ€™s methodè¿­ä»£æ›´è¿…é€Ÿï¼› Newtonâ€™s methodæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—Hessian matrixçš„é€†ï¼Œåœ¨é«˜ç»´featureæƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æ¯æ¬¡è¿­ä»£ä¼šæ¯”è¾ƒæ…¢ï¼› Newtonâ€™s methodåœ¨non-convex optimizationä¸­å¾ˆå—é™åˆ¶ï¼Œè€Œgradient descentåˆ™ä¸å—å½±å“ã€‚ å¥½äº†ï¼Œå…ˆå†™è¿™ä¹ˆå¤šï¼Œè¿™å…¶ä¸­çš„çŸ¥è¯†é‡è¿˜æ˜¯å¾ˆæ·±å¥¥çš„ï¼Œä¹Ÿä¸çŸ¥é“è‡ªå·±æœ‰æ²¡æœ‰å™è¿°æ˜ç™½ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥è®¨è®ºï¼ æœ€åæ„Ÿè°¢ä¼˜ç”·çš„å®è´µæ„è§ï¼ Reference UCLA courseware CCU courseware Taylor series","link":"/2017/08/11/ml-gd-and-nm/"},{"title":"æ·±å…¥èŠèŠæ­£åˆ™åŒ–","text":"æœ€è¿‘å’Œä¼˜ç”·ä¸€èµ·èŠåˆ°äº†L1å’ŒL2 regularizationï¼ŒæœŸé—´é‡åˆ°äº†å¾ˆå¤šæ²¡æœ‰æƒ³æ˜ç™½çš„é—®é¢˜ï¼ŒåŠ ä¸Šæœ€è¿‘å·¥ä½œæœ‰äº›å¿™ï¼Œç©ºä½™æ—¶é—´ç”¨æ¥å€’è…¾æ–°åˆ°è´§çš„å°ç±³è·¯ç”±å™¨ï¼Œåªèƒ½è¶å‘¨æœ«è‡ªå·±ç ”ç©¶ç ”ç©¶ï¼Œä¸‹é¢å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹regularizationä¸­ä¸€äº›æ·±å…¥çš„é—®é¢˜ã€‚ä¸è®¨è®ºåŸºç¡€çŸ¥è¯†ï¼Œç›´æ¥ä¸Šå¹²è´§ã€‚ MAP and regularizationæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå½“cost functionåœ¨æ²¡æœ‰åŠ regularizationçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯¹å‚æ•°ä½¿ç”¨çš„æ˜¯MLE(Maximum likelihood estimation)ï¼Œå¯¹åº”é¢‘ç‡å­¦æ´¾æ‰€è®¤ä¸ºçš„å‚æ•°æœ¬æ— åˆ†å¸ƒè§„å¾‹çš„è§‚ç‚¹ï¼›åœ¨Andrew Ngç»å…¸çš„CS229ä¸­ï¼Œè¿™ä½AIå¤§å¸ˆæ›¾ç»æåˆ°ï¼Œregularizationå…¶å®æ˜¯å¯¹å‚æ•°çš„MAP(Maximum a posteriori estimation)ï¼Œæ˜¯åŸºäºè´å¶æ–¯å­¦æ´¾è®¤ä¸ºçš„å‚æ•°æœ¬æœ‰priori distributionï¼ŒåŒæ—¶å¸çº³äº†MLEçš„ä¸€ç§ä¸­é—´è§‚ç‚¹ã€‚ è¿™é‡Œçš„priori distributionï¼Œå°±æ˜¯æ ¹æ®ç»éªŒï¼Œè®¤ä¸ºå‚æ•°åº”è¯¥å¤§è‡´ç¬¦åˆæŸä¸ªdistributionï¼Œè¿™æ ·ï¼Œæœ€ç»ˆè·å¾—çš„å‚æ•°ä¼°è®¡ç»“æœä¹Ÿä¼šå’Œè¿™ä¸ªè¢«è®¤ä¸ºçš„distributionæœ‰ä¸€äº›ç›¸è¿‘ è€Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„L1 regularizationï¼Œå…¶å®å°±æ˜¯è®¤ä¸ºå‚æ•°çš„priori distributionæ˜¯Laplacian distributionï¼Œè€ŒL2 regularizationï¼Œåˆ™è®¤ä¸ºå‚æ•°çš„priorit distributionæ˜¯Gaussian distributionï¼Œç›¸ä¿¡å¤§å®¶å¯¹Gaussian distributionæ˜¯å¾ˆç†Ÿæ‚‰çš„ï¼Œè€Œå¯¹äºLaplacian distributionï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯$$p(x;a)= \\frac{a}{2} e^{-a|x|}$$ä¸‹å›¾å°±æ˜¯ä¸¤è€…çš„ä¸€ä¸ªæ¯”è¾ƒï¼šåœ¨åæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¸¤ç§distributionçš„ç‰¹ç‚¹å†³å®šäº†ä¸¤ç§regularizationçš„æ€§è´¨ã€‚ Lasso regressionåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Laplacian distributionï¼Œcost functionå°±æˆäº†$$J( \\theta) = \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}{j=1}| \\theta^{(i)}|$$ä¸Šå¼å°±æ˜¯Lasso regression Ridge regressionåœ¨liner regressionä¸­ï¼Œæˆ‘ä»¬å‡è®¾å‚æ•°\\( \\theta\\)æœä»Gaussian distributionï¼Œcost functionå°±æˆäº†$$J( \\theta) = \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})+ \\lambda \\sum^{d}{j=1} ( \\theta^{(i)})^2$$ä¸Šå¼å°±æ˜¯Ridge regressionæˆ–shrinkage geometry of error surfacesåœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œcost functionçš„å½¢å¼æ˜¯$$J( \\theta) = \\frac{1}{2} \\sum^{m}_{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$ç”¨äºŒç»´æˆªé¢å›¾å±•ç¤ºå°±æ˜¯å›¾ä¸­åªæœ‰objective functionï¼Œæ¨ªçºµè½´æ˜¯å‚æ•°\\( \\theta\\)ï¼Œæˆªå–è¿‡æ¥çš„å›¾ï¼Œæ‰€ä»¥ä¸Šé¢çš„å‚æ•°æ˜¯\\(w\\)ï¼Œ\\(l\\)æ˜¯losså€¼ï¼Œç®­å¤´æŒ‡å‘çš„ç‚¹å°±æ˜¯cost functionçš„æå°ç‚¹ã€‚åœ¨ä¸è€ƒè™‘å‚æ•°priori distributionçš„æ—¶å€™ï¼Œè¿™ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬çš„optimization target. ä¸‹é¢å¤§å®¶æ¥æˆ‘ä¸€èµ·åšä¸€ä¸ªå¤´è„‘é£æš´ï¼Œæ‰€è°“å‚æ•°çš„priori distributionï¼Œå…¶å®å°±æ˜¯ç”¨æ¥é™åˆ¶æœ€åoptimizationç»“æœçš„ä¸€ä¸ªé™å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨åšä¸€ä¸ªå—é™åˆ¶çš„çš„convex optimizationï¼Œå³ï¼š$$ \\theta=argmin \\frac{1}{2} \\sum^{m}{i=1} (y^{(i)}- \\theta^T x^{(i)})^2$$$$ s.t. \\sum^{d}{j=1}| \\theta^{(i)}|^p \\geq \\beta$$å…¶ä¸­ï¼Œ\\( \\beta\\)æ˜¯ridgeæˆ–è€…lassoçš„æœ€å°å€¼ã€‚é‚£ä¹ˆæ­¤æ—¶çš„å›¾å°±å˜æˆäº†ï¼šæˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŠ å…¥äº†é™åˆ¶åï¼Œæœ€ç»ˆçš„optimizationä¸æ˜¯è½åœ¨æå°å€¼ç‚¹ï¼Œè€Œæ˜¯è½åœ¨å›¾ä¸­æ‰€ç¤ºçš„ä½ç½®ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥æƒ³ï¼Œregularization itemçš„åŠ å…¥ï¼Œä½¿å¾—æ•´ä¸ªcost functionåœ¨å¯»æ‰¾æœ€å°å€¼çš„æ—¶å€™ï¼Œè¦å‡è¡¡çš„è€ƒè™‘objective functionå’Œregularization itemçš„å¤§å°. åœ¨è¿™ä¸ªåœ°æ–¹ï¼Œæˆ‘å’Œä¼˜ç”·è®¨è®ºçš„æ—¶å€™æœ‰ä¸€ä¸ªåœ°æ–¹æ²¡æœ‰æƒ³é€šï¼Œä¾‹å¦‚ä½¿ç”¨gradient descentè¿›è¡Œoptimzationçš„æ—¶å€™ï¼Œæ€ä¹ˆä¿è¯ä¼˜åŒ–å¯ä»¥è½åˆ°å›¾ä¸­çš„ç‚¹å‘¢ï¼Œæˆ‘æ˜¯è¿™ä¹ˆè€ƒè™‘çš„ï¼šå½“åŠ å…¥regularizationåï¼Œcost functionæœ¬èº«å°±æœ‰äº†å˜åŒ–ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯gradientä¹Ÿå‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨gradient descentè¿­ä»£è¿‡ç¨‹ä¸­å°±å·²ç»æŠŠregularizationçš„å½±å“å¸¦äº†è¿›å»ï¼Œå› æ­¤åœ¨æ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œå®é™…ä¸Šåº”è¯¥éƒ½æ˜¯æŒ‰ç…§ä¸Šå¼çš„é™åˆ¶è¿›è¡Œä¼˜åŒ–çš„ã€‚ å½“ç„¶ï¼Œä¸Šå›¾ä¹Ÿå¯ä»¥ç”¨æ¥å°±æ˜¯ä¸ºä»€ä¹ˆlassoå¯ä»¥è·å¾—ç¨€ç–ç‰¹å¾ï¼Œé‚£å°±æ˜¯å› ä¸ºlassoæ›´å¯èƒ½åœ¨åæ ‡è½´ä¸Šå’Œobjective functionäº§ç”Ÿäº¤ç‚¹ï¼Œè¿›è€Œä½¿å¾—ä¸€äº›ç‰¹å¾å˜æˆ0. Reference CS 195-5: Machine Learning STAT 897D","link":"/2017/08/27/ml-ridge-lasso/"},{"title":"Hello World","text":"æˆ‘ç»ˆäºæŠŠblogæ­å»ºèµ·æ¥äº†! è¿™æ˜¯ä¸€ä¸ªå±äºAsir è‡ªå·±çš„åšå®¢,åœ¨è¿™é‡Œæˆ‘ä¼šå†™ä¸€äº›æŠ€æœ¯åˆ†äº«,è®°å½•è‡ªå·±å¹³æ—¶å­¦åˆ°çš„ä¸œè¥¿,ä¹Ÿä¼šæ•´ç‚¹åæ§½æˆ–è€…é¸¡æ±¤.æ€»ä¹‹,æœ‰äº†ä¸€ä¸ªçœŸæ­£çš„å±äºè‡ªå·±çš„å¤©åœ°,å¯ä»¥éšä¾¿æ•´,è¿™ç§æ„Ÿè§‰éå¸¸æ£’. å…¶å®è‡ªå·±åœ¨åšå®¢å›­ä¹Ÿå°è¯•è¿‡ä¸€æ¬¡,å¯æ˜¯ä½“éªŒå¹¶ä¸æ˜¯å¾ˆç†æƒ³,åœ¨è¿™é‡Œæˆ‘å¹¶æ²¡æœ‰æŠ¨å‡»çš„æ„æ€,å› ä¸ºè‡ªå·±æ­å»ºèµ·æ¥çš„æˆå°±æ„Ÿé‚£ä¸æ˜¯ä¸€ç‚¹ä¸¤ç‚¹.å¸Œæœ›åé¢å¯ä»¥è¶çƒ­æ‰“é“,å¼€å¯blogä¹‹æ—…. åœ¨è¿™é‡Œæ„Ÿè°¢ä¸‹äº²é“åœˆç¾Šä¸ºæˆ‘æä¾›çš„å®Œç¾è®¾å¤‡å’Œæ·±å¤œæŠ€æœ¯æ”¯æŒ,éå¸¸æ£’! æœ€å,ä½œä¸ºä¸€ä¸ªcoding man, åœ¨æ‰€æœ‰äº‹æƒ…çš„æœ€å¼€å§‹,éƒ½ä¸åº”è¯¥ç¼ºå°‘è¿™å¥è¯ Hello world!!!","link":"/2017/07/26/other-hello/"},{"title":"Imbalanced data é—®é¢˜æ€»ç»“æ–¹æ³•æ±‡æ€»","text":"Helloï¼Œå¤§å®¶å¥½ï¼ŒåŒåä¸€çœŸçš„å¾ˆç´¯ï¼Œä¸€ç›´åœ¨åŠ ç­ï¼Œå¿™é‡Œå·é—²çœ‹äº†A systematic study of the class imbalance problem in convolutional neural networksï¼Œæ„Ÿè§‰paperå‘ˆç°çš„ç ”ç©¶å†…å®¹æ„Ÿè§‰å¾ˆä¸€èˆ¬ï¼Œä½†æ˜¯ï¼Œpaperä¸­å…³äºimbalanced dataçš„solutionæ–¹æ³•å€’æ˜¯å†™çš„å¾ˆä¸é”™ï¼Œä¹Ÿå‹¾èµ·äº†æˆ‘å¯¹äºè¿™ä¸€å—æ€»ç»“çš„æ¬²æœ›ã€‚ä¹‹å‰ä¹Ÿå†™è¿‡ä¸€ç¯‡å…³äºimbalanced dataçš„paper notesï¼Œä½†æ˜¯å¯¹äºè¿™ä¸€å—çš„å…·ä½“æ–¹æ³•æ€»ç»“è¿˜ä¸æ˜¯å¾ˆè¶³å¤Ÿï¼Œäºæ˜¯ç”¨è¿™ç¯‡paperä¸ºä¸»çº¿å¥½å¥½sum upä¸€è®¡ã€‚ æˆ‘ä»¬æ¥ä¸€èµ·çœ‹çœ‹ã€‚ Data level methodsé¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹data level methodsï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªå…±æ€§ï¼Œé‚£å°±æ˜¯é€šè¿‡æ”¹å˜dataçš„æ•°é‡æ¥å®Œæˆå¯¹imbalanced data problemçš„è§£å†³ã€‚ OversamplingOversamplingå¯ä»¥è¯´æ˜¯æœ€ç›´è§‚çš„solutionä¹‹ä¸€ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¯¹äºè¾ƒå°‘ä¸€ç±»åˆ«çš„samplesï¼Œè¿‡æ­¤é‡å¤é‡‡æ ·ï¼Œä»¥æ­¤è®©ä¸¤ç§ç±»åˆ«çš„æ ·æœ¬æ¥è¿‘å¹³è¡¡ã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸€ä¸ªsampleå¤šæ¬¡é‡å¤è®­ç»ƒï¼Œå¾ˆæœ‰å¯èƒ½å¸¦æ¥overfittingï¼Œå› æ­¤ï¼Œç®€å•ç²—æš´çš„é‡å¤é‡‡æ ·å¹¶ä¸å¯å–ã€‚å› æ­¤ï¼Œå¾ˆå¤šæ”¹è¿›çš„ç‰ˆæœ¬åº”è¿è€Œç”Ÿï¼š SMOTESMOTEç®—æ³•æ˜¯ä¸€ç§ç»å…¸çš„oversamplingæ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯å¯¹è¾ƒå°‘æ•°ç›®ç±»åˆ«çš„æ ·æœ¬ï¼ŒéšæœºæŠ½å–\\(m\\)ä¸ªæ ·æœ¬ï¼Œå¯¹äºéšæœºæŠ½å–å‡ºçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é€‰å–è·ç¦»æœ€è¿‘çš„\\(n\\)ä¸ªæ ·æœ¬ï¼Œåœ¨ä»–ä»¬çš„è¿çº¿ä¸Šéšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼Œä½œä¸ºè¾ƒå°‘ç±»åˆ«çš„è¡¥å……æ ·æœ¬ã€‚å‡è®¾åŸæ ·æœ¬ç‚¹ä¸º\\(x\\)ï¼Œè¢«é€‰ä¸­çš„é™„è¿‘çš„ç‚¹ä¸º\\(xâ€™\\)ï¼Œåˆ™æ–°çš„æ ·æœ¬ç‚¹ä¸ºï¼š$$x_{new}= x + rand(0,1) \\cdot |x-xâ€™|$$ é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSMOTEå¯ä»¥å¯¹è¾ƒå°‘ç±»åˆ«æ ·æœ¬è¿›è¡Œæ‰©å……ï¼Œè¿›è€Œå®ç°oversamplingï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚ Cluster-base oversamplingCluster-basedæ–¹æ³•çš„æœ€å¤§ç‰¹ç‚¹è«è¿‡äºæœ€å¼€å§‹å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªèšç±»åˆ†æï¼Œæ•°æ®ä¼šå˜æˆæ•°ä¸ªclusterï¼Œç„¶åå¯¹äºæ¯ä¸€ä¸ªclusteråœ¨è¿›è¡Œæ•°æ®çš„oversamplingï¼ŒåŒæ—¶å…¼é¡¾ç±»åˆ«ä¹‹é—´çš„between-class imbalanceï¼Œè¿˜è¦è€ƒè™‘åˆ°ç±»å†…éƒ¨å„ä¸ªclusterçš„within-class imbalance. Its idea is to consider not only the between class imbalance (the imbalance occurring between the two classes) but also the with in class imbalance (the imbalance occurring between the sub clusters of each class) and to oversample the data set by rectifying these two types of imbalances simultaneously. åŸpaperå¤§è‡´å™è¿°äº†æ•´ä¸ªæµç¨‹ï¼Œé¦–å…ˆæˆ‘ä»¬å¯¹imbalanced dataè¿›è¡Œk-means(æˆ–è€…å…¶ä»–ç®—æ³•ä¹Ÿå¯ä»¥)èšç±»ï¼Œèšæˆå¤šä¸ªclusterä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹è¿›è¡Œoversamplingï¼Œå‡è®¾majority classæœ‰\\(m\\)ä¸ªclusterï¼Œminorityæœ‰\\(n\\)ä¸ªclusterï¼Œæˆ‘ä»¬ä»¥clusteræœ€å¤§çš„dataæ•°ç›®\\(k\\)ä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬å…ˆå¯¹majority classä¸­æ‰€æœ‰clusterï¼Œéƒ½è¿›è¡Œoversamplingï¼Œä½¿å¾—ä»–ä»¬çš„æ•°ç›®éƒ½è¾¾åˆ°\\(k\\)ï¼Œéšåï¼Œå¯¹äºminorityä¸­æ¯ä¸ªclusterè¿›è¡Œoversamplingï¼Œä½¿å¾—æ¯ä¸€ä¸ªclusteræ•°ç›®å˜æˆ\\(m * k /n\\)ï¼Œæœ€ç»ˆå®ç°between-class balanceå’Œwithin-class balance. Undersamplingä¸oversamplingç›¸å¯¹åº”çš„åˆ™æ˜¯undersamplingï¼Œundersamplingçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹äºè¾ƒå¤šç±»åˆ«çš„samplesæŠ½æ ·ï¼Œä½¿å¾—ä¸¤ä¸ªç±»åˆ«æ•°æ®è¶‹äºç›¸è¿‘ã€‚ä½†æ˜¯ï¼ŒéšæœºæŠ½æ ·è·å¾—ä¼šä½¿å¾—ç±»åˆ«ä¸§å¤±å¾ˆå¤šçš„ä¿¡æ¯ï¼Œç”šè‡³å¯¼è‡´æ•°æ®åˆ†å¸ƒå‘ç”Ÿæ”¹å˜ã€‚ One-sided selectionone-sided selectionçš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œä¸ºäº†ä¿è¯æ•°æ®æ•´ä½“çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ä¼˜å…ˆå»é™¤é è¿‘è¾¹ç•Œçš„æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¾ƒå¤šåˆ†ç±»çš„æ•°æ®åˆ†å¸ƒã€‚ Classifier level methodsä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹é€šè¿‡æ”¹å˜classifier levelæ¥è§£å†³imbalanced dataçš„æ–¹æ³•ï¼Œè¿™ç±»æ–¹æ³•ä¾§é‡äºåˆ†ç±»å™¨æœ¬èº«çš„ä¸€äº›æ€§è´¨è€Œå¹¶éä¸¤ç±»æ•°æ®çš„ä¸ªæ•°ã€‚ Thresholdingæˆ‘åœ¨ä¹‹å‰çš„åšå®¢ä¸­èŠåˆ°è¿‡ï¼Œimbalanced dataçš„åˆ†ç±»å¹³é¢ä¼šå€¾å‘äºè¾ƒå°‘æ•°æ®çš„åˆ†ç±»ä¸€ä¾§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜ç±»åˆ«é¢„æµ‹çš„probabiltyçš„thresholdæ¥ä¿®æ­£åˆ†ç±»å¹³é¢ã€‚å¸¸ç”¨çš„æ–¹æ³•å°±æ˜¯åŠ å…¥å…³äºç±»åˆ«æ•°ç›®çš„prior probabilityï¼š$$y_i(x)=p(i|x)= \\frac{p(i) \\cdot p(x|i)}{p(x)}$$ Cost sensitive learningThresholdingæ–¹æ³•å…¶å®å¯¹å·²ç»trainå¥½çš„æ¨¡å‹çš„é‡‡å–çš„ä¸€ç§æ–¹å¼ã€‚ç›¸åº”çš„ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒçš„æ—¶å€™å°±æ¥æ¶ˆé™¤imbalanced dataçš„ä¸€äº›å½±å“ï¼Œå¦‚ä½•åšåˆ°å‘¢ï¼Ÿç­”æ¡ˆå°±æ˜¯cost function. æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´learning rateï¼ŒåŠ å¼ºå¯¹costæ¯”è¾ƒå¤§çš„samplesï¼Œå¹¶ä¸”æœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡ä»æ ‡å‡†çš„cost functionå˜æˆmisclassification costï¼Œå¦‚æ­¤å°±å¯ä»¥è§£å†³imbalanced dataçš„é—®é¢˜äº† One-class classificationè¯¥æ–¹æ³•å¯ä»¥è¯´æ˜¯æ¢äº†ä¸€ç§æ€ç»´çœ‹é—®é¢˜ï¼Œæˆ‘ä»¬ä¸å†å°†classificationä½œä¸ºæˆ‘ä»¬çš„taskï¼Œè€Œæ˜¯å˜æˆäº†å¯¹äºä¸€ç§å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚æˆ‘ä»¬åªæ˜¯ç€çœ¼äºè¾ƒå¤šsamplesçš„ç±»åˆ«ï¼Œè®¤ä¸ºå¦ä¸€ç±»åˆ«çš„samplesæ˜¯ä¸€ç§å¼‚å¸¸å€¼ã€‚ å½“ç„¶ï¼Œè¿™ç§æ–¹æ³•é€‚åˆé‚£ç§æç«¯çš„imbalanced dataï¼Œå¯¹äºä¸€èˆ¬çš„æƒ…å†µå¹¶ä¸ä¸€å®šå¾ˆé€‚ç”¨ã€‚ RecommendationProjection:Imbalanced-learn Reference Buda, Mateusz, Atsuto Maki, and Maciej A. Mazurowski. â€œA systematic study of the class imbalance problem in convolutional neural networks.â€ arXiv preprint arXiv:1710.05381 (2017). Chawla, Nitesh V., et al. â€œSMOTE: synthetic minority over-sampling technique.â€ Journal of artificial intelligence research 16 (2002): 321-357. Jo, Taeho, and Nathalie Japkowicz. â€œClass imbalances versus small disjuncts.â€ ACM Sigkdd Explorations Newsletter 6.1 (2004): 40-49. Richard, Michael D., and Richard P. Lippmann. â€œNeural network classifiers estimate Bayesian a posteriori probabilities.â€ Neural computation 3.4 (1991): 461-483.","link":"/2017/11/11/ml-imbalanced-data-solution/"},{"title":"Reading Notes-Practical lessons from predicting clicks on ads at facebook","text":"OKï¼Œä»Šå¤©æˆ‘ä»¬æ¥reviewä¸€ç¯‡ç»å…¸çš„paperï¼Œè¿™ç¯‡paperæ˜¯3å¹´å‰facebookçš„ç ”ç©¶æˆæœï¼Œå…³äºgbtå’Œlrçš„ç»“åˆï¼Œè¿™ä¸ªæ­é…å¯¹äºè¿‘å‡ å¹´çš„CTRé¢„æµ‹ä»¥åŠæ¨èç³»ç»Ÿçš„å‘å±•éƒ½äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚è™½ç„¶å·²ç»å¾ˆéš¾è¢«ç§°ä¸ºä¸€ç¯‡æ–°paperäº†ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—æˆ‘ä»¬å»çœ‹çœ‹ã€‚ æˆ‘ä»¬ä¸€èµ·ç®€å•çœ‹çœ‹è¿™ç¯‡paperçš„æ ¸å¿ƒpoint. Notesä¼ ç»ŸCTRé¢„æµ‹ä¸­ï¼Œlogistic regressionä¸€ç›´æœ‰ç€å¾ˆå¥½çš„æ•ˆæœï¼Œlrä¸ä»…å¯ä»¥çº¿æ€§åˆ†ç±»ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç»™å‡ºæ ·æœ¬å±äºè¯¥ç±»åˆ«çš„posterior probability ä½†æ˜¯ä¼ ç»Ÿçš„lrä¹Ÿæœ‰ç€æœ¬èº«çš„ç¼ºæ†¾ï¼Œlræœ¬èº«å°±æ˜¯lineråˆ†ç±»å™¨ï¼Œå¯¹äºçº¿æ€§ä¸å¯åˆ†çš„featuresæ•ˆæœä¸æ˜¯å¾ˆç†æƒ³ã€‚åŒæ—¶åœ¨å¯¹äºè¿ç»­featureç¦»æ•£åŒ–çš„æ—¶å€™ï¼Œæ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¾èµ–äºç¦»æ•£åˆ†æ¡¶çš„äººä¸ºç»éªŒã€‚ è¯¥paperæå‡ºäº†ä¸€ç§ä¾é gbtè¿›è¡Œfeature transformçš„æ–¹æ³•ï¼Œä¸å¤šè¯´åºŸè¯ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šå›¾è¿™å°±æ˜¯è¿™ç¯‡paperæœ€æœ€æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ã€‚ Input features are transformed by means of boosted decision trees.The output of each individual tree is treated as a categorical input feature to a sparse linear classifier. Boosted decision trees prove to be very powerful feature transforms. ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå§‹featureè¢«gbtè¿›è¡Œäº†transformï¼Œæ ·æœ¬è½å…¥åˆ°å“ªä¸ªtree nodeï¼Œåˆ™è¯¥ä½ç½®1ï¼Œå…¶ä»–ä½ç½®0ï¼Œéšåå†è¿›å…¥çº¿æ€§åˆ†ç±»å™¨lrä¸­è¿›è¡Œæœ€åçš„åˆ†ç±»ã€‚ å‡è®¾æœ‰ä¸€ä¸ªsampleï¼Œåœ¨å›¾ä¸­æ‰€ç¤ºçš„æ¨¡å‹ä¸­ï¼Œgbtæœ‰ä¸¤æ£µæ ‘ï¼Œä»å·¦åˆ°å³æ˜¯tree1å’Œtree2ï¼Œtree1ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬ä¸€ä¸ªtree nodeï¼Œtree2ä¸­sampleè¢«åˆ†åˆ°äº†ç¬¬äºŒä¸ªtree nodeï¼Œé‚£ä¹ˆæœ€ç»ˆtransformå¾—åˆ°çš„new sampleå°±å˜æˆäº†(1,0,0,0,1) é€šè¿‡gbtçš„transformåï¼Œfeatureä¸ä»…ä»éçº¿æ€§è½¬æ¢æˆäº†çº¿æ€§ï¼ˆç±»ä¼¼äºSVMçš„kernelæ•ˆæœï¼‰ï¼Œè€Œä¸”featureè¢«å®Œå…¨çš„ç¦»æ•£æˆäº†0-1ç¨€ç–featureï¼Œæ— è®ºä»çº¿æ€§å¯åˆ†è¿˜æ˜¯ç‰¹å¾ç¨€ç–çš„è§’åº¦ä¸Šï¼Œéƒ½å˜å¾—æ¯”åŸå§‹featureæ›´åŠ ç†æƒ³ï¼ å› ä¸ºæ˜¯ä¸€ç¯‡ç›¸å¯¹è€ä¸€äº›çš„paperï¼Œæ‰€ä»¥æˆ‘å™è¿°çš„æ¯”è¾ƒç®€å•ï¼Œå¤§å®¶å¯ä»¥getåˆ°gbt+lrè¿™ä¸ªæ¨¡å‹çš„åŸºæœ¬åŸç†å°±å¯ä»¥äº†ã€‚æˆ‘è‡ªå·±åœ¨ç§ä¸‹ä¹Ÿç”¨pythonå†™äº†ä¸€ä¸ªç®€å•çš„demoï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥çœ‹çœ‹ï¼Œæ¬¢è¿æå‡ºæ„è§ï¼Œæ¬¢è¿folkï¼ Reference He, Xinran, et al. â€œPractical lessons from predicting clicks on ads at facebook.â€ Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.","link":"/2017/08/23/paper-facebook/"},{"title":"Reading Notes-Class Imbalance, Redux","text":"å†æ¬¡æ„Ÿè°¢ä¼˜ç”·ï¼Œå‘æˆ‘æå‡ºäº†åˆä¸€ä¸ªå°–é”çš„é—®é¢˜ï¼Œä½¿å¾—æˆ‘æœ‰æœºä¼šæ€è€ƒå’Œç ”ç©¶ï¼Œå¹¶ä¸”æœ€ç»ˆå¯ä»¥çœ‹åˆ°è¿™ç¯‡paperï¼Œå¹¶ä¸”æœ€åå¯ä»¥åˆ†äº«ç»™å¤§å®¶ã€‚ æˆ‘ä¸ªäººåœ¨å·¥ä½œä¹‹ä¸­é‡åˆ°è¿‡imbalanced dataçš„é—®é¢˜ï¼Œæˆ‘åªæ˜¯ç›´è§‚çš„æ„Ÿå—åˆ°ï¼Œimbalanced dataçš„æœ€åæ•ˆæœå¾€å¾€ä¸æ˜¯å¾ˆæ£’ï¼Œç½‘ä¸Šä¹Ÿåªæ˜¯ç»™å‡ºäº†oversamplingå’Œundersamplingçš„å»ºè®®ï¼Œå¹¶æ²¡æœ‰æåŠè¿™å…¶ä¸­çš„ä¸€äº›ç¼˜æ•…ï¼Œä»Šå¤©æˆ‘ä»¬ä¸€èµ·é€šè¿‡è¿™ç¯‡paperæ¥å­¦ä¹ å­¦ä¹ ã€‚ Notesæˆ‘ä»¬å‡è®¾æœ‰positiveå’Œnegativeä¸¤ç±»sampleï¼Œå…¶ä¸­positive samplesç¬¦åˆ\\(P(x)\\)çš„Guassianåˆ†å¸ƒï¼Œnegative samplesç¬¦åˆ\\(G(x)\\)çš„Guassianåˆ†å¸ƒï¼Œåˆ†ç±»å¹³é¢å°†ç©ºé—´åˆ’åˆ†æˆpositive region\\(\\cal R^{+} _{w}\\)å’Œnegative region\\(\\cal R^{-} _{w}\\)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šå›¾ä¸­\\(w^{ *}\\)æ˜¯ç†æƒ³çš„åˆ†å‰²å¹³é¢ï¼Œ\\(w^{ *}\\) åº”è¯¥æ˜¯ä½¿lossæœ€å°çš„å–å€¼ï¼Œå³$$w^{}= \\arg\\underset{w}{\\min} \\cal L^{}(w)$$å¯¹äºlosså€¼ï¼Œå…¶å®å°±æ˜¯åˆ†ç±»ä¸­è¢«é”™åˆ†çš„fn(false negative)å’Œfp(false positive)çš„æœŸæœ›å€¼ï¼Œæ˜¾ç„¶ï¼Œé€šè¿‡minimunè¯¥losså¾—åˆ°çš„ä¼šæ˜¯å›¾ä¸­çš„\\(w^{}\\)ï¼Œå› ä¸ºè¿™ä¸ªåˆ†ç±»å¹³é¢æ‰€å¸¦æ¥çš„erroræ˜æ˜¾æ˜¯æœ€å°‘çš„ã€‚$$\\cal L^{}(w) = \\cal C_{fn} \\int _{\\cal R^{w} {-}} \\it P(x)dx + \\cal C{fp} \\int _{\\cal R^{w} {+}} \\it G(x)dx$$å¯¹äºæ•´ä¸ªæ•°æ®é›†\\(\\cal D \\)æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®é‡è¾ƒå°‘çš„ä¸€ç±»(paperä¸­è®¾å®špositiveç±»è¾ƒå°‘)æ‰€å æ¯”ä¾‹ä¸º\\(\\pi\\)(å°äº0.5)ï¼Œé‚£ä¹ˆå¯¹äºå¸¦æœ‰æ¯”ä¾‹\\(\\pi\\)çš„æ•°æ®é›†\\(\\cal D{\\pi}\\)ï¼Œå…¨å±€æœŸæœ›æ˜¯$$\\bf E_{\\cal D_{ \\pi}} [\\cal L(w)]=\\pi \\cal C_{fn} \\int _{\\cal R^{w} {-}} \\it P(x)dx + (1- \\pi) \\cal C{fp} \\int _{\\cal R^{w} _{+}} \\it G(x)dx$$æ­¤å¤„ï¼Œæˆ‘ä¸ªäººçš„ç†è§£æ˜¯ï¼Œåœ¨ä¸¤ç±»æ•°æ®å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¨å±€æƒ…å†µä¸‹çš„æœŸæœ›å…¶å®æ˜¯å’Œä¸Šé¢çš„lossç­‰ä»·çš„ï¼Œä½†æ˜¯imbalanced dataå¸¦æ¥äº†ä¸å‡è¡¡çš„å› å­\\(\\pi\\)ï¼Œå› æ­¤ï¼Œä¸¤ä¸ªå…¬å¼ä¸å†ç­‰ä»·ã€‚ OKï¼Œæ—¢ç„¶ä¸ç­‰ä»·ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼Œpaperä¸Šè¯´ï¼Œé€šè¿‡æœ€å°åŒ–å…¨å±€æœŸæœ›è·å¾—çš„\\(\\hat w\\)ï¼Œæ˜¯å‘ç€è¾ƒå°‘æ•°é‡ç±»åˆ«çš„æ ·æœ¬å€¾æ–œï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€å¹…å›¾ä¸­ï¼Œå‘è¾ƒå°‘çš„postiveé‚£è¾¹skewedï¼ŒåŸå› æ˜¯å› ä¸º\\( \\cal R _{+} ^{ \\hat w} &lt; \\cal R _{+} ^{w^{*}}\\), ä¹Ÿå°±æ˜¯è¯´ï¼Œ\\(\\hat w\\)åˆ†å‰²çš„positive regioné¢ç§¯å°äº\\(w^{*}\\)åˆ†å‰²å‡ºçš„é¢ç§¯ï¼Œé¢ç§¯çš„å‡å°åŠ¿å¿…å¯¼è‡´åˆ†å‰²å¹³é¢å‘positiveç±»åˆ«æ–¹å‘åç§»ã€‚ é—æ†¾çš„æ˜¯ï¼Œå…³äºé¢ç§¯çš„è¯æ˜æˆ‘å®åœ¨çœ‹ä¸æ˜ç™½ï¼Œä¹Ÿemailäº†ä¸€äº›äººï¼Œä¹Ÿæ²¡æœ‰å¾—åˆ°ä¸€ä¸ªæ»¡æ„çš„ç­”æ¡ˆï¼Œå¦‚æœæœ‰æœ‹å‹çœ‹æ˜ç™½äº†çš„è¯ï¼Œè®°å¾—ç•™è¨€æˆ–è€…emailæˆ‘ï¼ åˆ°äº†è¿™é‡Œï¼Œpaperå¤§æ¦‚ä»‹ç»äº†undersamplingçš„è£¨ç›Šï¼Œundersamplingçš„æ ¸å¿ƒå…¶å®å°±æ˜¯æ¶ˆé™¤å‰é¢æåˆ°çš„æ¯”ä¾‹\\(\\pi\\)ï¼Œè®©å®ƒè¶‹è¿‘äº0.5åï¼Œåˆ†ç±»å¹³é¢\\(\\hat w\\)å°±ä¼šè¶‹è¿‘äºç†æƒ³åˆ†ç±»å¹³é¢\\(w^{*}\\)ã€‚ è¿™é‡Œï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªbaggingæ–¹æ³•ï¼Œå°±æ˜¯å¤šæ¬¡åšundersamplingï¼Œæœ€åæœ€ç»“æœåšbaggingå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¦‚ä¸‹å›¾paperè¿˜å¯¹æ¯”äº†å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Weighted Empirical Cost Minimization(å¦‚weighted SVM)å’ŒSMOTEæ–¹æ³•æ•ˆæœä¸å¦‚bagging undersamplingï¼Œæˆ‘ä¸Šä¸€å¹…å›¾è¯´æ˜ä¸‹SMOTEçš„ç¼ºç‚¹ï¼Œæ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥è¯¦ç»†çœ‹çœ‹paperï¼Œå¦‚å›¾ï¼šSMOTEæ–¹æ³•æ˜¯éšæœºé€‰æ‹©æ–¹å‘ç”Ÿæˆæ–°çš„sampleï¼Œä½†æ˜¯å¦‚æœæ–°çš„sampleäº§ç”Ÿäº†å›¾ä¸­ä½ç½®ï¼Œåˆ™æ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚ OKï¼Œä»Šå¤©å°±è¿™ä¹ˆå¤šï¼Œè®°å¾—çœ‹æ˜ç™½äº†ä¸­é—´çš„æ¨å¯¼ä¸€èµ·åˆ†äº«å•Šï¼ Reference Wallace, Byron C., et al. â€œClass imbalance, redux.â€ Data Mining (ICDM), 2011 IEEE 11th International Conference on. IEEE, 2011. PPT-Class Imbalance, Redux","link":"/2017/09/10/paper-imbalance/"},{"title":"Reading Notes-Swishï¼šA Self-gated Activation Function","text":"Hi allï¼Œä»Šå¤©å’Œå¤§å®¶åˆ†äº«ä¸€ç¯‡æ¯”è¾ƒæ–°çš„paperï¼Œæ˜¯å…³äºä¸€ç§æ–°çš„activation functionï¼Œå…³äºæˆ‘ä»¬çŸ¥é“çš„activation functionï¼Œæœ‰sigmoidï¼Œtanhï¼ŒReLUä»¥åŠReLUçš„ä¸€äº›å˜ç§ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©æ¥çœ‹çœ‹è¿™ç§æ–°æå‡ºçš„activation functionåˆ°åº•æœ‰ä»€ä¹ˆç‰¹è‰²ã€‚ Notesé¦–å…ˆå®šä¹‰ï¼Œswish activation function \\(f(x)=x \\cdot \\sigma (x)\\)ï¼Œå…¶ä¸­\\(\\sigma(x)\\)æ˜¯sigmoid functionï¼Œä¹Ÿå°±æ˜¯\\( \\sigma(x)=1/(1+ e^{-x})\\).Swish functinçš„å›¾åƒå¦‚å›¾æ‰€ç¤ºï¼šæˆ‘ä»¬å†æ¥çœ‹ä¸‹swish functionçš„1st and 2nd derivativesï¼Œä¸‹é¢æˆ‘ä»¬ä¸€èµ·æ¥é›†ä¸­çœ‹çœ‹swish functionçš„ä¼˜ç‚¹éƒ½æœ‰ä»€ä¹ˆï¼Œä½œè€…ç»™å‡ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼šå‡½æ•°å€¼æ²¡æœ‰ä¸Šé™ï¼Œå‡½æ•°å€¼æœ‰ä¸‹é™ï¼Œå‡½æ•°ä¸å•è°ƒï¼Œå‡½æ•°å…‰æ»‘è¿ç»­ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ï¼š Unbounded aboveUnbounded aboveçš„å®è´¨ï¼Œæ˜¯é˜²æ­¢activation functionåœ¨bounded valueå¤„å‘ç”Ÿsaturation. bounded above å¸¦æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯è¶Šæ¥è¿‘bounded valueçš„æ—¶å€™ï¼Œfunction gradientå°±ä¼šè¶Šå°ï¼Œé€æ¸æ¥è¿‘0ï¼Œè¿™å°±å¯¼è‡´gradient descentå¼‚å¸¸ç¼“æ…¢ç”šè‡³æ— æ³•convergeã€‚ä¾‹å¦‚sigmoid å’Œtanh functionï¼Œä»–ä»¬éƒ½æ˜¯bounded below and aboveï¼Œå½“æˆ‘ä»¬é‡‡ç”¨è¿™ä¸¤ç§activation functionçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¿…é¡»è°¨æ…å°å¿ƒçš„è®©åˆå§‹å€¼å°½é‡åœ¨functionçš„æ¥è¿‘linerçš„éƒ¨åˆ†æ¥é¿å…ä¸Šé¢é—®é¢˜çš„äº§ç”Ÿï¼Œå› æ­¤ï¼Œunbounded aboveæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¼˜ç‚¹ï¼Œä¾‹å¦‚ReLUåŠå…¶å˜ç§éƒ½é‡‡ç”¨äº†è¿™ä¸€åŸåˆ™ã€‚ Bounded below &amp; non-monotonicityBounded belowå…¶å®ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¹Ÿæœ‰activation functionå·²ç»é‡‡ç”¨äº†ï¼Œé‡‡ç”¨è¯¥æ–¹æ³•åï¼Œæ‰€æœ‰è´Ÿæ•°inputéƒ½ä¼šå¾—åˆ°ç›¸å·®æ— å‡ çš„activation valueï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ-1000å’Œ-1çš„å€¼å‡ ä¹æ²¡æœ‰åŒºåˆ«ï¼ŒæŒ‰ç…§authorçš„è¯æ¥è®²ï¼Œå°±æ˜¯æˆ‘ä»¬å°† make large negative input â€œfogottenâ€ è¿™å…¶å®ä¹Ÿæ˜¯regularzationçš„ä¸€ç§æ€æƒ³ï¼Œè¿™ç§æ–¹æ³•åœ¨ReLUç­‰æ–¹æ³•ä¸­ä¹Ÿæœ‰ä½“ç°ï¼Œä½†æ˜¯ï¼Œswishå¯ä»¥é€šè¿‡è‡ªèº«çš„éå•è°ƒæ€§è´¨ï¼Œå°†æ¯”è¾ƒå°çš„negative inputä»ç„¶ä»¥negative valueè¾“å‡ºï¼Œnon-monotonicityæä¾›äº†æ›´å¥½çš„gradient flow. Smothnesså…³äºsmoothnessçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€å¼ å›¾ï¼š æ€»è€Œè¨€ä¹‹ï¼Œä¸ªäººæ„Ÿè§‰swishåº”è¯¥ç®—æ˜¯ä¸€ä¸ªä¸é”™çš„activationï¼Œæœ¬äººç”±äºæ—¶é—´åŸå› ï¼Œè¿˜æ²¡æœ‰æ¥å¾—åŠè‡ªå·±æµ‹è¯•å®ƒï¼Œä½†æ˜¯æ®æˆ‘æ‰€çœ‹åˆ°çš„è®¨è®ºï¼Œswishçš„å®é™…æ•ˆæœè²Œä¼¼ä¸æ˜¯ååˆ†ç¨³å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒä¿ç•™æ„è§ï¼Œè¿›ä¸€æ­¥è§‚å¯Ÿå®ƒçš„è¡¨ç°ã€‚ Reference Ramachandran P, Zoph B, Le Q V. Swishï¼ša Self-Gated Activation Function[J]. arXiv preprint arXiv:1710.05941, 2017.","link":"/2017/10/22/paper-swish/"},{"title":"Catalyst Optimization in Spark SQL","text":"Spark SQL is one of the most important components of Apache Spark and has become the fundation of Structure Streaming, ML Pipeline, GraphFrames and so on since Spark 2.0. Also, Spark SQL provides SQL queries and DataFrame/Dataset API, both of which are optimized by Catalyst. Letâ€™s talk about Catalyst today.Catalyst Optimizer is a great optimization framework to improve Spark SQL performance, especially query capabilities of Spark SQL and it is written by functional programming construct in Scala. RBO(Rule Based Optimization) and CBO(Cost Based Optimization) are the optimization principles of Catalyst. The concept of Catalyst is familiar to many other query optimizers, so understanding Catalyst may help you learn the working pipeline of other query optimizers. Trees And RulesWe will have a quick review of trees and rules. You can learn more about them by the references. TreesThe tree is the basic datatype in Catalyst, which is consisted of node objects. Each node has a node type, and some children, which could be none. Letâ€™s have an example, the tree for expression x+(1+2) could be translated in Scala as: 1Add(Attribute(x),Add(Literal(1),Literal(2))) Actually, most of the SQL query optimization, like Catalyst, would transform the SQL to a huge tree structure as the first step of the optimization. Tree structure could be modified, tailored and optimized easily, also it represents the query plan briefly. Whatâ€™s more, the data can be thrown to every node of the tree by the query plan iteratively. Thatâ€™s why tree datatype is used and introduced firstly in Catalyst. RulesTrees can be manipulated by the rules. The optimization can be treated as some transformations from one tree to another under some rules we provide. With the help of rules, we can run arbitrary code and many other operations on the input tree. And the Catalyst will know which part of the tree that rules could match or apply, and will automatically skip over the tree that does not match. The rules are all applied by Scala case class, in other words, one rule can match multiple patterns. Letâ€™s see an example.12345tree.transform &#123; case Add(Literal(c1),Literal(c2)) =&gt; Literal(c1+c2) case Add(left, Literal(0)) =&gt; left case Add(Literal(0), right) =&gt; right&#125; CatalystFrom this section, we will know how Catalyst optimizes Spark SQL, which might improve your understanding of how Spark SQL works and how to manipulate it better. The figure below presents the workflow of Spark SQL, and the Analysis, Logical Optimization and Physical Planning are all working under the Catalyst, and we will go over one by one. ParserThe first stage in the figure above is called Parser. Despite not a part of Catalyst, Parser is also very crucial in the whole optimization. Spark SQL transform SQL queries to an AST(Abstract Syntax Tree), also called Unresolved Logical Plan in Catalyst, by ANTLR, which is also used in the Hive, presto and so on. However, DataFrame/Dataset object can be transformed to Unresolved Logical Plan by the API. AnalysisReturned by the Parser, Unresolved Logical Plan contains many unresolved attribute reference of relation. For example, you have no idea whether the column name provided is correct or type of the column, either. Spark SQL use Catalyst and Catalog object that tracks the data all the time to resolve the attributes. Looking up relations by name from Catalog, mapping all the named attributes to the input, checking the attributes which match to the same value and giving the unique ID and pushing the type information of the attributes, the Unresolved Logical Plan is transformed to Logical Plan by Catalyst. Logcial OptimizationLogical Optimization is mainly based on the rules, which is also called RBO(Rules Based Optimization). Lots of rules are provided in this step to accomplish RBO, including constant folding, predicate pushdown, project pruning and so on. As results, the Logical Plan is returned by Logical Optimization from Unresolved Logical Plan. Some figures below describe these ROB mentioned.Predicate pushdown can reduce the computation of join operation by filtering unnecessary data before join. Constant folding avoids calculating the same operation between constants for each record. Column Pruning makes Spark SQL only load data which would be used in the table. Physical PlanningSince we get the Logical Plan, Spark still doesnâ€™t know how to execute the Logical Plan. Transformed from Logical Plan by Physical Planning, Physical Plan would guide the Spark on how to handle the data. In Physical Planning, several Physical Plans are generated from Logical Plan, using physical operator matches the Spark execution engine. Using CBO(Cost Based Optimization), Catalyst will select the best performance one as the Physical Plan. However, the CBO is now only used for join algorithms selection. Also, RBO is used in Physical Planning to pipelining projections or filters into single Spark map() transformation. Code GenerationGetting the Physical Plan from the Physical Planning stage, Spark will translate the AST into Java bytecode to run on each machine. Thanks to quasiquotes, a greate feature of Scala, ASTs would be built by Scala language based on the Physical Plan. And the AST will be send to Scala Compile at runtime to generateava bytecode. We use Catalyst to transform a tree representing an expression in SQL to an AST for Scala code to evaluate that expression, and then compile and run the generated code. In Spark 2.x, the WholeStageCodeGen has been used in code generation stage, you can read more from here. At last, if you want to see the Logical Plan and Physical Plan of your own Spark SQL, you may do as follows:1234// for Logical Planspark.sql(\"your SQL\").queryExecution// for Physical Planspark.sql(\"your SQL\").explain References Deep Dive into Spark SQLâ€™s Catalyst Optimizer Spark SQL Optimization â€“ Understanding the Catalyst Optimizer Catalyst Source Code Quasiquotes Introduction","link":"/2018/09/25/spark-catalyst-optimization/"},{"title":"From Spark RDD to DataFrame/Dataset","text":"This article presents the relationship between Spark RDD, DataFrame and Dataset, and talks about both the advantages and disadvantages of them. RDD is the fundamental API since the inception of Spark and DataFrame/Dataset API is also pretty popular since Spark 2.0. Whatâ€™s the differences between them and how to decide which API to be imported, letâ€™s have a quick look. RDDRDD (aka Resilient Distributed Dataset) is the most fundamental API, itâ€™s so critical that all the computation in Spark are based on it. The RDD is distributed, immutable, type-safed, unstructured, partitioned and low-leveled API, which offers transformations and actions. Letâ€™s have a quick review to the attributes of RDD. RDD has some awesome characteristics which make it the foundation of the Apache Spark. Letâ€™s take a look and learn about the details one by one. Distributed data abstractionThe first attribute of RDD is the logical distributed abstraction, which makes RDD can run over the entire clusters. The RDD can be set across the storage and divided into many partitions so that the lambda function or any computation function you provide will execute on each partition separately. Thatâ€™s really awesome as RDD is the logical distributed abstraction, the computation will be much faster as data is divided and run in parallel on several executors. Resilient and immutableRDD is also resilient and immutable. When an RDD is transformed to the next RDD, and then to the third one, and all the RDDs get recorded as a lineage of where they come from. The lineage can also be recorded as the acyclic graph of the RDDs, from which we can recreate any RDD in it when something goes wrong, and thatâ€™s why RDD is resilient.As for the immutability, when you make a transformation, the original RDD remains unaltered so that you can go back through the acyclic graph and recreate it at any time and point during the execution. Remember, the transformation operation creates a new RDD from the previous one instead of altering the previous one. Compile-time type-safeRDD is compiled type-safe and you can name the RDD with particular type briefly. Spark application could be complicated and debugging in the distributed environment could be cumbersome, the compiled type-safe really save time as it could find the type error in the compile time. Unstructured/Structured dataThe fourth attribute is that data can be unstructured, streaming data from the media for instance, or semi-structured, log files with some particular date, time or url information for example. Since RDD would not care about the structure or schema of the data, itâ€™s good for those data without structures. Also, RDD can manipulate structured data, though it doesnâ€™t understand the different kinds of types and all depends on how you parse the data. Lazy evaluationLazy evaluation in Apache Spark means the execution will not start until an action is triggered. In another word, the RDD will not be loaded and computed until it is necessary. And there are some benefits of lazy evaluation. Lazy evaluation really reduces the time and space complexities and speeds up the whole execution. DataFrame/DatasetDataFrame/Dataset, unlike RDD, in high-level API dealing with structured data. Data is organized into named columns in DataFrame and can be manipulated type-safely. In Scala, DataFrame is just an alias for Dataset[Row], and in Java, there is only Dataset API, as for Python and R, there is only DataFrame API provided since Python and R have no compile-time type-safety.There are several benefits of DataFrame/Dataset API. I just want to talk about two of them, which I think are pretty awesome. Static-typing and runtime type-safetyDataFrame/Dataset presents static-typing and runtime type-safety. You may have a spelling error when you are typing a SQL such as typing form rather than from, and you would not find the syntax errors until the runtime, however, you will catch these errors at compile time in the DataFrame/Dataset API. Also, as for some analysis errors, the column you queried is not in the schema for example, you can catch these errors when compiling in Dataset while until running in SQL and DataFrame. Nice performanceDataFrame/Dataset API can make the execution more intelligent and efficient. You are telling Spark how-to-do a operation when using RDD, while what-to-do using DataFrame/Dataset. Letâ€™s have a look at the example.12345rdd.filter&#123;case(project, page, numRequests) =&gt; project=='en'&#125;. map&#123;case(_,page,numRequests) =&gt; (page, numRequests)&#125;. reduceByKey(_+_). filter&#123;case(page,_) =&gt; !isSpecialPage(page)&#125;. take(100).foreach &#123;case (project, requests) =&gt; println(s\"projec:$requests\"\")&#125; The code above can be run perfectly without any bug. But think about it, the RDD execute a filter followed by reduceByKey transformation, which means we filter some data after shuffling the entire data. Thatâ€™s really a waste because shuffle operation is very expensive. However, this problem will be solved in DataFrame/Dataset. Based on the Catalyst, DataFrame/Dataset will optimize the query operation by rules and cost, thus the execution is much smarter than the raw RDD. When to UseSince we take a look at the RDD and DataFrame/Dataset, I make a list about when to use RDD and when to use DataFrame/Dataset. When to use RDD When you want more about the low-level control of dataset When you are dealing with some unstructred data When you prefer manipulate data with lambda function When you donâ€™t care about schema or structure of data When to use DataFrame/Dataset When you are dealing with structured data When you want more code optimization and better performance All in all, I do recommend you to use DataFrame/Dataset API as you can for their easy-using and better optimization. Supporting by Catalyst and Tungsten, DataFrame/Dataset can reduce your time of optimization, thus you can pay more attention to the data itself. References A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets Apache Spark RDD vs DataFrame vs DataSet","link":"/2018/09/22/spark-from-rdd-to-dataframe-dataset/"},{"title":"Second Generation Tungsten Engine in Spark 2.x","text":"This article is about the 2nd generation Tungsten engine, which is the core project to optimize Spark performance. Compared with the 1st generation Tungsten engine, the 2nd one mainly focuses on optimizing query plan and speeding up query execution, which is a pretty aggressive goal to get orders of magnitude faster performance. Letâ€™s take a look! Project TungstenIn the past several years, both storage and network IO bandwidth have been largely improved, while CPU efficiency bound has not. As results, CPU now becomes the new bottleneck and we have to substantially try to improve the efficiency of memory and CPU and push the performance of Spark closer to the limits of modern hardware, which is the main propose of Tungsten. To achieve it, three initiatives in 1st generation Tungsten engine are proposed, including: Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection Cache-aware computation: algorithms and data structures to exploit memory hierarchy Code generation: using code generation to exploit modern compilers and CPUs As we can see, the first two initiatives mainly foucus on memory, and the last one are for CPUs. In the 2nd generation Tungsten engine, rather than code generation, WholeStageCodeGen and Vectorization are proposed for the order of magnitude faster. WholeStageCodeGenAs the name presents, WholeStageCodeGen, aka whole-stage-code-generation, collapses the entire query into a single stage, or maybe a single function. So why combining all the query into a single stage could significantly improve the CPU efficiency and gain performance? We may take a quick look at what it looks like in 1st Tungsten engine. Volcano Iterator ModelWhat a vividly name! Volcano iterator model, as presents in the figure, would generate an interface for each operator, and the each operator would get the results from its parent one by one, just like the volcano eruption from bottom to top.Although Volcano Model can combine arbitrary operators together without worry about the data type of each operator provides, which makes it a popular classic query evaluation strategy in the past 20 years, there are still many downsides and we will take about it later. Bottom-up ModelIn this blog, a hand-written code is proposed to implement the query in the figure above, itâ€™s just a so simple for-loop that even a college freshman can complete, which is:123456var count = 0for (ss_item_sk in store_sales) &#123; if (ss_item_sk == 1000) &#123; count += 1 &#125;&#125; Even though the code is pretty simple, the comparison of performance between Volcano Iterator Model and Bottom-up Model will do shake you.But why is that? Seems it turn out it to be caused by following downsides of Volcano Iterator Model: Too many virtual functions calls:In Volcano Iterator Model, when one operator call for the next operator, a virtual function next() would be called. But in Bottom-up Model, there is no virtual function called because all the operations are combined in a single loop function. Intermediate data of Volcano Iterator Model are in memory while of Bottom-up Model are in CPU registers:As one operator transforms data to another one in Volcano Iterator Model, the intermediate data can only be cached in Memory. But for Bottom-up Model, as there is no need to be transformed, data are always in CPU registers. Volcano Iterator Model donâ€™t take advantage of modern techniques, which Bottom-up Model do, such as loop-pipelining, loop-unrolling and so on:As Bottom-up Model evaluates the whole query into a single loop function, some modern technique can be used in the Bottom-up Model, I will show you two of them, which are loop-pipelining and loop-unrolling. Loop-pipeliningIn a loop iteration function, one iteration of loop usually begins when the previous one is complete, which means the iteration of the loop should be executed sequentially one by one. While loop-pipelining can make some differences. The figure below could learn about it clearly. Loop-pipelining increases the parallelism of the loop iteration by implementing a concurrent manner. Loop-unrollingLoop-unrolling is another technique to exploit parallelism between loop iterations. Letâ€™s learn about it by the code:1234567891011// without loop-unrollingint sum=0;for (int i=0; i&lt;10; i++) &#123; sum+=a[i];&#125;// with loop-unrollingint sum = 0;for (int i=0; i&lt;10; i+=2) &#123; sum += a[i]; sum += a[i+1];&#125; As shown above, loop-unrolling creates multiple copies of the loop body and also changes the loop iteration counter. Reducing loop iteration number, loop-unrolling also create more operation in each loop iteration and, as results, more parallisim could be exploited. Whole Stage Code GenerationFusing operators together to make the generated code looks like the hand-writing bottom-up model, WholeStageCodeGen makes chains of operators as a single stage, and it has been the alternatives for the Code Generation in Catalyst Optimization. It takes advantages of hand-writing and significantly optimizes the query evaluation and can be easily found in the DAG of your Spark application. VectorizationAlthough the WholeStageCodeGen makes a huge optimization of the query plan, there are still some problems. For example, when we import some external integrations, such as tensorflow, scikit-learn, and some python packages, these code cannot be optimized by the WholeStageCodeGen cause they cannot be merged in our code. Whatâ€™s more, the complicated IO cannot be fused, reading Parquet or ORC for instance. How to speed up this excution? The answer is improving the in-core parallelism for an operation of data, so Vector Processing and Column Format are used in 2nd generation Tungsten engine. Vector Processing In computing, a vector processor or array processor is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called vectors, compared to scalar processors, whose instructions operate on single data items. The following figure presents the differences between Scalar and Vector Processing. And we find out that Vector Processing really fasten the operation. How does Vector Processing be implemented? We may need to learn something about SIMD(Single Instruction, Multiple Data) Single instruction, multiple data (SIMD) is a class of parallel computers in Flynnâ€™s taxonomy. It describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. Let me show one figure to show whatâ€™s SIMD breifly.As presented above, SIMD can process multuple data via single instruction, and the data are all in an one-dimesional vector. As results, take advantage of SMID, Vector Processing can improve the in-core parallelism and thus make the operation faster. For the sake of the data are all in an one-dimesonal vector in SIMD, Colunm Format could be a better choice for Spark. Column FormatColumn Format has been widely used in many fields, such as disk storage. The following figure shows the differences between Row Format and Column Format. SummaryWholeStageCodeGen and Vectorization in 2nd generation Tungsten engine really optimize the query plan and speed up the query execution. Compared with 1st generation Tungsten engine, the 2nd one mainly foucses on improving the CPU parallelism to take advtange of some modern techniques. There are many terms and konwledges about CPU exection, which I learn so little that may make some mistakes in this blog, so it would be very nice of you to figure out any single mistake. References Project Tungsten: Bringing Apache Spark Closer to Bare Metal Apache Spark as a Compiler: Joining a Billion Rows per Second on a LaptopDeep dive into the new Tungsten execution engine Spark 2.x - 2nd generation Tungsten Engine Loop Pipelining and Loop Unrolling Vectorization: Ranger to Stampede Transition","link":"/2018/11/14/spark-second-generation-tungsten-in-spark/"},{"title":"Sparkå·¥ä½œæµç¨‹ç®€æ","text":"Helloï¼Œæœ‰ä¸€ä¸ªæœˆæ²¡å†™blogäº†æ„Ÿè§‰å¾ˆè‡ªè´£ï¼Œå¿…é¡»æ•´èµ·æ¥ï¼æœ€è¿‘ç”±äºå·¥ä½œä¸Šé‡åˆ°çš„ä¸€äº›è°ƒä¼˜å›°éš¾ï¼Œè®©æˆ‘å¯¹Sparkæœ‰äº›æ•¬ç•ï¼Œæ‰€ä»¥é›†ä¸­çš„ç ”ç©¶äº†ä¸‹é¬¼é­…ç„å­¦Sparkï¼Œå’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹ã€‚é¦–å…ˆå…ˆæ¥çœ‹çœ‹sparkçš„åŸºæœ¬å·¥ä½œæµç¨‹ã€‚ Work Flowå’Œhadoopä¸€æ ·ï¼Œsparkä¹Ÿæ˜¯master-slaveæœºåˆ¶ï¼ŒSparké€šè¿‡driverè¿›ç¨‹ï¼Œå°†taskåˆ†å‘åˆ°å¤šä¸ªexecutorsä¸Šå¹¶å‘è¿›è¡Œè®¡ç®—ã€‚æ•´ä¸ªdriverå’Œæ‰€æœ‰çš„executorsç»„æˆäº†ä¸€ä¸ªspark applicationï¼Œæ¯ä¸€ä¸ªapplicationæ˜¯è¿è¡Œåœ¨cluster managerä¸Šçš„ï¼ŒSparkæœ¬èº«é›†æˆäº†standalone clusterï¼Œå½“ç„¶ï¼ŒSparkè¿˜å¯ä»¥è¿è¡Œåœ¨èµ«èµ«æœ‰åçš„YARNå’ŒMesosä¸Šã€‚æˆ‘å¹³æ—¶ä½¿ç”¨çš„å…¬å¸é›†ç¾¤éƒ½æ˜¯åŸºäºYARN cluster managerçš„ï¼Œå› æ­¤æœ¬æ–‡é‡ç‚¹æ¢è®¨åŸºäºYARNçš„sparkã€‚ ä¸‹å›¾å°±æ˜¯sparkåœ¨cluster managerä¸‹çš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚ The DriverDriveræ˜¯æ•´ä¸ªapplicationæœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œä»–è¿è¡Œçš„æ˜¯applicationçš„mainæ–¹æ³•ï¼Œå®ƒä¼´éšè¿™æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼Œdriverè¿›ç¨‹çš„ç»“æŸå°±ä¼šå¸¦æ¥æ•´ä¸ªapplicationçš„ç»“æŸã€‚ å¯¹äºæ‰€æœ‰çš„Sparkä»»åŠ¡ï¼Œä»–ä»¬å…¶å®éƒ½æ˜¯å®ç°RDDçš„transformationå’Œactionæ“ä½œï¼Œè€Œè¿™äº›æ“ä½œï¼Œæœ€åæ˜¯éœ€è¦driverå°†ä»–ä»¬è½¬åŒ–å’Œåˆ†å‘æˆtasksï¼Œç„¶åæ‰å¯ä»¥å»æ‰§è¡Œã€‚æ‰€æœ‰çš„user programéƒ½ä¼šè¢«driveré€šè¿‡DAG(directed acyclic graph)è½¬åŒ–æˆå®é™…çš„tasksæ‰§è¡Œè®¡åˆ’ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œdriverè¿˜ä¼šåœ¨tasksæ‰§è¡Œçš„æœŸé—´ï¼Œç›‘æ§executorä¸Šçš„tasksï¼Œå¹¶ä¸”ä¿è¯ä»–ä»¬æ‹¥æœ‰å¥åº·è€Œåˆç†çš„èµ„æºã€‚ ExecutorsExecutorsæ˜¯Spark applicationçš„æ‰§è¡Œè€…ï¼Œä»–ä»¬ä¹Ÿæ˜¯ä¼´éšç€applicationçš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨çš„ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSpark jobåœ¨executorsæ‰§è¡Œå¤±è´¥çš„æƒ…å†µä¸‹ä¾ç„¶å¯ä»¥ç»§ç»­è¿›è¡Œã€‚Executorsä¼šå¯¹å…·ä½“çš„tasksçš„æ‰§è¡Œç»“æœè¿”å›ç»™driverï¼ŒåŒæ—¶ç»™ç¼“å­˜çš„RDDæä¾›å­˜å‚¨ç©ºé—´ã€‚ Some terms Job: Jobæ˜¯executorå±‚é¢æœ€å¤§çš„æ‰§è¡Œå•å…ƒï¼Œjobé€šè¿‡RDDçš„actionæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯ä¸€ä¸ªactionæ“ä½œå°±ä¼šè¿›è¡Œä¸€æ¬¡jobçš„åˆ’åˆ†ï¼› Stage: Stageæ˜¯åŒ…å«åœ¨jobä¸­çš„æ‰§è¡Œå•å…ƒï¼Œstageé€šè¿‡RDDçš„shuffleæ“ä½œæ¥åˆ†å‰²ï¼Œæ¯è¿›è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡stageçš„åˆ’åˆ†ï¼› Task: Taskæ˜¯executoræ‰§è¡Œä¸­æœ€ç»†çš„æ‰§è¡Œå•å…ƒï¼Œtaskçš„æ•°ç›®å–å’Œparent RDDçš„partitionæ•°ç›®æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚ Spark on Yarn-clusterä¸‹é¢ï¼Œæˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æ•´ä¸ªSpark applicationä¸­ï¼Œdriverå’Œexecutorsçš„éƒ½ä¼šèµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚æˆ‘ä»¥åŸºäºyarn-clusterçš„YARNçš„Sparkä½œä¸ºä¾‹å­æ¥ç®€è¿°æ•´ä¸ªæµç¨‹ï¼Œå…ˆçœ‹ä¸€å¼ å›¾ï¼šé¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ä¸€äº›YARNçš„æ¦‚å¿µï¼ŒYARNæ˜¯ä¸master-slaverçš„ä¸€ä¸ªCluster Managerï¼Œ åœ¨YARNä¸­ï¼ŒRM(ResourseManager)è´Ÿè´£æ•´ä¸ªè°ƒåº¦åˆ†å‘ï¼Œå³æˆ‘ä»¬å¸¸è¯´çš„masterï¼›è€ŒNM(NodeManager)ä»»åŠ¡åˆ†å‘çš„æ¥å—è€…ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„workerã€‚è¿™äº›æ¦‚å¿µåç»­æˆ‘ä¸“é—¨ä»‹ç»YARNçš„æ—¶å€™ä¼šè¯¦ç»†çš„è¯´æ˜ï¼Œä»–ä»¬çš„ä½œç”¨éƒ½æ˜¯å®ç°sparkå’ŒYARNä¹‹é—´è¯¸å¦‚èµ„æºç”³è¯·ç­‰æ“ä½œã€‚ é¦–å…ˆClientå‘ResourceManagerå‘å‡ºæäº¤applicationçš„è¯·æ±‚ï¼ŒResourseManagerä¼šåœ¨æŸä¸€ä¸ªNodeManagerä¸Šå¯åŠ¨AppManagerè¿›ç¨‹ï¼ŒAppManagerä¼šéšåå¯åŠ¨driverï¼Œå¹¶å°†driverç”³è¯·containersèµ„æºçš„ä¿¡æ¯å‘ç»™ResourceManagerï¼Œç”³è¯·å®Œæˆåï¼ŒResourceManagerå°†èµ„æºåˆ†é…æ¶ˆæ¯ä¼ é€’ç»™AppManagerå¹¶ç”±å®ƒå¯åŠ¨containerï¼Œæ¯ä¸€ä¸ªcontainerä¸­åªè¿è¡Œä¸€ä¸ªspark executorï¼Œç”±æ­¤å®Œæˆäº†èµ„æºçš„ç”³è¯·å’Œåˆ†é…ã€‚ ç„¶åæ•´ä¸ªapplicationå¼€å§‹æ‰§è¡Œï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®RDDçš„transformationæˆ–è€…actionï¼ŒdriveræŠŠè¿™äº›ä»»åŠ¡ä»¥tasksçš„å½¢å¼ï¼Œæºæºä¸æ–­çš„ä¼ é€ç»™executorsï¼Œäºæ˜¯executorsä¸åœåœ°è¿›è¡Œè®¡ç®—å’Œå­˜å‚¨çš„ä»»åŠ¡ã€‚å½“driverç»“æŸçš„æ—¶å€™ï¼Œä»–ä¼šç»“æŸæ‰executorså¹¶ä¸”é‡Šæ”¾æ‰èµ„æºã€‚è¿™å°±æ˜¯yarn-clusterä¸Šsparkçš„æ•´ä½“å·¥ä½œæµç¨‹ã€‚ é™¤äº†yarn-clusterï¼Œè¿˜æœ‰ä¸€ç§yarn-clientçš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œä»–çš„driverå¹¶éè¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Šï¼Œè€Œæ˜¯ä¸€ç›´è¿è¡Œåœ¨clientä¸­ã€‚è¿™æ ·çš„é—®é¢˜å°±æ˜¯clientä¸€æ—¦å…³é—­ï¼Œé‚£ä¹ˆæ•´ä¸ªä»»åŠ¡ä¹Ÿå°±éšä¹‹åœæ­¢æ‰§è¡Œã€‚å› æ­¤ç›¸è¾ƒè€Œè¨€ï¼Œyarn-clusteræ›´é€‚åˆçº¿ä¸Šä»»åŠ¡ï¼Œè€Œyarn-clientæ›´é€‚åˆè°ƒè¯•æ¨¡å¼ã€‚ Reference Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015. Spark:Yarn-clusterå’ŒYarn-clientåŒºåˆ«ä¸è”ç³»","link":"/2018/01/07/spark-spark-workflow/"},{"title":"Spark Tuning","text":"Hi, all, æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶spark tuningæ–¹é¢çš„é—®é¢˜ï¼Œæ·±æ„Ÿè¿™æ˜¯ä¸€ä¸ªç»éªŒæ´»ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæŠ€æœ¯æ´»ï¼ŒæŸ¥é˜…å’Œå¾ˆå¤šèµ„æ–™ï¼Œåœ¨è¿™é‡Œmarkä¸€ä¸‹ã€‚ä¸Šæ¬¡æˆ‘ä»¬reviewäº†ä¸€ä¸‹sparkçš„work-flowï¼Œä¸»è¦æ˜¯åŸºäºspark on yarnçš„ï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ¢è®¨çš„ä¹Ÿä¸»è¦æ˜¯åŸºäºspark on yarnã€‚ Resource AllocationSome ConfigurationResource allocationæ˜¯sparkä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ç¯èŠ‚ï¼Œç»™äºˆä¸€ä¸ªapplicationè¿‡å°‘çš„resourceä¼šå¸¦äº†æ‰§è¡Œæ•ˆç‡çš„ä½ä¸‹å’Œæ‰§è¡Œé€Ÿåº¦çš„ç¼“æ…¢ï¼›ç›¸åï¼Œè¿‡å¤šçš„resourceåˆ™ä¼šå¸¦æ¥èµ„æºæµªè´¹ï¼Œå½±å“clusterä¸Šå…¶ä»–appllicationçš„è¿è¡Œï¼Œå› æ­¤ï¼Œä¸€ä¸ªåˆé€‚çš„resource allocationæ˜¯éå¸¸éå¸¸é‡è¦çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å‡ ä¸ªæ¯”è¾ƒé‡è¦çš„parameterï¼š num-executors: è¡¨æ˜sparkç”³è¯·executorsçš„æ•°ç›®ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®spark.dynamicAllocation.enabledæ¥è®©sparkæ ¹æ®æ•°æ®åŠ¨æ€çš„åˆ†é…executorsï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆçš„æé«˜èµ„æºåˆ©ç”¨ç‡ï¼› executor-cores: æŒ‡å®šæ¯ä¸€ä¸ªexecutorçš„coreæ•°ç›®ï¼Œcoreæ•°ç›®å†³å®šäº†æ¯ä¸ªexecutorçš„æœ€å¤§å¹¶è¡Œtaskæ•°ç›® executor-memory: æŒ‡å®šåˆ†é…ç»™æ¯ä¸€ä¸ªexecutorçš„å†…å­˜å¤§å°ã€‚ Some Tips å¯¹äºexecutoræ¥è¯´ï¼Œåœ¨è¿‡äºå¤§çš„memoryä¸Šè¿è¡Œå¯èƒ½ä¼šå¸¦æ¥æ¯”è¾ƒé«˜çš„GC(gabage collection) timeï¼Œå¯¹äºä¸€ä¸ªexecutoræ¥è¯´ï¼Œå»ºè®®ç»™å‡ºçš„ä¸Šé™memoryæ˜¯64Gï¼› ç”±äºHDFSåœ¨å¹¶è¡Œè¯»å†™çš„æ—¶å€™å­˜åœ¨ä¸€äº›ç“¶é¢ˆï¼Œå› æ­¤æ¯ä¸€ä¸ªexecutorä¸­æœ€å¥½ä¸è¦è¶…è¿‡5ä¸ªå¹¶è¡Œä»»åŠ¡ï¼Œå³coresæ•°ä¸è¦è¶…è¿‡5ä¸ªï¼Œæœ‰å®éªŒå¯ä»¥è¯æ˜ï¼Œsparkåœ¨å¤šexecutorå°‘coreçš„é…ç½®ä¸‹æ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼› ç›¸åçš„ï¼Œå¯¹äºexecutoræ¥è¯´ï¼Œè¿‡åˆ†å°‘çš„coreï¼Œä¾‹å¦‚1ä¸ªï¼Œå°†ä¼šä½¿å¾—executorsæ•°ç›®å˜å¤šï¼Œä¾‹å¦‚æŸä¸ªbroadcastè¿‡ç¨‹ï¼Œéœ€è¦ä¼ æ’­åˆ°æ‰€æœ‰çš„executorsä¸Šï¼Œé‚£ä¹ˆè¿‡åˆ†å¤šçš„executorsä¼šé™ä½æ‰§è¡Œçš„æ•ˆç‡ã€‚ Memory Mangementå…³äºsparkä¸­çš„memory managementï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€å¼ å›¾ï¼šåœ¨å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒsparkæŠŠmemoryåˆ†æˆäº†ä¸‰éƒ¨åˆ†ï¼Œå³spark memoryã€user memoryå’Œreserved memoryï¼Œæˆ‘ä»¬é¡ºæ¬¡æ¥çœ‹çœ‹ï¼š Reserved Memoryæ‰€è°“reserved memoryï¼Œå®ƒå°±æ˜¯ç³»ç»Ÿé¢„ç•™ä¸‹çš„ä¸€éƒ¨åˆ†memoryï¼Œç”¨äºå­˜å‚¨sparkçš„å†…éƒ¨å¯¹è±¡ï¼Œé»˜è®¤å¤§å°ä¸º300mï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¸ä¼šä¿®æ”¹è¿™äº›å‚æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“executorè¢«åˆ†é…çš„memoryå°äº1.5å€çš„reserved memoryæ—¶ï¼Œå°†ä¼šæŠ›å‡ºâ€œplease use larger heap sizeâ€çš„é”™è¯¯ã€‚ User MemoryUser memoryç”¨äºå‚¨å­˜sparkçš„transfermationçš„ä¸€äº›ä¿¡æ¯ï¼Œæ¯”å¦‚RDDä¹‹é—´çš„ä¾èµ–ä¿¡æ¯ç­‰ç­‰ï¼Œè¿™éƒ¨åˆ†å†…å­˜é»˜è®¤å¤§å°ä¸º(Java Heap - 300M)*0.25ï¼Œå…¶ä¸­çš„300Må…¶å®å°±æ˜¯ä¸Šé¢æåˆ°çš„reserved memory.å…·ä½“çš„å¤§å°è¦ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°å†³å®šäº†user å’Œ ä¸‹é¢è¦è®²åˆ°çš„spark memoryçš„åˆ†é…æ¯”ä¾‹ã€‚ Spark Memoryä¸Šæ–‡å·²ç»åˆ°äº†ï¼Œspark memoryä¸»è¦æ˜¯sparkè‡ªå·±ä½¿ç”¨çš„memoryéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†çš„å¤§å°ä¾èµ–äºspark.memory.fractionå‚æ•°ï¼Œå³(Java Heap - 300M)*spark.memory.fractionï¼Œå…¶ä¸­fractionçš„defaultä¸º0.75ã€‚ Spark memoryä¸»è¦æœ‰ä¸¤ä¸ªç”¨é€”ï¼Œä¸€æ˜¯ç”¨äºsparkçš„shuffleç­‰æ“ä½œï¼Œè€Œæ˜¯ç”¨æ¥cache sparkä¸­çš„RDDï¼Œå› æ­¤spark memoryä¹Ÿè‡ªç„¶è€Œç„¶çš„åˆ†æˆäº†ä¸¤éƒ¨åˆ†ï¼Œå³è´Ÿè´£shuffleæ“ä½œçš„execution memoryå’Œè´Ÿè´£cacheçš„storage memoryï¼Œä¸¤è€…çš„å¤§å°é€šè¿‡spark.memory.storageFractionå‚æ•°æ¥åˆ†å‰²ï¼Œé»˜è®¤å€¼æ˜¯0.5ã€‚ åœ¨spark memoryä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ€§è´¨ï¼Œé‚£å°±æ˜¯storage å’Œ execution memoryçš„å…±äº«æœºåˆ¶ï¼Œè¯´çš„ç®€å•ä¸€äº›å°±æ˜¯ï¼Œå½“ä¸€è¾¹å†…å­˜ç©ºé—²è€Œå¦ä¸€æ–¹å†…å­˜ç´§å¼ çš„æ—¶å€™ï¼Œå¯ä»¥å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œæˆ‘ä»¬ä¸‹é¢çœ‹çœ‹åœ¨å†…å­˜å‡ºç°å†²çªçš„æ—¶å€™ï¼Œsparkæ€ä¹ˆåè°ƒï¼š å½“storageå ç”¨execution memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿexecution memoryä½¿ç”¨ç´§å¼ çš„æƒ…å†µæ—¶ï¼Œå¼ºåˆ¶å°†storageå æœ‰çš„å†…å­˜é‡Šæ”¾å¹¶å½’è¿˜executionï¼Œä¸¢å¤±çš„æ•°æ®å°†ä¼šåç»­é‡æ–°è®¡ç®—ï¼› å½“executionå ç”¨storage memoryçš„æ—¶å€™ï¼Œå‘ç”Ÿstorage memoryç´§å¼ çš„æƒ…å†µï¼Œè¢«å ç”¨çš„å†…å­˜ä¸ä¼šè¢«å¼ºåˆ¶é‡Šæ”¾ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥ä»»åŠ¡ä¸¢å¤±ï¼Œstorageä¼šè€å¿ƒç­‰å¾…çŸ¥é“executionæ‰§è¡Œå®Œé‡Šæ”¾å‡ºå†…å­˜ã€‚ Data Serializationåœ¨æ•´ä¸ªsparkä»»åŠ¡ä¸­ï¼Œæ•°æ®ä¼ è¾“éƒ½æ˜¯ç»è¿‡åºåˆ—åŒ–å(serialization)ä¹‹åä¼ è¾“çš„ï¼Œå› æ­¤æ•°æ®çš„åºåˆ—åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå†—ä½™çš„åºåˆ—åŒ–è¿‡ç¨‹ä¼šè®©æ•´ä¸ªsparkä»»åŠ¡å˜æ…¢ï¼Œsparkæä¾›ä¸¤ç§åºåˆ—åŒ–æ–¹å¼ï¼š Java serializationï¼šè¿™æ˜¯sparké»˜è®¤çš„åºåˆ—åŒ–æ–¹å¼ï¼Œjavaåºåˆ—åŒ–æ˜¯ä¸€ç§å¾ˆç»å…¸å’Œç¨³å®šçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œä½†æ˜¯æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯â€”â€”æ…¢ï¼ Kryo serializationï¼šKryo åºåˆ—åŒ–å¯ä»¥è®©sparkä»»åŠ¡æ›´åŠ å¿«é€Ÿï¼Œç”šè‡³10å€äºjavaåºåˆ—åŒ–ï¼›ä½†æ˜¯å®ƒä¸æ”¯æŒæ‰€æœ‰çš„Serializableç±»å‹ï¼ŒåŒæ—¶éœ€è¦ä¸ºç”¨æˆ·è‡ªå·±å¼€å‘çš„classè¿›è¡Œæ³¨å†Œåï¼Œæ‰å¯ä»¥ä½¿ç”¨Kyo. å…³äºKryoçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥æŸ¥çœ‹spark documentationï¼Œæˆ–è€…Kryo documentation Summaryå…³äºsparkè°ƒä¼˜çš„é—®é¢˜ï¼Œæœ‰å¾ˆå¤šå› ç´ ï¼Œæˆ‘ä¹Ÿæ˜¯ç®€å•çš„åšäº†ä¸€äº›äº†è§£å¹¶åˆ†äº«ç»™å¤§å®¶ï¼Œé™¤äº†æˆ‘æåˆ°çš„ï¼Œè¿˜æœ‰è¯¸å¦‚GCç­‰ç­‰å› ç´ ï¼Œå¤§å®¶å¯ä»¥æ ¹æ®æˆ‘ç»™å‡ºçš„referencesåšè¿›ä¸€æ­¥çš„äº†è§£ã€‚ References How-to: Tune Your Apache Spark Jobs (Part 2) Spark Documentation-Tuning Karau, Holden, et al. Learning spark: lightning-fast big data analysis. â€œ Oâ€™Reilly Media, Inc.â€, 2015.","link":"/2018/02/23/spark-spark-tune/"},{"title":"Spark Tips Sum-up Part-2","text":"This article presents some tips of Apache Spark, which is part 2 of the series. All the tips below are based on the real problems which I met. Despite the background, the tips below are of valuable reference. Iâ€™ve tried a lot to learn about Apache Spark but canâ€™t know the detail of every part of it. Iâ€™d appreciate it if you figure out the mistakes in this article. CoalesceChanging the number of partitions could benefit the performance of your Spark application. A large number of partitions may increase the parallelism of the process while too many tasks would be executed on a single executor, which could cost more running time. In contrast, a small number of partition might improve the complexity of each task and slow down the whole process. To solve this trade-off problem, repartition() and coalesce() is proposed in Apache Spark.Before talking about the detail about coalesce(), letâ€™s review the concept of transformation with wide-dependencies and narrow-dependencies. Wide-dependencies: each partition of the parent RDD is used by at most one partition of the child RDD. Narrow-dependencies: multiple child partitions may depend on each partition in the parent RDD. According to the definition above, repartition() is a typical transformation with wide-dependencies and thus can bring in shuffle operation when triggered. But what about coalesce()? To find out more about it, letâ€™s see the definition first.12345678910// coalesce() for RDD is defined in org.apache.spark.rdd.RDDdef coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope &#123;...&#125;// coalesce() for Dataset is defined in org.apache.spark.sql.Datasetdef coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123; Repartition(numPartitions, shuffle = false, logicalPlan)&#125; For RDDs, coalesce() has a boolean typed parameter called shuffle. The coalesce() can be treated as repartition() as shuffle is set to True, which is a transformation with wide-dependencies. In contrast, when shuffle is False, coalesce() is a transformation with narrow-dependencies and only can reduce the number of partitions, which means you cannot increase the number of partitions by setting the parameter numPartitions. As for DataFrame/Dataset API, coalesce() is a transformation with narrow-dependencies as there is no shuffle operation at all. As results, coalesce() cannot increase the number of DataFrame/Datasetâ€™s partitions and can only be used to reduce DataFrame/Datasetâ€™s partitions.After understanding the above, there is a crucial tip for you. When you use coalesce() and reduce the number of partitions, which may cause a problem that the partition of the whole stage would be decreased and the computation could be even slower than you expect. since no shuffle operation performed, the stage executes with the level of parallelism assigned by coalesce(). To avoid this problem, you can set shuffle=True for RDDs or use repartition() instead for DataFrame/Dataset to split the whole stage by a shuffle. Read ORC TableReading data from and writing data to HIVE tables is quite often when I use Apache Spark. And we are now using ROC format to store HIVE table. ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Generally, when Spark reads data from HDFS, the initial number of partitions are determined by the number of blocks of data store in HDFS, with one block represents one partition in Spark. However, when I read the ORC format HIVE table, the partition number is equal to the number of files stored on HDFS, not number block. Thatâ€™s caused by ORC split strategy set by hive.exec.orc.split.strategy, which determines what strategy ORC should use to create splits for execution. The available option includes â€œBIâ€, â€œETLâ€ and â€œHYBRIDâ€ The HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS block size. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS. As results, when the split strategy is set to ETL, Spark would take more time to generate tasks and the number of partitions is based on the size of HDFS blocks. In contrast, BI strategy would make Spark generate tasks immediately and the number of tasks is equal to the number of files of HIVE table. These two strategies seem to be a trade-off for us and itâ€™s better for us to decide by the actual situation. Reference Managing Spark Partitions with Coalesce and Repartition High Performence Spark Apache ORC Apache ORC Configuration Properties","link":"/2018/10/13/spark-sumup-part-2/"},{"title":"Spark Tips Sum-up Part-1","text":"This article is about things I learned about Apache Spark recently. Iâ€™ve been struggling with Spark tuning for more than one month, including shuffle tuning, GC time tuning and so on. Honestly speaking, Apache Spark is just like an untamed horse, it could be a beauty if you tune it well while a nightmare if not. So let me show you some tips Iâ€™ve learned from my work. This article is part 1 and here we go. RDD vs DataFrame Partition Number in ShuffleShuffle plays a pretty crucial role in MapReduce and Spark, which can influence a lot on the performance of the whole job. The shuffle would not only separate one stage but bring much more storage and I/O delay. However, is usually hard to avoid shuffle occurring, so how to make shuffle faster could be the key point.During my work in tuning the Spark SQL project, the join operation is unavoidable because it is the fundamental process in all the SQL program. While when I try to run the Spark SQL saving the joined DataFrame into a Hive table, I found the number of table partition is constant no matter how many partitions of two joining DataFrame, oh-gee! it would never happen in RDD join process. That makes the size of each file of Hive table on the HDFS very large, and I have to fix it up.After searching from the Internet, I found that the shuffle of DataFrame in Spark, join included, would have a partition number according to the configuration spark.sql.shuffle.partitions with 200 as default. And the problem above can be solved as this configuration changed. So, when you join two DataFrames or other process causing shuffle, remember the configuration spark.sql.shuffle.partitions when you want to modify the DataFrame partition number.What if I want to modify the partition number of a RDD? Actually, the configuration spark.default.parallelism, which seems to only working for raw RDD and is ignored when working with DataFrames in Spark SQL, may be a good choice. Alternatively, you can set the partition number by calling repartition( ) or coalese( ), which is effective for both DataFrames and RDDs. Smart ActionAs we know, RDDs support two types of operation, which are action and transformation. Transformation creates a new RDD from an existing one, and action returns a value to the Spark driver from computing on a RDD. Based on the lazy evaluation, all the transformations would run as a lineage when an action is triggered by the Spark. A lineage of transformation followed by an action consist of one job in Spark, and one wide-dependency between two transformations separate the job into two stages, which is aka shuffle.When Iâ€™m tuning a Spark project, I found that different action following the same lineage of transformation takes different period of time. For example, the action show( ) takes shorter time than createOrReplaceTempView( ), which makes me confuse a lot. After a long time thinking and searching, the answer finally comes out. Spark would draw a DAGSchedule when the program is submitted, the data would run through all the DAGSchedual and the result is sent to the Spark driver. Different action may generate different DAGSchedule even the the transformations are same, Spark is smart enough to know whether it needs to run everything in the RDD. Showed above, Spark may run less data for the show( ) action than those for createOrReplaceTempView( ). Broadcast JoinsBroadcast joins (aka map-side joins) is a good way to abort shuffle and reduce cost. Spark SQL provides two ways for developers, you can not only write SQL but also use DataFrame/Dataset API. When a large table joins a smaller one, a threshold defined by spark.sql.autoBroadcastJoinThreshold, with 10M as default value, determines whether the smaller one will be broadcast or not. If you use SQL in Spark SQL, as the smaller table size below the threshold, Spark would automatically broadcast it to all executors. However, if you use DataFrame/Dataset API, the broadcast function must be imported or Spark wouldnâ€™t broadcast data even if the size is below the threshold.1val df = largeDF.join(broadcast(smallDF),Seq(\"col1\",\"col2\"),\"left\") Also, you can enlarge the value of spark.sql.autoBroadcastJoinThreshold so that larger table can also be broadcast, but the memory of your application should be paid attention.Broadcast joins is really an awesome solution to optimize Spark SQL joins, after using broadcast joins instead of default SortMerge joins, my application runs more than 10 times faster. Avoiding shuffle is quite important for Spark tuning, and broadcast is born to kill shuffle. You will fall in love with her as long as you have a try! References Spark SQL Programming guide Mastering Spark SQL","link":"/2018/09/15/spark-sumup-part-1/"},{"title":"Spark Tips Sum-up Part-3","text":"This blog is the third part of Apache Spark tips sum-up learnt from my programing and debugging. Have been busy for such a long time, I have some time to carry on my personal blog. Today Iâ€™ll show you some tips about some functions in Spark SQL, and there may be some mistakes caused by my misunderstanding. Anyway, thanks if you figure out anything incorrect! ExplodeSpark SQL provides a varority of functions in org.apache.spark.sql.functions for you to restruct your data, one of which is the explode() function. Since Spark 2.3, explode() function has been optimized and it has been much more faster than it in the previous Spark versions. For more details about this optimization, this issue may help you a lot.However, what I want to share about is the number of partitions when you use explode(). Itâ€™s easy to understand that the number of rows grows several times after the exploding process, as a result, the size of data for each partition has become larger than ever, which may cause the jobs taking more time and more memory. So, it could be a wise choice to enlarge the number of partitions by repartition(), especially when the rows explode more than 10 times than previous, each task would process much more data and thatâ€™s possible to get an OOM error, or high GC time. Foreach vs ForeachPartition, Map vs MapPartitionYeah, foreach() vs foreachPartition() and map() vs mapPartition(), these four method do confuse me for a long time and let me share you my understanding about them.First of all, foreach() and foreachPartition() are actiona in Spark, while map() and mapPartition() are transformations. If you have no ideas about the defination of action and transformation, itâ€™s better to read about my previous blog or just ask help for dear google. foreach() and foreachPartition() are often used for writing data to external database while map() and mapPartition() are used to modify the data of each row in the RDD, also DataFrame or DataSet.Seondly, foreachPartition() and mapPartitionn() are respectively based on foreach() and map(). Instead of invoking function for each element, foreachPartition() and mapPartition() calls for each partition and provide an iterator to invoke the function. So whatâ€™s the advantages of invoking function for each partition? For example, when you invoke a function connecting to your database, redis for example, foreach() will build a connection for each row of your data, which means, number of connection could be pretty huge and you may get connection failed errors. Instead of invokeing for each element, foreachPartition() could invoke the function building connection to redis for each partition, and data in each partition would be written to redis iteratively. Amazing, isnâ€™t it! Reading ORC TableActually speaking, I have already talked about reading data from ORC table in Apache Spark in the previous blog, but I find something about reading from ORC table pretty awesome since Spark 2.3. Letâ€™s have a look. Spark supports a vectorized ORC reader with a new ORC file format for ORC files since 2.3 version, which means reading from and writing to ORC fromat file could be much faster!To enbale the vectotized ORC reader, you just need to set these configuration: â€“conf spark.sql.orc.impl=native â€“conf spark.sql.orc.enableVectorizedReader=true â€“conf spark.sql.hive.convertMetastoreOrc=true For more information, you can read the Spark Doc. References Apache Spark - foreach Vs foreachPartitions When to use What? Spark SQL Guide - ORC File","link":"/2019/03/06/spark-sumup-part-3/"}],"tags":[{"name":"regularization","slug":"regularization","link":"/tags/regularization/"},{"name":"gradient descent","slug":"gradient-descent","link":"/tags/gradient-descent/"},{"name":"hyperparameter","slug":"hyperparameter","link":"/tags/hyperparameter/"},{"name":"batch norm","slug":"batch-norm","link":"/tags/batch-norm/"},{"name":"covariate shift","slug":"covariate-shift","link":"/tags/covariate-shift/"},{"name":"moving averages","slug":"moving-averages","link":"/tags/moving-averages/"},{"name":"learning strategy","slug":"learning-strategy","link":"/tags/learning-strategy/"},{"name":"orthogonalization","slug":"orthogonalization","link":"/tags/orthogonalization/"},{"name":"transfer learning","slug":"transfer-learning","link":"/tags/transfer-learning/"},{"name":"multi-task learning","slug":"multi-task-learning","link":"/tags/multi-task-learning/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"convex optimization","slug":"convex-optimization","link":"/tags/convex-optimization/"},{"name":"newton's method","slug":"newton-s-method","link":"/tags/newton-s-method/"},{"name":"unconstrained optimization","slug":"unconstrained-optimization","link":"/tags/unconstrained-optimization/"},{"name":"MAP","slug":"MAP","link":"/tags/MAP/"},{"name":"ridge regression","slug":"ridge-regression","link":"/tags/ridge-regression/"},{"name":"lasso regression","slug":"lasso-regression","link":"/tags/lasso-regression/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"imbalanced data","slug":"imbalanced-data","link":"/tags/imbalanced-data/"},{"name":"gbt","slug":"gbt","link":"/tags/gbt/"},{"name":"logistic regression","slug":"logistic-regression","link":"/tags/logistic-regression/"},{"name":"undersampling","slug":"undersampling","link":"/tags/undersampling/"},{"name":"bagging","slug":"bagging","link":"/tags/bagging/"},{"name":"activtion function","slug":"activtion-function","link":"/tags/activtion-function/"},{"name":"spark","slug":"spark","link":"/tags/spark/"}],"categories":[{"name":"learning notes","slug":"learning-notes","link":"/categories/learning-notes/"},{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"reading notes","slug":"reading-notes","link":"/categories/reading-notes/"},{"name":"spark","slug":"spark","link":"/categories/spark/"}]}